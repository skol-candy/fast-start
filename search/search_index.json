{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the Application Modernization Fast-Start","text":""},{"location":"#welcome-to-the-application-modernization-fast-start","title":"Welcome to the Application Modernization Fast-Start","text":"<p>This set of labs focuses on processes used to modernize applications from traditional WebSphere Application Server (tWAS) to WebSphere Liberty.</p>"},{"location":"#liberty","title":"Liberty","text":"<p>Liberty is an application server designed for the cloud. It\u2019s small, lightweight, and designed with modern cloud-native application development in mind.  It supports the full MicroProfile and Jakarta EE APIs and is composable, meaning that you can use only the features that you need, keeping the server lightweight, which is great for microservices.  It also deploys to every major cloud platform, including Docker, Kubernetes, and Cloud Foundry.</p> <p>Liberty</p> <p>WebSphere Liberty is included in the JSphere Suite specifically as part of the Enterprise Application Runtimes (EAR) offerings. (As well as EASeJ).  Open Liberty is the community addition of this runtime.  JSphere Suite does also include other modernization paths such as EASeJ.</p>"},{"location":"#operational-modernization","title":"Operational Modernization","text":"<p>Operational Modernization gives an operations team the opportunity to embrace modern operations best practices without putting change requirements on the development team.  The scaling, routing, clustering, high availability, and continuous availability functionality that were previously provided by the application server middleware, can be provided by the hybrid cloud platform (for instance OCP / Kubernetes, K8s service, SaaS offering).</p>"},{"location":"#application-modernization-accelerator-ama","title":"Application Modernization Accelerator (AMA)","text":"<p>The AMA / TA tool provides the following value:</p> <ul> <li>Identifies the Java EE programming models in the traditional Java application</li> <li>Determines the complexity of re-platforming these applications by listing a high-level inventory of the content and structure of each application</li> <li>Highlights Java EE programming model and WebSphere API differences between the WebSphere runtime profile types</li> <li>Identifies Java EE specification implementation differences that might affect the app</li> <li>Generates accelerators for deploying the application to Liberty and containers in a target environment</li> </ul> <p>Additionally, the tool provides a recommendation for the right-fit IBM WebSphere Application Server edition and offers advice, best practices, and potential solutions to assess the ease of moving apps to Liberty or newer versions of WebSphere traditional. It automatically generates a migration bundle with the artifacts you will need to containerize your application running on Liberty and deploy it to OpenShift Cloud Platform, accelerating application migrating to cloud process, minimizing errors and risks and reducing time to market.</p> <p>This bootcamp activity presupposes you have some familiarization with Java, application servers and containerization / Kubernetes concepts.</p>"},{"location":"agenda-5day/","title":"5-Day Sample Agenda","text":""},{"location":"agenda-5day/#5-day-sample-agenda","title":"5-Day Sample Agenda","text":"<p>The following agenda is designed for a five-day bootcamp experience.</p>"},{"location":"agenda-5day/#day-1-cloud-native-and-kubernetes","title":"Day 1 - Cloud Native and Kubernetes","text":"Time Session 10:00 - 10:30 Introductions 10:30 - 11:00 App Dev and Containers Presentation Breakout Session - START 11:00 - 12:00 Image Registries &amp; Podman Hands on Lab 12:00 - 12:15 Break 12:15 - 13:00 Image Registries &amp; Podman Hands on Lab - Continued Breakout Session - PAUSE 13:00 - 14:00 K8s Basics Presentation 14:00 - 14:30 Lunch Break Breakout Session - START 14:30 - 16:00 K8s Hands on Lab 16:00 - 16:30 Break 16:30 - 17:30 K8s Hands on Lab - Continued Breakout Session - END 17:30 - 18:00 Office Hours"},{"location":"agenda-5day/#day-2-openshift-install-on-vmware","title":"Day 2 - OpenShift Install on VMware","text":"Time Session 10:00 - 11:00 Environment Onboarding Breakout Session - START 11:00 - 14:00 OpenShift Install Lab 14:00 - 14:30 Lunch Break 14:15 - 16:00 OpenShift Install Lab - Continued 16:00 - 16:15 Break 16:15 - 17:30 OpenShift Install Lab - Continued Breakout Session - END 17:30 - 18:00 Office Hours"},{"location":"agenda-5day/#day-3-devops-gitops-and-terraform","title":"Day 3 - DevOps, Gitops and Terraform","text":"Time Session 10:00 - 10:30 Tekton Introduction - Presentation Breakout Session - START 10:30 - 12:30 Tekton Lab 12:30 - 12:45 Break Breakout Session - PAUSE 12:45 - 13:15 GitOps Introduction - Presentation Breakout Session - START 13:15 - 14:00 GitOps Lab 14:00 - 14:30 Lunch Break 14:30 - 15:30 GitOps Lab - Continued Breakout Session - PAUSE 15:30 - 16:00 Terraform  - Presentation 16:00 - 16:15 Break Breakout Session - START 16:15 - 17:30 Terraform VSphere Lab Breakout Session - END 17:00 - 18:00 Office Hours"},{"location":"agenda-5day/#day-4-ibm-concert","title":"Day 4 - IBM Concert","text":"Time Session 10:00 - 11:00 IBM Concert Overview Breakout Session - START 11:00 - 12:00 IBM Concert Hands on Lab 12:00 - 12:15 Break 12:15 - 14:00 IBM Concert Hands on Lab - Continued 14:00 - 14:30 Lunch Break 14:45 - 16:00 IBM Concert Hands on Lab - Continued 16:00 - 16:15 Break 16:15 - 17:00 Cluster Teardown Breakout Session - END 17:00 - 17:30 Office Hours"},{"location":"agenda-5day/#day-5-air-gapped-openshift","title":"Day 5 - Air-Gapped OpenShift","text":"Time Session Breakout Session - START 10:00 - 14:00 OpenShift Air-Gapped Install Lab 14:00 - 14:30 Lunch Break 14:15 - 16:00 OpenShift Air-Gapped Install Lab - Continued 16:00 - 16:15 Break 16:15 - 17:30 OpenShift Air-Gapped Install Lab - Continued Breakout Session - END 17:30 - 18:00 Office Hours"},{"location":"agenda/","title":"DC Bootcamp Agenda April 22","text":""},{"location":"agenda/#dc-bootcamp-agenda-april-22","title":"DC Bootcamp Agenda April 22","text":"<p>Welcome to the CE Platform Engineer Bootcamp in Washington, DC.  Our Slack Channel for DC:  #dc-pe-bootcamp-2025</p>"},{"location":"agenda/#day-1-openshift","title":"Day 1 - OpenShift","text":"Time Session 9:00 - 9:30 Introductions to Bootcamp 9:30 - 11:00 Session 1: OCP Installation Presentation 11:00 - 11:15 Break 11:15 - 12:30 Session 2: OCP Install Lab 12:30 - 13:30 Lunch 13:30 - 15:00 Session 2: OCP Install Lab (continued) 15:00 - 15:15 Break 15:15 - 17:00 Session 3: App &amp; Event Automation Lab Prerequisites 17:00 - 18:30 Office Hours"},{"location":"agenda/#day-2-event-automation-application-development-modernization","title":"Day 2 - Event Automation - Application Development &amp; Modernization","text":"Time Session 9:00 - 9:30 Day 1 Recap 9:30 - 10:00 Session 1: Automation Platform Playbooks 9:30 - 10:00 Session 2: Application Development &amp; Modernization 11:00 - 11:15 Break 11:15 - 12:30 Session 2: Application Development &amp; Modernization (continued) 12:30 - 13:30 Lunch 13:30 - 15:00 Session 3: Event Automation Presentation 15:00 - 15:15 Break 15:15 - 17:00 Session 4: Event Automation Lab 17:00 - 18:30 Office Hours"},{"location":"agenda/#day-3-ibm-concert-hashicorp","title":"Day 3 - IBM Concert &amp; HashiCorp","text":"Time Session 9:00 - 9:30 Day 2 Recap 9:30 - 11:00 Session 1: Event Automation Lab (continued) 11:00 - 11:15 Break 11:15 - 12:30 Session 2: IBM Concert Presentation / Demo 12:30 - 13:30 Lunch 13:30 - 15:00 Session 3: IBM Concert Lab 15:00 - 15:15 Break 15:15 - 17:00 Session 4: IBM Concert Lab (continued) Additional Topics 17:00 - 18:30 Office Hours"},{"location":"workstation-setup/","title":"Workstation setup","text":""},{"location":"workstation-setup/#workstation-setup","title":"Workstation Setup","text":"Openshift (MacOS/Linux)Openshift (Windows)Kubernetes (MacOS/Linux)Kubernetes (Windows)"},{"location":"workstation-setup/#create-accounts","title":"Create accounts","text":"<p>You'll need these accounts to use the Developer Tools environment.</p> <ul> <li> <p>GitHub account (public, not enterprise): Create one if you do not have one already. If you have not logged in for a while, make sure your login is working.</p> </li> <li> <p>Red Hat Account: Request a Red Hat Partner Subscription. Ensure to follow the instructions closely.</p> </li> </ul>"},{"location":"workstation-setup/#run-system-check-script","title":"Run System Check Script","text":"<p>Run the following command in your terminal to check which tools need to be installed.</p> <p>Using <code>wget</code>:</p> <pre><code>wget -O - https://cloudbootcamp.dev/scripts/system-check.sh | sh\n</code></pre> <p>Using <code>curl</code>:</p> <pre><code>curl -s https://cloudbootcamp.dev/scripts/system-check.sh | sh\n</code></pre> <p>After the script is run, make sure to install any missing tools.</p> <p>Note</p> <p>Ignore the requirement for the Docker CLI, IBM Software Policies prohibit the use of the Docker CLI and encourages the use of Podman CLI instead.</p>"},{"location":"workstation-setup/#install-clis-and-tools","title":"Install CLIs and tools","text":"<p>The following is a list of desktop tools required to help with installation and development.</p> <ul> <li> <p>Git Client: Needs to be installed in your development operating system, it comes as standard for Mac OS</p> </li> <li> <p>IBM Cloud CLI: Required for management of IBM Cloud</p> </li> <li> <p>Podman Desktop: Required for building and running container images.</p> <ul> <li>Installed and running on your local machine</li> </ul> </li> <li> <p>Visual Studio Code: A popular code editor</p> <ul> <li>You will be required to edit some files, having a good quality editor is always best practice</li> <li>Enabling launching VSCode from a terminal</li> </ul> </li> <li> <p>JDK 21: Optional installed on your local machine</p> <ul> <li>Used for SpringBoot content</li> </ul> </li> </ul>"},{"location":"workstation-setup/#create-accounts-windows","title":"Create accounts (Windows)","text":"<p>You'll need these accounts to use the Developer Tools environment.</p> <ul> <li> <p>GitHub account (public, not enterprise): Create one if you do not have one already. If you have not logged in for a while, make sure your login is working.</p> </li> <li> <p>Red Hat Account: Request a Red Hat Partner Subscription. Ensure to follow the instructions closely.</p> </li> </ul>"},{"location":"workstation-setup/#cloud-native-vm","title":"Cloud Native VM","text":"<p>Use the Cloud Native VM it comes pre-installed with kubernetes and all cloud native CLIs.</p> <p>Is highly recommended for Windows users to use this VM.</p>"},{"location":"workstation-setup/#install-clis-and-tools-windows","title":"Install CLIs and tools (Windows)","text":"<p>The following is a list of desktop tools required to help with installation and development.</p> <ul> <li> <p>Git Client: Needs to be installed in your development operating system</p> </li> <li> <p>IBM Cloud CLI: Required for management of IBM Cloud</p> </li> <li> <p>Podman Desktop: Required for building and running container images.</p> <ul> <li>Installed and running on your local machine</li> </ul> </li> <li> <p>Visual Studio Code: A popular code editor</p> <ul> <li>You will be required to edit some files, having a good quality editor is always best practice</li> <li>Enabling launching VSCode from a terminal</li> </ul> </li> <li> <p>JDK 11: Optional installed on your local machine</p> <ul> <li>Used for SpringBoot content</li> </ul> </li> </ul> <p></p> <p>Warning: Make sure you have Cisco VPN turned off when using CRC.</p> <p></p>"},{"location":"workstation-setup/#create-accounts-macoslinux","title":"Create accounts (MacOS/Linux)","text":"<p>You'll need these accounts to use the Developer Tools environment.</p> <ul> <li>GitHub account (public, not enterprise): Create one if you do not have one already. If you have not logged in for a while, make sure your login is working.</li> </ul>"},{"location":"workstation-setup/#run-system-check-script_1","title":"Run System Check Script","text":"<p>Run the following command in your terminal to check which tools need to be installed.</p> <p>Using wget: </p><pre><code>wget -O - https://cloudbootcamp.dev/scripts/system-check.sh | sh\n</code></pre><p></p> <p>Using curl: </p><pre><code>curl -s https://cloudbootcamp.dev/scripts/system-check.sh | sh\n</code></pre><p></p> <p>After the script is run, make sure to install any missing tools.</p> <p>Note</p> <p>Ignore the requirement for the Docker CLI, IBM Software Policies prohibit the use of the Docker CLI and encourages the use of Podman CLI instead.</p>"},{"location":"workstation-setup/#install-clis-and-tools-macoslinux","title":"Install CLIs and tools (MacOS/Linux)","text":"<p>The following is a list of desktop tools required to help with installation and development.</p> <ul> <li> <p>Git Client: Needs to be installed in your development operating system, it comes as standard for Mac OS</p> </li> <li> <p>IBM Cloud CLI: Required for management of IBM Cloud.</p> </li> <li> <p>Podman Desktop: Required for building and running container images.</p> <ul> <li>Installed and running on your local machine</li> </ul> </li> <li> <p>Visual Studio Code: A popular code editor</p> <ul> <li>You will be required to edit some files, having a good quality editor is always best practice</li> <li>Enabling launching VSCode from a terminal</li> </ul> </li> <li> <p>JDK 11: Optional installed on your local machine</p> <ul> <li>Used for SpringBoot content</li> </ul> </li> </ul>"},{"location":"workstation-setup/#create-accounts-kubernetes-windows","title":"Create accounts (Kubernetes (Windows))","text":"<p>You'll need these accounts to use the Developer Tools environment.</p> <ul> <li>GitHub account (public, not enterprise): Create one if you do not have one already. If you have not logged in for a while, make sure your login is working.</li> </ul>"},{"location":"workstation-setup/#cloud-native-vm-kubernetes-windows","title":"Cloud Native VM (Kubernetes (Windows))","text":"<p>Use the Cloud Native VM it comes pre-installed with kubernetes and all cloud native CLIs.</p> <p>Is highly recommended for Windows users to use this VM.</p>"},{"location":"workstation-setup/#install-clis-and-tools-kubernetes-windows","title":"Install CLIs and tools (Kubernetes (Windows))","text":"<p>The following is a list of desktop tools required to help with installation and development.</p> <ul> <li> <p>Git Client: Needs to be installed in your development operating system, it comes as standard for Mac OS</p> </li> <li> <p>IBM Cloud CLI: Required for management of IBM Cloud</p> </li> <li> <p>Podman Desktop: Required for building and running container images.</p> <ul> <li>Installed and running on your local machine</li> </ul> </li> <li> <p>Visual Studio Code: A popular code editor</p> <ul> <li>You will be required to edit some files, having a good quality editor is always best practice</li> <li>Enabling launching VSCode from a terminal</li> </ul> </li> <li> <p>JDK 11: Optional installed on your local machine</p> <ul> <li>Used for SpringBoot content</li> </ul> </li> </ul>"},{"location":"workstation-setup/#environment-setup","title":"Environment Setup","text":"OpenShift LocalMiniKube <p>Ensure OpenShift Local is installed. Check out the  OpenShift Local Page.</p> <ul> <li> <p>Setup OpenShift Local</p> <pre><code>crc setup\n</code></pre> </li> <li> <p>Start OpenShift Local</p> <pre><code>crc start\n</code></pre> </li> </ul> <p> Warning: Make sure you have Cisco VPN turned off when using OpenShift Local. </p> <p>Ensure Minikube is installed. Check out the Minikube Page.</p> <ul> <li> <p>Verify your <code>driver</code> is set for <code>podman</code></p> <pre><code>minikube config set driver podman\n</code></pre> </li> <li> <p>Start minikube</p> <pre><code>minikube start --driver=podman --container-runtime=cri-o\n</code></pre> </li> <li> <p>In case memory is not set, or need to increase set the memory and recreate the VM</p> <pre><code>minikube config set memory 4096\nminikube delete\nminikube start --driver=podman --container-runtime=cri-o\n</code></pre> </li> <li> <p>Kubernetes should be v1.31+</p> <pre><code>kubectl version\n</code></pre> </li> </ul> <p> Warning: Make sure you have Cisco VPN turned off when using Minikube. </p>"},{"location":"airgapped/","title":"Solution Architecture","text":"<p>Shout-out </p> <p>Big shout-out to the SPGI CSM Team for creating the majority of this workshop. A huge thank you to them.</p> <p>Disclaimer</p> <p>This material has been created for training and learning purposes. It is not, by any means, official documentation supported by either IBM or Red Hat.</p> <p>The following diagram illustrates the architecture of the OpenShift cluster air-gapped installation we are conducting in this training course. The servers within the cluster operate in an air-gapped environment, completely isolated from the Internet, with no inbound or outbound access. To facilitate the installation, a specialized machine external to the cluster and connected to the Internet is employed. This machine, designated as the online bastion, downloads all necessary packages and binaries required for the installation, mirroring real-world client scenarios.</p> <p>Packages and binaries are then transferred from the online bastion to a machine situated within the internal network of the cluster, which has no Internet connectivity whatsoever. This internal machine is referred to as the offline bastion. The OpenShift installation will be initiated from this offline bastion, as it holds exclusive access to the internal network where the cluster resides.</p> <p></p> <p>During the air-gapped OpenShift installation conducted in this training course, we will create and configure 3 control plane nodes (aka master nodes), 3 infrastructure nodes (aka infra nodes), 3 storage nodes and 3 compute nodes (aka worker nodes), along with both 'offline' and 'online' bastion nodes. Should the need arise, additional compute nodes can be added at a later stage.</p> <p>The version of the OpenShift Container Platform (OCP) to be installed is 4.16. Correspondingly, an OpenShift (oc) client of this same version, 4.16, will be downloaded and installed on the bastion machines for subsequent operations against the OpenShift cluster.</p> <p>Servers specifications:</p> Type Number of servers vCPU RAM Memory Storage (system) Storage (data) Bastion 2 4 vCPU 16 GB 700 GB 0 GB Control Plane 3 4 vCPU 16 GB 120 GB 0 GB Compute 3 4 vCPU 16 GB 120 GB 0 GB Infra 3 4 vCPU 16 GB 120 GB 0 GB Storage 3 16 vCPU 64 GB 120 GB 512 GB Bootstrap 1 4 vCPU 16 GB 120 GB 0 GB <p>Lab environment</p> <p>Use this  link to the IBM Technology Zone to request an environment to carry out this tutorial. Make sure you select at least 3TB for the DataStore and VPN is enabled:</p> <p></p> <p>Important</p> <p>While utilizing a VMware vSphere IBM Technology Zone environment, we will not leverage any of the VMware features integrated within the OpenShift Installer for deployment purposes. Instead, we are considering this setup as an elevated \"bare-metal\" environment, where virtual machines emulate bare-metal servers closely.</p>"},{"location":"airgapped/10-offlinebastion/","title":"Configure Offline Bastion","text":""},{"location":"airgapped/10-offlinebastion/#disable-selinux-in-the-offline-bastion","title":"Disable SELINUX in the offline bastion","text":"<p>In order to perform the correct configuration of the Bastion node, it is recommended that the <code>SELINUX</code> is deactivated first.</p> <p>Note</p> <p>It is not strictly necessary that Linux Security is deactivated in order to install OpenShift. However, it is recommended as it eases the installation. Otherwise, you would need to open several specific ports and add rules to the firewall so that the needed communication is allowed.</p> <ol> <li> <p>Access the offline bastion:</p> [student laptop]<pre><code>ssh root@192.168.252.23\n</code></pre> </li> <li> <p>Open the <code>/etc/selinux/config</code> to disable the <code>SELINUX</code></p> [root@localhost ~]<pre><code>vi /etc/selinux/config\n</code></pre> </li> <li> <p>Make sure your file looks like below, where the highlighted line has been modified:</p> /etc/selinux/config<pre><code># This file controls the state of SELinux on the system.\n# SELINUX= can take one of these three values:\n# enforcing - SELinux security policy is enforced.\n# permissive - SELinux prints warnings instead of enforcing.\n# disabled - No SELinux policy is loaded.\nSELINUX=disabled\n# SELINUXTYPE= can take one of these three values:\n# targeted - Targeted processes are protected,\n# minimum - Modification of targeted policy. Only selected processes are protected.\n# mls - Multi Level Security protection. SELINUXTYPE=targeted\n</code></pre> <p>Once the change is made, save and exit the file (<code>esc</code> and <code>:wq</code>) </p> </li> <li> <p>Restart the server for the changes to take effect.</p> [root@localhost ~]<pre><code>init 6\n</code></pre> </li> </ol>"},{"location":"airgapped/10-offlinebastion/#disable-the-firewall-in-the-offline-bastion","title":"Disable the firewall in the offline bastion","text":"<p>Also, and since it is an internal network, the firewall of the machine is deactivated</p> <ol> <li> <p>Access the offline bastion:</p> [student laptop]<pre><code>ssh root@192.168.252.23\n</code></pre> </li> <li> <p>Stop the firewall:</p> [root@localhost ~]<pre><code>systemctl stop firewalld\nsystemctl disable firewalld\n</code></pre> Output<pre><code>Removed /etc/systemd/system/multi-user.target.wants/firewalld.service. \nRemoved /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service.\n</code></pre> </li> </ol>"},{"location":"airgapped/10-offlinebastion/#configure-the-hostname-for-the-offline-bastion","title":"Configure the hostname for the offline bastion","text":"<p>We need to change the online bastion machine's default hostname as we will be referring to this machine from the DNS, load balancer, etc using a different hostname.</p> <ol> <li> <p>Edit the hostname of the machine</p> [root@localhost ~]<pre><code>vi /etc/hostname\n</code></pre> <p>set the hostname to <code>bastion.ocp4.platformengineers.xyz</code></p> </li> <li> <p>Once the change has been made, restart the server for it to take effect.</p> [root@localhost ~]<pre><code>init 6\n</code></pre> </li> <li> <p>Access the offline bastion again:</p> [student laptop]<pre><code>ssh root@192.168.252.23\n</code></pre> </li> <li> <p>Check the hostname was successfully modified:</p> [root@bastion ~]<pre><code>ping $HOSTNAME\n</code></pre> <p>You should see similar output as below:</p> Output<pre><code>PING bastion.ocp4.platformengineers.xyz(bastion.ocp4.platformengineers.xyz (fe80::250:56ff:fe8a:23c2%ens192)) 56 data bytes\n64 bytes from bastion.ocp4.platformengineers.xyz (fe80::250:56ff:fe8a:23c2%ens192): icmp_seq=1 ttl=64 time=0.040 ms\n64 bytes from bastion.ocp4.platformengineers.xyz (fe80::250:56ff:fe8a:23c2%ens192): icmp_seq=2 ttl=64 time=0.055 ms\n64 bytes from bastion.ocp4.platformengineers.xyz (fe80::250:56ff:fe8a:23c2%ens192): icmp_seq=3 ttl=64 time=0.047 ms\n</code></pre> </li> <li> <p>Finish the ping command with <code>ctrl+c</code></p> </li> </ol>"},{"location":"airgapped/10-offlinebastion/#install-required-red-hat-packages-in-the-offline-bastion","title":"Install required Red Hat packages in the offline bastion","text":"<p>For the installation of rpm packages on the offline machine, the RHEL 8.7 OS ISO file that created the VM from will help us. </p> <p>Repeat the steps taken on the onlinebastion for creating a offline repository from the RHEL 8.7 ISO</p> <ol> <li> <p>Make sure that, in the vCenter, the offline bastion has the CD/DVD drive as connected:</p> <p></p> </li> </ol> <p>Once the offline repository is has been created and enabled, the required packages are installed with the yum utility.</p> <p>The following packages have been installed on the online bastion server. Some packages may not be used during the execution of online commands, but they are installed so that the \"online\" and \"offline\" bastion settings are as identical as possible.</p> [root@bastion ~]<pre><code>yum install -y podman \\\n                    jq openssl httpd-tools curl wget telnet nfs-utils \\\n                    httpd.x86_64 \\\n                    bind bind-utils rsync mkisofs haproxy\n</code></pre> Output<pre><code>Updating Subscription Management repositories.\nUnable to read consumer identity\n\nThis system is not registered with an entitlement server. You can use subscription-manager to register.\n\nRed Hat Enterprise Linux 8.7.0 Base                                                                                          40 MB/s | 2.4 MB     00:00    \nRed Hat Enterprise Linux 8.7.0 App                                                                                           53 MB/s | 7.8 MB     00:00    \nPackage openssl-1:1.1.1k-7.el8_6.x86_64 is already installed.\nPackage curl-7.61.1-25.el8.x86_64 is already installed.\nDependencies resolved.\n============================================================================================================================================================\n Package                                   Architecture        Version                                                  Repository                     Size\n============================================================================================================================================================\nInstalling:\n bind                                      x86_64              32:9.11.36-5.el8                                         InstallMediaApps              2.1 M\n bind-utils                                x86_64              32:9.11.36-5.el8                                         InstallMediaApps              452 k\n genisoimage                               x86_64              1.1.11-39.el8                                            InstallMediaApps              316 k\n httpd                                     x86_64              2.4.37-51.module+el8.7.0+16050+02173b8e                  InstallMediaApps              1.4 M\n httpd-tools                               x86_64              2.4.37-51.module+el8.7.0+16050+02173b8e                  InstallMediaApps              109 k\n jq                                        x86_64              1.6-3.el8                                                InstallMediaApps              202 k\n nfs-utils                                 x86_64              1:2.3.3-57.el8                                           InstallMediaBase              515 k\n podman                                    x86_64              3:4.2.0-1.module+el8.7.0+16772+33343656                  InstallMediaApps               12 M\n rsync                                     x86_64              3.1.3-19.el8                                             InstallMediaBase              410 k\n telnet                                    x86_64              1:0.17-76.el8                                            InstallMediaApps               72 k\n wget                                      x86_64              1.19.5-10.el8                                            InstallMediaApps              734 k\n...\n...\nComplete!\n</code></pre>"},{"location":"airgapped/10-offlinebastion/#copy-downloaded-artifacts-from-online-bastion-to-offline-bastion","title":"Copy downloaded artifacts from online bastion to offline bastion","text":"<ol> <li> <p>Create a directory for the private image registry in the offline bastion</p> [root@bastion ~]<pre><code>mkdir /root/registry\ncd registry/\n</code></pre> </li> <li> <p>Create the necessary directories for such private image registry.</p> [root@bastion registry]<pre><code>mkdir auth certs data downloads\ncd downloads/\n</code></pre> [root@bastion downloads]<pre><code>mkdir images tools secrets\n</code></pre> <p>The folder structure that should have got created should look like this:</p> Directory structure<pre><code>/root\n`-- registry\n    |-- auth\n    |-- certs\n    |-- data\n    `-- downloads\n        |-- images\n        |-- secrets\n        `-- tools\n</code></pre> </li> <li> <p>Exit the offline bastion</p> [root@bastion downloads]<pre><code>exit\n</code></pre> </li> <li> <p>Access the online bastion:</p> [student laptop]<pre><code>ssh root@192.168.252.22\n</code></pre> </li> <li> <p>Verify that the <code>.tar</code> files have been generated.</p> [root@bastiononline ~]<pre><code>cd /root/registry/data\n</code></pre> [root@bastiononline data]<pre><code>ls -hla\n</code></pre> Output<pre><code>total 61G\ndrwxr-xr-x 4 root root 4.0K Sep  8 17:28 .\ndrwxr-xr-x 6 root root  110 Sep  8 17:08 ..\n-rw-r--r-- 1 root root 4.0G Sep  8 17:23 mirror_seq1_000000.tar\n-rw-r--r-- 1 root root 3.9G Sep  8 17:23 mirror_seq1_000001.tar\n-rw-r--r-- 1 root root 3.6G Sep  8 17:24 mirror_seq1_000002.tar\n-rw-r--r-- 1 root root 4.0G Sep  8 17:24 mirror_seq1_000003.tar\n-rw-r--r-- 1 root root 4.0G Sep  8 17:24 mirror_seq1_000004.tar\n-rw-r--r-- 1 root root 3.4G Sep  8 17:25 mirror_seq1_000005.tar\n-rw-r--r-- 1 root root 3.7G Sep  8 17:25 mirror_seq1_000006.tar\n-rw-r--r-- 1 root root 3.5G Sep  8 17:25 mirror_seq1_000007.tar\n-rw-r--r-- 1 root root 4.0G Sep  8 17:26 mirror_seq1_000008.tar\n-rw-r--r-- 1 root root 4.0G Sep  8 17:26 mirror_seq1_000009.tar\n-rw-r--r-- 1 root root 4.0G Sep  8 17:26 mirror_seq1_000010.tar\n-rw-r--r-- 1 root root 3.9G Sep  8 17:27 mirror_seq1_000011.tar\n-rw-r--r-- 1 root root 4.0G Sep  8 17:27 mirror_seq1_000012.tar\n-rw-r--r-- 1 root root 3.9G Sep  8 17:27 mirror_seq1_000013.tar\n-rw-r--r-- 1 root root 3.9G Sep  8 17:28 mirror_seq1_000014.tar\n-rw-r--r-- 1 root root 3.8G Sep  8 17:28 mirror_seq1_000015.tar\ndrwxr-xr-x 2 root root    6 Sep  8 17:28 oc-mirror-workspace\ndrwxr-x--- 2 root root   28 Sep  8 17:28 publish\n</code></pre> </li> <li> <p>Copy the tar files to the offline bastion using the scp command as there is currently network connectivity between the online bastion and the offline bastion.</p> <p>Note</p> <p>In a customer air-gapped environment, you would likely need to copy the data to the offline bastion via another method, a USB drive for example.</p> [root@bastiononline data]<pre><code>scp *.tar root@192.168.252.23:/root/registry/data\n</code></pre> Output<pre><code>mirror_seq1_000000tar   100% 4090MB 167.6MB/s   00:24\nmirror_seq1_000001.tar  100% 3922MB 165.8MB/s   00:23\nmirror_seq1_000002.tar  100% 3673MB 167.1MB/s   00:21\nmirror_seq1_000003.tar  100% 4093MB 168.0MB/s   00:24\nmirror_seq1_000004.tar  100% 4061MB 169.4MB/s   00:23\nmirror_seq1_000005.tar  100% 3391MB 165.0MB/s   00:20\nmirror_seq1_000006.tar  100% 3698MB 128.1MB/s   00:28\nmirror_seq1_000007.tar  100% 3567MB 168.4MB/s   00:21\nmirror_seq1_000008.tar  100% 4083MB 124.5MB/s   00:32\nmirror_seq1_000009.tar  100% 3893MB 165.2MB/s   00:23\nmirror_seq1_000010.tar  100% 4086MB 158.6MB/s   00:25\nmirror_seq1_000011.tar  100% 3894MB 165.3MB/s   00:23\nmirror_seq1_000012.tar  100% 3871MB 137.9MB/s   00:28\nmirror_seq1_000013.tar  100% 3895MB 162.9MB/s   00:23\nmirror_seq1_000014.tar  100% 3913MB 156.7MB/s   00:24\nmirror_seq1_000015.tar  100% 4094MB 133.8MB/s   00:30\nmirror_seq1_000016.tar  100% 4094MB 133.8MB/s   00:30\n</code></pre> </li> <li> <p>Copy the remaining components downloaded in the online bastion (client oc, installer, images, image registry, butane...) to the offline bastion </p> [root@bastiononline data]<pre><code>scp -r /root/registry/downloads/* root@192.168.252.23:/root/registry/downloads/\n</code></pre> Output<pre><code>rhcos-4.16.3-x86_64-metal.x86_64.raw.gz                  100% 1208MB 391.3MB/s   00:03    \nrhcos-4.16.3-x86_64-live.x86_64.iso                      100% 1164MB 188.5MB/s   00:06    \npull-secret.txt                                          100% 2759     1.3MB/s   00:00    \npull-secret.json                                         100% 2875     1.4MB/s   00:00    \nmirror-registry-amd64.tar.gz                             100%  579MB 407.7MB/s   00:01    \nimage-archive.tar                                        100% 1388MB 139.8MB/s   00:09    \nexecution-environment.tar                                100%  302MB 383.2MB/s   00:00    \nmirror-registry                                          100% 9595KB 295.8MB/s   00:00    \nsqlite3.tar                                              100%  108MB 193.5MB/s   00:00    \nbutane                                                   100% 7881KB 218.3MB/s   00:00    \nopenshift-client-linux-amd64-rhel8-4.16.9.tar.gz         100%   64MB 497.5MB/s   00:00    \noc                                                       100%  152MB 373.1MB/s   00:00    \nkubectl                                                  100%  152MB 665.6MB/s   00:00    \nopenshift-install-linux-4.16.9.tar.gz                    100%  487MB 497.8MB/s   00:00    \nREADME.md                                                100%  706   395.9KB/s   00:00    \nopenshift-install                                        100%  675MB 191.8MB/s   00:03    \noc-mirror.tar.gz                                         100%   64MB 525.6MB/s   00:00    \noc-mirror                                                100%  158MB 386.7MB/s   00:00    \n.oc-mirror.log                                           100%    0     0.0KB/s   00:00\n</code></pre> </li> <li> <p>Exit the online bastion</p> [root@bastiononline data]<pre><code>exit\n</code></pre> </li> <li> <p>Access the offline bastion:</p> [student laptop]<pre><code>ssh root@192.168.252.23\n</code></pre> </li> <li> <p>Copy the binaries into <code>/usr/bin</code> or <code>/usr/local/bin</code> in the same way as it was done for the online bastion</p> [root@bastion ~]<pre><code>cd /root/registry/downloads/tools\n</code></pre> [root@bastion tools]<pre><code>ls -lart\n</code></pre> Output<pre><code>total 4245772\ndrwxr-xr-x 5 root root         48 Oct 25 14:08 ..\n-rw-r--r-- 1 root root  607056480 Oct 25 15:43 mirror-registry-amd64.tar.gz\n-rw-r--r-- 1 root root 1454929920 Oct 25 15:44 image-archive.tar\n-rw-r--r-- 1 root root  316753920 Oct 25 15:44 execution-environment.tar\n-rwxr-xr-x 1 root root    9824888 Oct 25 15:44 mirror-registry\n-rw-r--r-- 1 root root  113418240 Oct 25 15:44 sqlite3.tar\n-rw-r--r-- 1 root root    8070568 Oct 25 15:44 butane\n-rw-r--r-- 1 root root   66705673 Oct 25 15:44 openshift-client-linux-amd64-rhel8-4.16.9.tar.gz\n-rwxr-xr-x 1 root root  159905720 Oct 25 15:44 oc\n-rwxr-xr-x 1 root root  159905720 Oct 25 15:44 kubectl\n-rw-r--r-- 1 root root  510253136 Oct 25 15:44 openshift-install-linux-4.16.9.tar.gz\n-rw-r--r-- 1 root root        706 Oct 25 15:44 README.md\n-rwxr-xr-x 1 root root  707731456 Oct 25 15:44 openshift-install\n-rw-r--r-- 1 root root   67385243 Oct 25 15:44 oc-mirror.tar.gz\n-rwxr-x--x 1 root root  165699280 Oct 25 15:44 oc-mirror\n-rw------- 1 root root          0 Oct 25 15:44 .oc-mirror.log\ndrwxr-xr-x 2 root root       4096 Oct 25 15:44 .\n</code></pre> [root@bastion tools]<pre><code>cp /root/registry/downloads/tools/oc /usr/bin/oc\ncp /root/registry/downloads/tools/openshift-install /usr/bin/openshift-install\ncp /root/registry/downloads/tools/oc-mirror /usr/local/bin/oc-mirror\n</code></pre> </li> </ol>"},{"location":"airgapped/10-offlinebastion/#create-the-private-image-registry-in-the-offline-bastion","title":"Create the private image registry in the offline bastion","text":"<p>The images to be used during the installation of OpenShift are to be supplied from the bastion. To do this, a local registry must be configured as follows:</p> <ol> <li> <p>Run the <code>mirror-registry install</code> command</p> [root@bastion downloads]<pre><code>cd /root/registry/downloads/tools\n</code></pre> [root@bastion tools]<pre><code>./mirror-registry install --quayRoot /root/registry \\\n                          --quayHostname bastion.ocp4.platformengineers.xyz \\\n                          --initPassword passw0rd\n</code></pre> <p>Important</p> <p>On a client installation, please specify a different password in agreement with the client. If the <code>--initPassword</code> is not specified during the Quay private image registry install command, a random password is generated.</p> <p>IMPORTANT: If you let the install process generate a random password, please DO WRITE IT DOWN as it can not be retrieved again.</p> Output<pre><code>__   __\n/  \\ /  \\     ______   _    _     __   __   __\n/ /\\ / /\\ \\   /  __  \\ | |  | |   /  \\  \\ \\ / /\n/ /  / /  \\ \\  | |  | | | |  | |  / /\\ \\  \\   /\n\\ \\  \\ \\  / /  | |__| | | |__| | / ____ \\  | |\n\\ \\/ \\ \\/ /   \\_  ___/  \\____/ /_/    \\_\\ |_|\n\\__/ \\__/      \\ \\__\n                \\___\\ by Red Hat\nBuild, Store, and Distribute your Containers\n\nINFO[2023-11-06 17:01:33] Install has begun  \n...\n...\nPLAY RECAP *************************************************************************************************************************************************\nroot@bastion.ocp4.platformengineers.xyz : ok=50   changed=30   unreachable=0    failed=0    skipped=17   rescued=0    ignored=0   \n\nINFO[2023-11-06 17:04:14] Quay installed successfully, config data is stored in /root/registry \nINFO[2023-11-06 17:04:14] Quay is available at https://bastion.ocp4.platformengineers.xyz:8443 with credentials (init, passw0rd) \n</code></pre> </li> <li> <p>If you let the Quay's private image registry install process generate a random password for the <code>init</code> user, log such password    </p> Output<pre><code>INFO[2023-09-11 18:31:13] Quay installed successfully, config data is stored in /root/registry\nINFO[2023-09-11 18:31:13] Quay is available at https://bastion.ocp4.platformengineers.xyz:8443 with credentials (init, cw9q3GC10aPQMiunkSZ6Ag7r52H8h4ve)\n</code></pre> <p>Warning</p> <p>Take note of this user and password because you will need to use it when you create the <code>install-config.yaml</code> file on next steps.</p> </li> <li> <p>Add the <code>init</code> user as a Quay super user on the following Quay private image registry configuration file within the <code>SUPER_USERS</code> section</p> [root@bastion tools]<pre><code>vi /root/registry/quay-config/config.yaml\n</code></pre> Output<pre><code>...\nSUPER_USERS:\n- admin\n- init\n...\n</code></pre> </li> <li> <p>Restart the offline bastion node.</p> [root@bastion tools]<pre><code>init 6\n</code></pre> </li> </ol> <p>Important</p> <ul> <li> <p>The private image registry sets Quay to use the autogenerated TLS certificates to enable SSL connectivity by default. These certificates have a validity of 365 days and will expire after 1 year after which they need to be rotated. The rotation procedure is not done automatically by Quay, it needs to be done manually, otherwise x509 errors will be present during pushes and pulls to/from the registry.</p> </li> <li> <p>You can find the instructions to rotate the certificates here.</p> </li> <li> <p>Newly created certificate will need to be added to all OpenShift clusters that pull images from this private image registry instance. This will ensure certificate trust. You can find the link to add certficiates to OpenShift here</p> </li> </ul>"},{"location":"airgapped/10-offlinebastion/#publish-the-downloaded-images-to-the-offline-bastion-image-registry","title":"Publish the downloaded images to the offline bastion Image Registry","text":"<p>The downloaded images must be published in the Quay private image registry installed on the previous section. </p> <ol> <li> <p>Access the offline bastion again:</p> [student laptop]<pre><code>ssh root@192.168.252.23\n</code></pre> </li> <li> <p>Execute the following command to set the image registry certificate as a trusted certificate by the system:</p> [root@bastion ~]<pre><code>cp /root/registry/quay-rootCA/rootCA.pem /usr/share/pki/ca-trust-source/anchors/\nupdate-ca-trust\n</code></pre> </li> <li> <p>Log into the private image registry:</p> <p>Warning</p> <p>Remember to use the user (<code>init</code>) and password (<code>passw0rd</code> if you used the command outlined in this training material) you installed the Quay private image registry with during its installation proccess. </p> [root@bastion ~]<pre><code>podman login bastion.ocp4.platformengineers.xyz:8443\n</code></pre> Output<pre><code>Login Succeeded!\n</code></pre> </li> <li> <p>Launch the image mirror process to load the previously downloaded images, now in <code>.tar</code> files, into the Quay private image registry:</p> <p>Info</p> <p>This process can take around 30 mins</p> <p>Tip</p> <p>You should not need the <code>--dest-skip-tls</code> flag in the command below if you set your container image private registry CA as a trusted CA in the steps above. We've added it to avoid issues during workshops.</p> [root@bastion ~]<pre><code>oc mirror --from=/root/registry/data docker://bastion.ocp4.csm-spgi.acme.es:8443 --dest-skip-tls\n</code></pre> Output<pre><code>[...]\nWrote release signatures to oc-mirror-workspace/results-1694473158\nRendering catalog image \"bastion.ocp4.platformengineers.xyz:8443/redhat/redhat-operator-index:v4.12\" with file-based catalog\nWriting image mapping to oc-mirror-workspace/results-1694473158/mapping.txt\nWriting CatalogSource manifests to oc-mirror-workspace/results-1694473158\nWriting ICSP manifests to oc-mirror-workspace/results-1694473158\n</code></pre> </li> <li> <p>You can check the result of the image mirror process by pointing your browser to:</p> <p>https://192.168.252.23:8443</p> <p></p> </li> <li> <p>Check that the <code>catalogSource-redhat-operator-index.yaml</code> and <code>imageContentSourcePolicy.yaml</code> files have been generated in the <code>/root/oc-mirror-workspace/results-XXXXXX</code> directory.</p> [root@bastion ~]<pre><code>ls -lart ~/oc-mirror-workspace/results-XXXXXX/\n</code></pre> Output<pre><code>total 96\ndrwxr-xr-x 2 root root     6 Sep 11 18:59 charts\ndrwxr-xr-x 2 root root    98 Sep 11 19:25 release-signatures\ndrwxr-xr-x 4 root root    47 Sep 11 19:25 ..\n-rw-r--r-- 1 root root 87467 Sep 11 19:25 mapping.txt\n-rwxr-xr-x 1 root root   241 Sep 11 19:25 catalogSource-redhat-operator-index.yaml\n-rwxr-xr-x 1 root root  1741 Sep 11 19:25 imageContentSourcePolicy.yaml\ndrwxr-xr-x 4 root root   150 Sep 11 19:25 .\n</code></pre> </li> </ol> <p>Note</p> <p>The contents of the <code>imageContentSourcePolicy.yaml</code> must be written down for use when creating the <code>install-config.yaml</code> file later in the chapter Create the install-config.yaml file.</p> <p>The contents of the <code>catalogSource-redhat-operator-index.yaml</code> file must be used when creating the RedHat Marketplace later in the chapter Create the catalog sources to access RedHat Marketplace operators.</p>"},{"location":"airgapped/10-offlinebastion/#configuring-dns-service-in-offline-bastion","title":"Configuring DNS service in offline bastion","text":"<ol> <li> <p>The DNS service configuration file is located in the <code>/etc</code> directory. To start, copy <code>named.conf</code> to a backup file:</p> [root@bastion ~]<pre><code>cp /etc/named.conf /etc/named.conf.bak\n</code></pre> </li> <li> <p>Open the <code>named.conf</code> file.</p> [root@bastion ~]<pre><code>vi /etc/named.conf\n</code></pre> </li> <li> <p>Edit the file to look like the one shown below </p> named.conf<pre><code>//\n// named.conf\n//\n// Provided by Red Hat bind package to configure the ISC BIND named(8) DNS\n// server as a caching only nameserver (as a localhost DNS resolver only).\n//\n// See /usr/share/doc/bind*/sample/ for example named configuration files.\n//\n\noptions {\n#   listen-on port 53 { 127.0.0.1; };\n#   listen-on-v6 port 53 { ::1; };\n    directory   \"/var/named\";\n    dump-file   \"/var/named/data/cache_dump.db\";\n    statistics-file \"/var/named/data/named_stats.txt\";\n    memstatistics-file \"/var/named/data/named_mem_stats.txt\";\n    secroots-file   \"/var/named/data/named.secroots\";\n    recursing-file  \"/var/named/data/named.recursing\";\n    allow-query     { any;};\n\n    /*\n    - If you are building an AUTHORITATIVE DNS server, do NOT enable recursion.\n    - If you are building a RECURSIVE (caching) DNS server, you need to enable\n    recursion.\n    - If your recursive DNS server has a public IP address, you MUST enable access\n    control to limit queries to your legitimate users. Failing to do so will\n    cause your server to become part of large scale DNS amplification\n    attacks. Implementing BCP38 within your network would greatly\n    reduce such attack surface\n    */\n    recursion yes;\n\n    dnssec-enable yes;\n#   dnssec-validation yes;\n\n    managed-keys-directory \"/var/named/dynamic\";\n\n    pid-file \"/run/named/named.pid\";\n    session-keyfile \"/run/named/session.key\";\n#    forwarders { 192.168.252.1;};\n    /* https://fedoraproject.org/wiki/Changes/CryptoPolicy */\n    include \"/etc/crypto-policies/back-ends/bind.config\";\n};\n\nlogging {\n        channel default_debug {\n                file \"data/named.run\";\n                severity dynamic;\n        };\n};\n\nzone \".\" IN {\n    type hint;\n    file \"named.ca\";\n};\n\ninclude \"/etc/named.rfc1912.zones\";\ninclude \"/etc/named.root.key\";\nzone \"acme.es\" {\n    type master;\n    file \"platformengineers.xyz.db\";\n    allow-update { none; };\n};\nzone \"252.168.192.in-addr.arpa\" IN {\n    type master;\n    file \"platformengineers.xyz.reverse.db\";\n    allow-update { none; };\n};\n</code></pre> </li> <li> <p>Create the <code>platformengineers.xyz.db</code> file in <code>/var/named</code> for name resolution.</p> [root@bastion ~]<pre><code>vi /var/named/platformengineers.xyz.db\n</code></pre> platformengineers.xyz.db<pre><code>$TTL 1D\n@ IN SOA bastion.ocp4.platformengineers.xyz. root.ocp4.platformengineers.xyz. (\n2019022409 ; serial\n3h ; refresh\n15 ; retry\n1w ; expire\n3h ; minimum\n)\n       IN NS bastion.ocp4.platformengineers.xyz.\napi.ocp4.platformengineers.xyz. IN A 192.168.252.24\napi-int.ocp4.platformengineers.xyz. IN A 192.168.252.24\n*.apps.ocp4.platformengineers.xyz. IN A 192.168.252.25\nbootstrap.ocp4.platformengineers.xyz. IN A 192.168.252.3\nbastion.ocp4.platformengineers.xyz. IN A 192.168.252.23\ncontrolplane01.ocp4.platformengineers.xyz. IN A 192.168.252.4\ncontrolplane02.ocp4.platformengineers.xyz. IN A 192.168.252.5\ncontrolplane03.ocp4.platformengineers.xyz. IN A 192.168.252.6\ninfra01.ocp4.platformengineers.xyz. IN A 192.168.252.7\ninfra02.ocp4.platformengineers.xyz. IN A 192.168.252.8\ninfra03.ocp4.platformengineers.xyz. IN A 192.168.252.9\ncompute01.ocp4.platformengineers.xyz. IN A 192.168.252.10\ncompute02.ocp4.platformengineers.xyz. IN A 192.168.252.11\ncompute03.ocp4.platformengineers.xyz. IN A 192.168.252.12\nstorage01.ocp4.platformengineers.xyz. IN A 192.168.252.13\nstorage02.ocp4.platformengineers.xyz. IN A 192.168.252.14\nstorage03.ocp4.platformengineers.xyz. IN A 192.168.252.15\n</code></pre> </li> <li> <p>Create <code>platformengineers.xyz.reverse.db</code> file in <code>/var/named</code> for reverse name resolution:</p> [root@bastion ~]<pre><code>vi /var/named/platformengineers.xyz.reverse.db\n</code></pre> platformengineers.xyz.reverse.db<pre><code>$TTL 1D\n@ IN SOA bastion.ocp4.platformengineers.xyz. root.ocp4.platformengineers.xyz. (\n2019022409 ; serial\n3h ; refresh\n15 ; retry\n1w ; expire\n3h ; minimum\n)\n252.168.192.in-addr.arpa. IN NS bastion.ocp4.platformengineers.xyz.\n23 IN PTR bastion.ocp4.platformengineers.xyz.\n3 IN PTR bootstrap.ocp4.platformengineers.xyz.\n4 IN PTR controlplane01.ocp4.platformengineers.xyz.\n5 IN PTR controlplane02.ocp4.platformengineers.xyz.\n6 IN PTR controlplane03.ocp4.platformengineers.xyz.\n7 IN PTR infra01.ocp4.platformengineers.xyz.\n8 IN PTR infra02.ocp4.platformengineers.xyz.\n9 IN PTR infra03.ocp4.platformengineers.xyz.\n10 IN PTR compute01.ocp4.platformengineers.xyz.\n11 IN PTR compute02.ocp4.platformengineers.xyz.\n12 IN PTR compute03.ocp4.platformengineers.xyz.\n13 IN PTR storage01.ocp4.platformengineers.xyz.\n14 IN PTR storage02.ocp4.platformengineers.xyz.\n15 IN PTR storage03.ocp4.platformengineers.xyz.\n</code></pre> </li> <li> <p>Enable the DNS service: </p> [root@bastion ~]<pre><code>systemctl enable named\n</code></pre> Output<pre><code>Created symlink /etc/systemd/system/multi-user.target.wants/named.service \u2192 /usr/lib/systemd/system/named.service.\n</code></pre> </li> <li> <p>Start the DNS service:</p> [root@bastion ~]<pre><code>systemctl start named\n</code></pre> </li> </ol>"},{"location":"airgapped/10-offlinebastion/#set-up-haproxy-in-the-offline-bastion","title":"Set up HAProxy in the offline bastion","text":"<p>The following link provides an example of the HAProxy provided by RedHat as a reference:</p> <p>https://access.redhat.com/articles/5127211 </p> <ol> <li> <p>Open the HAProxy configuration file:</p> [root@bastion ~]<pre><code>vi /etc/haproxy/haproxy.cfg\n</code></pre> </li> <li> <p>Make sure the HAProxy configuration file looks like below:</p> haproxy.cfg<pre><code>#---------------------------------------------------------------------\n# Global settings\n#---------------------------------------------------------------------\nglobal\nlog 127.0.0.1 local2\nchroot /var/lib/haproxy\npidfile /var/run/haproxy.pid\nmaxconn 4000\nuser haproxy\ngroup haproxy\ndaemon\n# turn on stats unix socket\nstats socket /var/lib/haproxy/stats\n# utilize system-wide crypto-policies\n#ssl-default-bind-ciphers PROFILE=SYSTEM\n#ssl-default-server-ciphers PROFILE=SYSTEM\n#---------------------------------------------------------------------\n# common defaults that all the 'listen' and 'backend' sections will\n# use if not designated in their block\n#---------------------------------------------------------------------\ndefaults\nmode tcp\nlog global\n#option httplog\noption dontlognull\noption http-server-close\noption forwardfor except 127.0.0.0/8\noption redispatch\nretries 3\ntimeout http-request 10s\ntimeout queue 1m\ntimeout connect 10s\ntimeout client 5m\ntimeout server 5m\ntimeout http-keep-alive 10s\ntimeout check 10s\nmaxconn 3000\n#---------------------------------------------------------------------\n# balancing for RHOCP Kubernetes API Server\n#---------------------------------------------------------------------\nfrontend k8s_api\nbind *:6443\n#mode tcp\ndefault_backend k8s_api_backend\nbackend k8s_api_backend\n#balance roundrobin\nbalance source\n#mode tcp\nserver bootstrap.ocp4.platformengineers.xyz 192.168.252.3:6443 check\nserver controlplane01.ocp4.platformengineers.xyz 192.168.252.4:6443 check\nserver controlplane02.ocp4.platformengineers.xyz 192.168.252.5:6443 check\nserver controlplane03.ocp4.platformengineers.xyz 192.168.252.6:6443 check\n# ---------------------------------------------------------------------\n# balancing for RHOCP Machine Config Server\n# ---------------------------------------------------------------------\nfrontend machine_config\nbind *:22623\n#mode tcp\ndefault_backend machine_config_backend\nbackend machine_config_backend\n#balance roundrobin\nbalance source\n#mode tcp\nserver bootstrap.ocp4.platformengineers.xyz 192.168.252.3:22623 check\nserver controlplane01.ocp4.platformengineers.xyz 192.168.252.4:22623 check\nserver controlplane02.ocp4.platformengineers.xyz 192.168.252.5:22623 check\nserver controlplane03.ocp4.platformengineers.xyz 192.168.252.6:22623 check\n# --------------------------------------------------------------------\n# balancing for RHOCP Ingress Insecure Port\n# ---------------------------------------------------------------------\nfrontend ingress_insecure\nbind *:80\n#mode tcp\ndefault_backend ingress_insecure_backend\nbackend ingress_insecure_backend\n#balance roundrobin\nbalance source\n#mode tcp\nserver infra01.ocp4.platformengineers.xyz 192.168.252.7:80 check\nserver infra02.ocp4.platformengineers.xyz 192.168.252.8:80 check\nserver infra03.ocp4.platformengineers.xyz 192.168.252.9:80 check\nserver compute01.ocp4.platformengineers.xyz 192.168.252.10:80 check\nserver compute02.ocp4.platformengineers.xyz 192.168.252.11:80 check\nserver compute03.ocp4.platformengineers.xyz 192.168.252.12:80 check\nserver storage01.ocp4.platformengineers.xyz 192.168.252.13:80 check\nserver storage02.ocp4.platformengineers.xyz 192.168.252.14:80 check\nserver storage03.ocp4.platformengineers.xyz 192.168.252.15:80 check\n\n# ---------------------------------------------------------------------\n# balancing for RHOCP Ingress Secure Port\n# ---------------------------------------------------------------------\nfrontend ingress_secure\nbind *:443\n#mode tcp\ndefault_backend ingress_secure_backend\nbackend ingress_secure_backend\n#balance roundrobin\nbalance source\n#mode tcp\nserver infra01.ocp4.platformengineers.xyz 192.168.252.7:443 check\nserver infra02.ocp4.platformengineers.xyz 192.168.252.8:443 check\nserver infra03.ocp4.platformengineers.xyz 192.168.252.9:443 check\nserver compute01.ocp4.platformengineers.xyz 192.168.252.10:443 check\nserver compute02.ocp4.platformengineers.xyz 192.168.252.11:443 check\nserver compute03.ocp4.platformengineers.xyz 192.168.252.12:443 check\nserver storage01.ocp4.platformengineers.xyz 192.168.252.13:80 check\nserver storage02.ocp4.platformengineers.xyz 192.168.252.14:80 check\nserver storage03.ocp4.platformengineers.xyz 192.168.252.15:80 check\n\n\n# ---------------------------------------------------------------------\n# Exposing HAProxy Statistic Page\n# ---------------------------------------------------------------------\n#listen stats\n#bind :32700\n#stats enable\n#stats uri /\n#stats hide-version\n#stats auth admin:RedH@t322\n</code></pre> </li> <li> <p>Check the configuration file:</p> [root@bastion ~]<pre><code>haproxy -c -f /etc/haproxy/haproxy.cfg\n</code></pre> Output<pre><code>Configuration file is valid\n</code></pre> </li> <li> <p>Enable the service:</p> [root@bastion ~]<pre><code>systemctl enable haproxy\n</code></pre> Output<pre><code>Created symlink /etc/systemd/system/multi-user.target.wants/haproxy.service \u2192 /usr/lib/systemd/system/haproxy.service.\n</code></pre> </li> <li> <p>Start the service:</p> [root@bastion ~]<pre><code>systemctl start haproxy\n</code></pre> </li> </ol>"},{"location":"airgapped/10-offlinebastion/#install-the-http-server-in-the-offline-bastion","title":"Install the HTTP server in the offline bastion","text":"<p>For the provisioning of the ignition files to configure the OpenShift cluster nodes, an HTTP server is required. In this installation, that server resides in the offline bastion. In order to install an Apache HTTPD web server, follow the steps below.</p> <ol> <li> <p>Check the http package is installed:</p> [root@bastion ~]<pre><code>yum list installed | grep http\n</code></pre> Output<pre><code>httpd.x86_64     2.4.37-56.module+el8.8.0+19808+379766d6.7     @rhel-8-for-x86_64-appstream-rpms\n</code></pre> </li> <li> <p>Configure the httpd server to boot on port <code>8080</code> by configuring the file in the path <code>/etc/httpd/conf/httpd.conf</code> and changing the default port <code>80</code> to <code>8080</code>.</p> [root@bastion ~]<pre><code>vi /etc/httpd/conf/httpd.conf\n</code></pre> Output<pre><code>[...]\n#\n#Listen 12.34.56.78:80\nListen 8080\n\n#\n[...]\n</code></pre> </li> <li> <p>Start the service:</p> [root@bastion ~]<pre><code>systemctl enable httpd\n</code></pre> Output<pre><code>Created symlink /etc/systemd/system/multi-user.target.wants/httpd.service \u2192 /usr/lib/systemd/system/httpd.service.\n</code></pre> [root@bastion ~]<pre><code>systemctl start httpd\n</code></pre> </li> <li> <p>Create the following directory to host the images and the ignition files: </p> [root@bastion ~]<pre><code>mkdir /var/www/html/ign\n</code></pre> </li> </ol>"},{"location":"airgapped/10-offlinebastion/#generate-an-ssh-key-in-the-offline-bastion","title":"Generate an SSH key in the offline bastion","text":"<p>In order to be able to log on to the OpenShift cluster nodes from the bastion server using SSH after the installation has been done, a public ssh key needs to be added to the file <code>install-config.yaml</code>.</p> <ol> <li> <p>Generate an SSH key</p> [root@bastion ~]<pre><code>ssh-keygen -t ed25519 -N '' -f ~/.ssh/id_ed25519\n</code></pre> Output<pre><code>Generating public/private ed25519 key pair.\nYour identification has been saved in /root/.ssh/id_ed25519.\nYour public key has been saved in /root/.ssh/id_ed25519.pub.\nThe key fingerprint is:\nSHA256:bldGYez9jXDnXs+6luiy7dDnEFVIoKqVtJpTyaMg/wU root@bastion.ocp4.platformengineers.xyz\nThe key's randomart image is:\n+--[ED25519 256]--+\n|           .+o...|\n|           o.... |\n|        . .....  |\n|       o = .o.o .|\n|  . . E S   +o =.|\n|   o . X . + .. =|\n|    . * + o o..+o|\n|     . + ..o.+o +|\n|      .   .=+.+o |\n+----[SHA256]-----+\n</code></pre> </li> <li> <p>Display the public key:</p> [root@bastion ~]<pre><code>cat /root/.ssh/id_ed25519.pub\n</code></pre> Output<pre><code>ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIET7OJRP4wD7ibIvcQxA9/9B80qUp0IPog0M9E7zofED root@bastion.ocp4.platformengineers.xyz\n</code></pre> </li> <li> <p>Start the SSH agent</p> [root@bastion ~]<pre><code>eval \"$(ssh-agent -s)\"\n</code></pre> Output<pre><code>Agent pid 7233\n</code></pre> </li> <li> <p>Add the key to the SSH agent</p> [root@bastion ~]<pre><code>ssh-add ~/.ssh/id_ed25519\n</code></pre> Output<pre><code>Identity added: /root/.ssh/id_ed25519 (root@bastion.ocp4.platformengineers.xyz)\n</code></pre> </li> </ol>"},{"location":"airgapped/11-coreosisomaker/","title":"Create Red Hat CoreOS Custom Image","text":""},{"location":"airgapped/11-coreosisomaker/#configure-the-coreos-iso-maker-in-the-online-bastion","title":"Configure the CoreOS ISO Maker in the online bastion","text":"<ol> <li> <p>Exit the offline bastion:</p> [root@bastion ~]<pre><code>exit\n</code></pre> </li> <li> <p>Access the online bastion:</p> [student laptop]<pre><code>ssh root@192.168.252.22\n</code></pre> </li> <li> <p>Change to the route where the CoreOS ISO Maker is</p> [root@bastiononline ~]<pre><code>cd /root/Coreos-iso-maker/coreos-iso-maker/\n</code></pre> </li> </ol> <p>In the file <code>group_vars/all.yml</code> the following configuration is defined:</p> Variable Definition Value gateway default IP router 192.168.252.1 netmask default netmask 255.255.255.0 interface NIC device name ens192 dns dns server. This can be done as a list. Don't add more than 3 192.168.252.23 webserver_url webserver that holds the Ignition file bastion.ocp4.platformengineers.xyz webserver_port webserver port for the webserver above 80 webserver_ignition_path Ignition subpath in http server ign install_drive drive to install RHCOS on sda ocp_version Full CoreOS version you are going for 4.16.3 is_checksum sha256 checksum of the ISO. Got it from https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/4.16/4.16.3/sha256sum.txt b4b2bbe4462258e9d30cab2f4a9d94b45960bc03ffa578be3400b9cbcac4912c iso_name Name of the ISO to download. Makes certain assumptions that should be verified rhcos-{{ocp_version }}-x86_64-live.x86_64.iso rhcos_bios Name of the BIOS image to boot from. This is how the file is named on your webserver. Make certain assumptions that should be verified rhcos-{{ocp_version }}-x86_64-metal.x86_64.raw.gz arch The CPU Architecture type. Must be one of x86_64 (default) or ppc64le x86_64 <ol> <li> <p>Open the <code>group_vars/all.yml</code> file:</p> [root@bastiononline core-iso-maker]<pre><code>vi group_vars/all.yml\n</code></pre> </li> <li> <p>Make sure it looks like below:</p> <p>Tip</p> <p>Although you can read <code>ocp_version</code> in the file below, it refers to the RedHat CoreOS version you are getting installed (4.16.3). The OpenShift version that will eventually get installed remains 4.16.9 which is the OpentShift install binaries, CLI and container images version we have downloaded previously in this tutorial.</p> all.yml<pre><code>---\n# If only one network interface\ngateway: 192.168.252.1\nnetmask: 255.255.255.0\n# VMWare default ens192\n# KVM default ens3\n# Libvirt default enp1s0\n# Intel NUC default eno1\ninterface: ens192\n\ndns:\n- 192.168.252.23\n\nwebserver_url: bastion.ocp4.platformengineers.xyz\nwebserver_port: 8080\n# Ignition subpath in http server (optionnal, defaults to nothing)\nwebserver_ignition_path: /ign\n# Path to download master ignition file will be\n# http://192.168.1.20:8080/ignition/master.ign\n\n# Drive to install RHCOS\n# Libvirt - can be vda\ninstall_drive: sda\n\n# Timeout for selection menu during first boot\n# '-1' for infinite timeout. Default '10'\nboot_timeout: -1\n\n# Chose the binary architecture\n# x86_64 or ppc64le\narch: \"x86_64\"\n\nocp_version: 4.16.3\n#iso_checksum: d15bd7ae942573eece34ba9c59e110e360f15608f36e9b83ab9f2372d235bef2\niso_checksum: b4b2bbe4462258e9d30cab2f4a9d94b45960bc03ffa578be3400b9cbcac4912c\n#iso_checksum_ppc64: ff3ef20a0c4c29022f52ad932278b9040739dc48f4062411b5a3255af863c95e\niso_name: rhcos-{{ ocp_version }}-x86_64-live.x86_64.iso\n#iso_name_ppc64: rhcos-{{ ocp_version }}-ppc64le-installer.ppc64le.iso\nrhcos_bios: rhcos-{{ ocp_version }}-x86_64-metal.x86_64.raw.gz\n...\n</code></pre> <p>Tip</p> <p>Dont forget to set <code>boot_timeout: -1</code> so that you have no time limit when asked for the machine type at boot time</p> </li> </ol> <p>The machines of the OpenShift cluster are defined in the <code>inventory.yml</code> file.</p> <ol> <li> <p>Open the <code>inventory.yml</code> file:</p> [root@bastiononline core-iso-maker]<pre><code>vi inventory.yml\n</code></pre> </li> <li> <p>Make sure the file looks like below:</p> inventory.yml<pre><code>--- \nall:\n  children:\n    bootstrap:\n      hosts: \n        bootstrap.ocp4.platformengineers.xyz:\n          dhcp: false\n          ipv4: 192.168.252.3\n    controlplane:\n      hosts:\n        controlplane01.ocp4.platformengineers.xyz: \n          dhcp: false\n          ipv4: 192.168.252.4\n        controlplane02.ocp4.platformengineers.xyz: \n          dhcp: false\n          ipv4: 192.168.252.5\n        controlplane03.ocp4.platformengineers.xyz: \n          dhcp: false\n          ipv4: 192.168.252.6\n    compute:\n      hosts:\n        infra01.ocp4.platformengineers.xyz: \n          dhcp: false\n          ipv4: 192.168.252.7\n        infra02.ocp4.platformengineers.xyz: \n          dhcp: false\n          ipv4: 192.168.252.8\n        infra03.ocp4.platformengineers.xyz: \n          dhcp: false\n          ipv4: 192.168.252.9\n        compute01.ocp4.platformengineers.xyz: \n          dhcp: false\n          ipv4: 192.168.252.10                \n        compute02.ocp4.platformengineers.xyz: \n          dhcp: false\n          ipv4: 192.168.252.11\n        compute03.ocp4.platformengineers.xyz: \n          dhcp: false\n          ipv4: 192.168.252.12\n        storage01.ocp4.platformengineers.xyz: \n          dhcp: false\n          ipv4: 192.168.252.13                \n        storage02.ocp4.platformengineers.xyz: \n          dhcp: false\n          ipv4: 192.168.252.14\n        storage03.ocp4.platformengineers.xyz: \n          dhcp: false\n          ipv4: 192.168.252.15 \n...\n</code></pre> </li> <li> <p>Copy the RHCOS ISO live image and the metal raw file for the OpenShift version we are going to install to the temporary directory:</p> [root@bastiononline core-iso-maker]<pre><code>cp /root/registry/downloads/images/rhcos-4.16.3-x86_64-live.x86_64.iso /tmp\ncp /root/registry/downloads/images/rhcos-4.16.3-x86_64-metal.x86_64.raw.gz /tmp\n</code></pre> </li> <li> <p>Run the following Ansible playbook to create the ISO image:</p> [root@bastiononline core-iso-maker]<pre><code>ansible-playbook playbook-single.yml\n</code></pre> Output<pre><code>[...]\nPLAY RECAP *********************************************************************************************************************\nbootstrap.ocp4.platformengineers.xyz : ok=4    changed=2    unreachable=0    failed=0    skipped=2    rescued=0    ignored=0\ninfra01.ocp4.platformengineers.xyz : ok=4    changed=1    unreachable=0    failed=0    skipped=2    rescued=0    ignored=0\ninfra02.ocp4.platformengineers.xyz : ok=4    changed=1    unreachable=0    failed=0    skipped=2    rescued=0    ignored=0\ninfra03.ocp4.platformengineers.xyz : ok=4    changed=1    unreachable=0    failed=0    skipped=2    rescued=0    ignored=0\nlocalhost                  : ok=7    changed=4    unreachable=0    failed=0    skipped=4    rescued=0    ignored=0\ncontrolplane01.ocp4.platformengineers.xyz : ok=4    changed=2    unreachable=0    failed=0    skipped=2    rescued=0    ignored=0\ncontrolplane02.ocp4.platformengineers.xyz : ok=4    changed=1    unreachable=0    failed=0    skipped=2    rescued=0    ignored=0\ncontrolplane03.ocp4.platformengineers.xyz : ok=4    changed=1    unreachable=0    failed=0    skipped=2    rescued=0    ignored=0\ncompute01.ocp4.platformengineers.xyz : ok=4    changed=1    unreachable=0    failed=0    skipped=2    rescued=0    ignored=0\ncompute02.ocp4.platformengineers.xyz : ok=4    changed=1    unreachable=0    failed=0    skipped=2    rescued=0    ignored=0\ncompute03.ocp4.platformengineers.xyz : ok=4    changed=1    unreachable=0    failed=0    skipped=2    rescued=0    ignored=0\nstorage01.ocp4.platformengineers.xyz : ok=4    changed=1    unreachable=0    failed=0    skipped=2    rescued=0    ignored=0\nstorage02.ocp4.platformengineers.xyz : ok=4    changed=1    unreachable=0    failed=0    skipped=2    rescued=0    ignored=0\nstorage03.ocp4.platformengineers.xyz : ok=4    changed=1    unreachable=0    failed=0    skipped=2    rescued=0    ignored=0\n</code></pre> </li> </ol> <p>Copy the resulting ISO files to the image directory <code>/root/registry/downloads/images/</code> and upload the <code>rhcos-install-cluster.iso</code> file to the vCenter datastore to be used as an ISO to install the OpenShift cluster master and worker nodes.</p> <ol> <li> <p>List the ISO files in the <code>/tmp</code> folder</p> [root@bastiononline core-iso-maker]<pre><code>ls -lart /tmp/*iso\n</code></pre> Output<pre><code>-rw-r--r-- 1 root root 1220542464 Oct 27 11:58 /tmp/rhcos-4.16.3-x86_64-live.x86_64.iso\n-rw-r--r-- 1 root root 1220235264 Oct 27 11:59 /tmp/rhcos-install-cluster.iso\n</code></pre> </li> <li> <p>Move original ISO files to their own folder:</p> [root@bastiononline core-iso-maker]<pre><code>mkdir /root/registry/downloads/images/orig\nmv /root/registry/downloads/images/rhcos-4.16.3-x86_64-* /root/registry/downloads/images/orig/\ncp /tmp/*.iso /root/registry/downloads/images/\nchmod 777 /tmp/rhcos-install-cluster.iso\n</code></pre> </li> <li> <p>SCP the final ISO file into the Guacamole VM.</p> <p>Tip</p> <p>The following command is executed from within the Guacamole VM. That is, you need to connect to the vCenter, launch the web console for the Guacamole VM, open a terminal window in there and execute the command below. To upload ISO files, as we did in the section Guacamole VM at the beginning of the course for uploading the RHEL 8.7 OS ISO file for the online bastion and offline bastion, we must do it from the Guacamole VM because the VPN bandwidth is very limited.</p> <p>Extra tip for free ;-)</p> <p>To get the <code>@</code> symbol typed on the terminal, because the layout of the keyboard set up in the Guacamole VM might very well be different than your laptops keyboard layout, we strongly recommened you open the browser and google \"at symbol\". From the results displayed by Google, copy the <code>@</code> symbol and paste it in your terminal</p> [Guacamole VM]<pre><code>scp root@192.168.252.22:/tmp/rhcos-install-cluster.iso ./\n</code></pre> </li> </ol> <p>Once you have copied the ISO file from the online bastion, you must add the file to vCenter into a datastore:</p> <p></p>"},{"location":"airgapped/12-createocpvms/","title":"Create OpenShift VMs","text":"<p>We need to create the following virtual machines in vCenter according to the following specs:</p> Type Number of servers vCPU RAM Memory Storage (system) Storage (data) Control Plane 3 8 vCPU 32 GB 300 GB 0 GB Infra 3 8 vCPU 32 GB 300 GB 0 GB Compute 3 16 vCPU 64 GB 300 GB 0 GB Storage 3 16 vCPU 64 GB 300 GB 500 GB Bootstrap 1 4 vCPU 16 GB 320 GB 0 GB <p>And with the following nomenclature:</p> Type FQDN hostname Bootstrap bootstrap.ocp4.platformengineers.xyz Control Plane controlplane01.ocp4.platformengineers.xyz controlplane02.ocp4.platformengineers.xyz controlplane03.ocp4.platformengineers.xyz Infra infra01.ocp4.platformengineers.xyz infra02.ocp4.platformengineers.xyz infra03.ocp4.platformengineers.xyz Compute compute01.ocp4.platformengineers.xyz compute02.ocp4.platformengineers.xyz compute03.ocp4.platformengineers.xyz Storage storage01.ocp4.platformengineers.xyz storage02.ocp4.platformengineers.xyz storage03.ocp4.platformengineers.xyz <p>You can create folders for the different type of VMs if you wish.</p> <p></p> <p>You can review the process for creating the VMs in the section Create the online bastion.</p> <p></p> <p>You need to configure the machines to boot from the ISO that was created in the previous chapter Configure CoreOS ISO Maker in \"online\" bastion</p> <p></p> <p></p> <p>At the end, you should have the following VMs created in your vCenter</p> <p></p>"},{"location":"airgapped/13-createocpinstallconfig/","title":"Create install-config.yaml file","text":"<ol> <li> <p>Exit the online bastion:</p> [root@bastiononline coreos-iso-maker]<pre><code>exit\n</code></pre> </li> <li> <p>Access the offline bastion:</p> [student laptop]<pre><code>ssh root@192.168.252.23\n</code></pre> </li> <li> <p>Create the directory where the <code>install-config.yaml</code> file will be placed:</p> [root@bastion ~]<pre><code>cd /root/registry/downloads/tools/\n</code></pre> [root@bastion tools]<pre><code>mkdir ocp4\ncd ocp4\n</code></pre> </li> </ol> <p>You need to cat the following files to create the <code>install-config.yaml</code> in the next step:</p> <ol> <li> <p>Quay private image registry CA</p> [root@bastion ocp4]<pre><code>cat /root/registry/quay-rootCA/rootCA.pem\n</code></pre> rootCA.pem<pre><code>-----BEGIN CERTIFICATE-----\nMIID3jCCAsagAwIBAgIUHj/Ux66Fpoktc/qtzoP6wgWZ2aIwDQYJKoZIhvcNAQEL\nBQAwdzELMAkGA1UEBhMCVVMxCzAJBgNVBAgMAlZBMREwDwYDVQQHDAhOZXcgWW9y\nazENMAsGA1UECgwEUXVheTERMA8GA1UECwwIRGl2aXNpb24xJjAkBgNVBAMMHWJh\nc3Rpb24ub2NwNC5jc20tc3BnaS5hY21lLmVzMB4XDTIzMDkyMTE4NDAzM1oXDTI2\nMDcxMTE4NDAzM1owdzELMAkGA1UEBhMCVVMxCzAJBgNVBAgMAlZBMREwDwYDVQQH\nDAhOZXcgWW9yazENMAsGA1UECgwEUXVheTERMA8GA1UECwwIRGl2aXNpb24xJjAk\nBgNVBAMMHWJhc3Rpb24ub2NwNC5jc20tc3BnaS5hY21lLmVzMIIBIjANBgkqhkiG\n9w0BAQEFAAOCAQ8AMIIBCgKCAQEAv3eDBDh1GH6i+hptdlbMPYztWDsHCihyjRta\nWgD3SEyHobuWM/lSepwMC2mmyejZKsv/x4PYFsNrWBwt1oVicP3+sGNuNfKheFgn\nLAiXAgt35MLPR7oXZKCb82rb5jFUYWU6Ro2xHv8qxLAAv+BRzq5fDYczvsps3pq0\nrvdNLfCORBmtQnoRuKXTmOreQgZqHZkhb1g8P4jE+BJugSAQT4gy1cnP5HXFqpdC\nqaehQeQEi+k3yHVdYnMyVC2Ya609SUUeSUxuLrlO2EQTSwLzSZ02sMsIfBKbOgmu\nTJPGgd5jqrSu1LDem8YKws8u0MVEhkl3LkuUuOYBbt4kPxoTIwIDAQABo2IwYDAL\nBgNVHQ8EBAMCAuQwEwYDVR0lBAwwCgYIKwYBBQUHAwEwKAYDVR0RBCEwH4IdYmFz\ndGlvbi5vY3A0LmNzbS1zcGdpLmFjbWUuZXMwEgYDVR0TAQH/BAgwBgEB/wIBATAN\nBgkqhkiG9w0BAQsFAAOCAQEAEIanwOhmODUlshlmpXUMeAa7g8S/cEUy19tpo932\n/sHoxVJogMVdxBY2V1EjNUPr/6PImesUq0IA1piYXqdxw23kiTgAM01THn6xq9Bn\nef7jKPuD7dVY1V8xMmt9EF1Bh4aTObCotF/6XOnBA6NaxWNq5jJ7dfRVU7Vqh5xa\nhyGRglgJw1LmrhwXTHLi6XBZXuwVlAw5HZFEsKsqD/6YqhmTMV7DA3Er/g3Qv1iJ\nNmnM5Ftr1huVtwXuETzm0VHIteSWXBV8/c4+COTlQpwVH2mQRcAZQsW8FibQIjP5\nk/26D0f5XWdf0u6cLowCmWRWhHrTe0ttUUm5ZcP7xeZNjA==\n-----END CERTIFICATE-----\n</code></pre> <p>Info</p> <p>The content of this file will be pasted under <code>additionalTrustBundle</code> tag in the <code>install-config.yaml</code> file.</p> </li> <li> <p>The Image Content Source Policy for the mirrored images</p> [root@bastion ocp4]<pre><code>cat /root/oc-mirror-workspace/results-XXXXXX/imageContentSourcePolicy.yaml\n</code></pre> imageContentSourcePolicy.yaml<pre><code>---\napiVersion: operator.openshift.io/v1alpha1\nkind: ImageContentSourcePolicy\nmetadata:\nname: generic-0\nspec:\nrepositoryDigestMirrors:\n- mirrors:\n    - bastion.ocp4.platformengineers.xyz:8443/ubi8\n    source: registry.redhat.io/ubi8\n---\napiVersion: operator.openshift.io/v1alpha1\nkind: ImageContentSourcePolicy\nmetadata:\nlabels:\n    operators.openshift.org/catalog: \"true\"\nname: operator-0\nspec:\nrepositoryDigestMirrors:\n- mirrors:\n    - bastion.ocp4.platformengineers.xyz:8443/openshift-logging\n    source: registry.redhat.io/openshift-logging\n- mirrors:\n    - bastion.ocp4.platformengineers.xyz:8443/openshift-pipelines\n    source: registry.redhat.io/openshift-pipelines\n- mirrors:\n    - bastion.ocp4.platformengineers.xyz:8443/rhel8\n    source: registry.redhat.io/rhel8\n- mirrors:\n    - bastion.ocp4.platformengineers.xyz:8443/ubi8\n    source: registry.redhat.io/ubi8\n- mirrors:\n    - bastion.ocp4.platformengineers.xyz:8443/redhat\n    source: registry.redhat.io/redhat\n- mirrors:\n    - bastion.ocp4.platformengineers.xyz:8443/openshift-serverless-1\n    source: registry.redhat.io/openshift-serverless-1\n- mirrors:\n    - bastion.ocp4.platformengineers.xyz:8443/ocp-tools-4-tech-preview\n    source: registry.redhat.io/ocp-tools-4-tech-preview\n- mirrors:\n    - bastion.ocp4.platformengineers.xyz:8443/openshift4\n    source: registry.redhat.io/openshift4\n---\napiVersion: operator.openshift.io/v1alpha1\nkind: ImageContentSourcePolicy\nmetadata:\nname: release-0\nspec:\nrepositoryDigestMirrors:\n- mirrors:\n    - bastion.ocp4.platformengineers.xyz:8443/openshift/release\n    source: quay.io/openshift-release-dev/ocp-v4.0-art-dev\n- mirrors:\n    - bastion.ocp4.platformengineers.xyz:8443/openshift/release-images\n    source: quay.io/openshift-release-dev/ocp-release\n</code></pre> <p>Info</p> <p>The highlighted content will be pasted under <code>imageContentSources</code> tag in the <code>install-config.yaml</code> file.</p> </li> <li> <p>Create the <code>install-config.yaml</code> file:</p> [root@bastion ocp4]<pre><code>vi install-config.yaml\n</code></pre> </li> <li> <p>Make sure it looks like below:</p> install-config.yaml<pre><code>apiVersion: v1\nbaseDomain: platformengineers.xyz\ncompute:\n- hyperthreading: Enabled\n  name: worker\n  replicas: 9\ncontrolPlane:\n  hyperthreading: Enabled\n  name: master\n  replicas: 3\nmetadata:\n  name: ocp4\nnetworking:\n  clusterNetworks:\n  - cidr: 9.248.0.0/14\n    hostPrefix: 24\n  machineNetwork:\n  - cidr: 192.168.252.0/24\n  networkType: OVNKubernetes\n  serviceNetwork:\n  - 9.31.0.0/16\nplatform:\n  none: {}\nfips: false\npullSecret: '{\"auths\": {\"bastion.ocp4.platformengineers.xyz:8443\": { \"auth\": \"aW5pdDpwYXNzdzByZA==\", \"email\": \"you@example.com\"}}}'\nsshKey: 'ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAICIl3vZvi6eh1SkTE9EWKnZZwG6bVvZZzZzBHuKgbrVW root@bastion.ocp4.platformengineers.xyz'\nadditionalTrustBundle: |\n    -----BEGIN CERTIFICATE-----\n    MIID3jCCAsagAwIBAgIUHj/Ux66Fpoktc/qtzoP6wgWZ2aIwDQYJKoZIhvcNAQEL\n    BQAwdzELMAkGA1UEBhMCVVMxCzAJBgNVBAgMAlZBMREwDwYDVQQHDAhOZXcgWW9y\n    azENMAsGA1UECgwEUXVheTERMA8GA1UECwwIRGl2aXNpb24xJjAkBgNVBAMMHWJh\n    c3Rpb24ub2NwNC5jc20tc3BnaS5hY21lLmVzMB4XDTIzMDkyMTE4NDAzM1oXDTI2\n    MDcxMTE4NDAzM1owdzELMAkGA1UEBhMCVVMxCzAJBgNVBAgMAlZBMREwDwYDVQQH\n    DAhOZXcgWW9yazENMAsGA1UECgwEUXVheTERMA8GA1UECwwIRGl2aXNpb24xJjAk\n    BgNVBAMMHWJhc3Rpb24ub2NwNC5jc20tc3BnaS5hY21lLmVzMIIBIjANBgkqhkiG\n    9w0BAQEFAAOCAQ8AMIIBCgKCAQEAv3eDBDh1GH6i+hptdlbMPYztWDsHCihyjRta\n    WgD3SEyHobuWM/lSepwMC2mmyejZKsv/x4PYFsNrWBwt1oVicP3+sGNuNfKheFgn\n    LAiXAgt35MLPR7oXZKCb82rb5jFUYWU6Ro2xHv8qxLAAv+BRzq5fDYczvsps3pq0\n    rvdNLfCORBmtQnoRuKXTmOreQgZqHZkhb1g8P4jE+BJugSAQT4gy1cnP5HXFqpdC\n    qaehQeQEi+k3yHVdYnMyVC2Ya609SUUeSUxuLrlO2EQTSwLzSZ02sMsIfBKbOgmu\n    TJPGgd5jqrSu1LDem8YKws8u0MVEhkl3LkuUuOYBbt4kPxoTIwIDAQABo2IwYDAL\n    BgNVHQ8EBAMCAuQwEwYDVR0lBAwwCgYIKwYBBQUHAwEwKAYDVR0RBCEwH4IdYmFz\n    dGlvbi5vY3A0LmNzbS1zcGdpLmFjbWUuZXMwEgYDVR0TAQH/BAgwBgEB/wIBATAN\n    BgkqhkiG9w0BAQsFAAOCAQEAEIanwOhmODUlshlmpXUMeAa7g8S/cEUy19tpo932\n    /sHoxVJogMVdxBY2V1EjNUPr/6PImesUq0IA1piYXqdxw23kiTgAM01THn6xq9Bn\n    ef7jKPuD7dVY1V8xMmt9EF1Bh4aTObCotF/6XOnBA6NaxWNq5jJ7dfRVU7Vqh5xa\n    hyGRglgJw1LmrhwXTHLi6XBZXuwVlAw5HZFEsKsqD/6YqhmTMV7DA3Er/g3Qv1iJ\n    NmnM5Ftr1huVtwXuETzm0VHIteSWXBV8/c4+COTlQpwVH2mQRcAZQsW8FibQIjP5\n    k/26D0f5XWdf0u6cLowCmWRWhHrTe0ttUUm5ZcP7xeZNjA==\n    -----END CERTIFICATE-----\nimageContentSources:\n- mirrors:\n  - bastion.ocp4.platformengineers.xyz:8443/openshift/release\n  source: quay.io/openshift-release-dev/ocp-v4.0-art-dev\n- mirrors:\n  - bastion.ocp4.platformengineers.xyz:8443/openshift/release-images\n  source: quay.io/openshift-release-dev/ocp-release\n</code></pre> <p>Important</p> <p>Remember the <code>additionalTrusBundle</code> and <code>imageContentSources</code> sections come from the data output in the steps above.</p> <p>The highlighted content on the <code>pullSecret</code> tag is the base64-encoded username and password obtained on the Create the private image registry in the offline bastion section when we ran the <code>mirror-registry install</code> command.</p> [root@bastion ocp4]<pre><code>echo -n 'init:passw0rd' | base64 -w0\n</code></pre> Output<pre><code>aW5pdDpwYXNzdzByZA==\n</code></pre> <p>Tip</p> <p>If you used the default <code>passw0rd</code> when installing your Quay private image registry, the <code>pullSecret</code> value in the <code>instal-config.yaml</code> above will be exactly the same value you need to use.</p> <p>The content for the <code>sshKey</code> tag is the output of:</p> [root@bastion ocp4]<pre><code>cat /root/.ssh/id_ed25519.pub\n</code></pre> Output<pre><code>ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAICIl3vZvi6eh1SkTE9EWKnZZwG6bVvZZzZzBHuKgbrVW root@bastion.ocp4.platformengineers.xyz\n</code></pre> </li> <li> <p>Create a copy of the <code>install-config.yaml</code> file in another directory, as it is automatically deleted in the following steps:</p> [root@bastion ocp4]<pre><code>cp install-config.yaml ../install-config.yaml\n</code></pre> </li> </ol>"},{"location":"airgapped/14-createocpmanifests/","title":"Create OpenShift Manifests and Ignition files","text":"<ol> <li> <p>Create the OpenShift manifest files</p> [root@bastion ocp4]<pre><code>openshift-install create manifests --dir /root/registry/downloads/tools/ocp4\n</code></pre> Output<pre><code>INFO Consuming Install Config from target directory\nINFO Manifests created in: /root/registry/downloads/tools/ocp4/manifests and /root/registry/downloads/tools/ocp4/openshift\n</code></pre> </li> <li> <p>In order to create the ignition files, install butane in the bastion node:</p> [root@bastion ocp4]<pre><code>cd /root/registry/downloads/tools/\n</code></pre> [root@bastion tools]<pre><code>chmod u+x butane\ncp butane /usr/local/bin/butane\nbutane -V\n</code></pre> Output<pre><code>Butane 0.18.0\n</code></pre> </li> </ol> <p>Create two butane files for the master and worker nodes. In these files, you set the chrony configuration for master and worker nodes.</p> <p>99-worker-chrony-conf-override.bu</p> <ol> <li> <p>Create the <code>99-worker-chrony-conf-override.bu</code> file</p> [root@bastion tools]<pre><code>vi 99-worker-chrony-conf-override.bu\n</code></pre> </li> <li> <p>Make sure it looks like below</p> 99-worker-chrony-conf-override.bu<pre><code>variant: openshift\nversion: 4.16.0\nmetadata:\n  name: 99-worker-chrony-conf-override\n  labels:\n    machineconfiguration.openshift.io/role: worker\nstorage:\n  files:\n    - path: /etc/chrony.conf\n      mode: 0644\n      overwrite: true\n      contents:\n        inline: |\n          # The Machine Config Operator manages this file.\n          server controlplane01.ocp4.platformengineers.xyz iburst\n          server controlplane02.ocp4.platformengineers.xyz iburst\n          server controlplane03.ocp4.platformengineers.xyz iburst\n\n          stratumweight 0\n          driftfile /var/lib/chrony/drift\n          rtcsync\n          makestep 10 3\n          bindcmdaddress 127.0.0.1\n          bindcmdaddress ::1\n          keyfile /etc/chrony.keys\n          commandkey 1\n          generatecommandkey\n          noclientlog\n          logchange 0.5\n          logdir /var/log/chrony\n</code></pre> </li> </ol> <p>99-master-chrony-conf-override.bu</p> <ol> <li> <p>Create the <code>99-master-chrony-conf-override.bu</code> file</p> [root@bastion tools]<pre><code>vi 99-master-chrony-conf-override.bu\n</code></pre> </li> <li> <p>Make sure it looks like below</p> 99-master-chrony-conf-override.bu<pre><code>variant: openshift\nversion: 4.16.0\nmetadata:\n  name: 99-master-chrony-conf-override\n  labels:\n    machineconfiguration.openshift.io/role: master\nstorage:\n  files:\n    - path: /etc/chrony.conf\n      mode: 0644\n      overwrite: true\n      contents:\n        inline: |\n          # Use public servers from the pool.ntp.org project.\n          # Please consider joining the pool (https://www.pool.ntp.org/join.html).\n\n          # The Machine Config Operator manages this file\n          server controlplane01.ocp4.platformengineers.xyz iburst\n          server controlplane02.ocp4.platformengineers.xyz iburst\n          server controlplane03.ocp4.platformengineers.xyz iburst\n\n          stratumweight 0\n          driftfile /var/lib/chrony/drift\n          rtcsync\n          makestep 10 3\n          bindcmdaddress 127.0.0.1\n          bindcmdaddress ::1\n          keyfile /etc/chrony.keys\n          commandkey 1\n          generatecommandkey\n          noclientlog\n          logchange 0.5\n          logdir /var/log/chrony\n\n          # Configure the control plane nodes to serve as local NTP servers\n          # for all worker nodes, even if they are not in sync with an\n          # upstream NTP server.\n\n          # Allow NTP client access from the local network.\n          allow all\n          # Serve time even if not synchronized to a time source.\n          local stratum 3 orphan\n</code></pre> </li> <li> <p>Generate the yaml files with butane:</p> [root@bastion tools]<pre><code>butane 99-worker-chrony-conf-override.bu -o 99-worker-chrony-conf-override.yaml\n</code></pre> [root@bastion tools]<pre><code>butane 99-master-chrony-conf-override.bu -o 99-master-chrony-conf-override.yaml\n</code></pre> </li> <li> <p>Copy them to the installation directory:</p> [root@bastion tools]<pre><code>cp 99-*.yaml /root/registry/downloads/tools/ocp4/manifests/\n</code></pre> </li> <li> <p>Make a copy of the installation directory:</p> [root@bastion tools]<pre><code>cp -r ocp4 ocp4_backup\n</code></pre> </li> <li> <p>Create the ignition files:</p> [root@bastion tools]<pre><code>openshift-install create ignition-configs --dir /root/registry/downloads/tools/ocp4/\n</code></pre> Output<pre><code>INFO Consuming Common Manifests from target directory\nINFO Consuming OpenShift Install (Manifests) from target directory\nINFO Consuming Master Machines from target directory\nINFO Consuming Openshift Manifests from target directory\nINFO Consuming Worker Machines from target directory\nINFO Ignition-Configs created in: /root/registry/downloads/tools/ocp4 and /root/registry/downloads/tools/ocp4/auth\n</code></pre> </li> <li> <p>Provide read and execute permissions for other users and groups to the ignition files:</p> [root@bastion tools]<pre><code>chmod 755 /root/registry/downloads/tools/ocp4/*.ign\n</code></pre> </li> <li> <p>Copy the ignition files to the serving path of the HTTP Server we installed on the Install the http server in the offline bastion section before.</p> [root@bastion tools]<pre><code>cp /root/registry/downloads/tools/ocp4/*.ign /var/www/html/ign/\nll /var/www/html/ign/\n</code></pre> Output<pre><code>total 288\n-rwxr-xr-x 1 root root 284065 Sep 14 05:19 bootstrap.ign\n-rwxr-xr-x 1 root root   1723 Sep 14 05:19 master.ign\n-rwxr-xr-x 1 root root   1723 Sep 14 05:19 worker.ign\n</code></pre> </li> <li> <p>It is recommended to back the directory <code>ocp4/auth</code> up, because it contains the certificates and key of the initial user and administrator of OpenShift: the user kubeadmin.</p> [root@bastion tools]<pre><code>cp -r /root/registry/downloads/tools/ocp4/auth/ /root/registry/downloads/tools/ocp4_auth/\n</code></pre> </li> </ol>"},{"location":"airgapped/15-deployocpcluster/","title":"Deploy OpenShift Cluster","text":"<p>In order to get our OpenShift cluster deployed, we first need to start up the bootstrap node that will operate as a temporary control plane for the Control Plane nodes to join later.</p>"},{"location":"airgapped/15-deployocpcluster/#control-plane-nodes","title":"Control Plane Nodes","text":"<ol> <li> <p>In your vCenter, start up the bootstrap node. Open the web console for this node in the vCenter so that you can interact with the ISO bootup menu. Remember, this is the bootup menu we have baked into the ISO with the ISO maker from the previous section. Select the bootstrap option from the bootup menu:</p> <p></p> <p>Once the specific bootstrap configuration has been loaded on the machine, you should see the following login screen:</p> <p></p> </li> </ol> <p>At this point, that temporary control plane node for the rest of the Contol Plane nodes to join has been created and started so that you can you can start the control machines.</p> <p>Important</p> <p>Remember to select the appropriate bootup option for the machine you will be starting up on the following steps.</p> <ol> <li> <p>Start the controlplane01 machine:</p> <p></p> <p>Make sure it starts up properly:</p> <p></p> </li> <li> <p>Start the controlplane02 machine:</p> <p></p> <p>Make sure it starts up properly:</p> <p></p> </li> <li> <p>Start the controlplane03 machine:</p> <p></p> <p>Make sure it starts up properly:</p> <p></p> </li> <li> <p>Access the offline bastion:</p> [root@bastion ~]<pre><code>ssh root@192.168.252.23\n</code></pre> </li> <li> <p>Go to <code>/root/registry/downloads/</code> to kick off the wait for the bootstrap to complete command:</p> [root@bastion ~]<pre><code>cd /root/registry/downloads/\n</code></pre> </li> <li> <p>Kick off the wait for the bootstrap to complete command:</p> [root@bastion downloads]<pre><code>openshift-install wait-for bootstrap-complete --dir=/root/registry/downloads/tools/ocp4 --log-level=debug\n</code></pre> <p>If the bootstrap of the control plane nodes for your OpenShift cluster has completed correctly, you should see the following ouput:</p> Output<pre><code>DEBUG OpenShift Installer 4.16.9\nDEBUG Built from commit 194f806390b2b20668f5ac32d7918f9773b29a5e\nINFO Waiting up to 20m0s (until 9:53AM) for the Kubernetes API at https://api.ocp4.platformengineers.xyz:6443...\nDEBUG Loading Agent Config...\nINFO API v1.25.11+1485cc9 up\nDEBUG Loading Install Config...\nDEBUG   Loading SSH Key...\nDEBUG   Loading Base Domain...\nDEBUG     Loading Platform...\nDEBUG   Loading Cluster Name...\nDEBUG     Loading Base Domain...\nDEBUG     Loading Platform...\nDEBUG   Loading Networking...\nDEBUG     Loading Platform...\nDEBUG   Loading Pull Secret...\nDEBUG   Loading Platform...\nDEBUG Using Install Config loaded from state file\nINFO Waiting up to 30m0s (until 10:03AM) for bootstrapping to complete...\nDEBUG Bootstrap status: complete\nINFO It is now safe to remove the bootstrap resources\nDEBUG Time elapsed per stage:\nDEBUG Bootstrap Complete: 1m48s\nINFO Time elapsed: 1m48s\n</code></pre> <p>Tip</p> <p>If the previous command fails or takes too long, you can do the following to troubleshoot/debug:</p> [root@bastion ~]<pre><code>ssh core@bootstrap.ocp4.platformengineers.xyz\n</code></pre> <p>Once inside the bootstrap machine, execute the following command to see if there are any errors that ocurred during the installation.</p> [core@bootstrap ~]<pre><code>journalctl -f\n</code></pre> </li> <li> <p>At this point, we can stop the bootstrap machine and delete it from the vCenter as it will no longer be used during installation.</p> </li> <li> <p>Execute the following command to be able to use oc commands as an administrator against the control plane of your OpenShift cluster:</p> [root@bastion downloads]<pre><code>export KUBECONFIG=/root/registry/downloads/tools/ocp4/auth/kubeconfig\n</code></pre> </li> <li> <p>Execute the following command that should output what user are you authenticated as by using the above <code>kubeconfig</code> credentials file.</p> [root@bastion downloads]<pre><code>oc whoami\n</code></pre> Output<pre><code>system:admin\n</code></pre> </li> <li> <p>Get the nodes of your OpenShift cluster</p> [root@bastion downloads]<pre><code>oc get nodes\n</code></pre> Output<pre><code>NAME                             STATUS   ROLES                  AGE     VERSION\ncontrolplane01.ocp4.platformengineers.xyz   Ready    control-plane,master   25m     v1.29.7+4510e9c\ncontrolplane02.ocp4.platformengineers.xyz   Ready    control-plane,master   19m     v1.29.7+4510e9c\ncontrolplane03.ocp4.platformengineers.xyz   Ready    control-plane,master   7m41s   v1.29.7+4510e9c\n</code></pre> <p>Warning</p> <p>Control Plane nodes must appear in Ready state.</p> </li> </ol>"},{"location":"airgapped/15-deployocpcluster/#compute-nodes","title":"Compute Nodes","text":"<p>Once the control plane nodes are ready, we can start all the compute machines, including the infra and storage machines, and to do this, again, make sure you select the appropriate option on the bootup menu:</p> <ol> <li> <p>Start the compute01 machine:</p> <p></p> </li> <li> <p>Start the compute02 machine:</p> <p></p> </li> <li> <p>Start the compute03 machine:</p> <p></p> </li> <li> <p>Start the infra01 machine:</p> <p></p> </li> <li> <p>Start the infra02 machine:</p> <p></p> </li> <li> <p>Start the infra03 machine:</p> <p></p> </li> <li> <p>Start the storage01 machine:</p> <p></p> </li> <li> <p>Start the storage02 machine:</p> <p></p> </li> <li> <p>Start the storage03 machine:</p> <p></p> </li> </ol> <p>Once the compute nodes have successfully started, we should see that the certificate signing request to join the OpenShift cluster should be pending. In order to authorize these workers to join the OpenShift cluster, we need to approve those certificate signing requests.</p> <p>Tip</p> <p>Give around 10mins for the machines to start up so that you can see all the CSR at once. You should have one per machine and you will need to approve a CSR per machine twice.</p> <ol> <li> <p>Get the certificate signing requests (csr):</p> [root@bastion downloads]<pre><code>oc get csr\n</code></pre> Output<pre><code>NAME        AGE   SIGNERNAME                                    REQUESTOR                                                                   REQUESTEDDURATION   CONDITION\ncsr-dncq9   96s   kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   &lt;none&gt;              Pending\ncsr-fxhxg   62s   kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   &lt;none&gt;              Pending\ncsr-kgkgs   6s    kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   &lt;none&gt;              Pending\ncsr-ntmtw   12s   kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   &lt;none&gt;              Pending\ncsr-vzbgt   76s   kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   &lt;none&gt;              Pending\ncsr-zrr6n   41s   kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   &lt;none&gt;              Pending\n</code></pre> </li> <li> <p>Approve any pending certificate signing request:</p> [root@bastion downloads]<pre><code>oc get csr -o go-template='{{range .items}}{{if not .status}}{{.metadata.name}}{{\"\\n\"}}{{end}}{{end}}' | xargs oc adm certificate approve\n</code></pre> Output<pre><code>certificatesigningrequest.certificates.k8s.io/csr-dncq9 approved\ncertificatesigningrequest.certificates.k8s.io/csr-fxhxg approved\ncertificatesigningrequest.certificates.k8s.io/csr-kgkgs approved\ncertificatesigningrequest.certificates.k8s.io/csr-ntmtw approved\ncertificatesigningrequest.certificates.k8s.io/csr-vzbgt approved\ncertificatesigningrequest.certificates.k8s.io/csr-zrr6n approved\n</code></pre> </li> </ol> <p>Tip</p> <p>You may need to get and approve all pending certificate signing request few times as these will appear as machines come up. Also, to get a node to join the OpenShift cluster, two cerficate signing requests must be approved. Therefore, repeat the previous two steps few times until there is no pending certificate signing request.</p> <p>After a reasonable amount of time (15 minutes aprox), confirm that the cluster operators are available and there is none in degraded state if no pending certificate signing request is listed. </p> <ol> <li> <p>Get the not ready cluster operators</p> [root@bastion downloads]<pre><code>oc get co | grep -e \"4.16.9 False\" -e \"False True\" -e \"True          False\" -e \"NAME\"\n</code></pre> Output<pre><code>NAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE\nauthentication                             4.16.9    False       True          False      7h40m   WellKnownAvailable: The well-known endpoint is not yet available: kube-apiserver oauth endpoint https://192.168.252.5:6443/.well-known/oauth-authorization-server is not yet served and authentication operator keeps waiting (check kube-apiserver operator, and check that instances roll out successfully, which can take several minutes per instance)\nkube-apiserver                             4.16.9   True        True          False      7h36m   NodeInstallerProgressing: 2 nodes are at revision 6; 1 nodes are at revision 7\n</code></pre> <p>Warning</p> <p>Repeat the command until there is no cluster operator listed. That will mean all cluster operators are healthy</p> </li> <li> <p>Get all the cluster operators</p> [root@bastion downloads]<pre><code>oc get co\n</code></pre> Output<pre><code>NAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE\nauthentication                             4.16.9    True        False         False      2m22s\nbaremetal                                  4.16.9    True        False         False      47m\ncloud-controller-manager                   4.16.9    True        False         False      50m\ncloud-credential                           4.16.9    True        False         False      55m\ncluster-autoscaler                         4.16.9    True        False         False      47m\nconfig-operator                            4.16.9    True        False         False      48m\nconsole                                    4.16.9    True        False         False      7m59s\ncontrol-plane-machine-set                  4.16.9    True        False         False      48m\ncsi-snapshot-controller                    4.16.9    True        False         False      48m\ndns                                        4.16.9    True        False         False      47m\netcd                                       4.16.9    True        False         False      46m\nimage-registry                             4.16.9    True        False         False      23m\ningress                                    4.16.9    True        False         False      10m\ninsights                                   4.16.9    True        False         False      35m\nkube-apiserver                             4.16.9    True        False         False      34m\nkube-controller-manager                    4.16.9    True        False         False      45m\nkube-scheduler                             4.16.9    True        False         False      45m\nkube-storage-version-migrator              4.16.9    True        False         False      48m\nmachine-api                                4.16.9    True        False         False      47m\nmachine-approver                           4.16.9    True        False         False      47m\nmachine-config                             4.16.9    True        False         False      47m\nmarketplace                                4.16.9    True        False         False      47m\nmonitoring                                 4.16.9    True        False         False      9m37s\nnetwork                                    4.16.9    True        False         False      40m\nnode-tuning                                4.16.9    True        False         False      13m\nopenshift-apiserver                        4.16.9    True        False         False      30m\nopenshift-controller-manager               4.16.9    True        False         False      40m\nopenshift-samples                          4.16.9    True        False         False      25m\noperator-lifecycle-manager                 4.16.9    True        False         False      47m\noperator-lifecycle-manager-catalog         4.16.9    True        False         False      47m\noperator-lifecycle-manager-packageserver   4.16.9    True        False         False      33m\nservice-ca                                 4.16.9    True        False         False      48m\nstorage                                    4.16.9    True        False         False      48m\n</code></pre> </li> <li> <p>Execute the wait for install to complete command</p> [root@bastion downloads]<pre><code>cd /root/registry/downloads/tools/ocp4\n</code></pre> [root@bastion ocp4]<pre><code>openshift-install wait-for install-complete\n</code></pre> Output<pre><code>INFO Waiting up to 40m0s (until 4:28PM) for the cluster at https://api.ocp4.platformengineers.xyz:6443 to initialize...\nINFO Checking to see if there is a route at openshift-console/console...\nINFO Install complete!\nINFO To access the cluster as the system:admin user when using 'oc', run 'export KUBECONFIG=/root/registry/downloads/tools/ocp4/auth/kubeconfig'\nINFO Access the OpenShift web-console here: https://console-openshift-console.apps.ocp4.platformengineers.xyz\nINFO Login to the console with user: \"kubeadmin\", and password: \"o8o6u-zgz8h-CobwL-pWb9i\"\nINFO Time elapsed: 0s\n</code></pre> <p>Tip</p> <p>Alternatively, the same credentials can be found at <code>/root/registry/downloads/tools/ocp4/auth</code></p> </li> </ol> <p>Let's now open up our OpenShift cluster web console in our laptop's web browser.</p> <ol> <li> <p>Add the following urls to your laptop hosts file with the IP address of the load balancer so that you can access them.</p> [student laptop]<pre><code>vi /etc/hosts\n</code></pre> <p>Add the following entry to your hosts file:</p> /etc/hosts<pre><code>192.168.252.24 console-openshift-console.apps.ocp4.platformengineers.xyz oauth-openshift.apps.ocp4.platformengineers.xyz\n</code></pre> </li> <li> <p>Now, you can connect via a web browser to the OpenShift web console using the credentials and url from the OpenShift wait for install to complete command:</p> <p>https://console-openshift-console.apps.ocp4.platformengineers.xyz</p> <p></p> </li> </ol>"},{"location":"airgapped/16-configinfranodes/","title":"Configure Infrastructure Nodes","text":"<p>This operation is performed to ready the infrastructure nodes for OpenShift infrastructure components. The infrastructure nodes do not consume OpenShift licenses.</p> <ol> <li> <p>Create the labels for the infra nodes</p> [root@bastion ocp4]<pre><code>oc label node infra01.ocp4.platformengineers.xyz node-role.kubernetes.io/infra=\n</code></pre> Output<pre><code>node/infra01.ocp4.platformengineers.xyz labeled\n</code></pre> [root@bastion ocp4]<pre><code>oc label node infra02.ocp4.platformengineers.xyz node-role.kubernetes.io/infra=\n</code></pre> Output<pre><code>node/infra02.ocp4.platformengineers.xyz labeled\n</code></pre> [root@bastion ocp4]<pre><code>oc label node infra03.ocp4.platformengineers.xyz node-role.kubernetes.io/infra=\n</code></pre> Output<pre><code>node/infra03.ocp4.platformengineers.xyz labeled\n</code></pre> </li> <li> <p>Taint the infra nodes to prevent them from receiving other workloads</p> [root@bastion ocp4]<pre><code>oc adm taint nodes -l node-role.kubernetes.io/infra infra=reserved:NoSchedule infra=reserved:NoExecute\n</code></pre> Output<pre><code>node/infra01.ocp4.platformengineers.xyz tainted\nnode/infra02.ocp4.platformengineers.xyz tainted\nnode/infra03.ocp4.platformengineers.xyz tainted\n</code></pre> </li> </ol>"},{"location":"airgapped/17-day2ops/","title":"Day 2 Operations","text":""},{"location":"airgapped/17-day2ops/#move-ingress-routing-and-internal-registry-loads-to-infrastructure-nodes","title":"Move ingress routing and internal registry loads to infrastructure nodes","text":"<ol> <li> <p>Move the OpenShift cluster ingress controller to the infra nodes.</p> [root@bastion ocp4]<pre><code>oc edit ingresscontroller default -n openshift-ingress-operator\n</code></pre> </li> <li> <p>Copy the highlighted lines into your default Ingress Controller specification.</p> deault<pre><code># Please edit the object below. Lines beginning with a '#' will be ignored,\n# and an empty file will abort the edit. If an error occurs while saving this file will be\n# reopened with the relevant failures.\n#\napiVersion: operator.openshift.io/v1\nkind: IngressController\nmetadata:\ncreationTimestamp: \"2023-09-14T13:30:51Z\"\nfinalizers:\n- ingresscontroller.operator.openshift.io/finalizer-ingresscontroller\ngeneration: 2\nname: default\nnamespace: openshift-ingress-operator\nresourceVersion: \"2658200\"\nuid: 40d5648f-98c3-46ac-9223-773e52b65f6a\nspec:\nclientTLS:\n    clientCA:\n    name: \"\"\n    clientCertificatePolicy: \"\"\nhttpCompression: {}\nhttpEmptyRequestsPolicy: Respond\nhttpErrorCodePages:\n    name: \"\"\nnodePlacement:\n  nodeSelector:\n    matchLabels:\n      node-role.kubernetes.io/infra: \"\"\n  tolerations:\n  - effect: NoSchedule\n    key: infra\n    value: reserved\n  - effect: NoExecute\n    key: infra\n    value: reserved\nreplicas: 2\ntuningOptions:\n    reloadInterval: 0s\n[...]\n</code></pre> </li> <li> <p>Move the internal OpenShift image registry to the infra nodes</p> [root@bastion ocp4]<pre><code>oc patch configs.imageregistry.operator.openshift.io/cluster --type=merge -p '{\"spec\":{\"nodeSelector\": {\"node-role.kubernetes.io/infra\": \"\"},\"tolerations\": [{\"effect\":\"NoSchedule\",\"key\": \"infra\",\"value\": \"reserved\"},{\"effect\":\"NoExecute\",\"key\": \"infra\",\"value\": \"reserved\"}]}}'\n</code></pre> Output<pre><code>config.imageregistry.operator.openshift.io/cluster patched\n</code></pre> </li> </ol>"},{"location":"airgapped/17-day2ops/#create-the-catalog-sources-to-access-the-redhat-marketplace-operators","title":"Create the catalog sources to access the RedHat Marketplace operators","text":"<p>The OpenShift cluster does not have internet access. Therefore the operator catalog for available operators has to be configured to point to the image mirrored to the Quay private image registry installed on the offline bastion.</p> <p>To do this, we will use the <code>catalogSource-redhat-operator-index.yaml</code> file previously created during installation and available in <code>/root/oc-mirror-workspace/results-XXXXXX</code>.</p> <p>Before applying the catalog source, the default catalogs that have been installed with Openshift cluster need to be deactivated:</p> [root@bastion ~]<pre><code>oc patch OperatorHub cluster --type json -p '[{\"op\": \"add\", \"path\": \"/spec/disableAllDefaultSources\", \"value\": true}]'\n</code></pre> Output<pre><code>operatorhub.config.openshift.io/cluster patched\n</code></pre> <p>We should see no operator available in the OperatorHub of our OpenShift cluster:</p> <p></p> <p>Now, to create that custom catalog source that points to our Quay private image registry, execute the following commands:</p> [root@bastion ~]<pre><code>cd /root/oc-mirror-workspace/results-XXXXX/\n</code></pre> [root@bastion results-XXXXX]<pre><code>oc apply -f catalogSource-cs-redhat-operator-index.yaml\n</code></pre> Output<pre><code>catalogsource.operators.coreos.com/redhat-operator-index created\n</code></pre> <p>Check now if the operators are listed:</p> <p></p>"},{"location":"airgapped/17-day2ops/#deactivate-the-cluster-update-channel","title":"Deactivate the cluster update channel","text":"<p>Openshift defines an update channel to search for and install updates. Since this cluster does not have any internet access, it is recommended to turn off the update channel.</p> <p>Run the following command to modify the update channel and set it to null, leaving the cluster without an update channel.</p> [root@bastion transfer-files]<pre><code>oc adm upgrade channel --allow-explicit-channel\n</code></pre> Output<pre><code>warning: Clearing channel \"stable-4.16\"; cluster will no longer request available update recommendations.\n</code></pre>"},{"location":"airgapped/18-configstoragenodes/","title":"Air-gapped OpenShift Installation","text":""},{"location":"airgapped/18-configstoragenodes/#deploy-openshift-data-foundation-on-storage-nodes","title":"Deploy OpenShift Data Foundation on Storage nodes","text":"<p>This operation is performed to provide OpenShift with Block, Object and File persistent storage. Storage nodes do not consume OpenShift Licences.</p> <ol> <li> <p>Create the labels for the Storage nodes</p> [root@bastion ocp4]<pre><code>oc label node storage01.ocp4.platformengineers.xyz node-role.kubernetes.io/infra=\noc label node storage01.ocp4.platformengineers.xyz cluster.ocs.openshift.io/openshift-storage= \n</code></pre> Output<pre><code>node/storage01.ocp4.platformengineers.xyz labeled\n</code></pre> [root@bastion ocp4]<pre><code>oc label node storage02.ocp4.platformengineers.xyz node-role.kubernetes.io/infra=\noc label node storage02.ocp4.platformengineers.xyz cluster.ocs.openshift.io/openshift-storage= \n</code></pre> Output<pre><code>node/storage02.ocp4.platformengineers.xyz labeled\n</code></pre> [root@bastion ocp4]<pre><code>oc label node storage03.ocp4.platformengineers.xyz node-role.kubernetes.io/infra=\noc label node storage03.ocp4.platformengineers.xyz cluster.ocs.openshift.io/openshift-storage= \n</code></pre> Output<pre><code>node/storage03.ocp4.platformengineers.xyz labeled\n</code></pre> </li> <li> <p>Taint the storage nodes to prevent them from receiving other workloads</p> [root@bastion ocp4]<pre><code>oc adm taint nodes storage01.ocp4.platformengineers.xyz node.ocs.openshift.io/storage=true:NoSchedule\n</code></pre> Output<pre><code>node/storage01.ocp4.platformengineers.xyz tainted\n</code></pre> [root@bastion ocp4]<pre><code>oc adm taint nodes storage02.ocp4.platformengineers.xyz node.ocs.openshift.io/storage=true:NoSchedule\n</code></pre> Output<pre><code>node/storage02.ocp4.platformengineers.xyz tainted\n</code></pre> [root@bastion ocp4]<pre><code>oc adm taint nodes storage03.ocp4.platformengineers.xyz node.ocs.openshift.io/storage=true:NoSchedule\n</code></pre> Output<pre><code>node/storage03.ocp4.platformengineers.xyz tainted\n</code></pre> </li> </ol>"},{"location":"airgapped/18-configstoragenodes/#install-the-local-storage-operator","title":"Install the Local Storage Operator","text":"<p>Install the Local Storage Operator from the Operator Hub before creating Red Hat OpenShift Data Foundation clusters on local storage devices.</p> <ol> <li>Log in to the OpenShift Web Console.</li> <li>Click Operators &gt; OperatorHub.</li> <li>Type local storage in the Filter by keyword\u200b box to find the Local Storage Operator from the list of operators, and click on it.</li> <li> <p>Set the following options on the Install Operator page:</p> <ul> <li>Update channel as stable.</li> <li>Installation mode as A specific namespace on the cluster.</li> <li>Installed Namespace as Operator recommended namespace openshift-local-storage.</li> <li>Update approval as Automatic.</li> </ul> </li> <li> <p>Click Install.</p> </li> </ol>"},{"location":"airgapped/18-configstoragenodes/#install-openshift-data-foundation","title":"Install OpenShift Data Foundation","text":"<p>The OpenShift web console provides an easy way to install the ODF operator and create a storage cluster.</p> <ol> <li> <p>Open the OpenShift web console and log in as user <code>admin</code>.</p> </li> <li> <p>Click Operators &gt; OperatorHub.</p> </li> <li> <p>Scroll or type <code>odf</code> into the Filter by keyword box to find the OpenShift Data Foundation Operator.</p> </li> <li> <p>Select the OpenShift Data Foundation operator, click Install, leave the values at their default, and click Install.</p> </li> <li> <p>After the operator installed a pop-up appears stating a new version of the web console is available, ensure you click Refresh web console.</p> </li> <li> <p>Click Create StorageSystem.</p> </li> <li> <p>In the Backing storage page, Check Use Ceph RBD as the default StorageClass and leave the remaining values at their default and click Next.</p> </li> <li> <p>Set the Requested capacity to 0.5 TiB, notice that the infrastructure nodes have been preselected, and click Next.</p> </li> <li> <p>In the next two screens leave the values at their default and click Next.</p> </li> <li> <p>Review the settings and click Create StorageSystem.</p> </li> <li> <p>Detailed progress monitoring can be done on the command line.</p> <pre><code>oc login -u admin\n</code></pre> <pre><code>watch oc -n openshift-storage get storagecluster,pods\n</code></pre> Wait until the StorageCluster reaches phase Ready<pre><code>NAME                                                 AGE     PHASE   EXTERNAL   CREATED AT             VERSION\nstoragecluster.ocs.openshift.io/ocs-storagecluster   6m37s   Ready              2024-11-18T12:42:21Z   4.17.3\n\nNAME                                                                  READY   STATUS      RESTARTS   AGE\npod/csi-addons-controller-manager-79589c64b9-7nrnx                    2/2     Running     0          7m22s\npod/csi-cephfsplugin-25vdt                                            2/2     Running     0          6m36s\npod/csi-cephfsplugin-599v5                                            2/2     Running     0          6m36s\npod/csi-cephfsplugin-crkmb                                            2/2     Running     0          6m36s\npod/csi-cephfsplugin-gzsfw                                            2/2     Running     0          6m36s\npod/csi-cephfsplugin-provisioner-5d549d97d5-5z8ct                     6/6     Running     0          6m36s\npod/csi-cephfsplugin-provisioner-5d549d97d5-qgmkd                     6/6     Running     0          6m36s\npod/csi-cephfsplugin-rbm79                                            2/2     Running     0          6m36s\npod/csi-rbdplugin-9htkb                                               3/3     Running     0          6m36s\npod/csi-rbdplugin-b5g7l                                               3/3     Running     0          6m36s\npod/csi-rbdplugin-j52sh                                               3/3     Running     0          6m36s\npod/csi-rbdplugin-provisioner-5869c4d666-2mgnq                        6/6     Running     0          6m36s\npod/csi-rbdplugin-provisioner-5869c4d666-dz85n                        6/6     Running     0          6m36s\npod/csi-rbdplugin-zcq6q                                               3/3     Running     0          6m36s\npod/csi-rbdplugin-zwcdd                                               3/3     Running     0          6m36s\npod/noobaa-core-0                                                     1/1     Running     0          2m4s\npod/noobaa-db-pg-0                                                    1/1     Running     0          3m40s\npod/noobaa-endpoint-7684858565-8h2d4                                  1/1     Running     0          2m35s\npod/noobaa-operator-785bf6967-7jdb8                                   1/1     Running     0          8m5s\npod/ocs-metrics-exporter-6db44f5fc-88lwn                              1/1     Running     0          3m39s\npod/ocs-operator-bb969496f-5t9c7                                      1/1     Running     0          7m33s\npod/odf-console-6d7f89bfcd-cvg2l                                      1/1     Running     0          7m58s\npod/odf-operator-controller-manager-6ccd96449c-m94mc                  2/2     Running     0          7m58s\npod/rook-ceph-crashcollector-ocpinstall-ntfsr-infra-0-5bfqj-75vd7ws   1/1     Running     0          5m4s\npod/rook-ceph-crashcollector-ocpinstall-ntfsr-infra-0-cscfp-58k872l   1/1     Running     0          4m49s\npod/rook-ceph-crashcollector-ocpinstall-ntfsr-infra-0-p42ml-64tlpnb   1/1     Running     0          5m3s\npod/rook-ceph-exporter-ocpinstall-ntfsr-infra-0-5bfqj-75fc99752b872   1/1     Running     0          5m4s\npod/rook-ceph-exporter-ocpinstall-ntfsr-infra-0-cscfp-74c7b944fdvqb   1/1     Running     0          4m49s\npod/rook-ceph-exporter-ocpinstall-ntfsr-infra-0-p42ml-56c9847b27pw8   1/1     Running     0          5m3s\npod/rook-ceph-mds-ocs-storagecluster-cephfilesystem-a-698ff486cf478   2/2     Running     0          4m\npod/rook-ceph-mds-ocs-storagecluster-cephfilesystem-b-6dfc6bf4pprfw   2/2     Running     0          3m57s\npod/rook-ceph-mgr-a-76cfdff9c5-xf62q                                  3/3     Running     0          5m1s\npod/rook-ceph-mgr-b-6844cbb666-khcpw                                  3/3     Running     0          5m\npod/rook-ceph-mon-a-7dbf8d8f47-thx42                                  2/2     Running     0          6m6s\npod/rook-ceph-mon-b-7bfb497f67-j5rsn                                  2/2     Running     0          5m34s\npod/rook-ceph-mon-c-779c7c4697-b2lx7                                  2/2     Running     0          5m19s\npod/rook-ceph-operator-58d57dfcc9-h7wc6                               1/1     Running     0          6m35s\npod/rook-ceph-osd-0-85f667c594-q6ghs                                  2/2     Running     0          4m23s\npod/rook-ceph-osd-1-6dbb47996d-nqmbk                                  2/2     Running     0          4m21s\npod/rook-ceph-osd-2-685cd49949-v9r6r                                  2/2     Running     0          4m18s\npod/rook-ceph-osd-prepare-09c4822102c3025ef86f32f0690ab78c-p9bw7      0/1     Completed   0          4m38s\npod/rook-ceph-osd-prepare-0b70eb979aae8c4e011459b9b6e8e76a-vpw8q      0/1     Completed   0          4m39s\npod/rook-ceph-osd-prepare-40250bca7f0ab640af4d3aadd9aafcac-jz7ss      0/1     Completed   0          4m39s\npod/rook-ceph-rgw-ocs-storagecluster-cephobjectstore-a-7976995qrs8h   2/2     Running     0          3m43s\npod/ux-backend-server-84b877b699-mr9wg                                2/2     Running     0          7m32s\n</code></pre> </li> <li> <p>Verify that the OpenShift Data Foundation storage classes are available.</p> <pre><code>oc get sc\n</code></pre> Example Output<pre><code>NAME                          PROVISIONER                             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE\nocs-storagecluster-ceph-rbd   openshift-storage.rbd.csi.ceph.com      Delete          Immediate              true                   106s\nocs-storagecluster-ceph-rgw   openshift-storage.ceph.rook.io/bucket   Delete          Immediate              false                  4m55s\nocs-storagecluster-cephfs     openshift-storage.cephfs.csi.ceph.com   Delete          Immediate              true                   106s\nopenshift-storage.noobaa.io   openshift-storage.noobaa.io/obc         Delete          Immediate              false                  48s\nthin-csi (default)            csi.vsphere.vmware.com                  Delete          WaitForFirstConsumer   true                   43m\n</code></pre> </li> <li> <p>Smoke test persistent volume provisioning.</p> <p>Create a PersistentVolumeClaim (PVC) for ODF block storage.</p> <pre><code>oc create -f - &lt;&lt;EOF\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: test\n  namespace: default\nspec:\n  accessModes:\n    - ReadWriteOnce\n  storageClassName: ocs-storagecluster-ceph-rbd\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 1Gi\nEOF\n</code></pre> <p>Check if the PVC is bound to a PersistentVolume.</p> <pre><code>oc get pvc test\n</code></pre> Example Output<pre><code>NAME   STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS                  AGE\ntest   Bound    pvc-419958c8-1126-4601-ba3f-4f2c14bfb88c   1Gi        RWO            ocs-storagecluster-ceph-rbd   6s\n</code></pre> <p>Delete the test PVC.</p> <pre><code>oc delete pvc test\n</code></pre> </li> </ol>"},{"location":"airgapped/2-infraservices/","title":"Infrastructure Services","text":"<p>In the 'offline' bastion node, the HTTP server, load balancer, image registry, DNS, and OCP deployment utilities will all be configured.</p> <p>Given that the network lacks a DHCP server and maintains static IP addresses for all machines, we are required to manually assign each machine's IP, DNS, and hostname details upon creation.</p>"},{"location":"airgapped/3-networkconfig/","title":"Network Configuration","text":"<p>The following table contains the network configuration where the cluster will be installed:</p> Configuration Option Subnet name ocp4.platformengineers.xyz IP address space 192.168.252.0/24 <p>The following table contains the OCP network configuration:</p> Option Network: External IP address space 192.168.252.0/24 Assigned IP address range 192.168.252.1-192.168.252.254 Number of available IP addresses 254 Network: Cluster IP address space 9.248.0.0/14 IP address range 9.248.0.1-9.248.255.254 Subnet prefix 24 Number of available IP addresses 262.142 Number of IP addresses per node 254 Maximum number of nodes 1024 Network: Service IP address space 10.248.0.0/16 IP address range 10.248.0.1-10.248.255.254 Number of available IP addresses 65.534"},{"location":"airgapped/4-dnsserverentries/","title":"Entries in DNS server","text":"<p>These are the DNS entries for the DNS servers:</p> Type FQDN hostname IP Address Bastion bastion.ocp4.platformengineers.xyz 192.168.252.23 bastiononline.ocp4.platformengineers.xyz 192.168.252.22 Bootstrap bootstrap.ocp4.platformengineers.xyz 192.168.252.3 Control Plane controlplane01.ocp4.platformengineers.xyz 192.168.252.4 controlplane02.ocp4.platformengineers.xyz 192.168.252.5 controlplane03.ocp4.platformengineers.xyz 192.168.252.6 Infra infra01.ocp4.platformengineers.xyz 192.168.252.7 infra02.ocp4.platformengineers.xyz 192.168.252.8 infra03.ocp4.platformengineers.xyz 192.168.252.9 Compute compute01.ocp4.platformengineers.xyz 192.168.252.10 compute02.ocp4.platformengineers.xyz 192.168.252.11 compute03.ocp4.platformengineers.xyz 192.168.252.12 Storage storage01.ocp4.platformengineers.xyz 192.168.252.13 storage02.ocp4.platformengineers.xyz 192.168.252.14 storage03.ocp4.platformengineers.xyz 192.168.252.15 <p>And these are the service names:</p> Name FQDN hostname Type IP VIP API api.ocp4.platformengineers.xyz A/AAA or CNAME 192.168.252.24 VIP API-INT api-int.ocp4.platformengineers.xyz A/AAA or CNAME 192.168.252.24 VIP Ingress *.apps.ocp4.platformengineers.xyz A/AAA or CNAME 192.168.252.25"},{"location":"airgapped/5-lbconfig/","title":"Load Balancer Configuration","text":"<p>We will configure the HAProxy load balancer within the \"offline\" Bastion node, as follows:</p> Front End Targets Port *.apps.ocp4.platformengineers.xyz infra01.ocp4.platformengineers.xyz 80 infra02.ocp4.platformengineers.xyz infra03.ocp4.platformengineers.xyz compute01.ocp4.platformengineers.xyz compute02.ocp4.platformengineers.xyz compute03.ocp4.platformengineers.xyz storage01.ocp4.platformengineers.xyz storage02.ocp4.platformengineers.xyz storage03.ocp4.platformengineers.xyz *.apps.ocp4.platformengineers.xyz infra01.ocp4.platformengineers.xyz 443 infra02.ocp4.platformengineers.xyz infra03.ocp4.platformengineers.xyz compute01.ocp4.platformengineers.xyz compute02.ocp4.platformengineers.xyz compute03.ocp4.platformengineers.xyz storage01.ocp4.platformengineers.xyz storage02.ocp4.platformengineers.xyz storage03.ocp4.platformengineers.xyz api.ocp4.platformengineers.xyz bootstrap.ocp4.platformengineers.xyz 6443 controlplane01.ocp4.platformengineers.xyz controlplane02.ocp4.platformengineers.xyz controlplane03.ocp4.platformengineers.xyz api-int.ocp4.platformengineers.xyz bootstrap.ocp4.platformengineers.xyz 22623 controlplane01.ocp4.platformengineers.xyz controlplane02.ocp4.platformengineers.xyz controlplane03.ocp4.platformengineers.xyz"},{"location":"airgapped/6-vpnconfig/","title":"VPN Configuration","text":"<p>In order to connect via ssh to the machines created during the OpenShift installation, which are placed in the <code>192.168.252.0/24</code> private network, and to the VMware vSphere vCenter we need to complete the following steps:</p> <ol> <li> <p>Download and install the VPN manager Wireguard (available in the App Store).</p> <p></p> </li> <li> <p>Download the VPN config file from the IBM Tech Zone using the following button:</p> <p></p> </li> <li> <p>Open Wireguard and import the downloaded file as a new VPN tunel.</p> </li> <li> <p>Active the VPN.</p> <p></p> </li> </ol>"},{"location":"airgapped/7-guacamolevm/","title":"Guacamole VM","text":"<p>Once we have configured and activated the VPN, we need to navigate to our VMware vSphere vCenter home page using the URL at the bottom of our IBM Tech Zone reservation. VMware vSphere vCenter login credentials can also be found on the same reservation.</p> <p></p> <p>Once we have successfully logged into our VMware vSphere vCenter, we should see the following screen:</p> <p></p> <p>In our VMware vSphere vCenter environment, there exists a unique Virtual Machine (VM), referred to as the Guacamole VM. This term does not pertain to OpenShift bastion nodes but rather denotes a specific VM managed by the IBM TechZone infrastructure team.</p> <p>The purpose of this Guacamole VM is to facilitate tasks requiring substantial internet bandwidth within the VMware vSphere vCenter network environment. By doing so, it avoids potential lag or slowdowns associated with VPN tunnel connections. Essentially, placing this VM on the vCenter network allows for rapid access to heavy components necessary for OpenShift installations directly from within the internal network.</p> <p>This setup significantly enhances efficiency when downloading large files or elements crucial for the OpenShift installation process at the VMware vSphere vCenter level. It ensures smoother and quicker data transfers compared to relying on VPN tunnel connections, which could otherwise lead to slower speeds and potential connectivity issues.</p> <p></p> <p>In order to create the bastion machines, we first must upload a RHEL 8.7 OS ISO file to the vCenter datastore. As previously explained, for this heavy task (~11GB), we will utilize the Guacamole VM.</p> <p>Important</p> <p>Due to potential issues, accessing the Guacamole VM through these three steps can be problematic. Persistence and multiple attempts are often necessary to successfully gain entry.</p> <ol> <li> <p>Open the Guacamole VM from your IBM TechZone reservation page:</p> <p></p> </li> <li> <p>This will open the following web app. Expand the All Connections section to see the two options for accessing the Guacamole VM. Use the Remote Desktop option:</p> <p></p> </li> <li> <p>Once you have gained access to the Guacamole VM, you should see a RHEL regular desktop.</p> <p></p> </li> <li> <p>Click on Activities at the top left corner of the desktop and click on Firefox to open the web browser.</p> </li> <li> <p>Download the full install image ISO from Red Hat portal (You will need a Red Hat Account to access):</p> <ul> <li>Link to RH 8.7 SO ISO file</li> </ul> </li> <li> <p>Point the browser to the link in order to download the files.</p> </li> <li> <p>Once we have downloaded the ISO file, open the vCenter using the bookmark located at the bookmarks toolbar. You can find the credentials for the vCenter in your IBM TechZone reservation.</p> <p></p> </li> <li> <p>Upload the downloaded ISO file to the vCenter datastore to make it available for the bastion VMs we will create in later sections.</p> <p></p> <p>Info</p> <p>Name the folder created in step 3 as ISOs folder</p> </li> <li> <p>Once the ISO image have been fully uploaded to the DataStore, you will be able to see it listed.</p> <p></p> </li> </ol> <p>Tip</p> <p>We can continue to work with vCenter from our laptop's web browser. We should not need to work with the Guacamole VM anymore.</p>"},{"location":"airgapped/8-bastionnodes/","title":"Bastion Nodes","text":"<p>For this OpenShift installation, we'll employ two bastion nodes: an online bastion with internet access for downloading necessary components (images, executables, installation programs), and an offline bastion within our internal network without direct internet connectivity for transferring these components and executing the installation.</p> <p>First on our agenda is to establish both online and offline bastion nodes.</p>"},{"location":"airgapped/8-bastionnodes/#create-the-online-bastion-node","title":"Create the online bastion node","text":"<p>Now, we shall proceed to create the virtual machine designated as the online bastion, which will operate using RHEL 8.7 and feature internet connectivity.</p> Option Configuration FQDN hostname bastiononline.ocp4.platformengineers.xyz Static IP Address 192.168.252.22 Operating System Red Hat Enterprise Linux OS Version 8.7 OS Language English-US <ol> <li> <p>Click on the VMs tab. Then, right click on the folder assigned to our vCenter user and select \"New Virtual Machine\".</p> <p></p> </li> <li> <p>Click next on the first page to go to the page where we will assign a name to the bastion online virtual machine. In our case we will use <code>bastiononline.ocp4.platformengineers.xyz</code> for the name of the VM we are creating.</p> </li> <li> <p>Skip step 3.</p> </li> <li> <p>On the step 4, select datastore started with <code>gym-</code> and click next:</p> <p></p> </li> <li> <p>Skip the step 5.</p> </li> <li> <p>On the step 6, select the \"Guest OS Family\" as <code>Linux</code> and \"Guest OS Version\" as <code>Red Hat Enterprise Linux 8 (64-bit)</code> and click next.</p> <p></p> </li> <li> <p>On the step 7, specify the details of the virtual machine according to the table below</p> Type vCPU RAM Memory Storage (system) Storage (data) Bastion 4 vCPU 16 GB 700 GB 0 GB <p></p> <p>On \"New CD/DVD Drive\" select <code>Datastore ISO File</code> and then navigate to the ISO folder where we uploaded the RH 8.7 OS ISO file in the previous section and select such ISO file.</p> <p></p> <p>Make sure \"New CD/DVD Drive\" connect is checked and click next.</p> <p></p> </li> <li> <p>Finish the creation of the VM.</p> </li> </ol> <p>Tip</p> <p>We might need to click on the refresh arrow icon at the top center of the vCenter web application in order to see our newly created VM.</p> <p>Once the VM has been created successfully we must start the machine and proceed with the OS installation.</p> <ol> <li> <p>Select our newly created VM and click on the play button to start the VM.</p> <p></p> </li> <li> <p>Click on launch web console to proceed with the OS installation.</p> <p></p> </li> <li> <p>Click on the screen that will pop up and select install Red Hat OS.</p> <p></p> </li> <li> <p>Click Continue</p> <p></p> </li> <li> <p>Add disk.</p> <p></p> </li> <li> <p>Select <code>Custom</code> for the \"Storage Configuration\" and click done.</p> <p></p> </li> <li> <p>Click on the <code>Click here to create them automatically</code> link so that a default mount point and partition are created.</p> <p></p> </li> <li> <p>Set the amount of disk for the <code>/home</code> and <code>/</code> mount points as described in the picture below. This will allow us to have the required space for the things to be installed at the root mount point which is where the <code>root</code> user will inherit from.</p> <p></p> <p>Tip</p> <p>When we set the disk space for a mount point in the text box, we need to click on the <code>Update Settings</code> button that will get enabled to take effect.</p> </li> <li> <p>Click Done and Accept Changes on the message with the changes to be applied that will pop up.</p> <p></p> </li> <li> <p>On the main menu, click now on the Software Selection option at the bottom of the center options colunm. Then, select the \"Minimal install\", and \"Headless Management\" and click done.</p> <p></p> </li> <li> <p>Create the root credentials, for example <code>passw0rd</code> and click done.</p> </li> <li> <p>Click Begin Installation.</p> </li> <li> <p>When the installation finishes, click on Reboot System.</p> </li> </ol> <p>After the OS is installed and we restarted the server, login with root credentials (<code>root/passw0rd</code>). We will be able to connect using the web console link which will open a new tab in our browser. </p> <ol> <li> <p>Use Network Manager Tool UI (nmtui) for configuring the network properties.</p> [root@localhost ~]#<pre><code>nmtui\n</code></pre> </li> <li> <p>Click on Edit a connection</p> <p></p> </li> <li> <p>Select \"ens192\".</p> </li> <li>Use the arrows and intro to change the IPv4 CONFIGURATION from <code>&lt;Automatic&gt;</code> to <code>&lt;Manual&gt;</code></li> <li>Then, use the arrows and intro to <code>&lt;Show&gt;</code> details.</li> <li> <p>Fill the properties as shown in the pictures below</p> <p></p> </li> <li> <p>Use the arrows and space bar to select the <code>Automatically connect</code> option at the bottom.</p> <p></p> </li> <li> <p>Click <code>&lt;OK&gt;</code></p> </li> <li>Click <code>&lt;Back&gt;</code></li> <li>Click <code>&lt;OK&gt;</code></li> </ol>"},{"location":"airgapped/8-bastionnodes/#create-the-offline-bastion-node","title":"Create the offline bastion node","text":"<p>Follow the same steps from the previous section in order to create the offline bastion with the following specs:</p> Option Configuration FQDN hostname bastion.ocp4.platformengineers.xyz Static IP Addresses 192.168.252.23, 192.168.252.24, 192.168.252.25 Operating System Red Hat Enterprise Linux OS Version 8.7 OS Language English-US <p>We also need to edit the network configuration but this time we need to configure it so that the offline bastion does not have internet access but we can still connect to it via ssh. </p> <ol> <li> <p>We will use Network Manager Tool UI (nmtui) for configuring the network properties.</p> [root@bastion ~]<pre><code>nmtui\n</code></pre> </li> <li> <p>Click on Edit a connection</p> <p></p> </li> <li> <p>Select \"ens192\".</p> </li> <li>Use the arrows and intro to change the IPv4 CONFIGURATION from <code>&lt;Automatic&gt;</code> to <code>&lt;Manual&gt;</code></li> <li>Then, use the arrows and intro to <code>&lt;Show&gt;</code> details.</li> <li> <p>Fill the properties as shown in the pictures below</p> <p></p> </li> <li> <p>Click '' under the addresses field to add the additional IP addresses listed in the table above.</p> </li> <li> <p>Configure the routing so that we simulate no internet connection:</p> <p></p> </li> <li> <p>Use the arrows and space bar to select the <code>Automatically connect</code> option at the bottom.</p> <p></p> </li> <li> <p>Click <code>&lt;OK&gt;</code></p> </li> <li>Click <code>&lt;OK&gt;</code></li> <li>Click <code>&lt;Back&gt;</code></li> <li>Click <code>&lt;OK&gt;</code></li> </ol> <p>After doing this configuration, try the following commands from the offline bastion terminal to check the connection to internet:</p> [root@localhost ~]<pre><code>ping 192.168.252.1\n</code></pre> Output<pre><code>PING 192.168.252.1 (192.168.252.1) 56(84) bytes of data.\n64 bytes from 192.168.252.1: icmp_seq=1 ttl=64 time=0.398 ms\n^C\n--- 192.168.252.1 ping statistics ---\n1 packets transmitted, 1 received, 0% packet loss, time 0ms\nrtt min/avg/max/mdev = 0.398/0.398/0.398/0.000 ms\n</code></pre> [root@localhost ~]<pre><code>ping 8.8.8.8\n</code></pre> Output<pre><code>connect: Network is unreachable\n</code></pre> [root@localhost ~]<pre><code>ping www.google.com\n</code></pre> Output<pre><code>ping: www.google.com: Name or service not known\n</code></pre> <p>Important</p> <p>From now on, we can use our own laptop's terminal to ssh into the different VMs and carry on with the OpenShift installation. We should not need to use the web console of the bastions from the vCenter UI.</p> <p>From your laptop's terminal, make sure you can ssh into the online bastion and offline bastion:</p> [student laptop]<pre><code>ssh root@192.168.252.22\n</code></pre> [student laptop]<pre><code>ssh root@192.168.253.22\n</code></pre>"},{"location":"airgapped/9-onlinebastion/","title":"Configure Online Bastion","text":""},{"location":"airgapped/9-onlinebastion/#disable-selinux-in-the-online-bastion","title":"Disable SELINUX in the online bastion","text":"<p>In order to perform the correct configuration of the bastion node, it is recommended that the <code>SELINUX</code> is deactivated first.</p> <p>Note</p> <p>It is not strictly necessary that Linux Security is deactivated in order to install OpenShift. However, it is recommended as it eases the installation. Otherwise, you would need to open several specific ports and add rules to the firewall so that the needed communication is allowed.</p> <ol> <li> <p>Access the online bastion:</p> [student laptop]<pre><code>ssh root@192.168.252.22\n</code></pre> </li> <li> <p>Open the <code>/etc/selinux/config</code> to disable the <code>SELINUX</code></p> [root@localhost ~]<pre><code>vi /etc/selinux/config\n</code></pre> </li> <li> <p>Make sure your file looks like below, where the highlighted line has been modified:</p> /etc/selinux/config<pre><code># This file controls the state of SELinux on the system.\n# SELINUX= can take one of these three values:\n# enforcing - SELinux security policy is enforced.\n# permissive - SELinux prints warnings instead of enforcing.\n# disabled - No SELinux policy is loaded.\nSELINUX=disabled\n# SELINUXTYPE= can take one of these three values:\n# targeted - Targeted processes are protected,\n# minimum - Modification of targeted policy. Only selected processes are protected.\n# mls - Multi Level Security protection. SELINUXTYPE=targeted\n</code></pre> <p>Once the change is made, save and exit the file (<code>esc</code> and <code>:wq</code>) </p> </li> <li> <p>Restart the server for the changes to take effect.</p> [root@localhost ~]<pre><code>init 6\n</code></pre> </li> </ol>"},{"location":"airgapped/9-onlinebastion/#disable-firewall-in-the-online-bastion","title":"Disable firewall in the online bastion","text":"<p>Also, and since it is an internal network, the firewall of the machine is deactivated</p> <ol> <li> <p>Access the online bastion:</p> [student laptop]<pre><code>ssh root@192.168.252.22\n</code></pre> </li> <li> <p>Stop the firewall:</p> [root@localhost ~]<pre><code>systemctl stop firewalld\nsystemctl disable firewalld\n</code></pre> Output<pre><code>Removed /etc/systemd/system/multi-user.target.wants/firewalld.service. \nRemoved /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service.\n</code></pre> </li> </ol>"},{"location":"airgapped/9-onlinebastion/#configure-the-hostname-for-the-online-bastion","title":"Configure the hostname for the online bastion","text":"<p>We need to change the online bastion machine's default hostname as we will be referring to this machine from the DNS, load balancer, etc using a different hostname.</p> <ol> <li> <p>Edit the hostname of the machine</p> [root@localhost ~]<pre><code>vi /etc/hostname\n</code></pre> <p>set the hostname to <code>bastiononline.ocp4.platformengineers.xyz</code></p> </li> <li> <p>Once the change has been made, restart the server for it to take effect.</p> [root@localhost ~]<pre><code>init 6\n</code></pre> </li> <li> <p>Access the online bastion again:</p> [student laptop]<pre><code>ssh root@192.168.252.22\n</code></pre> </li> <li> <p>Check the hostname was successfully modified:</p> [root@bastiononline ~]<pre><code>ping bastiononline.ocp4.platformengineers.xyz\n</code></pre> <p>You should see similar output as below:</p> Output<pre><code>PING bastiononline.ocp4.platformengineers.xyz(bastiononline.ocp4.platformengineers.xyz (fe80::250:56ff:fe8a:3e00%ens192)) 56 data bytes\n64 bytes from bastiononline.ocp4.platformengineers.xyz (fe80::250:56ff:fe8a:3e00%ens192): icmp_seq=1 ttl=64 time=0.036 ms\n64 bytes from bastiononline.ocp4.platformengineers.xyz (fe80::250:56ff:fe8a:3e00%ens192): icmp_seq=2 ttl=64 time=0.040 ms\n64 bytes from bastiononline.ocp4.platformengineers.xyz (fe80::250:56ff:fe8a:3e00%ens192): icmp_seq=3 ttl=64 time=0.042 ms\n</code></pre> </li> <li> <p>Finish the ping command with <code>ctrl+c</code></p> </li> </ol>"},{"location":"airgapped/9-onlinebastion/#install-required-red-hat-packages-in-the-online-bastion","title":"Install required Red Hat packages in the online bastion","text":"<p>For the installation of rpm packages on the online machine, the RHEL 8.7 OS ISO file that we created the VM from will help us.</p> <p>Note</p> <p>We can skip this step if we register the machine with a Red Hat subscription, which falls outside this workshop.</p> <ol> <li> <p>Make sure that, in the vCenter, the online bastion has the CD/DVD drive as connected:</p> <p></p> </li> <li> <p>The purpose of having the CD/DVD drive connected is that we can now mount this drive in the file system:</p> [root@bastiononline ~]<pre><code>mkdir /mnt/disc\nmount /dev/sr0 /mnt/disc\n</code></pre> Output<pre><code>mount: /mnt/disc: WARNING: device write-protected, mounted read-only.\n</code></pre> </li> <li> <p>Create the following following <code>rhel87dvd.repo</code> file that will define our RHEL OS repos:</p> [root@bastiononline ~]<pre><code>vi /etc/yum.repos.d/rhel87dvd.repo\n</code></pre> rhel87dvd.repo<pre><code>[InstallMediaBase]\nname=Red Hat Enterprise Linux 8.7.0 Base\nmediaid=None\nmetadata_expire=-1\ngpgcheck=0\ncost=500\nenabled=1\nbaseurl=file:///mnt/disc/BaseOS\n\n[InstallMediaApps]\nname=Red Hat Enterprise Linux 8.7.0 App\nmediaid=None\nmetadata_expire=-1\ngpgcheck=0\ncost=500\nenabled=1\nbaseurl=file:///mnt/disc/AppStream\n[InstallMediaParches]\nname=Red Hat Enterprise Linux KErnel\nmediaid=None\nmetadata_expire=-1\ngpgcheck=0\ncost=500\nenabled=0\nbaseurl=file:///mnt/parches\n</code></pre> </li> <li> <p>Make sure the cache is cleaned:</p> [root@bastiononline ~]<pre><code>yum clean all\n</code></pre> Output<pre><code>Updating Subscription Management repositories.\nUnable to read consumer identity\n\nThis system is not registered with an entitlement server. You can use subscription-manager to register.\n\n0 files removed\n</code></pre> </li> <li> <p>Make sure the new repositories are enabled:</p> [root@bastiononline ~]<pre><code>yum repolist enabled\n</code></pre> Output<pre><code>Updating Subscription Management repositories.\nUnable to read consumer identity\n\nThis system is not registered with an entitlement server. You can use subscription-manager to register.\n\nrepo id                                   repo name\nInstallMediaApps                          Red Hat Enterprise Linux 8.7.0 App\nInstallMediaBase                          Red Hat Enterprise Linux 8.7.0 Base\n</code></pre> </li> </ol> <p>Once you have created the offline repository and confirmed it is enabled, the required packages can be installed with the yum utility.</p> <p>The following packages have been installed on the online bastion server. Some packages may not be used during the execution of online commands, but they are installed so that the \"online\" and \"offline\" bastion settings are as identical as possible.</p> [root@bastiononline ~]<pre><code>yum install -y  podman \\\n                jq openssl httpd-tools curl wget telnet nfs-utils \\\n                httpd.x86_64 \\\n                bind bind-utils rsync mkisofs\n</code></pre> Output<pre><code>Updating Subscription Management repositories.\nUnable to read consumer identity\n\nThis system is not registered with an entitlement server. You can use subscription-manager to register.\n\nRed Hat Enterprise Linux 8.7.0 Base                                                                                          40 MB/s | 2.4 MB     00:00    \nRed Hat Enterprise Linux 8.7.0 App                                                                                           53 MB/s | 7.8 MB     00:00    \nPackage openssl-1:1.1.1k-7.el8_6.x86_64 is already installed.\nPackage curl-7.61.1-25.el8.x86_64 is already installed.\nDependencies resolved.\n============================================================================================================================================================\n Package                                   Architecture        Version                                                  Repository                     Size\n============================================================================================================================================================\nInstalling:\n bind                                      x86_64              32:9.11.36-5.el8                                         InstallMediaApps              2.1 M\n bind-utils                                x86_64              32:9.11.36-5.el8                                         InstallMediaApps              452 k\n genisoimage                               x86_64              1.1.11-39.el8                                            InstallMediaApps              316 k\n httpd                                     x86_64              2.4.37-51.module+el8.7.0+16050+02173b8e                  InstallMediaApps              1.4 M\n httpd-tools                               x86_64              2.4.37-51.module+el8.7.0+16050+02173b8e                  InstallMediaApps              109 k\n jq                                        x86_64              1.6-3.el8                                                InstallMediaApps              202 k\n nfs-utils                                 x86_64              1:2.3.3-57.el8                                           InstallMediaBase              515 k\n podman                                    x86_64              3:4.2.0-1.module+el8.7.0+16772+33343656                  InstallMediaApps               12 M\n rsync                                     x86_64              3.1.3-19.el8                                             InstallMediaBase              410 k\n telnet                                    x86_64              1:0.17-76.el8                                            InstallMediaApps               72 k\n wget                                      x86_64              1.19.5-10.el8                                            InstallMediaApps              734 k\n...\n...\nComplete!\n</code></pre> <p>Finally, we need to install Ansible on the system. However, this package is not included in the RHEL repositories configured previously. As a result, we will need to install it manually.</p> <ol> <li> <p>Download the Ansible RPM package to your laptop from the Red Hat portal.</p> <ul> <li>Link to Ansible rpm</li> </ul> </li> <li> <p>Source copy the rpm package to the online bastion</p> [student laptop]<pre><code>scp ansible-2.9.27-1.el8ap.noarch.rpm root@192.168.252.22:/root\n</code></pre> </li> <li> <p>Install it:</p> [root@bastiononline ~]<pre><code>yum localinstall -y ansible-2.9.27-1.el8ap.noarch.rpm\n</code></pre> Output<pre><code>Updating Subscription Management repositories.\nUnable to read consumer identity\n\nThis system is not registered with an entitlement server. You can use subscription-manager to register.\n\nLast metadata expiration check: 0:09:27 ago on Mon 06 Nov 2023 05:40:03 PM CET.\nDependencies resolved.\n============================================================================================================================================================\nPackage                                  Architecture          Version                                               Repository                       Size\n============================================================================================================================================================\nInstalling:\nansible                                  noarch                2.9.27-1.el8ap                                        @commandline                     17 M\n...\n...\nComplete!\n</code></pre> </li> </ol>"},{"location":"airgapped/9-onlinebastion/#download-openshift-utilities-to-the-online-bastion","title":"Download OpenShift utilities to the online bastion","text":"<p>A directory structure is created to accommodate the different binaries that are downloaded.</p> <ul> <li> <p>Create a directory for the registry and create the necessary directories for the registry.</p> [root@bastiononline ~]<pre><code>mkdir /root/registry\ncd registry/\n</code></pre> [root@bastiononline registry]<pre><code>mkdir auth certs data downloads\nls -lart\n</code></pre> Output<pre><code>total 4\ndr-xr-x---. 7 root root 4096 Sep  8 07:43 ..\ndrwxr-xr-x  2 root root    6 Sep  8 07:48 downloads\ndrwxr-xr-x  2 root root    6 Sep  8 07:48 data\ndrwxr-xr-x  2 root root    6 Sep  8 07:48 certs\ndrwxr-xr-x  2 root root    6 Sep  8 07:48 auth\ndrwxr-xr-x  6 root root   60 Sep  8 07:48 .\n</code></pre> [root@bastiononline registry]<pre><code>cd downloads/\n</code></pre> [root@bastiononline downloads]<pre><code>mkdir images tools secrets\nls -lart\n</code></pre> Output<pre><code>total 0\ndrwxr-xr-x 6 root root 60 Sep  8 07:48 ..\ndrwxr-xr-x 2 root root  6 Sep  8 07:59 tools\ndrwxr-xr-x 2 root root  6 Sep  8 07:59 secrets\ndrwxr-xr-x 2 root root  6 Sep  8 07:59 images\ndrwxr-xr-x 5 root root 48 Sep  8 07:59 .\n</code></pre> </li> </ul> <p>The folder structure that should have got created should look like this:</p> Directory structure<pre><code>/root\n`-- registry\n    |-- auth\n    |-- certs\n    |-- data\n    `-- downloads\n        |-- images\n        |-- secrets\n        `-- tools\n</code></pre> <p>For the OpenShift client, the following steps have been followed:</p> <ul> <li> <p>OpenShift client download:</p> [root@bastiononline downloads]<pre><code>cd tools/\n</code></pre> [root@bastiononline tools]<pre><code>wget https://mirror.openshift.com/pub/openshift-v4/clients/ocp/4.16.9/openshift-client-linux-amd64-rhel8-4.16.9.tar.gz\n</code></pre> </li> <li> <p>Extracting the contents of the downloaded file:</p> [root@bastiononline tools]<pre><code>tar -xvf openshift-client-linux-amd64-rhel8-4.16.9.tar.gz\n</code></pre> Output<pre><code>README.md\noc\nkubectl\n</code></pre> [root@bastiononline tools]<pre><code>ls -lart\n</code></pre> Output<pre><code>total 1156008\n-rwxr-xr-x 2 root root  159905720 Aug 21 01:54 oc\n-rwxr-xr-x 2 root root  159905720 Aug 21 01:54 kubectl\n-rw-r--r-- 1 root root        950 Aug 21 01:54 README.md\n-rw-r--r-- 1 root root   66705673 Aug 22 22:05 openshift-client-linux-amd64-rhel8-4.16.9.tar.gz\ndrwxr-xr-x 5 root root         48 Sep 21 07:55 ..\ndrwxr-xr-x 2 root root        164 Sep 21 11:19 .\n</code></pre> </li> <li> <p>Copy to a location accessible from your $PATH (e.g. /usr/bin):</p> [root@bastiononline tools]<pre><code>cp /root/registry/downloads/tools/oc /usr/bin/oc\n</code></pre> </li> </ul> <p>For the OpenShift installer, the following steps should be followed:</p> <ul> <li> <p>Downloading the OpenShift Installer</p> [root@bastiononline tools]<pre><code>wget https://mirror.openshift.com/pub/openshift-v4/clients/ocp/4.16.9/openshift-install-linux-4.16.9.tar.gz\n</code></pre> </li> <li> <p>Extract the contents of the downloaded file:</p> [root@bastiononline tools]<pre><code>tar -xvf openshift-install-linux-4.16.9.tar.gz\n</code></pre> Output<pre><code>README.md\nopenshift-install\n</code></pre> [root@bastiononline tools]<pre><code>ls -lart\n</code></pre> Output<pre><code>total 1156008\n-rwxr-xr-x 2 root root  159905720 Aug 21 01:54 oc\n-rwxr-xr-x 2 root root  159905720 Aug 21 01:54 kubectl\n-rwxr-xr-x 1 root root  707731456 Aug 21 03:44 openshift-install\n-rw-r--r-- 1 root root        706 Aug 21 03:44 README.md\n-rw-r--r-- 1 root root   66705673 Aug 22 22:05 openshift-client-linux-amd64-rhel8-4.16.9.tar.gz\n-rw-r--r-- 1 root root  510253136 Aug 22 22:05 openshift-install-linux-4.16.9.tar.gz\ndrwxr-xr-x 5 root root         48 Sep 21 07:55 ..\ndrwxr-xr-x 2 root root        164 Sep 21 11:19 .\n</code></pre> </li> <li> <p>Copy to a location accessible from your $PATH (e.g. /usr/bin):</p> [root@bastiononline tools]<pre><code>cp /root/registry/downloads/tools/openshift-install /usr/bin/openshift-install\n</code></pre> </li> </ul> <p>For the Mirror registry, the following steps should be followed:</p> <ul> <li> <p>Download the mirror-registry</p> [root@bastiononline tools]<pre><code>wget https://mirror.openshift.com/pub/cgw/mirror-registry/latest/mirror-registry-amd64.tar.gz\n</code></pre> </li> <li> <p>Extracting the contents of the downloaded file:</p> [root@bastiononline tools]<pre><code>tar -xvf mirror-registry-amd64.tar.gz\n</code></pre> Output<pre><code>image-archive.tar\nexecution-environment.tar\nmirror-registry\n</code></pre> [root@bastiononline tools]<pre><code>ls -lart\n</code></pre> Output<pre><code>total 4019476\n-rwxr-xr-x 2 root root  160465008 Aug 21 00:49 oc\n-rwxr-xr-x 2 root root  160465008 Aug 21 00:49 kubectl\n-rwxr-xr-x 1 root root  707731456 Aug 21 03:44 openshift-install\n-rw-r--r-- 1 root root        706 Aug 21 03:44 README.md\n-rw-r--r-- 1 root root   66941882 Aug 22 22:05 openshift-client-linux-4.16.9.tar.gz\n-rw-r--r-- 1 root root  510253136 Aug 22 22:05 openshift-install-linux-4.16.9.tar.gz\n-rwxr-xr-x 1 root root    9824888 Sep 23 18:46 mirror-registry\n-rw-r--r-- 1 root root  113418240 Sep 23 18:47 sqlite3.tar\n-rw-r--r-- 1 root root  316753920 Sep 23 18:50 execution-environment.tar\n-rw-r--r-- 1 root root 1454929920 Sep 23 18:50 image-archive.tar\n-rw-r--r-- 1 root root  607056480 Sep 26 11:44 mirror-registry-amd64.tar.gz\ndrwxr-xr-x 5 root root         48 Oct 24 19:16 ..\ndrwxr-xr-x 2 root root       4096 Oct 24 19:41 .\n</code></pre> </li> </ul> <p>For the oc-mirror plugin, the following steps should be followed:</p> <ul> <li> <p>Download the oc mirror program to make mirror of OCP and its operators</p> [root@bastiononline tools]<pre><code>wget https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/ocp/4.16.19/oc-mirror.tar.gz\n</code></pre> </li> <li> <p>Extracting the contents of the downloaded file:</p> [root@bastiononline tools]<pre><code>tar xvfz oc-mirror.tar.gz\n</code></pre> Output<pre><code>oc-mirror\n</code></pre> [root@bastiononline tools]<pre><code>ls -lart\n</code></pre> Output<pre><code>total 4245772\n-rwxr-xr-x 2 root root  159905720 Aug 21 01:54 oc\n-rwxr-xr-x 2 root root  159905720 Aug 21 01:54 kubectl\n-rwxr-xr-x 1 root root  707731456 Aug 21 03:44 openshift-install\n-rw-r--r-- 1 root root        706 Aug 21 03:44 README.md\n-rw-r--r-- 1 root root   66705673 Aug 22 22:05 openshift-client-linux-amd64-rhel8-4.16.9.tar.gz\n-rw-r--r-- 1 root root  510253136 Aug 22 22:05 openshift-install-linux-4.16.9.tar.gz\n-rwxr-xr-x 1 root root    9824888 Sep 23 18:46 mirror-registry\n-rw-r--r-- 1 root root  113418240 Sep 23 18:47 sqlite3.tar\n-rw-r--r-- 1 root root  316753920 Sep 23 18:50 execution-environment.tar\n-rw-r--r-- 1 root root 1454929920 Sep 23 18:50 image-archive.tar\n-rw-r--r-- 1 root root  607056480 Sep 26 11:44 mirror-registry-amd64.tar.gz\n-rw-r----- 1  984  984  165699280 Oct 18 02:15 oc-mirror\n-rw-r--r-- 1 root root   67385243 Oct 25 03:02 oc-mirror.tar.gz\ndrwxr-xr-x 5 root root         48 Oct 24 19:16 ..\ndrwxr-xr-x 2 root root       4096 Oct 25 09:22 .\n</code></pre> </li> <li> <p>Copy to a location accessible from your $PATH (e.g. /usr/local/bin):</p> [root@bastiononline tools]<pre><code>chmod +x /root/registry/downloads/tools/oc-mirror\ncp /root/registry/downloads/tools/oc-mirror /usr/local/bin/\noc-mirror help\n</code></pre> </li> </ul> <p>For the Butane tool, the following steps should be followed:</p> <ul> <li> <p>Download butane</p> [root@bastiononline tools]<pre><code>curl https://mirror.openshift.com/pub/openshift-v4/clients/butane/latest/butane-amd64 --output butane\n</code></pre> [root@bastiononline tools]<pre><code>ls -lart\n</code></pre> Output<pre><code>total 4019476\n-rwxr-xr-x 2 root root  160465008 Aug 21 00:49 oc\n-rwxr-xr-x 2 root root  160465008 Aug 21 00:49 kubectl\n-rwxr-xr-x 1 root root  707731456 Aug 21 03:44 openshift-install\n-rw-r--r-- 1 root root        706 Aug 21 03:44 README.md\n-rw-r--r-- 1 root root   66941882 Aug 22 22:05 openshift-client-linux-4.16.9.tar.gz\n-rw-r--r-- 1 root root  510253136 Aug 22 22:05 openshift-install-linux-4.16.9.tar.gz\n-rwxr-xr-x 1 root root    9824888 Sep 23 18:46 mirror-registry\n-rw-r--r-- 1 root root  113418240 Sep 23 18:47 sqlite3.tar\n-rw-r--r-- 1 root root  316753920 Sep 23 18:50 execution-environment.tar\n-rw-r--r-- 1 root root 1454929920 Sep 23 18:50 image-archive.tar\n-rw-r--r-- 1 root root  607056480 Sep 26 11:44 mirror-registry-amd64.tar.gz\ndrwxr-xr-x 5 root root         48 Oct 24 19:16 ..\ndrwxr-xr-x 2 root root       4096 Oct 24 19:41 .\n-rw-r--r-- 1 root root    8070568 Oct 24 19:41 butane\n</code></pre> </li> </ul> <p>For the ISO Maker tool, the following steps should be followed:</p> <ul> <li> <p>Download the ISO Maker</p> [root@bastiononline ~]<pre><code>cd /root/\nmkdir Coreos-iso-maker\ncd Coreos-iso-maker/\n</code></pre> [root@bastiononline Coreos-iso-maker]<pre><code>yum install -y git\n</code></pre> Output<pre><code>Updating Subscription Management repositories.\nUnable to read consumer identity\n\nThis system is not registered with an entitlement server. You can use subscription-manager to register.\n\nLast metadata expiration check: 0:30:40 ago on Mon 06 Nov 2023 05:40:03 PM CET.\nDependencies resolved.\n============================================================================================================================================================\nPackage                               Architecture          Version                                                  Repository                       Size\n============================================================================================================================================================\nInstalling:\n git                                   x86_64                2.31.1-2.el8                                             InstallMediaApps                161 k\n...\n...\nComplete!\n</code></pre> [root@bastiononline Coreos-iso-maker]<pre><code>git clone https://github.com/chuckersjp/coreos-iso-maker\n</code></pre> Output<pre><code>Cloning into 'coreos-iso-maker'...\nremote: Enumerating objects: 391, done.\nremote: Counting objects: 100% (129/129), done.\nremote: Compressing objects: 100% (37/37), done.\nremote: Total 391 (delta 116), reused 93 (delta 92), pack-reused 262\nReceiving objects: 100% (391/391), 70.81 KiB | 1.16 MiB/s, done.\nResolving deltas: 100% (246/246), done.\n</code></pre> [root@bastiononline Coreos-iso-maker]<pre><code>ls -lart\n</code></pre> Output<pre><code>total 8\ndr-xr-x---. 8 root root 4096 Sep  8 11:40 ..\ndrwxr-xr-x  3 root root   30 Sep  8 11:42 .\ndrwxr-xr-x  4 root root 4096 Sep  8 11:42 coreos-iso-maker\n</code></pre> </li> </ul>"},{"location":"airgapped/9-onlinebastion/#optional-expand-bastion-boot-disk","title":"(Optional) Expand bastion boot disk","text":"<p>Info</p> <p>You dont need to complete this section. It is only for reference to understand what you would need to do should you require to increase disk space on any of your bastion machines.</p> <p>In case you need to increase the bastion online\u2019s disk size because at the creation moment you didn\u2019t set the correct amount of storage, you must follow the next steps in vCenter with the vm machine stopped:</p> <p></p> <p>Set the correct space you need on the disk and then start the machine and follow these commands to create a new partition:</p> <ol> <li> <p>Access the online bastion:</p> [student laptop]<pre><code>ssh root@192.168.252.22\n</code></pre> </li> <li> <p>Check the actual partitions size</p> [root@bastiononline ~]<pre><code>df -h\n</code></pre> Output<pre><code>Filesystem             Size  Used Avail Use% Mounted on\ndevtmpfs                16G     0   16G   0% /dev\ntmpfs                   16G     0   16G   0% /dev/shm\ntmpfs                   16G  8.7M   16G   1% /run\ntmpfs                   16G     0   16G   0% /sys/fs/cgroup\n/dev/mapper/rhel-root  621G   12G  610G   2% /\n/dev/sda2             1014M  211M  804M  21% /boot\n/dev/sda1              599M  5.8M  594M   1% /boot/efi\n/dev/mapper/rhel-home   62G  473M   62G   1% /home\ntmpfs                  3.2G     0  3.2G   0% /run/user/0\n</code></pre> </li> <li> <p>List the partition tables</p> [root@bastiononline ~]<pre><code>fdisk -l\n</code></pre> Output<pre><code>Register this system with Red Hat Insights: insights-client --register\nCreate an account or view all your systems at https://red.ht/insights-dashboard\nLast login: Mon Nov  6 18:32:23 2023 from 192.168.253.2\n[root@bastiononline ~]# fdisk -l\nDisk /dev/sda: 1 TiB, 1099511627776 bytes, 2147483648 sectors\nUnits: sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisklabel type: gpt\nDisk identifier: 68D049D0-0D01-44BA-AF0F-9371047971AE\n\nDevice          Start        End    Sectors   Size Type\n/dev/sda1        2048    1230847    1228800   600M EFI System\n/dev/sda2     1230848    3327999    2097152     1G Linux filesystem\n/dev/sda3     3328000 1468004351 1464676352 698.4G Linux LVM\n/dev/sda4  1468004352 2147483614  679479263   324G Linux filesystem\n\n\nDisk /dev/mapper/rhel-root: 621 GiB, 666793672704 bytes, 1302331392 sectors\nUnits: sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\n\n\nDisk /dev/mapper/rhel-swap: 15.8 GiB, 16924016640 bytes, 33054720 sectors\nUnits: sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\n\n\nDisk /dev/mapper/rhel-home: 61.7 GiB, 66194505728 bytes, 129286144 sectors\nUnits: sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\n</code></pre> </li> <li> <p>Start the disk partition table tool for <code>/dev/sda</code></p> [root@bastiononline ~]<pre><code>fdisk /dev/sda\n</code></pre> <p>Note</p> <p>We select <code>n</code> in the first command. Then, we press enter to leave the defaults. Finally, for the second command, we select <code>w</code> (to write the partitions)</p> Output<pre><code>Welcome to fdisk (util-linux 2.32.1).\nChanges will remain in memory only, until you decide to write them.\nBe careful before using the write command.\n\n\nCommand (m for help): n\nPartition number (4-128, default 4): \nFirst sector (1468004352-2147483614, default 1468004352): \nLast sector, +sectors or +size{K,M,G,T,P} (1468004352-2147483614, default 2147483614): \n\nCreated a new partition 4 of type 'Linux filesystem' and of size 324 GiB.\n\nCommand (m for help): w\nThe partition table has been altered.\nSyncing disks.\n</code></pre> </li> <li> <p>Expand the logical volume that has the operating system boot with the following commands</p> [root@bastiononline ~]<pre><code>pvcreate /dev/sda4\n</code></pre> Output<pre><code>Physical volume \"/dev/sda4\" successfully created.\n</code></pre> [root@bastiononline ~]<pre><code>vgs\n</code></pre> Output<pre><code>VG   #PV #LV #SN Attr   VSize   VFree\nrhel   1   3   0 wz--n- 698.41g    0\n</code></pre> [root@bastiononline ~]<pre><code>vgextend rhel /dev/sda4\n</code></pre> Output<pre><code>Volume group \"rhel\" successfully extended\n</code></pre> [root@bastiononline ~]<pre><code>lvs\n</code></pre> Output<pre><code>LV   VG   Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert\nhome rhel -wi-ao---- &lt;61.65g                                                    \nroot rhel -wi-ao---- 621.00g                                                    \nswap rhel -wi-ao----  15.76g\n</code></pre> [root@bastiononline ~]<pre><code>lvextend -l +100%FREE /dev/rhel/root\n</code></pre> Output<pre><code>Size of logical volume rhel/root changed from 621.00 GiB (158976 extents) to &lt;945.00 GiB (241919 extents).\nLogical volume rhel/root successfully resized.\n</code></pre> [root@bastiononline ~]<pre><code>lsblk -f\n</code></pre> Output<pre><code>NAME          FSTYPE      LABEL                    UUID                                   MOUNTPOINT\nsda                                                                                       \n|-sda1        vfat                                 6793-7237                              /boot/efi\n|-sda2        xfs                                  c43ee8e7-77b7-40af-8367-ba7ef90fcbb5   /boot\n|-sda3        LVM2_member                          kOrAn9-K2h6-RbB9-gINB-pWNL-dxVB-VkxYzZ \n| |-rhel-root xfs                                  ba1bba3c-dea8-4e2e-9882-fd866f6d13e8   /\n| |-rhel-swap swap                                 868795de-6965-44bd-aee3-3181691531f7   [SWAP]\n| `-rhel-home xfs                                  2bb2a05b-9811-4c10-ba3b-f1ab8084460a   /home\n`-sda4        LVM2_member                          9apLnq-zxPM-HNZ3-ncc7-jgwZ-hhAg-T3GVk6 \n  `-rhel-root xfs                                  ba1bba3c-dea8-4e2e-9882-fd866f6d13e8   /\nsr0           iso9660     RHEL-8-7-0-BaseOS-x86_64 2022-10-13-06-03-04-00\n</code></pre> [root@bastiononline ~]<pre><code>xfs_growfs /dev/rhel/root\n</code></pre> Output<pre><code>meta-data=/dev/mapper/rhel-root  isize=512    agcount=4, agsize=40697856 blks\n        =                       sectsz=512   attr=2, projid32bit=1\n        =                       crc=1        finobt=1, sparse=1, rmapbt=0\n        =                       reflink=1    bigtime=0 inobtcount=0\ndata     =                       bsize=4096   blocks=162791424, imaxpct=25\n        =                       sunit=0      swidth=0 blks\nnaming   =version 2              bsize=4096   ascii-ci=0, ftype=1\nlog      =internal log           bsize=4096   blocks=79488, version=2\n        =                       sectsz=512   sunit=0 blks, lazy-count=1\nrealtime =none                   extsz=4096   blocks=0, rtextents=0\ndata blocks changed from 162791424 to 247725056\n</code></pre> [root@bastiononline ~]<pre><code>df -h\n</code></pre> Output<pre><code>Filesystem             Size  Used Avail Use% Mounted on\ndevtmpfs                16G     0   16G   0% /dev\ntmpfs                   16G     0   16G   0% /dev/shm\ntmpfs                   16G  8.7M   16G   1% /run\ntmpfs                   16G     0   16G   0% /sys/fs/cgroup\n/dev/mapper/rhel-root  945G   14G  932G   2% /\n/dev/sda2             1014M  211M  804M  21% /boot\n/dev/sda1              599M  5.8M  594M   1% /boot/efi\n/dev/mapper/rhel-home   62G  473M   62G   1% /home\ntmpfs                  3.2G     0  3.2G   0% /run/user/0\n</code></pre> </li> </ol> <p>Repeat on the offline bastion if required.</p>"},{"location":"airgapped/9-onlinebastion/#download-the-red-hat-coreos-images-to-the-online-bastion","title":"Download the Red Hat CoreOS images to the online bastion","text":"<p>We need to download the following CoreOS images for the installation of the different OpenShift nodes: </p> [root@bastiononline ~]<pre><code>cd /root/registry/downloads/images/\n</code></pre> [root@bastiononline images]<pre><code>wget https://mirror.openshift.com/pub/openshift-v4/x86_64/dependencies/rhcos/4.16/4.16.3/rhcos-4.16.3-x86_64-metal.x86_64.raw.gz\nwget https://mirror.openshift.com/pub/openshift-v4/x86_64/dependencies/rhcos/4.16/4.16.3/rhcos-4.16.3-x86_64-live.x86_64.iso\n</code></pre> [root@bastiononline images]<pre><code>ls -lart\n</code></pre> Output<pre><code>total 2363592\n-rw-r--r-- 1 root root 1213202432 Apr  4  2023 rhcos-4.16.3-x86_64-live.x86_64.iso\n-rw-r--r-- 1 root root 1207111745 Apr  4  2023 rhcos-4.16.3-x86_64-metal.x86_64.raw.gz\ndrwxr-xr-x 5 root root         48 Nov  6 17:57 ..\ndrwxr-xr-x 2 root root         98 Nov  6 20:40 .\n</code></pre>"},{"location":"airgapped/9-onlinebastion/#mirror-images-on-the-online-bastion","title":"Mirror images on the online bastion","text":"<p>For accessing your Red Hat OpenShift pull secret \u2013 containing credentials that allow for downloading OpenShift and associated artifacts \u2013 please proceed as follows:</p> <p>https://cloud.redhat.com/openshift/install/pull-secret</p> <p>and download or copy the contents of the pull secret. </p> <p> </p> <ol> <li>Open the link above and copy the contents of the Red Hat OpenShift pull secret</li> <li> <p>Paste the contents of the Red Hat OpenShift pull secret into a file.</p> [root@bastiononline images]<pre><code>cd /root/registry/downloads/secrets/\n</code></pre> [root@bastiononline secrets]<pre><code>vi pull-secret.txt\n</code></pre> Output<pre><code>{\"auths\":{\"cloud.openshift.com\":{\"auth\":\"&lt;auth_secret&gt;\",\"email\":\"Miguel.Ariza.Colmenares2@ibm.com\"},\"quay.io\":{\"auth\":\" [\u2026\u2026\u2026\u2026\u2026]TdUpHajdDRGJfUUNFUk9ncjJ4Qi01V0pUVUNWenpLcmJlZU5yWGo0Y2ZTTkYzVWU3NC05N3hQMVBvQjBCYmRiOEhMemgwc0NfYjdtNzlkVDVmVGRyY3M2ZENqZzV4dEZNWnhlTTFTWEg0UWJqeDJYb2xUQ2RmekRfNzRhOXRueGVLMnFHaFZPZVVnXzBSY2t5MXZQUzVFMjFvMA==\",\"email\":\"Miguel.Ariza.Colmenares2@ibm.com\"}}}\n</code></pre> </li> <li> <p>Make a copy of your pull secret in JSON format:</p> [root@bastiononline secrets]<pre><code>cat ./pull-secret.txt | jq . &gt; ./pull-secret.json\n</code></pre> <p>The contents of the file resemble the following example:</p> Output<pre><code>{\n\"auths\": {\n    \"cloud.openshift.com\": {\n    \"auth\": \"b3BlbnNo...\",\n    \"email\": \"you@example.com\"\n    },\n    \"quay.io\": {\n    \"auth\": \"b3BlbnNo...\",\n    \"email\": \"you@example.com\"\n    },\n    \"registry.connect.redhat.com\": {\n    \"auth\": \"NTE3Njg5Nj...\",\n    \"email\": \"you@example.com\"\n    },\n    \"registry.redhat.io\": {\n    \"auth\": \"NTE3Njg5Nj...\",\n    \"email\": \"you@example.com\"\n    }\n}\n}\n</code></pre> </li> </ol> <p>We need to copy the contents of the file to the container runtime directory so that it is accessible by the image download procedure. Save the file either as </p> <ul> <li><code>~/.docker/config.json</code> or </li> <li><code>$XDG_RUNTIME_DIR/containers/auth.json</code></li> </ul> [root@bastiononline secrets]<pre><code>echo $XDG_RUNTIME_DIR\n</code></pre> Output<pre><code>/run/user/0\n</code></pre> [root@bastiononline secrets]<pre><code>mkdir /run/user/0/containers\ncp /root/registry/downloads/secrets/pull-secret.json /run/user/0/containers/auth.json\nls -lart /run/user/0/containers/auth.json\n</code></pre> Output<pre><code>-rw-r--r-- 1 root root 2943 Sep 21 12:31 /run/user/0/containers/auth.json\n</code></pre> <p>Change directory in order to mirror the images</p> [root@bastiononline secrets]<pre><code>cd /root/registry/\n</code></pre> <p>Use the <code>oc mirror init</code> command to create a template for the image set configuration and save it to a file called <code>imageset-config.yaml</code>:</p> [root@bastiononline secrets]<pre><code>cd /root/registry/\n</code></pre> [root@bastiononline registry]<pre><code>oc mirror init  &gt; imageset-config.yaml\nls -lart\n</code></pre> Output<pre><code>total 12\ndrwxr-xr-x  2 root root    6 Sep  8 07:48 data\ndrwxr-xr-x  2 root root    6 Sep  8 07:48 certs\ndrwxr-xr-x  2 root root    6 Sep  8 07:48 auth\ndrwxr-xr-x  5 root root   48 Sep  8 07:59 downloads\ndr-xr-x---. 8 root root 4096 Sep  8 14:07 ..\ndrwxr-xr-x  6 root root  110 Sep  8 16:49 .\n-rw-------  1 root root  419 Sep  8 16:50 .oc-mirror.log\n-rw-r--r--  1 root root  419 Sep  8 16:50 imageset-config.yaml\n</code></pre> <p>We edit the file to look exactly as shown below:</p> [root@bastiononline registry]<pre><code>vi imageset-config.yaml\n</code></pre> imageset-config.yaml<pre><code>kind: ImageSetConfiguration\napiVersion: mirror.openshift.io/v1alpha2\narchiveSize: 4\nstorageConfig:\n  local:\n    path: /root/registry/data\nmirror:\n  platform:\n    architectures:\n      - \"amd64\"\n    channels:\n    - name: stable-4.16\n      type: ocp\n      minVersion: 4.16.9\n      maxVersion: 4.16.9\n      shortestPath: true\n  operators:\n  - catalog: registry.redhat.io/redhat/redhat-operator-index:v4.16\n    packages:\n    - name: openshift-pipelines-operator-rh\n      defaultChannel: latest\n      channels:\n      - name: latest\n    - name: local-storage-operator\n      channels:\n      - name: stable\n    - name: odf-operator\n      channels:\n      - name: stable-4.16\n      minVersion: '4.16.0-rhodf'\n    - name: ocs-operator\n      channels:\n      - name: stable-4.16\n      minVersion: '4.16.0-rhodf'\n    - name: odf-csi-addons-operator\n      channels:\n      - name: stable-4.16\n      minVersion: '4.16.0-rhodf'\n    - name: mcg-operator\n      channels: \n      - name: stable-4.16\n      minVersion: '4.16.0-rhodf'\n  additionalImages:\n  - name: registry.redhat.io/ubi8/ubi:latest\n  helm: {}\n</code></pre> <p>Execute the platform and operators mirror process.</p> <p>Time</p> <p>Bear in mind that the mirror process can take around 10 mins to complete. Be patient.</p> [root@bastiononline registry]<pre><code>oc mirror --config=./imageset-config.yaml file:///root/registry/data\n</code></pre> Output<pre><code>[...]\ninfo: Mirroring completed in 7m53.51s (134.2MB/s)\nCreating archive /root/registry/data/mirror_seq1_000000.tar\nCreating archive /root/registry/data/mirror_seq1_000001.tar\nCreating archive /root/registry/data/mirror_seq1_000002.tar\nCreating archive /root/registry/data/mirror_seq1_000003.tar\nCreating archive /root/registry/data/mirror_seq1_000004.tar\nCreating archive /root/registry/data/mirror_seq1_000005.tar\nCreating archive /root/registry/data/mirror_seq1_000006.tar\nCreating archive /root/registry/data/mirror_seq1_000007.tar\nCreating archive /root/registry/data/mirror_seq1_000008.tar\nCreating archive /root/registry/data/mirror_seq1_000009.tar\nCreating archive /root/registry/data/mirror_seq1_000010.tar\nCreating archive /root/registry/data/mirror_seq1_000011.tar\nCreating archive /root/registry/data/mirror_seq1_000012.tar\nCreating archive /root/registry/data/mirror_seq1_000013.tar\nCreating archive /root/registry/data/mirror_seq1_000014.tar\nCreating archive /root/registry/data/mirror_seq1_000015.tar\n</code></pre> <p>Tip</p> <p>You can find the default channel by running the following command:</p> <pre><code>oc mirror list operators --catalog=&lt;catalog_name&gt; --package=&lt;package_name&gt;\n</code></pre> <p>Examples:</p> <pre><code>oc-mirror list operators --catalog registry.redhat.io/redhat/redhat-operator-index:v4.16\noc-mirror list operators --catalog registry.redhat.io/redhat/redhat-operator-index:v4.16 --package openshift-pipelines-operator-rh\noc mirror list releases --channels --version 4.16\noc mirror list releases --channel stable-4.16\n</code></pre>"},{"location":"airgapped/orig/","title":"Air-gapped OpenShift Installation","text":"<p>Shout-out </p> <p>Big shout-out to the SPGI CSM Team for creating the majority of this workshop. A huge thank you to them.</p> <p>Disclaimer</p> <p>This material has been created for training and learning purposes. It is not, by any means, official documentation supported by either IBM or Red Hat.</p>"},{"location":"airgapped/orig/#1-solution-architecture","title":"1. Solution Architecture","text":"<p>The following diagram illustrates the architecture of the OpenShift cluster air-gapped installation we are conducting in this training course. The servers within the cluster operate in an air-gapped environment, completely isolated from the Internet, with no inbound or outbound access. To facilitate the installation, a specialized machine external to the cluster and connected to the Internet is employed. This machine, designated as the online bastion, downloads all necessary packages and binaries required for the installation, mirroring real-world client scenarios.</p> <p>Packages and binaries are then transferred from the online bastion to a machine situated within the internal network of the cluster, which has no Internet connectivity whatsoever. This internal machine is referred to as the offline bastion. The OpenShift installation will be initiated from this offline bastion, as it holds exclusive access to the internal network where the cluster resides.</p> <p></p> <p>During the air-gapped OpenShift installation conducted in this training course, we will create and configure 3 control plane nodes (aka master nodes), 3 infrastructure nodes (aka infra nodes), 3 storage nodes and 3 compute nodes (aka worker nodes), along with both 'offline' and 'online' bastion nodes. Should the need arise, additional compute nodes can be added at a later stage.</p> <p>The version of the OpenShift Container Platform (OCP) to be installed is 4.16. Correspondingly, an OpenShift (oc) client of this same version, 4.16, will be downloaded and installed on the bastion machines for subsequent operations against the OpenShift cluster.</p> <p>Servers specifications:</p> Type Number of servers vCPU RAM Memory Storage (system) Storage (data) Bastion 2 4 vCPU 16 GB 700 GB 0 GB Control Plane 3 4 vCPU 16 GB 120 GB 0 GB Compute 3 8 vCPU 32 GB 300 GB 0 GB Infra 3 8 vCPU 32 GB 120 GB 0 GB Storage 3 16 vCPU 64 GB 120 GB 512 GB Bootstrap 1 4 vCPU 16 GB 120 GB 0 GB <p>Lab environment</p> <p>Use this link to the IBM Technology Zone to request an environment to carry out this tutorial. Make sure you select at least 3TB for the DataStore and VPN is enabled:</p> <p></p> <p>Important</p> <p>While utilizing a VMware vSphere IBM Technology Zone environment, we will not leverage any of the VMware features integrated within the OpenShift Installer for deployment purposes. Instead, we are considering this setup as an elevated \"bare-metal\" environment, where virtual machines emulate bare-metal servers closely.</p>"},{"location":"airgapped/orig/#2-infrastructure-services","title":"2. Infrastructure Services","text":"<p>In the 'offline' bastion node, the HTTP server, load balancer, image registry, DNS, and OCP deployment utilities will all be configured.</p> <p>Given that the network lacks a DHCP server and maintains static IP addresses for all machines, we are required to manually assign each machine's IP, DNS, and hostname details upon creation.</p>"},{"location":"airgapped/orig/#3-network-configuration","title":"3. Network Configuration","text":"<p>The following table contains the network configuration where the cluster will be installed:</p> Configuration Option Subnet name ocp4.platformengineers.xyz IP address space 192.168.252.0/24 <p>The following table contains the OCP network configuration:</p> Option Network: External IP address space 192.168.252.0/24 Assigned IP address range 192.168.252.1-192.168.252.254 Number of available IP addresses 254 Network: Cluster IP address space 9.248.0.0/14 IP address range 9.248.0.1-9.248.255.254 Subnet prefix 24 Number of available IP addresses 262.142 Number of IP addresses per node 254 Maximum number of nodes 1024 Network: Service IP address space 10.248.0.0/16 IP address range 10.248.0.1-10.248.255.254 Number of available IP addresses 65.534"},{"location":"airgapped/orig/#4-entries-in-the-dns-server","title":"4. Entries in the DNS server","text":"<p>These are the DNS entries for the DNS servers:</p> Type FQDN hostname IP Address Bastion bastion.ocp4.platformengineers.xyz 192.168.252.23 bastiononline.ocp4.platformengineers.xyz 192.168.252.22 Bootstrap bootstrap.ocp4.platformengineers.xyz 192.168.252.3 Control Plane controlplane01.ocp4.platformengineers.xyz 192.168.252.4 controlplane02.ocp4.platformengineers.xyz 192.168.252.5 controlplane03.ocp4.platformengineers.xyz 192.168.252.6 Infra infra01.ocp4.platformengineers.xyz 192.168.252.7 infra02.ocp4.platformengineers.xyz 192.168.252.8 infra03.ocp4.platformengineers.xyz 192.168.252.9 Compute compute01.ocp4.platformengineers.xyz 192.168.252.10 compute02.ocp4.platformengineers.xyz 192.168.252.11 compute03.ocp4.platformengineers.xyz 192.168.252.12 Storage storage01.ocp4.platformengineers.xyz 192.168.252.13 storage02.ocp4.platformengineers.xyz 192.168.252.14 storage03.ocp4.platformengineers.xyz 192.168.252.15 <p>And these are the service names:</p> Name FQDN hostname Type IP VIP API api.ocp4.platformengineers.xyz A/AAA or CNAME 192.168.252.24 VIP API-INT api-int.ocp4.platformengineers.xyz A/AAA or CNAME 192.168.252.24 VIP Ingress *.apps.ocp4.platformengineers.xyz A/AAA or CNAME 192.168.252.25"},{"location":"airgapped/orig/#5-load-balancer-configuration","title":"5. Load Balancer Configuration","text":"<p>We will configure the HAProxy load balancer within the \"offline\" Bastion node, as follows:</p> Front End Targets Port *.apps.ocp4.platformengineers.xyz infra01.ocp4.platformengineers.xyz 80 infra02.ocp4.platformengineers.xyz infra03.ocp4.platformengineers.xyz compute01.ocp4.platformengineers.xyz compute02.ocp4.platformengineers.xyz compute03.ocp4.platformengineers.xyz storage01.ocp4.platformengineers.xyz storage02.ocp4.platformengineers.xyz storage03.ocp4.platformengineers.xyz *.apps.ocp4.platformengineers.xyz infra01.ocp4.platformengineers.xyz 443 infra02.ocp4.platformengineers.xyz infra03.ocp4.platformengineers.xyz compute01.ocp4.platformengineers.xyz compute02.ocp4.platformengineers.xyz compute03.ocp4.platformengineers.xyz storage01.ocp4.platformengineers.xyz storage02.ocp4.platformengineers.xyz storage03.ocp4.platformengineers.xyz api.ocp4.platformengineers.xyz bootstrap.ocp4.platformengineers.xyz 6443 controlplane01.ocp4.platformengineers.xyz controlplane02.ocp4.platformengineers.xyz controlplane03.ocp4.platformengineers.xyz api-int.ocp4.platformengineers.xyz bootstrap.ocp4.platformengineers.xyz 22623 controlplane01.ocp4.platformengineers.xyz controlplane02.ocp4.platformengineers.xyz controlplane03.ocp4.platformengineers.xyz"},{"location":"airgapped/orig/#6-vpn-configuration","title":"6. VPN Configuration","text":"<p>In order to connect via ssh to the machines created during the OpenShift installation, which are placed in the <code>192.168.252.0/24</code> private network, and to the VMware vSphere vCenter we need to complete the following steps:</p> <ol> <li> <p>Download and install the VPN manager Wireguard (available in the App Store).</p> <p></p> </li> <li> <p>Download the VPN config file from the IBM Tech Zone using the following button:</p> <p></p> </li> <li> <p>Open Wireguard and import the downloaded file as a new VPN tunel.</p> </li> <li> <p>Active the VPN.</p> <p></p> </li> </ol>"},{"location":"airgapped/orig/#7-guacamole-vm","title":"7. Guacamole VM","text":"<p>Once we have configured and activated the VPN, we need to navigate to our VMware vSphere vCenter home page using the URL at the bottom of our IBM Tech Zone reservation. VMware vSphere vCenter login credentials can also be found on the same reservation.</p> <p></p> <p>Once we have successfully logged into our VMware vSphere vCenter, we should see the following screen:</p> <p></p> <p>In our VMware vSphere vCenter environment, there exists a unique Virtual Machine (VM), referred to as the Guacamole VM. This term does not pertain to OpenShift bastion nodes but rather denotes a specific VM managed by the IBM TechZone infrastructure team.</p> <p>The purpose of this Guacamole VM is to facilitate tasks requiring substantial internet bandwidth within the VMware vSphere vCenter network environment. By doing so, it avoids potential lag or slowdowns associated with VPN tunnel connections. Essentially, placing this VM on the vCenter network allows for rapid access to heavy components necessary for OpenShift installations directly from within the internal network.</p> <p>This setup significantly enhances efficiency when downloading large files or elements crucial for the OpenShift installation process at the VMware vSphere vCenter level. It ensures smoother and quicker data transfers compared to relying on VPN tunnel connections, which could otherwise lead to slower speeds and potential connectivity issues.</p> <p></p> <p>In order to create the bastion machines, we first must upload a RHEL 8.7 OS ISO file to the vCenter datastore. As previously explained, for this heavy task (~11GB), we will utilize the Guacamole VM.</p> <p>Important</p> <p>Due to potential issues, accessing the Guacamole VM through these three steps can be problematic. Persistence and multiple attempts are often necessary to successfully gain entry.</p> <ol> <li> <p>Open the Guacamole VM from your IBM TechZone reservation page:</p> <p></p> </li> <li> <p>This will open the following web app. Expand the All Connections section to see the two options for accessing the Guacamole VM. Use the Remote Desktop option:</p> <p></p> </li> <li> <p>Once you have gained access to the Guacamole VM, you should see a RHEL regular desktop.</p> <p></p> </li> <li> <p>Click on Activities at the top left corner of the desktop and click on Firefox to open the web browser.</p> </li> <li> <p>Download the ISO from Red Hat portal:</p> <ul> <li>Link to RH 8.7 SO ISO file</li> </ul> </li> <li> <p>Point the browser to the link in order to download the files.</p> </li> <li> <p>Once we have downloaded the ISO file, open the vCenter using the bookmark located at the bookmarks toolbar. You can find the credentials for the vCenter in your IBM TechZone reservation.</p> <p></p> </li> <li> <p>Upload the downloaded ISO file to the vCenter datastore to make it available for the bastion VMs we will create in later sections.</p> <p></p> <p>Info</p> <p>Name the folder created in step 3 as ISOs folder</p> </li> <li> <p>Once the ISO image have been fully uploaded to the DataStore, you will be able to see it listed.</p> <p></p> </li> </ol> <p>Tip</p> <p>We can continue to work with vCenter from our laptop's web browser. We should not need to work with the Guacamole VM anymore. </p>"},{"location":"airgapped/orig/#8-bastion-nodes","title":"8. Bastion Nodes","text":"<p>For this OpenShift installation, we'll employ two bastion nodes: an online bastion with internet access for downloading necessary components (images, executables, installation programs), and an offline bastion within our internal network without direct internet connectivity for transferring these components and executing the installation.</p> <p>First on our agenda is to establish both online and offline bastion nodes.</p>"},{"location":"airgapped/orig/#81-create-the-online-bastion-node","title":"8.1. Create the online bastion node","text":"<p>Now, we shall proceed to create the virtual machine designated as the online bastion, which will operate using RHEL 8.7 and feature internet connectivity.</p> Option Configuration FQDN hostname bastiononline.ocp4.platformengineers.xyz Static IP Address 192.168.252.22 Operating System Red Hat Enterprise Linux OS Version 8.7 OS Language English-US <ol> <li> <p>Click on the VMs tab. Then, right click on the folder assigned to our vCenter user and select \"New Virtual Machine\".</p> <p></p> </li> <li> <p>Click next on the first page to go to the page where we will assign a name to the bastion online virtual machine. In our case we will use <code>bastiononline.ocp4.platformengineers.xyz</code> for the name of the VM we are creating.</p> </li> <li> <p>Skip step 3.</p> </li> <li> <p>On the step 4, select datastore started with <code>gym-</code> and click next:</p> <p></p> </li> <li> <p>Skip the step 5.</p> </li> <li> <p>On the step 6, select the \"Guest OS Family\" as <code>Linux</code> and \"Guest OS Version\" as <code>Red Hat Enterprise Linux 8 (64-bit)</code> and click next.</p> <p></p> </li> <li> <p>On the step 7, specify the details of the virtual machine according to the table below</p> Type vCPU RAM Memory Storage (system) Storage (data) Bastion 8 vCPU 32 GB 700 GB 0 GB <p></p> <p>On \"New CD/DVD Drive\" select <code>Datastore ISO File</code> and then navigate to the ISO folder where we uploaded the RH 8.7 OS ISO file in the previous section and select such ISO file.</p> <p></p> <p>Make sure \"New CD/DVD Drive\" connect is checked and click next.</p> <p></p> </li> <li> <p>Finish the creation of the VM.</p> </li> </ol> <p>Tip</p> <p>We might need to click on the refresh arrow icon at the top center of the vCenter web application in order to see our newly created VM.</p> <p>Once the VM has been created successfully we must start the machine and proceed with the OS installation.</p> <ol> <li> <p>Select our newly created VM and click on the play button to start the VM.</p> <p></p> </li> <li> <p>Click on launch web console to proceed with the OS installation.</p> <p></p> </li> <li> <p>Click on the screen that will pop up and select install Red Hat OS.</p> <p></p> </li> <li> <p>Click Continue</p> <p></p> </li> <li> <p>Add disk.</p> <p></p> </li> <li> <p>Select <code>Custom</code> for the \"Storage Configuration\" and click done.</p> <p></p> </li> <li> <p>Click on the <code>Click here to create them automatically</code> link so that a default mount point and partition are created.</p> <p></p> </li> <li> <p>Set the amount of disk for the <code>/home</code> and <code>/</code> mount points as described in the picture below. This will allow us to have the required space for the things to be installed at the root mount point which is where the <code>root</code> user will inherit from.</p> <p></p> <p>Tip</p> <p>When we set the disk space for a mount point in the text box, we need to click on the <code>Update Settings</code> button that will get enabled to take effect.</p> </li> <li> <p>Click Done and Accept Changes on the message with the changes to be applied that will pop up.</p> <p></p> </li> <li> <p>On the main menu, click now on the Software Selection option at the bottom of the center options colunm. Then, select the \"Minimal install\" and click done.</p> <p></p> </li> <li> <p>Create the root credentials, for example <code>passw0rd</code> and click done.</p> </li> <li> <p>Click Begin Installation.</p> </li> <li> <p>When the installation finishes, click on Reboot System.</p> </li> </ol> <p>After the OS is installed and we restarted the server, login with root credentials (<code>root/passw0rd</code>). We will be able to connect using the web console link which will open a new tab in our browser. </p> <ol> <li> <p>Use Network Manager Tool UI (nmtui) for configuring the network properties.</p> [root@localhost ~]#<pre><code>nmtui\n</code></pre> </li> <li> <p>Click on Edit a connection</p> <p></p> </li> <li> <p>Select \"ens192\".</p> </li> <li>Use the arrows and intro to change the IPv4 CONFIGURATION from <code>&lt;Automatic&gt;</code> to <code>&lt;Manual&gt;</code></li> <li>Then, use the arrows and intro to <code>&lt;Show&gt;</code> details.</li> <li> <p>Fill the properties as shown in the pictures below</p> <p></p> </li> <li> <p>Use the arrows and space bar to select the <code>Automatically connect</code> option at the bottom.</p> <p></p> </li> <li> <p>Click <code>&lt;OK&gt;</code></p> </li> <li>Click <code>&lt;Back&gt;</code></li> <li>Click <code>&lt;OK&gt;</code></li> </ol>"},{"location":"airgapped/orig/#82-create-the-offline-bastion-node","title":"8.2. Create the offline bastion node","text":"<p>Follow the same steps from the previous section in order to create the offline bastion with the following specs:</p> Option Configuration FQDN hostname bastion.ocp4.platformengineers.xyz Static IP Address 192.168.252.23 Operating System Red Hat Enterprise Linux OS Version 8.7 OS Language English-US <p>We also need to edit the network configuration but this time we need to configure it so that the offline bastion does not have internet access but we can still connect to it via ssh. </p> <ol> <li> <p>We will use Network Manager Tool UI (nmtui) for configuring the network properties.</p> [root@bastion ~]<pre><code>nmtui\n</code></pre> </li> <li> <p>Click on Edit a connection</p> <p></p> </li> <li> <p>Select \"ens192\".</p> </li> <li>Use the arrows and intro to change the IPv4 CONFIGURATION from <code>&lt;Automatic&gt;</code> to <code>&lt;Manual&gt;</code></li> <li>Then, use the arrows and intro to <code>&lt;Show&gt;</code> details.</li> <li> <p>Fill the properties as shown in the pictures below</p> <p></p> </li> <li> <p>Configure the routing so that we simulate no internet connection:</p> <p></p> </li> <li> <p>Use the arrows and space bar to select the <code>Automatically connect</code> option at the bottom.</p> <p></p> </li> <li> <p>Click <code>&lt;OK&gt;</code></p> </li> <li>Click <code>&lt;OK&gt;</code></li> <li>Click <code>&lt;Back&gt;</code></li> <li>Click <code>&lt;OK&gt;</code></li> </ol> <p>After doing this configuration, try the following commands from the offline bastion terminal to check the connection to internet:</p> [root@localhost ~]<pre><code>ping 192.168.252.1\n</code></pre> Output<pre><code>PING 192.168.252.1 (192.168.252.1) 56(84) bytes of data.\n64 bytes from 192.168.252.1: icmp_seq=1 ttl=64 time=0.398 ms\n^C\n--- 192.168.252.1 ping statistics ---\n1 packets transmitted, 1 received, 0% packet loss, time 0ms\nrtt min/avg/max/mdev = 0.398/0.398/0.398/0.000 ms\n</code></pre> [root@localhost ~]<pre><code>ping 8.8.8.8\n</code></pre> Output<pre><code>connect: Network is unreachable\n</code></pre> [root@localhost ~]<pre><code>ping www.google.com\n</code></pre> Output<pre><code>ping: www.google.com: Name or service not known\n</code></pre> <p>Important</p> <p>From now on, we can use our own laptop's terminal to ssh into the different VMs and carry on with the OpenShift installation. We should not need to use the web console of the bastions from the vCenter UI.</p> <p>From your laptop's terminal, make sure you can ssh into the online bastion and offline bastion:</p> [student laptop]<pre><code>ssh root@192.168.252.22\n</code></pre> [student laptop]<pre><code>ssh root@192.168.253.22\n</code></pre>"},{"location":"airgapped/orig/#9-disable-selinux-in-the-online-bastion","title":"9. Disable SELINUX in the online bastion","text":"<p>In order to perform the correct configuration of the bastion node, it is recommended that the <code>SELINUX</code> is deactivated first.</p> <p>Note</p> <p>It is not strictly necessary that Linux Security is deactivated in order to install OpenShift. However, it is recommended as it eases the installation. Otherwise, you would need to open several specific ports and add rules to the firewall so that the needed communication is allowed.</p> <ol> <li> <p>Access the online bastion:</p> [student laptop]<pre><code>ssh root@192.168.252.22\n</code></pre> </li> <li> <p>Open the <code>/etc/selinux/config</code> to disable the <code>SELINUX</code></p> [root@localhost ~]<pre><code>vi /etc/selinux/config\n</code></pre> </li> <li> <p>Make sure your file looks like below, where the highlighted line has been modified:</p> /etc/selinux/config<pre><code># This file controls the state of SELinux on the system.\n# SELINUX= can take one of these three values:\n# enforcing - SELinux security policy is enforced.\n# permissive - SELinux prints warnings instead of enforcing.\n# disabled - No SELinux policy is loaded.\nSELINUX=disabled\n# SELINUXTYPE= can take one of these three values:\n# targeted - Targeted processes are protected,\n# minimum - Modification of targeted policy. Only selected processes are protected.\n# mls - Multi Level Security protection. SELINUXTYPE=targeted\n</code></pre> <p>Once the change is made, save and exit the file (<code>esc</code> and <code>:wq</code>) </p> </li> <li> <p>Restart the server for the changes to take effect.</p> [root@localhost ~]<pre><code>init 6\n</code></pre> </li> </ol>"},{"location":"airgapped/orig/#10-disable-firewall-in-the-online-bastion","title":"10. Disable firewall in the online bastion","text":"<p>Also, and since it is an internal network, the firewall of the machine is deactivated</p> <ol> <li> <p>Access the online bastion:</p> [student laptop]<pre><code>ssh root@192.168.252.22\n</code></pre> </li> <li> <p>Stop the firewall:</p> [root@localhost ~]<pre><code>systemctl stop firewalld\nsystemctl disable firewalld\n</code></pre> Output<pre><code>Removed /etc/systemd/system/multi-user.target.wants/firewalld.service. \nRemoved /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service.\n</code></pre> </li> </ol>"},{"location":"airgapped/orig/#11-configure-the-hostname-for-the-online-bastion","title":"11. Configure the hostname for the online bastion","text":"<p>We need to change the online bastion machine's default hostname as we will be referring to this machine from the DNS, load balancer, etc using a different hostname.</p> <ol> <li> <p>Edit the hostname of the machine</p> [root@localhost ~]<pre><code>vi /etc/hostname\n</code></pre> <p>set the hostname to <code>bastiononline.ocp4.platformengineers.xyz</code></p> </li> <li> <p>Once the change has been made, restart the server for it to take effect.</p> [root@localhost ~]<pre><code>init 6\n</code></pre> </li> <li> <p>Access the online bastion again:</p> [student laptop]<pre><code>ssh root@192.168.252.22\n</code></pre> </li> <li> <p>Check the hostname was successfully modified:</p> [root@bastiononline ~]<pre><code>ping bastiononline.ocp4.platformengineers.xyz\n</code></pre> <p>You should see similar output as below:</p> Output<pre><code>PING bastiononline.ocp4.platformengineers.xyz(bastiononline.ocp4.platformengineers.xyz (fe80::250:56ff:fe8a:3e00%ens192)) 56 data bytes\n64 bytes from bastiononline.ocp4.platformengineers.xyz (fe80::250:56ff:fe8a:3e00%ens192): icmp_seq=1 ttl=64 time=0.036 ms\n64 bytes from bastiononline.ocp4.platformengineers.xyz (fe80::250:56ff:fe8a:3e00%ens192): icmp_seq=2 ttl=64 time=0.040 ms\n64 bytes from bastiononline.ocp4.platformengineers.xyz (fe80::250:56ff:fe8a:3e00%ens192): icmp_seq=3 ttl=64 time=0.042 ms\n</code></pre> </li> <li> <p>Finish the ping command with <code>ctrl+c</code></p> </li> </ol>"},{"location":"airgapped/orig/#12-install-required-redhat-packages-in-the-online-bastion","title":"12. Install required RedHat packages in the online bastion","text":"<p>For the installation of rpm packages on the online machine, the RHEL 8.7 OS ISO file that we created the VM from will help us.</p> <p>Note</p> <p>We can skip this step if we register the machine with a Red Hat subscription, which falls outside this workshop.</p> <ol> <li> <p>Make sure that, in the vCenter, the online bastion has the CD/DVD drive as connected:</p> <p></p> </li> <li> <p>The purpose of having the CD/DVD drive connected is that we can now mount this drive in the file system:</p> [root@bastiononline ~]<pre><code>mkdir /mnt/disc\nmount /dev/sr0 /mnt/disc\n</code></pre> Output<pre><code>mount: /mnt/disc: WARNING: device write-protected, mounted read-only.\n</code></pre> </li> <li> <p>Create the following following <code>rhel87dvd.repo</code> file that will define our RHEL OS repos:</p> [root@bastiononline ~]<pre><code>vi /etc/yum.repos.d/rhel87dvd.repo\n</code></pre> rhel87dvd.repo<pre><code>[InstallMediaBase]\nname=Red Hat Enterprise Linux 8.7.0 Base\nmediaid=None\nmetadata_expire=-1\ngpgcheck=0\ncost=500\nenabled=1\nbaseurl=file:///mnt/disc/BaseOS\n\n[InstallMediaApps]\nname=Red Hat Enterprise Linux 8.7.0 App\nmediaid=None\nmetadata_expire=-1\ngpgcheck=0\ncost=500\nenabled=1\nbaseurl=file:///mnt/disc/AppStream\n[InstallMediaParches]\nname=Red Hat Enterprise Linux KErnel\nmediaid=None\nmetadata_expire=-1\ngpgcheck=0\ncost=500\nenabled=0\nbaseurl=file:///mnt/parches\n</code></pre> </li> <li> <p>Make sure the cache is cleaned:</p> [root@bastiononline ~]<pre><code>yum clean all\n</code></pre> Output<pre><code>Updating Subscription Management repositories.\nUnable to read consumer identity\n\nThis system is not registered with an entitlement server. You can use subscription-manager to register.\n\n0 files removed\n</code></pre> </li> <li> <p>Make sure the new repositories are enabled:</p> [root@bastiononline ~]<pre><code>yum repolist enabled\n</code></pre> Output<pre><code>Updating Subscription Management repositories.\nUnable to read consumer identity\n\nThis system is not registered with an entitlement server. You can use subscription-manager to register.\n\nrepo id                                   repo name\nInstallMediaApps                          Red Hat Enterprise Linux 8.7.0 App\nInstallMediaBase                          Red Hat Enterprise Linux 8.7.0 Base\n</code></pre> </li> </ol> <p>Once you have created the offline repository and confirmed it is enabled, the required packages can be installed with the yum utility.</p> <p>The following packages have been installed on the online bastion server. Some packages may not be used during the execution of online commands, but they are installed so that the \"online\" and \"offline\" bastion settings are as identical as possible.</p> [root@bastiononline ~]<pre><code>yum install -y  podman \\\n                jq openssl httpd-tools curl wget telnet nfs-utils \\\n                httpd.x86_64 \\\n                bind bind-utils rsync mkisofs\n</code></pre> Output<pre><code>Updating Subscription Management repositories.\nUnable to read consumer identity\n\nThis system is not registered with an entitlement server. You can use subscription-manager to register.\n\nRed Hat Enterprise Linux 8.7.0 Base                                                                                          40 MB/s | 2.4 MB     00:00    \nRed Hat Enterprise Linux 8.7.0 App                                                                                           53 MB/s | 7.8 MB     00:00    \nPackage openssl-1:1.1.1k-7.el8_6.x86_64 is already installed.\nPackage curl-7.61.1-25.el8.x86_64 is already installed.\nDependencies resolved.\n============================================================================================================================================================\n Package                                   Architecture        Version                                                  Repository                     Size\n============================================================================================================================================================\nInstalling:\n bind                                      x86_64              32:9.11.36-5.el8                                         InstallMediaApps              2.1 M\n bind-utils                                x86_64              32:9.11.36-5.el8                                         InstallMediaApps              452 k\n genisoimage                               x86_64              1.1.11-39.el8                                            InstallMediaApps              316 k\n httpd                                     x86_64              2.4.37-51.module+el8.7.0+16050+02173b8e                  InstallMediaApps              1.4 M\n httpd-tools                               x86_64              2.4.37-51.module+el8.7.0+16050+02173b8e                  InstallMediaApps              109 k\n jq                                        x86_64              1.6-3.el8                                                InstallMediaApps              202 k\n nfs-utils                                 x86_64              1:2.3.3-57.el8                                           InstallMediaBase              515 k\n podman                                    x86_64              3:4.2.0-1.module+el8.7.0+16772+33343656                  InstallMediaApps               12 M\n rsync                                     x86_64              3.1.3-19.el8                                             InstallMediaBase              410 k\n telnet                                    x86_64              1:0.17-76.el8                                            InstallMediaApps               72 k\n wget                                      x86_64              1.19.5-10.el8                                            InstallMediaApps              734 k\n...\n...\nComplete!\n</code></pre> <p>Finally, we need to install Ansible on the system. However, this package is not included in the RHEL repositories configured previously. As a result, we will need to install it manually.</p> <ol> <li> <p>Download the Ansible RPM package to your laptop from the Red Hat portal.</p> <ul> <li>Link to Ansible rpm</li> </ul> </li> <li> <p>Source copy the rpm package to the online bastion</p> [student laptop]<pre><code>scp ansible-2.9.27-1.el8ap.noarch.rpm root@192.168.252.22:/root\n</code></pre> </li> <li> <p>Install it:</p> [root@bastiononline ~]<pre><code>yum localinstall -y ansible-2.9.27-1.el8ap.noarch.rpm\n</code></pre> Output<pre><code>Updating Subscription Management repositories.\nUnable to read consumer identity\n\nThis system is not registered with an entitlement server. You can use subscription-manager to register.\n\nLast metadata expiration check: 0:09:27 ago on Mon 06 Nov 2023 05:40:03 PM CET.\nDependencies resolved.\n============================================================================================================================================================\nPackage                                  Architecture          Version                                               Repository                       Size\n============================================================================================================================================================\nInstalling:\nansible                                  noarch                2.9.27-1.el8ap                                        @commandline                     17 M\n...\n...\nComplete!\n</code></pre> </li> </ol>"},{"location":"airgapped/orig/#13-download-openshift-utilities-to-the-online-bastion","title":"13. Download OpenShift utilities to the online bastion","text":"<p>A directory structure is created to accommodate the different binaries that are downloaded.</p> <ul> <li> <p>Create a directory for the registry and create the necessary directories for the registry.</p> [root@bastiononline ~]<pre><code>mkdir /root/registry\ncd registry/\n</code></pre> [root@bastiononline registry]<pre><code>mkdir auth certs data downloads\nls -lart\n</code></pre> Output<pre><code>total 4\ndr-xr-x---. 7 root root 4096 Sep  8 07:43 ..\ndrwxr-xr-x  2 root root    6 Sep  8 07:48 downloads\ndrwxr-xr-x  2 root root    6 Sep  8 07:48 data\ndrwxr-xr-x  2 root root    6 Sep  8 07:48 certs\ndrwxr-xr-x  2 root root    6 Sep  8 07:48 auth\ndrwxr-xr-x  6 root root   60 Sep  8 07:48 .\n</code></pre> [root@bastiononline registry]<pre><code>cd downloads/\n</code></pre> [root@bastiononline downloads]<pre><code>mkdir images tools secrets\nls -lart\n</code></pre> Output<pre><code>total 0\ndrwxr-xr-x 6 root root 60 Sep  8 07:48 ..\ndrwxr-xr-x 2 root root  6 Sep  8 07:59 tools\ndrwxr-xr-x 2 root root  6 Sep  8 07:59 secrets\ndrwxr-xr-x 2 root root  6 Sep  8 07:59 images\ndrwxr-xr-x 5 root root 48 Sep  8 07:59 .\n</code></pre> </li> </ul> <p>The folder structure that should have got created should look like this:</p> Directory structure<pre><code>/root\n`-- registry\n    |-- auth\n    |-- certs\n    |-- data\n    `-- downloads\n        |-- images\n        |-- secrets\n        `-- tools\n</code></pre> <p>For the OpenShift client, the following steps have been followed:</p> <ul> <li> <p>OpenShift client download:</p> [root@bastiononline downloads]<pre><code>cd tools/\n</code></pre> [root@bastiononline tools]<pre><code>wget https://mirror.openshift.com/pub/openshift-v4/clients/ocp/4.16.9/openshift-client-linux-amd64-rhel8-4.16.9.tar.gz\n</code></pre> </li> <li> <p>Extracting the contents of the downloaded file:</p> [root@bastiononline tools]<pre><code>tar -xvf openshift-client-linux-amd64-rhel8-4.16.9.tar.gz\n</code></pre> Output<pre><code>README.md\noc\nkubectl\n</code></pre> [root@bastiononline tools]<pre><code>ls -lart\n</code></pre> Output<pre><code>total 1156008\n-rwxr-xr-x 2 root root  159905720 Aug 21 01:54 oc\n-rwxr-xr-x 2 root root  159905720 Aug 21 01:54 kubectl\n-rw-r--r-- 1 root root        950 Aug 21 01:54 README.md\n-rw-r--r-- 1 root root   66705673 Aug 22 22:05 openshift-client-linux-amd64-rhel8-4.16.9.tar.gz\ndrwxr-xr-x 5 root root         48 Sep 21 07:55 ..\ndrwxr-xr-x 2 root root        164 Sep 21 11:19 .\n</code></pre> </li> <li> <p>Copy to a location accessible from your $PATH (e.g. /usr/bin):</p> [root@bastiononline tools]<pre><code>cp /root/registry/downloads/tools/oc /usr/bin/oc\n</code></pre> </li> </ul> <p>For the OpenShift installer, the following steps should be followed:</p> <ul> <li> <p>Downloading the OpenShift Installer</p> [root@bastiononline tools]<pre><code>wget https://mirror.openshift.com/pub/openshift-v4/clients/ocp/4.16.9/openshift-install-linux-4.16.9.tar.gz\n</code></pre> </li> <li> <p>Extract the contents of the downloaded file:</p> [root@bastiononline tools]<pre><code>tar -xvf openshift-install-linux-4.16.9.tar.gz\n</code></pre> Output<pre><code>README.md\nopenshift-install\n</code></pre> [root@bastiononline tools]<pre><code>ls -lart\n</code></pre> Output<pre><code>total 1156008\n-rwxr-xr-x 2 root root  159905720 Aug 21 01:54 oc\n-rwxr-xr-x 2 root root  159905720 Aug 21 01:54 kubectl\n-rwxr-xr-x 1 root root  707731456 Aug 21 03:44 openshift-install\n-rw-r--r-- 1 root root        706 Aug 21 03:44 README.md\n-rw-r--r-- 1 root root   66705673 Aug 22 22:05 openshift-client-linux-amd64-rhel8-4.16.9.tar.gz\n-rw-r--r-- 1 root root  510253136 Aug 22 22:05 openshift-install-linux-4.16.9.tar.gz\ndrwxr-xr-x 5 root root         48 Sep 21 07:55 ..\ndrwxr-xr-x 2 root root        164 Sep 21 11:19 .\n</code></pre> </li> <li> <p>Copy to a location accessible from your $PATH (e.g. /usr/bin):</p> [root@bastiononline tools]<pre><code>cp /root/registry/downloads/tools/openshift-install /usr/bin/openshift-install\n</code></pre> </li> </ul> <p>For the Mirror registry, the following steps should be followed:</p> <ul> <li> <p>Download the mirror-registry</p> [root@bastiononline tools]<pre><code>wget https://mirror.openshift.com/pub/cgw/mirror-registry/latest/mirror-registry-amd64.tar.gz\n</code></pre> </li> <li> <p>Extracting the contents of the downloaded file:</p> [root@bastiononline tools]<pre><code>tar -xvf mirror-registry-amd64.tar.gz\n</code></pre> Output<pre><code>image-archive.tar\nexecution-environment.tar\nmirror-registry\n</code></pre> [root@bastiononline tools]<pre><code>ls -lart\n</code></pre> Output<pre><code>total 4019476\n-rwxr-xr-x 2 root root  160465008 Aug 21 00:49 oc\n-rwxr-xr-x 2 root root  160465008 Aug 21 00:49 kubectl\n-rwxr-xr-x 1 root root  707731456 Aug 21 03:44 openshift-install\n-rw-r--r-- 1 root root        706 Aug 21 03:44 README.md\n-rw-r--r-- 1 root root   66941882 Aug 22 22:05 openshift-client-linux-4.16.9.tar.gz\n-rw-r--r-- 1 root root  510253136 Aug 22 22:05 openshift-install-linux-4.16.9.tar.gz\n-rwxr-xr-x 1 root root    9824888 Sep 23 18:46 mirror-registry\n-rw-r--r-- 1 root root  113418240 Sep 23 18:47 sqlite3.tar\n-rw-r--r-- 1 root root  316753920 Sep 23 18:50 execution-environment.tar\n-rw-r--r-- 1 root root 1454929920 Sep 23 18:50 image-archive.tar\n-rw-r--r-- 1 root root  607056480 Sep 26 11:44 mirror-registry-amd64.tar.gz\ndrwxr-xr-x 5 root root         48 Oct 24 19:16 ..\ndrwxr-xr-x 2 root root       4096 Oct 24 19:41 .\n</code></pre> </li> </ul> <p>For the oc-mirror plugin, the following steps should be followed:</p> <ul> <li> <p>Download the oc mirror program to make mirror of OCP and its operators</p> [root@bastiononline tools]<pre><code>wget https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/ocp/4.16.19/oc-mirror.tar.gz\n</code></pre> </li> <li> <p>Extracting the contents of the downloaded file:</p> [root@bastiononline tools]<pre><code>tar xvfz oc-mirror.tar.gz\n</code></pre> Output<pre><code>oc-mirror\n</code></pre> [root@bastiononline tools]<pre><code>ls -lart\n</code></pre> Output<pre><code>total 4245772\n-rwxr-xr-x 2 root root  159905720 Aug 21 01:54 oc\n-rwxr-xr-x 2 root root  159905720 Aug 21 01:54 kubectl\n-rwxr-xr-x 1 root root  707731456 Aug 21 03:44 openshift-install\n-rw-r--r-- 1 root root        706 Aug 21 03:44 README.md\n-rw-r--r-- 1 root root   66705673 Aug 22 22:05 openshift-client-linux-amd64-rhel8-4.16.9.tar.gz\n-rw-r--r-- 1 root root  510253136 Aug 22 22:05 openshift-install-linux-4.16.9.tar.gz\n-rwxr-xr-x 1 root root    9824888 Sep 23 18:46 mirror-registry\n-rw-r--r-- 1 root root  113418240 Sep 23 18:47 sqlite3.tar\n-rw-r--r-- 1 root root  316753920 Sep 23 18:50 execution-environment.tar\n-rw-r--r-- 1 root root 1454929920 Sep 23 18:50 image-archive.tar\n-rw-r--r-- 1 root root  607056480 Sep 26 11:44 mirror-registry-amd64.tar.gz\n-rw-r----- 1  984  984  165699280 Oct 18 02:15 oc-mirror\n-rw-r--r-- 1 root root   67385243 Oct 25 03:02 oc-mirror.tar.gz\ndrwxr-xr-x 5 root root         48 Oct 24 19:16 ..\ndrwxr-xr-x 2 root root       4096 Oct 25 09:22 .\n</code></pre> </li> <li> <p>Copy to a location accessible from your $PATH (e.g. /usr/local/bin):</p> [root@bastiononline tools]<pre><code>chmod +x /root/registry/downloads/tools/oc-mirror\ncp /root/registry/downloads/tools/oc-mirror /usr/local/bin/\noc-mirror help\n</code></pre> </li> </ul> <p>For the Butane tool, the following steps should be followed:</p> <ul> <li> <p>Download butane</p> [root@bastiononline tools]<pre><code>curl https://mirror.openshift.com/pub/openshift-v4/clients/butane/latest/butane-amd64 --output butane\n</code></pre> [root@bastiononline tools]<pre><code>ls -lart\n</code></pre> Output<pre><code>total 4019476\n-rwxr-xr-x 2 root root  160465008 Aug 21 00:49 oc\n-rwxr-xr-x 2 root root  160465008 Aug 21 00:49 kubectl\n-rwxr-xr-x 1 root root  707731456 Aug 21 03:44 openshift-install\n-rw-r--r-- 1 root root        706 Aug 21 03:44 README.md\n-rw-r--r-- 1 root root   66941882 Aug 22 22:05 openshift-client-linux-4.16.9.tar.gz\n-rw-r--r-- 1 root root  510253136 Aug 22 22:05 openshift-install-linux-4.16.9.tar.gz\n-rwxr-xr-x 1 root root    9824888 Sep 23 18:46 mirror-registry\n-rw-r--r-- 1 root root  113418240 Sep 23 18:47 sqlite3.tar\n-rw-r--r-- 1 root root  316753920 Sep 23 18:50 execution-environment.tar\n-rw-r--r-- 1 root root 1454929920 Sep 23 18:50 image-archive.tar\n-rw-r--r-- 1 root root  607056480 Sep 26 11:44 mirror-registry-amd64.tar.gz\ndrwxr-xr-x 5 root root         48 Oct 24 19:16 ..\ndrwxr-xr-x 2 root root       4096 Oct 24 19:41 .\n-rw-r--r-- 1 root root    8070568 Oct 24 19:41 butane\n</code></pre> </li> </ul> <p>For the ISO Maker tool, the following steps should be followed:</p> <ul> <li> <p>Download the ISO Maker</p> [root@bastiononline ~]<pre><code>cd /root/\nmkdir Coreos-iso-maker\ncd Coreos-iso-maker/\n</code></pre> [root@bastiononline Coreos-iso-maker]<pre><code>yum install -y git\n</code></pre> Output<pre><code>Updating Subscription Management repositories.\nUnable to read consumer identity\n\nThis system is not registered with an entitlement server. You can use subscription-manager to register.\n\nLast metadata expiration check: 0:30:40 ago on Mon 06 Nov 2023 05:40:03 PM CET.\nDependencies resolved.\n============================================================================================================================================================\nPackage                               Architecture          Version                                                  Repository                       Size\n============================================================================================================================================================\nInstalling:\n git                                   x86_64                2.31.1-2.el8                                             InstallMediaApps                161 k\n...\n...\nComplete!\n</code></pre> [root@bastiononline Coreos-iso-maker]<pre><code>git clone https://github.com/chuckersjp/coreos-iso-maker\n</code></pre> Output<pre><code>Cloning into 'coreos-iso-maker'...\nremote: Enumerating objects: 391, done.\nremote: Counting objects: 100% (129/129), done.\nremote: Compressing objects: 100% (37/37), done.\nremote: Total 391 (delta 116), reused 93 (delta 92), pack-reused 262\nReceiving objects: 100% (391/391), 70.81 KiB | 1.16 MiB/s, done.\nResolving deltas: 100% (246/246), done.\n</code></pre> [root@bastiononline Coreos-iso-maker]<pre><code>ls -lart\n</code></pre> Output<pre><code>total 8\ndr-xr-x---. 8 root root 4096 Sep  8 11:40 ..\ndrwxr-xr-x  3 root root   30 Sep  8 11:42 .\ndrwxr-xr-x  4 root root 4096 Sep  8 11:42 coreos-iso-maker\n</code></pre> </li> </ul>"},{"location":"airgapped/orig/#14-optional-expand-bastion-boot-disk","title":"14. (optional) Expand bastion boot disk","text":"<p>Info</p> <p>You dont need to complete this section. It is only for reference to understand what you would need to do should you require to increase disk space on any of your bastion machines.</p> <p>In case you need to increase the bastion online\u2019s disk size because at the creation moment you didn\u2019t set the correct amount of storage, you must follow the next steps in vCenter with the vm machine stopped:</p> <p></p> <p>Set the correct space you need on the disk and then start the machine and follow these commands to create a new partition:</p> <ol> <li> <p>Access the online bastion:</p> [student laptop]<pre><code>ssh root@192.168.252.22\n</code></pre> </li> <li> <p>Check the actual partitions size</p> [root@bastiononline ~]<pre><code>df -h\n</code></pre> Output<pre><code>Filesystem             Size  Used Avail Use% Mounted on\ndevtmpfs                16G     0   16G   0% /dev\ntmpfs                   16G     0   16G   0% /dev/shm\ntmpfs                   16G  8.7M   16G   1% /run\ntmpfs                   16G     0   16G   0% /sys/fs/cgroup\n/dev/mapper/rhel-root  621G   12G  610G   2% /\n/dev/sda2             1014M  211M  804M  21% /boot\n/dev/sda1              599M  5.8M  594M   1% /boot/efi\n/dev/mapper/rhel-home   62G  473M   62G   1% /home\ntmpfs                  3.2G     0  3.2G   0% /run/user/0\n</code></pre> </li> <li> <p>List the partition tables</p> [root@bastiononline ~]<pre><code>fdisk -l\n</code></pre> Output<pre><code>Register this system with Red Hat Insights: insights-client --register\nCreate an account or view all your systems at https://red.ht/insights-dashboard\nLast login: Mon Nov  6 18:32:23 2023 from 192.168.253.2\n[root@bastiononline ~]# fdisk -l\nDisk /dev/sda: 1 TiB, 1099511627776 bytes, 2147483648 sectors\nUnits: sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisklabel type: gpt\nDisk identifier: 68D049D0-0D01-44BA-AF0F-9371047971AE\n\nDevice          Start        End    Sectors   Size Type\n/dev/sda1        2048    1230847    1228800   600M EFI System\n/dev/sda2     1230848    3327999    2097152     1G Linux filesystem\n/dev/sda3     3328000 1468004351 1464676352 698.4G Linux LVM\n/dev/sda4  1468004352 2147483614  679479263   324G Linux filesystem\n\n\nDisk /dev/mapper/rhel-root: 621 GiB, 666793672704 bytes, 1302331392 sectors\nUnits: sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\n\n\nDisk /dev/mapper/rhel-swap: 15.8 GiB, 16924016640 bytes, 33054720 sectors\nUnits: sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\n\n\nDisk /dev/mapper/rhel-home: 61.7 GiB, 66194505728 bytes, 129286144 sectors\nUnits: sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\n</code></pre> </li> <li> <p>Start the disk partition table tool for <code>/dev/sda</code></p> [root@bastiononline ~]<pre><code>fdisk /dev/sda\n</code></pre> <p>Note</p> <p>We select <code>n</code> in the first command. Then, we press enter to leave the defaults. Finally, for the second command, we select <code>w</code> (to write the partitions)</p> Output<pre><code>Welcome to fdisk (util-linux 2.32.1).\nChanges will remain in memory only, until you decide to write them.\nBe careful before using the write command.\n\n\nCommand (m for help): n\nPartition number (4-128, default 4): \nFirst sector (1468004352-2147483614, default 1468004352): \nLast sector, +sectors or +size{K,M,G,T,P} (1468004352-2147483614, default 2147483614): \n\nCreated a new partition 4 of type 'Linux filesystem' and of size 324 GiB.\n\nCommand (m for help): w\nThe partition table has been altered.\nSyncing disks.\n</code></pre> </li> <li> <p>Expand the logical volume that has the operating system boot with the following commands</p> [root@bastiononline ~]<pre><code>pvcreate /dev/sda4\n</code></pre> Output<pre><code>Physical volume \"/dev/sda4\" successfully created.\n</code></pre> [root@bastiononline ~]<pre><code>vgs\n</code></pre> Output<pre><code>VG   #PV #LV #SN Attr   VSize   VFree\nrhel   1   3   0 wz--n- 698.41g    0\n</code></pre> [root@bastiononline ~]<pre><code>vgextend rhel /dev/sda4\n</code></pre> Output<pre><code>Volume group \"rhel\" successfully extended\n</code></pre> [root@bastiononline ~]<pre><code>lvs\n</code></pre> Output<pre><code>LV   VG   Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert\nhome rhel -wi-ao---- &lt;61.65g                                                    \nroot rhel -wi-ao---- 621.00g                                                    \nswap rhel -wi-ao----  15.76g\n</code></pre> [root@bastiononline ~]<pre><code>lvextend -l +100%FREE /dev/rhel/root\n</code></pre> Output<pre><code>Size of logical volume rhel/root changed from 621.00 GiB (158976 extents) to &lt;945.00 GiB (241919 extents).\nLogical volume rhel/root successfully resized.\n</code></pre> [root@bastiononline ~]<pre><code>lsblk -f\n</code></pre> Output<pre><code>NAME          FSTYPE      LABEL                    UUID                                   MOUNTPOINT\nsda                                                                                       \n|-sda1        vfat                                 6793-7237                              /boot/efi\n|-sda2        xfs                                  c43ee8e7-77b7-40af-8367-ba7ef90fcbb5   /boot\n|-sda3        LVM2_member                          kOrAn9-K2h6-RbB9-gINB-pWNL-dxVB-VkxYzZ \n| |-rhel-root xfs                                  ba1bba3c-dea8-4e2e-9882-fd866f6d13e8   /\n| |-rhel-swap swap                                 868795de-6965-44bd-aee3-3181691531f7   [SWAP]\n| `-rhel-home xfs                                  2bb2a05b-9811-4c10-ba3b-f1ab8084460a   /home\n`-sda4        LVM2_member                          9apLnq-zxPM-HNZ3-ncc7-jgwZ-hhAg-T3GVk6 \n  `-rhel-root xfs                                  ba1bba3c-dea8-4e2e-9882-fd866f6d13e8   /\nsr0           iso9660     RHEL-8-7-0-BaseOS-x86_64 2022-10-13-06-03-04-00\n</code></pre> [root@bastiononline ~]<pre><code>xfs_growfs /dev/rhel/root\n</code></pre> Output<pre><code>meta-data=/dev/mapper/rhel-root  isize=512    agcount=4, agsize=40697856 blks\n        =                       sectsz=512   attr=2, projid32bit=1\n        =                       crc=1        finobt=1, sparse=1, rmapbt=0\n        =                       reflink=1    bigtime=0 inobtcount=0\ndata     =                       bsize=4096   blocks=162791424, imaxpct=25\n        =                       sunit=0      swidth=0 blks\nnaming   =version 2              bsize=4096   ascii-ci=0, ftype=1\nlog      =internal log           bsize=4096   blocks=79488, version=2\n        =                       sectsz=512   sunit=0 blks, lazy-count=1\nrealtime =none                   extsz=4096   blocks=0, rtextents=0\ndata blocks changed from 162791424 to 247725056\n</code></pre> [root@bastiononline ~]<pre><code>df -h\n</code></pre> Output<pre><code>Filesystem             Size  Used Avail Use% Mounted on\ndevtmpfs                16G     0   16G   0% /dev\ntmpfs                   16G     0   16G   0% /dev/shm\ntmpfs                   16G  8.7M   16G   1% /run\ntmpfs                   16G     0   16G   0% /sys/fs/cgroup\n/dev/mapper/rhel-root  945G   14G  932G   2% /\n/dev/sda2             1014M  211M  804M  21% /boot\n/dev/sda1              599M  5.8M  594M   1% /boot/efi\n/dev/mapper/rhel-home   62G  473M   62G   1% /home\ntmpfs                  3.2G     0  3.2G   0% /run/user/0\n</code></pre> </li> </ol> <p>Repeat on the offline bastion if required.</p>"},{"location":"airgapped/orig/#15-download-the-red-hat-coreos-images-to-the-online-bastion","title":"15. Download the Red Hat CoreOS images to the online bastion","text":"<p>We need to download the following CoreOS images for the installation of the different OpenShift nodes: </p> [root@bastiononline ~]<pre><code>cd /root/registry/downloads/images/\n</code></pre> [root@bastiononline images]<pre><code>wget https://mirror.openshift.com/pub/openshift-v4/x86_64/dependencies/rhcos/4.16/4.16.3/rhcos-4.16.3-x86_64-metal.x86_64.raw.gz\nwget https://mirror.openshift.com/pub/openshift-v4/x86_64/dependencies/rhcos/4.16/4.16.3/rhcos-4.16.3-x86_64-live.x86_64.iso\n</code></pre> [root@bastiononline images]<pre><code>ls -lart\n</code></pre> Output<pre><code>total 2363592\n-rw-r--r-- 1 root root 1213202432 Apr  4  2023 rhcos-4.16.3-x86_64-live.x86_64.iso\n-rw-r--r-- 1 root root 1207111745 Apr  4  2023 rhcos-4.16.3-x86_64-metal.x86_64.raw.gz\ndrwxr-xr-x 5 root root         48 Nov  6 17:57 ..\ndrwxr-xr-x 2 root root         98 Nov  6 20:40 .\n</code></pre>"},{"location":"airgapped/orig/#16-mirror-images-on-the-online-bastion","title":"16. Mirror images on the online bastion","text":"<p>For accessing your Red Hat OpenShift pull secret \u2013 containing credentials that allow for downloading OpenShift and associated artifacts \u2013 please proceed as follows:</p> <p>https://cloud.redhat.com/openshift/install/pull-secret</p> <p>and download or copy the contents of the pull secret. </p> <p> </p> <ol> <li>Open the link above and copy the contents of the Red Hat OpenShift pull secret</li> <li> <p>Paste the contents of the Red Hat OpenShift pull secret into a file.</p> [root@bastiononline images]<pre><code>cd /root/registry/downloads/secrets/\n</code></pre> [root@bastiononline secrets]<pre><code>vi pull-secret.txt\n</code></pre> Output<pre><code>{\"auths\":{\"cloud.openshift.com\":{\"auth\":\"&lt;auth_secret&gt;\",\"email\":\"Miguel.Ariza.Colmenares2@ibm.com\"},\"quay.io\":{\"auth\":\" [\u2026\u2026\u2026\u2026\u2026]TdUpHajdDRGJfUUNFUk9ncjJ4Qi01V0pUVUNWenpLcmJlZU5yWGo0Y2ZTTkYzVWU3NC05N3hQMVBvQjBCYmRiOEhMemgwc0NfYjdtNzlkVDVmVGRyY3M2ZENqZzV4dEZNWnhlTTFTWEg0UWJqeDJYb2xUQ2RmekRfNzRhOXRueGVLMnFHaFZPZVVnXzBSY2t5MXZQUzVFMjFvMA==\",\"email\":\"Miguel.Ariza.Colmenares2@ibm.com\"}}}\n</code></pre> </li> <li> <p>Make a copy of your pull secret in JSON format:</p> [root@bastiononline secrets]<pre><code>cat ./pull-secret.txt | jq . &gt; ./pull-secret.json\n</code></pre> <p>The contents of the file resemble the following example:</p> Output<pre><code>{\n\"auths\": {\n    \"cloud.openshift.com\": {\n    \"auth\": \"b3BlbnNo...\",\n    \"email\": \"you@example.com\"\n    },\n    \"quay.io\": {\n    \"auth\": \"b3BlbnNo...\",\n    \"email\": \"you@example.com\"\n    },\n    \"registry.connect.redhat.com\": {\n    \"auth\": \"NTE3Njg5Nj...\",\n    \"email\": \"you@example.com\"\n    },\n    \"registry.redhat.io\": {\n    \"auth\": \"NTE3Njg5Nj...\",\n    \"email\": \"you@example.com\"\n    }\n}\n}\n</code></pre> </li> </ol> <p>We need to copy the contents of the file to the container runtime directory so that it is accessible by the image download procedure. Save the file either as </p> <ul> <li><code>~/.docker/config.json</code> or </li> <li><code>$XDG_RUNTIME_DIR/containers/auth.json</code></li> </ul> [root@bastiononline secrets]<pre><code>echo $XDG_RUNTIME_DIR\n</code></pre> Output<pre><code>/run/user/0\n</code></pre> [root@bastiononline secrets]<pre><code>mkdir /run/user/0/containers\ncp /root/registry/downloads/secrets/pull-secret.json /run/user/0/containers/auth.json\nls -lart /run/user/0/containers/auth.json\n</code></pre> Output<pre><code>-rw-r--r-- 1 root root 2943 Sep 21 12:31 /run/user/0/containers/auth.json\n</code></pre> <p>Change directory in order to mirror the images</p> [root@bastiononline secrets]<pre><code>cd /root/registry/\n</code></pre> <p>Use the <code>oc mirror init</code> command to create a template for the image set configuration and save it to a file called <code>imageset-config.yaml</code>:</p> [root@bastiononline secrets]<pre><code>cd /root/registry/\n</code></pre> [root@bastiononline registry]<pre><code>oc mirror init  &gt; imageset-config.yaml\nls -lart\n</code></pre> Output<pre><code>total 12\ndrwxr-xr-x  2 root root    6 Sep  8 07:48 data\ndrwxr-xr-x  2 root root    6 Sep  8 07:48 certs\ndrwxr-xr-x  2 root root    6 Sep  8 07:48 auth\ndrwxr-xr-x  5 root root   48 Sep  8 07:59 downloads\ndr-xr-x---. 8 root root 4096 Sep  8 14:07 ..\ndrwxr-xr-x  6 root root  110 Sep  8 16:49 .\n-rw-------  1 root root  419 Sep  8 16:50 .oc-mirror.log\n-rw-r--r--  1 root root  419 Sep  8 16:50 imageset-config.yaml\n</code></pre> <p>We edit the file to look exactly as shown below:</p> [root@bastiononline registry]<pre><code>vi imageset-config.yaml\n</code></pre> imageset-config.yaml<pre><code>kind: ImageSetConfiguration\napiVersion: mirror.openshift.io/v1alpha2\narchiveSize: 4\nstorageConfig:\n  local:\n    path: /root/registry/data\nmirror:\n  platform:\n    architectures:\n      - \"amd64\"\n    channels:\n    - name: stable-4.16\n      type: ocp\n      minVersion: 4.16.9\n      maxVersion: 4.16.9\n      shortestPath: true\n  operators:\n  - catalog: registry.redhat.io/redhat/redhat-operator-index:v4.16\n    packages:\n    - name: openshift-pipelines-operator-rh\n      defaultChannel: latest\n      channels:\n      - name: latest\n  additionalImages:\n  - name: registry.redhat.io/ubi8/ubi:latest\n  helm: {}\n</code></pre> <p>Execute the platform and operators mirror process.</p> <p>Time</p> <p>Bear in mind that the mirror process can take around 10 mins to complete. Be patient.</p> [root@bastiononline registry]<pre><code>oc mirror --config=./imageset-config.yaml file:///root/registry/data\n</code></pre> Output<pre><code>[...]\ninfo: Mirroring completed in 7m53.51s (134.2MB/s)\nCreating archive /root/registry/data/mirror_seq1_000000.tar\nCreating archive /root/registry/data/mirror_seq1_000001.tar\nCreating archive /root/registry/data/mirror_seq1_000002.tar\nCreating archive /root/registry/data/mirror_seq1_000003.tar\nCreating archive /root/registry/data/mirror_seq1_000004.tar\nCreating archive /root/registry/data/mirror_seq1_000005.tar\nCreating archive /root/registry/data/mirror_seq1_000006.tar\nCreating archive /root/registry/data/mirror_seq1_000007.tar\nCreating archive /root/registry/data/mirror_seq1_000008.tar\nCreating archive /root/registry/data/mirror_seq1_000009.tar\nCreating archive /root/registry/data/mirror_seq1_000010.tar\nCreating archive /root/registry/data/mirror_seq1_000011.tar\nCreating archive /root/registry/data/mirror_seq1_000012.tar\nCreating archive /root/registry/data/mirror_seq1_000013.tar\nCreating archive /root/registry/data/mirror_seq1_000014.tar\nCreating archive /root/registry/data/mirror_seq1_000015.tar\n</code></pre> <p>Tip</p> <p>You can find the default channel by running the following command:</p> <pre><code>oc mirror list operators --catalog=&lt;catalog_name&gt; --package=&lt;package_name&gt;\n</code></pre> <p>Examples:</p> <pre><code>oc-mirror list operators --catalog registry.redhat.io/redhat/redhat-operator-index:v4.16\noc-mirror list operators --catalog registry.redhat.io/redhat/redhat-operator-index:v4.16 --package openshift-pipelines-operator-rh\noc mirror list releases --channels --version 4.16\noc mirror list releases --channel stable-4.16\n</code></pre>"},{"location":"airgapped/orig/#17-disable-selinux-in-the-offline-bastion","title":"17. Disable SELINUX in the offline bastion","text":"<p>In order to perform the correct configuration of the Bastion node, it is recommended that the <code>SELINUX</code> is deactivated first.</p> <p>Note</p> <p>It is not strictly necessary that Linux Security is deactivated in order to install OpenShift. However, it is recommended as it eases the installation. Otherwise, you would need to open several specific ports and add rules to the firewall so that the needed communication is allowed.</p> <ol> <li> <p>Access the offline bastion:</p> [student laptop]<pre><code>ssh root@192.168.252.23\n</code></pre> </li> <li> <p>Open the <code>/etc/selinux/config</code> to disable the <code>SELINUX</code></p> [root@localhost ~]<pre><code>vi /etc/selinux/config\n</code></pre> </li> <li> <p>Make sure your file looks like below, where the highlighted line has been modified:</p> /etc/selinux/config<pre><code># This file controls the state of SELinux on the system.\n# SELINUX= can take one of these three values:\n# enforcing - SELinux security policy is enforced.\n# permissive - SELinux prints warnings instead of enforcing.\n# disabled - No SELinux policy is loaded.\nSELINUX=disabled\n# SELINUXTYPE= can take one of these three values:\n# targeted - Targeted processes are protected,\n# minimum - Modification of targeted policy. Only selected processes are protected.\n# mls - Multi Level Security protection. SELINUXTYPE=targeted\n</code></pre> <p>Once the change is made, save and exit the file (<code>esc</code> and <code>:wq</code>) </p> </li> <li> <p>Restart the server for the changes to take effect.</p> [root@localhost ~]<pre><code>init 6\n</code></pre> </li> </ol>"},{"location":"airgapped/orig/#18-disable-the-firewall-in-the-offline-bastion","title":"18. Disable the firewall in the offline bastion","text":"<p>Also, and since it is an internal network, the firewall of the machine is deactivated</p> <ol> <li> <p>Access the offline bastion:</p> [student laptop]<pre><code>ssh root@192.168.252.23\n</code></pre> </li> <li> <p>Stop the firewall:</p> [root@localhost ~]<pre><code>systemctl stop firewalld\nsystemctl disable firewalld\n</code></pre> Output<pre><code>Removed /etc/systemd/system/multi-user.target.wants/firewalld.service. \nRemoved /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service.\n</code></pre> </li> </ol>"},{"location":"airgapped/orig/#19-configure-the-hostname-for-the-offline-bastion","title":"19. Configure the hostname for the offline bastion","text":"<p>We need to change the online bastion machine's default hostname as we will be referring to this machine from the DNS, load balancer, etc using a different hostname.</p> <ol> <li> <p>Edit the hostname of the machine</p> [root@localhost ~]<pre><code>vi /etc/hostname\n</code></pre> <p>set the hostname to <code>bastion.ocp4.platformengineers.xyz</code></p> </li> <li> <p>Once the change has been made, restart the server for it to take effect.</p> [root@localhost ~]<pre><code>init 6\n</code></pre> </li> <li> <p>Access the offline bastion again:</p> [student laptop]<pre><code>ssh root@192.168.252.23\n</code></pre> </li> <li> <p>Check the hostname was successfully modified:</p> [root@bastion ~]<pre><code>ping $HOSTNAME\n</code></pre> <p>You should see similar output as below:</p> Output<pre><code>PING bastion.ocp4.platformengineers.xyz(bastion.ocp4.platformengineers.xyz (fe80::250:56ff:fe8a:23c2%ens192)) 56 data bytes\n64 bytes from bastion.ocp4.platformengineers.xyz (fe80::250:56ff:fe8a:23c2%ens192): icmp_seq=1 ttl=64 time=0.040 ms\n64 bytes from bastion.ocp4.platformengineers.xyz (fe80::250:56ff:fe8a:23c2%ens192): icmp_seq=2 ttl=64 time=0.055 ms\n64 bytes from bastion.ocp4.platformengineers.xyz (fe80::250:56ff:fe8a:23c2%ens192): icmp_seq=3 ttl=64 time=0.047 ms\n</code></pre> </li> <li> <p>Finish the ping command with <code>ctrl+c</code></p> </li> </ol>"},{"location":"airgapped/orig/#20-install-required-redhat-packages-in-the-offline-bastion","title":"20. Install required RedHat packages in the offline bastion","text":"<p>For the installation of rpm packages on the offline machine, the RHEL 8.7 OS ISO file that created the VM from will help us. </p> <p>Repeat the steps taken on the onlinebastion for creating a offline repository from the RHEL 8.7 ISO</p> <ol> <li> <p>Make sure that, in the vCenter, the offline bastion has the CD/DVD drive as connected:</p> <p></p> </li> </ol> <p>Once the offline repository is has been created and enabled, the required packages are installed with the yum utility.</p> <p>The following packages have been installed on the online bastion server. Some packages may not be used during the execution of online commands, but they are installed so that the \"online\" and \"offline\" bastion settings are as identical as possible.</p> [root@bastion ~]<pre><code>yum install -y podman \\\n                    jq openssl httpd-tools curl wget telnet nfs-utils \\\n                    httpd.x86_64 \\\n                    bind bind-utils rsync mkisofs haproxy\n</code></pre> Output<pre><code>Updating Subscription Management repositories.\nUnable to read consumer identity\n\nThis system is not registered with an entitlement server. You can use subscription-manager to register.\n\nRed Hat Enterprise Linux 8.7.0 Base                                                                                          40 MB/s | 2.4 MB     00:00    \nRed Hat Enterprise Linux 8.7.0 App                                                                                           53 MB/s | 7.8 MB     00:00    \nPackage openssl-1:1.1.1k-7.el8_6.x86_64 is already installed.\nPackage curl-7.61.1-25.el8.x86_64 is already installed.\nDependencies resolved.\n============================================================================================================================================================\n Package                                   Architecture        Version                                                  Repository                     Size\n============================================================================================================================================================\nInstalling:\n bind                                      x86_64              32:9.11.36-5.el8                                         InstallMediaApps              2.1 M\n bind-utils                                x86_64              32:9.11.36-5.el8                                         InstallMediaApps              452 k\n genisoimage                               x86_64              1.1.11-39.el8                                            InstallMediaApps              316 k\n httpd                                     x86_64              2.4.37-51.module+el8.7.0+16050+02173b8e                  InstallMediaApps              1.4 M\n httpd-tools                               x86_64              2.4.37-51.module+el8.7.0+16050+02173b8e                  InstallMediaApps              109 k\n jq                                        x86_64              1.6-3.el8                                                InstallMediaApps              202 k\n nfs-utils                                 x86_64              1:2.3.3-57.el8                                           InstallMediaBase              515 k\n podman                                    x86_64              3:4.2.0-1.module+el8.7.0+16772+33343656                  InstallMediaApps               12 M\n rsync                                     x86_64              3.1.3-19.el8                                             InstallMediaBase              410 k\n telnet                                    x86_64              1:0.17-76.el8                                            InstallMediaApps               72 k\n wget                                      x86_64              1.19.5-10.el8                                            InstallMediaApps              734 k\n...\n...\nComplete!\n</code></pre>"},{"location":"airgapped/orig/#21-copy-downloaded-artefacts-from-online-bastion-to-offline-bastion","title":"21. Copy downloaded artefacts from online bastion to offline bastion","text":"<ol> <li> <p>Create a directory for the private image registry in the offline bastion</p> [root@bastion ~]<pre><code>mkdir /root/registry\ncd registry/\n</code></pre> </li> <li> <p>Create the necessary directories for such private image registry.</p> [root@bastion registry]<pre><code>mkdir auth certs data downloads\ncd downloads/\n</code></pre> [root@bastion downloads]<pre><code>mkdir images tools secrets\n</code></pre> <p>The folder structure that should have got created should look like this:</p> Directory structure<pre><code>/root\n`-- registry\n    |-- auth\n    |-- certs\n    |-- data\n    `-- downloads\n        |-- images\n        |-- secrets\n        `-- tools\n</code></pre> </li> <li> <p>Exit the offline bastion</p> [root@bastion downloads]<pre><code>exit\n</code></pre> </li> <li> <p>Access the online bastion:</p> [student laptop]<pre><code>ssh root@192.168.252.22\n</code></pre> </li> <li> <p>Verify that the <code>.tar</code> files have been generated.</p> [root@bastiononline ~]<pre><code>cd /root/registry/data\n</code></pre> [root@bastiononline data]<pre><code>ls -hla\n</code></pre> Output<pre><code>total 61G\ndrwxr-xr-x 4 root root 4.0K Sep  8 17:28 .\ndrwxr-xr-x 6 root root  110 Sep  8 17:08 ..\n-rw-r--r-- 1 root root 4.0G Sep  8 17:23 mirror_seq1_000000.tar\n-rw-r--r-- 1 root root 3.9G Sep  8 17:23 mirror_seq1_000001.tar\n-rw-r--r-- 1 root root 3.6G Sep  8 17:24 mirror_seq1_000002.tar\n-rw-r--r-- 1 root root 4.0G Sep  8 17:24 mirror_seq1_000003.tar\n-rw-r--r-- 1 root root 4.0G Sep  8 17:24 mirror_seq1_000004.tar\n-rw-r--r-- 1 root root 3.4G Sep  8 17:25 mirror_seq1_000005.tar\n-rw-r--r-- 1 root root 3.7G Sep  8 17:25 mirror_seq1_000006.tar\n-rw-r--r-- 1 root root 3.5G Sep  8 17:25 mirror_seq1_000007.tar\n-rw-r--r-- 1 root root 4.0G Sep  8 17:26 mirror_seq1_000008.tar\n-rw-r--r-- 1 root root 4.0G Sep  8 17:26 mirror_seq1_000009.tar\n-rw-r--r-- 1 root root 4.0G Sep  8 17:26 mirror_seq1_000010.tar\n-rw-r--r-- 1 root root 3.9G Sep  8 17:27 mirror_seq1_000011.tar\n-rw-r--r-- 1 root root 4.0G Sep  8 17:27 mirror_seq1_000012.tar\n-rw-r--r-- 1 root root 3.9G Sep  8 17:27 mirror_seq1_000013.tar\n-rw-r--r-- 1 root root 3.9G Sep  8 17:28 mirror_seq1_000014.tar\n-rw-r--r-- 1 root root 3.8G Sep  8 17:28 mirror_seq1_000015.tar\ndrwxr-xr-x 2 root root    6 Sep  8 17:28 oc-mirror-workspace\ndrwxr-x--- 2 root root   28 Sep  8 17:28 publish\n</code></pre> </li> <li> <p>Copy the tar files to the offline bastion using the scp command as there is currently network connectivity between the online bastion and the offline bastion.</p> <p>Note</p> <p>In a customer air-gapped environment, you would likely need to copy the data to the offline bastion via another method, a USB drive for example.</p> [root@bastiononline data]<pre><code>scp *.tar root@192.168.252.23:/root/registry/data\n</code></pre> Output<pre><code>mirror_seq1_000000tar   100% 4090MB 167.6MB/s   00:24\nmirror_seq1_000001.tar  100% 3922MB 165.8MB/s   00:23\nmirror_seq1_000002.tar  100% 3673MB 167.1MB/s   00:21\nmirror_seq1_000003.tar  100% 4093MB 168.0MB/s   00:24\nmirror_seq1_000004.tar  100% 4061MB 169.4MB/s   00:23\nmirror_seq1_000005.tar  100% 3391MB 165.0MB/s   00:20\nmirror_seq1_000006.tar  100% 3698MB 128.1MB/s   00:28\nmirror_seq1_000007.tar  100% 3567MB 168.4MB/s   00:21\nmirror_seq1_000008.tar  100% 4083MB 124.5MB/s   00:32\nmirror_seq1_000009.tar  100% 3893MB 165.2MB/s   00:23\nmirror_seq1_000010.tar  100% 4086MB 158.6MB/s   00:25\nmirror_seq1_000011.tar  100% 3894MB 165.3MB/s   00:23\nmirror_seq1_000012.tar  100% 3871MB 137.9MB/s   00:28\nmirror_seq1_000013.tar  100% 3895MB 162.9MB/s   00:23\nmirror_seq1_000014.tar  100% 3913MB 156.7MB/s   00:24\nmirror_seq1_000015.tar  100% 4094MB 133.8MB/s   00:30\nmirror_seq1_000016.tar  100% 4094MB 133.8MB/s   00:30\n</code></pre> </li> <li> <p>Copy the remaining components downloaded in the online bastion (client oc, installer, images, image registry, butane...) to the offline bastion </p> [root@bastiononline data]<pre><code>scp -r /root/registry/downloads/* root@192.168.252.23:/root/registry/downloads/\n</code></pre> Output<pre><code>rhcos-4.16.3-x86_64-metal.x86_64.raw.gz                  100% 1208MB 391.3MB/s   00:03    \nrhcos-4.16.3-x86_64-live.x86_64.iso                      100% 1164MB 188.5MB/s   00:06    \npull-secret.txt                                          100% 2759     1.3MB/s   00:00    \npull-secret.json                                         100% 2875     1.4MB/s   00:00    \nmirror-registry-amd64.tar.gz                             100%  579MB 407.7MB/s   00:01    \nimage-archive.tar                                        100% 1388MB 139.8MB/s   00:09    \nexecution-environment.tar                                100%  302MB 383.2MB/s   00:00    \nmirror-registry                                          100% 9595KB 295.8MB/s   00:00    \nsqlite3.tar                                              100%  108MB 193.5MB/s   00:00    \nbutane                                                   100% 7881KB 218.3MB/s   00:00    \nopenshift-client-linux-amd64-rhel8-4.16.9.tar.gz         100%   64MB 497.5MB/s   00:00    \noc                                                       100%  152MB 373.1MB/s   00:00    \nkubectl                                                  100%  152MB 665.6MB/s   00:00    \nopenshift-install-linux-4.16.9.tar.gz                    100%  487MB 497.8MB/s   00:00    \nREADME.md                                                100%  706   395.9KB/s   00:00    \nopenshift-install                                        100%  675MB 191.8MB/s   00:03    \noc-mirror.tar.gz                                         100%   64MB 525.6MB/s   00:00    \noc-mirror                                                100%  158MB 386.7MB/s   00:00    \n.oc-mirror.log                                           100%    0     0.0KB/s   00:00\n</code></pre> </li> <li> <p>Exit the online bastion</p> [root@bastiononline data]<pre><code>exit\n</code></pre> </li> <li> <p>Access the offline bastion:</p> [student laptop]<pre><code>ssh root@192.168.252.23\n</code></pre> </li> <li> <p>Copy the binaries into <code>/usr/bin</code> or <code>/usr/local/bin</code> in the same way as it was done for the online bastion</p> [root@bastion ~]<pre><code>cd /root/registry/downloads/tools\n</code></pre> [root@bastion tools]<pre><code>ls -lart\n</code></pre> Output<pre><code>total 4245772\ndrwxr-xr-x 5 root root         48 Oct 25 14:08 ..\n-rw-r--r-- 1 root root  607056480 Oct 25 15:43 mirror-registry-amd64.tar.gz\n-rw-r--r-- 1 root root 1454929920 Oct 25 15:44 image-archive.tar\n-rw-r--r-- 1 root root  316753920 Oct 25 15:44 execution-environment.tar\n-rwxr-xr-x 1 root root    9824888 Oct 25 15:44 mirror-registry\n-rw-r--r-- 1 root root  113418240 Oct 25 15:44 sqlite3.tar\n-rw-r--r-- 1 root root    8070568 Oct 25 15:44 butane\n-rw-r--r-- 1 root root   66705673 Oct 25 15:44 openshift-client-linux-amd64-rhel8-4.16.9.tar.gz\n-rwxr-xr-x 1 root root  159905720 Oct 25 15:44 oc\n-rwxr-xr-x 1 root root  159905720 Oct 25 15:44 kubectl\n-rw-r--r-- 1 root root  510253136 Oct 25 15:44 openshift-install-linux-4.16.9.tar.gz\n-rw-r--r-- 1 root root        706 Oct 25 15:44 README.md\n-rwxr-xr-x 1 root root  707731456 Oct 25 15:44 openshift-install\n-rw-r--r-- 1 root root   67385243 Oct 25 15:44 oc-mirror.tar.gz\n-rwxr-x--x 1 root root  165699280 Oct 25 15:44 oc-mirror\n-rw------- 1 root root          0 Oct 25 15:44 .oc-mirror.log\ndrwxr-xr-x 2 root root       4096 Oct 25 15:44 .\n</code></pre> [root@bastion tools]<pre><code>cp /root/registry/downloads/tools/oc /usr/bin/oc\ncp /root/registry/downloads/tools/openshift-install /usr/bin/openshift-install\ncp /root/registry/downloads/tools/oc-mirror /usr/local/bin/oc-mirror\n</code></pre> </li> </ol>"},{"location":"airgapped/orig/#22-create-the-private-image-registry-in-the-offline-bastion","title":"22. Create the private image registry in the offline bastion","text":"<p>The images to be used during the installation of OpenShift are to be supplied from the bastion. To do this, a local registry must be configured as follows:</p> <ol> <li> <p>Run the <code>mirror-registry install</code> command</p> [root@bastion downloads]<pre><code>cd /root/registry/downloads/tools\n</code></pre> [root@bastion tools]<pre><code>./mirror-registry install --quayRoot /root/registry \\\n                          --quayHostname bastion.ocp4.platformengineers.xyz \\\n                          --initPassword passw0rd\n</code></pre> <p>Important</p> <p>On a client installation, please specify a different password in agreement with the client. If the <code>--initPassword</code> is not specified during the Quay private image registry install command, a random password is generated.</p> <p>IMPORTANT: If you let the install process generate a random password, please DO WRITE IT DOWN as it can not be retrieved again.</p> Output<pre><code>__   __\n/  \\ /  \\     ______   _    _     __   __   __\n/ /\\ / /\\ \\   /  __  \\ | |  | |   /  \\  \\ \\ / /\n/ /  / /  \\ \\  | |  | | | |  | |  / /\\ \\  \\   /\n\\ \\  \\ \\  / /  | |__| | | |__| | / ____ \\  | |\n\\ \\/ \\ \\/ /   \\_  ___/  \\____/ /_/    \\_\\ |_|\n\\__/ \\__/      \\ \\__\n                \\___\\ by Red Hat\nBuild, Store, and Distribute your Containers\n\nINFO[2023-11-06 17:01:33] Install has begun  \n...\n...\nPLAY RECAP *************************************************************************************************************************************************\nroot@bastion.ocp4.platformengineers.xyz : ok=50   changed=30   unreachable=0    failed=0    skipped=17   rescued=0    ignored=0   \n\nINFO[2023-11-06 17:04:14] Quay installed successfully, config data is stored in /root/registry \nINFO[2023-11-06 17:04:14] Quay is available at https://bastion.ocp4.platformengineers.xyz:8443 with credentials (init, passw0rd) \n</code></pre> </li> <li> <p>If you let the Quay's private image registry install process generate a random password for the <code>init</code> user, log such password    </p> Output<pre><code>INFO[2023-09-11 18:31:13] Quay installed successfully, config data is stored in /root/registry\nINFO[2023-09-11 18:31:13] Quay is available at https://bastion.ocp4.platformengineers.xyz:8443 with credentials (init, cw9q3GC10aPQMiunkSZ6Ag7r52H8h4ve)\n</code></pre> <p>Warning</p> <p>Take note of this user and password because you will need to use it when you create the <code>install-config.yaml</code> file on next steps.</p> </li> <li> <p>Add the <code>init</code> user as a Quay super user on the following Quay private image registry configuration file within the <code>SUPER_USERS</code> section</p> [root@bastion tools]<pre><code>vi /root/registry/quay-config/config.yaml\n</code></pre> Output<pre><code>...\nSUPER_USERS:\n- admin\n- init\n...\n</code></pre> </li> <li> <p>Restart the offline bastion node.</p> [root@bastion tools]<pre><code>init 6\n</code></pre> </li> </ol> <p>Important</p> <ul> <li> <p>The private image registry sets Quay to use the autogenerated TLS certificates to enable SSL connectivity by default. These certificates have a validity of 365 days and will expire after 1 year after which they need to be rotated. The rotation procedure is not done automatically by Quay, it needs to be done manually, otherwise x509 errors will be present during pushes and pulls to/from the registry.</p> </li> <li> <p>You can find the instructions to rotate the certificates here.</p> </li> <li> <p>Newly created certificate will need to be added to all OpenShift clusters that pull images from this private image registry instance. This will ensure certificate trust. You can find the link to add certficiates to OpenShift here</p> </li> </ul>"},{"location":"airgapped/orig/#23-publish-the-downloaded-images-to-the-offline-bastion-image-registry","title":"23 Publish the downloaded images to the offline bastion Image Registry","text":"<p>The downloaded images must be published in the Quay private image registry installed on the previous section. </p> <ol> <li> <p>Access the offline bastion again:</p> [student laptop]<pre><code>ssh root@192.168.252.23\n</code></pre> </li> <li> <p>Execute the following command to set the image registry certificate as a trusted certificate by the system:</p> [root@bastion ~]<pre><code>cp /root/registry/quay-rootCA/rootCA.pem /usr/share/pki/ca-trust-source/anchors/\nupdate-ca-trust\n</code></pre> </li> <li> <p>Log into the private image registry:</p> <p>Warning</p> <p>Remember to use the user (<code>init</code>) and password (<code>passw0rd</code> if you used the command outlined in this training material) you installed the Quay private image registry with during its installation proccess. </p> [root@bastion ~]<pre><code>podman login bastion.ocp4.platformengineers.xyz:8443\n</code></pre> Output<pre><code>Login Succeeded!\n</code></pre> </li> <li> <p>Launch the image mirror process to load the previously downloaded images, now in <code>.tar</code> files, into the Quay private image registry:</p> <p>Info</p> <p>This process can take around 30 mins</p> <p>Tip</p> <p>You should not need the <code>--dest-skip-tls</code> flag in the command below if you set your container image private registry CA as a trusted CA in the steps above. We've added it to avoid issues during workshops.</p> [root@bastion ~]<pre><code>oc mirror --from=/root/registry/data docker://bastion.ocp4.csm-spgi.acme.es:8443 --dest-skip-tls\n</code></pre> Output<pre><code>[...]\nWrote release signatures to oc-mirror-workspace/results-1694473158\nRendering catalog image \"bastion.ocp4.platformengineers.xyz:8443/redhat/redhat-operator-index:v4.12\" with file-based catalog\nWriting image mapping to oc-mirror-workspace/results-1694473158/mapping.txt\nWriting CatalogSource manifests to oc-mirror-workspace/results-1694473158\nWriting ICSP manifests to oc-mirror-workspace/results-1694473158\n</code></pre> </li> <li> <p>You can check the result of the image mirror process by pointing your browser to:</p> <p>https://192.168.252.23:8443</p> <p></p> </li> <li> <p>Check that the <code>catalogSource-redhat-operator-index.yaml</code> and <code>imageContentSourcePolicy.yaml</code> files have been generated in the <code>/root/oc-mirror-workspace/results-XXXXXX</code> directory.</p> [root@bastion ~]<pre><code>ls -lart ~/oc-mirror-workspace/results-XXXXXX/\n</code></pre> Output<pre><code>total 96\ndrwxr-xr-x 2 root root     6 Sep 11 18:59 charts\ndrwxr-xr-x 2 root root    98 Sep 11 19:25 release-signatures\ndrwxr-xr-x 4 root root    47 Sep 11 19:25 ..\n-rw-r--r-- 1 root root 87467 Sep 11 19:25 mapping.txt\n-rwxr-xr-x 1 root root   241 Sep 11 19:25 catalogSource-redhat-operator-index.yaml\n-rwxr-xr-x 1 root root  1741 Sep 11 19:25 imageContentSourcePolicy.yaml\ndrwxr-xr-x 4 root root   150 Sep 11 19:25 .\n</code></pre> </li> </ol> <p>Note</p> <p>The contents of the <code>imageContentSourcePolicy.yaml</code> must be written down for use when creating the <code>install-config.yaml</code> file later in the chapter Create the install-config.yaml file.</p> <p>The contents of the <code>catalogSource-redhat-operator-index.yaml</code> file must be used when creating the RedHat Marketplace later in the chapter Create the catalog sources to access RedHat Marketplace operators.</p>"},{"location":"airgapped/orig/#24-configuring-dns-service-in-offline-bastion","title":"24. Configuring DNS service in offline bastion","text":"<ol> <li> <p>The DNS service configuration file is located in the <code>/etc</code> directory. To start, copy <code>named.conf</code> to a backup file:</p> [root@bastion ~]<pre><code>cp /etc/named.conf /etc/named.conf.bak\n</code></pre> </li> <li> <p>Open the <code>named.conf</code> file.</p> [root@bastion ~]<pre><code>vi /etc/named.conf\n</code></pre> </li> <li> <p>Edit the file to look like the one shown below </p> named.conf<pre><code>//\n// named.conf\n//\n// Provided by Red Hat bind package to configure the ISC BIND named(8) DNS\n// server as a caching only nameserver (as a localhost DNS resolver only).\n//\n// See /usr/share/doc/bind*/sample/ for example named configuration files.\n//\n\noptions {\n#   listen-on port 53 { 127.0.0.1; };\n#   listen-on-v6 port 53 { ::1; };\n    directory   \"/var/named\";\n    dump-file   \"/var/named/data/cache_dump.db\";\n    statistics-file \"/var/named/data/named_stats.txt\";\n    memstatistics-file \"/var/named/data/named_mem_stats.txt\";\n    secroots-file   \"/var/named/data/named.secroots\";\n    recursing-file  \"/var/named/data/named.recursing\";\n    allow-query     { any;};\n\n    /*\n    - If you are building an AUTHORITATIVE DNS server, do NOT enable recursion.\n    - If you are building a RECURSIVE (caching) DNS server, you need to enable\n    recursion.\n    - If your recursive DNS server has a public IP address, you MUST enable access\n    control to limit queries to your legitimate users. Failing to do so will\n    cause your server to become part of large scale DNS amplification\n    attacks. Implementing BCP38 within your network would greatly\n    reduce such attack surface\n    */\n    recursion yes;\n\n    dnssec-enable yes;\n#   dnssec-validation yes;\n\n    managed-keys-directory \"/var/named/dynamic\";\n\n    pid-file \"/run/named/named.pid\";\n    session-keyfile \"/run/named/session.key\";\n#    forwarders { 192.168.252.1;};\n    /* https://fedoraproject.org/wiki/Changes/CryptoPolicy */\n    include \"/etc/crypto-policies/back-ends/bind.config\";\n};\n\nlogging {\n        channel default_debug {\n                file \"data/named.run\";\n                severity dynamic;\n        };\n};\n\nzone \".\" IN {\n    type hint;\n    file \"named.ca\";\n};\n\ninclude \"/etc/named.rfc1912.zones\";\ninclude \"/etc/named.root.key\";\nzone \"acme.es\" {\n    type master;\n    file \"platformengineers.xyz.db\";\n    allow-update { none; };\n};\nzone \"252.168.192.in-addr.arpa\" IN {\n    type master;\n    file \"platformengineers.xyz.reverse.db\";\n    allow-update { none; };\n};\n</code></pre> </li> <li> <p>Create the <code>platformengineers.xyz.db</code> file in <code>/var/named</code> for name resolution.</p> [root@bastion ~]<pre><code>vi /var/named/platformengineers.xyz.db\n</code></pre> platformengineers.xyz.db<pre><code>$TTL 1D\n@ IN SOA bastion.ocp4.platformengineers.xyz. root.ocp4.platformengineers.xyz. (\n2019022409 ; serial\n3h ; refresh\n15 ; retry\n1w ; expire\n3h ; minimum\n)\n       IN NS bastion.ocp4.platformengineers.xyz.\napi.ocp4.platformengineers.xyz. IN A 192.168.252.24\napi-int.ocp4.platformengineers.xyz. IN A 192.168.252.24\n*.apps.ocp4.platformengineers.xyz. IN A 192.168.252.25\nbootstrap.ocp4.platformengineers.xyz. IN A 192.168.252.3\nbastion.ocp4.platformengineers.xyz. IN A 192.168.252.23\ncontrolplane01.ocp4.platformengineers.xyz. IN A 192.168.252.4\ncontrolplane02.ocp4.platformengineers.xyz. IN A 192.168.252.5\ncontrolplane03.ocp4.platformengineers.xyz. IN A 192.168.252.6\ninfra01.ocp4.platformengineers.xyz. IN A 192.168.252.7\ninfra02.ocp4.platformengineers.xyz. IN A 192.168.252.8\ninfra03.ocp4.platformengineers.xyz. IN A 192.168.252.9\ncompute01.ocp4.platformengineers.xyz. IN A 192.168.252.10\ncompute02.ocp4.platformengineers.xyz. IN A 192.168.252.11\ncompute03.ocp4.platformengineers.xyz. IN A 192.168.252.12\nstorage01.ocp4.platformengineers.xyz. IN A 192.168.252.13\nstorage02.ocp4.platformengineers.xyz. IN A 192.168.252.14\nstorage03.ocp4.platformengineers.xyz. IN A 192.168.252.15\n</code></pre> </li> <li> <p>Create <code>platformengineers.xyz.reverse.db</code> file in <code>/var/named</code> for reverse name resolution:</p> [root@bastion ~]<pre><code>vi /var/named/platformengineers.xyz.reverse.db\n</code></pre> platformengineers.xyz.reverse.db<pre><code>$TTL 1D\n@ IN SOA bastion.ocp4.platformengineers.xyz. root.ocp4.platformengineers.xyz. (\n2019022409 ; serial\n3h ; refresh\n15 ; retry\n1w ; expire\n3h ; minimum\n)\n252.168.192.in-addr.arpa. IN NS bastion.ocp4.platformengineers.xyz.\n23 IN PTR bastion.ocp4.platformengineers.xyz.\n3 IN PTR bootstrap.ocp4.platformengineers.xyz.\n4 IN PTR controlplane01.ocp4.platformengineers.xyz.\n5 IN PTR controlplane02.ocp4.platformengineers.xyz.\n6 IN PTR controlplane03.ocp4.platformengineers.xyz.\n7 IN PTR infra01.ocp4.platformengineers.xyz.\n8 IN PTR infra02.ocp4.platformengineers.xyz.\n9 IN PTR infra03.ocp4.platformengineers.xyz.\n10 IN PTR compute01.ocp4.platformengineers.xyz.\n11 IN PTR compute02.ocp4.platformengineers.xyz.\n12 IN PTR compute03.ocp4.platformengineers.xyz.\n13 IN PTR storage01.ocp4.platformengineers.xyz.\n14 IN PTR storage02.ocp4.platformengineers.xyz.\n15 IN PTR storage03.ocp4.platformengineers.xyz.\n</code></pre> </li> <li> <p>Enable the DNS service: </p> [root@bastion ~]<pre><code>systemctl enable named\n</code></pre> Output<pre><code>Created symlink /etc/systemd/system/multi-user.target.wants/named.service \u2192 /usr/lib/systemd/system/named.service.\n</code></pre> </li> <li> <p>Start the DNS service:</p> [root@bastion ~]<pre><code>systemctl start named\n</code></pre> </li> </ol>"},{"location":"airgapped/orig/#25-set-up-haproxy-in-the-offline-bastion","title":"25. Set up HAProxy in the offline bastion","text":"<p>The following link provides an example of the HAProxy provided by RedHat as a reference:</p> <p>https://access.redhat.com/articles/5127211 </p> <ol> <li> <p>Open the HAProxy configuration file:</p> [root@bastion ~]<pre><code>vi /etc/haproxy/haproxy.cfg\n</code></pre> </li> <li> <p>Make sure the HAProxy configuration file looks like below:</p> haproxy.cfg<pre><code>#---------------------------------------------------------------------\n# Global settings\n#---------------------------------------------------------------------\nglobal\nlog 127.0.0.1 local2\nchroot /var/lib/haproxy\npidfile /var/run/haproxy.pid\nmaxconn 4000\nuser haproxy\ngroup haproxy\ndaemon\n# turn on stats unix socket\nstats socket /var/lib/haproxy/stats\n# utilize system-wide crypto-policies\n#ssl-default-bind-ciphers PROFILE=SYSTEM\n#ssl-default-server-ciphers PROFILE=SYSTEM\n#---------------------------------------------------------------------\n# common defaults that all the 'listen' and 'backend' sections will\n# use if not designated in their block\n#---------------------------------------------------------------------\ndefaults\nmode tcp\nlog global\n#option httplog\noption dontlognull\noption http-server-close\noption forwardfor except 127.0.0.0/8\noption redispatch\nretries 3\ntimeout http-request 10s\ntimeout queue 1m\ntimeout connect 10s\ntimeout client 5m\ntimeout server 5m\ntimeout http-keep-alive 10s\ntimeout check 10s\nmaxconn 3000\n#---------------------------------------------------------------------\n# balancing for RHOCP Kubernetes API Server\n#---------------------------------------------------------------------\nfrontend k8s_api\nbind *:6443\n#mode tcp\ndefault_backend k8s_api_backend\nbackend k8s_api_backend\n#balance roundrobin\nbalance source\n#mode tcp\nserver bootstrap.ocp4.platformengineers.xyz 192.168.252.3:6443 check\nserver controlplane01.ocp4.platformengineers.xyz 192.168.252.4:6443 check\nserver controlplane02.ocp4.platformengineers.xyz 192.168.252.5:6443 check\nserver controlplane03.ocp4.platformengineers.xyz 192.168.252.6:6443 check\n# ---------------------------------------------------------------------\n# balancing for RHOCP Machine Config Server\n# ---------------------------------------------------------------------\nfrontend machine_config\nbind *:22623\n#mode tcp\ndefault_backend machine_config_backend\nbackend machine_config_backend\n#balance roundrobin\nbalance source\n#mode tcp\nserver bootstrap.ocp4.platformengineers.xyz 192.168.252.3:22623 check\nserver controlplane01.ocp4.platformengineers.xyz 192.168.252.4:22623 check\nserver controlplane02.ocp4.platformengineers.xyz 192.168.252.5:22623 check\nserver controlplane03.ocp4.platformengineers.xyz 192.168.252.6:22623 check\n# --------------------------------------------------------------------\n# balancing for RHOCP Ingress Insecure Port\n# ---------------------------------------------------------------------\nfrontend ingress_insecure\nbind *:80\n#mode tcp\ndefault_backend ingress_insecure_backend\nbackend ingress_insecure_backend\n#balance roundrobin\nbalance source\n#mode tcp\nserver infra01.ocp4.platformengineers.xyz 192.168.252.7:80 check\nserver infra02.ocp4.platformengineers.xyz 192.168.252.8:80 check\nserver infra03.ocp4.platformengineers.xyz 192.168.252.9:80 check\nserver compute01.ocp4.platformengineers.xyz 192.168.252.10:80 check\nserver compute02.ocp4.platformengineers.xyz 192.168.252.11:80 check\nserver compute03.ocp4.platformengineers.xyz 192.168.252.12:80 check\nserver storage01.ocp4.platformengineers.xyz 192.168.252.13:80 check\nserver storage02.ocp4.platformengineers.xyz 192.168.252.14:80 check\nserver storage03.ocp4.platformengineers.xyz 192.168.252.15:80 check\n\n# ---------------------------------------------------------------------\n# balancing for RHOCP Ingress Secure Port\n# ---------------------------------------------------------------------\nfrontend ingress_secure\nbind *:443\n#mode tcp\ndefault_backend ingress_secure_backend\nbackend ingress_secure_backend\n#balance roundrobin\nbalance source\n#mode tcp\nserver infra01.ocp4.platformengineers.xyz 192.168.252.7:443 check\nserver infra02.ocp4.platformengineers.xyz 192.168.252.8:443 check\nserver infra03.ocp4.platformengineers.xyz 192.168.252.9:443 check\nserver compute01.ocp4.platformengineers.xyz 192.168.252.10:443 check\nserver compute02.ocp4.platformengineers.xyz 192.168.252.11:443 check\nserver compute03.ocp4.platformengineers.xyz 192.168.252.12:443 check\nserver storage01.ocp4.platformengineers.xyz 192.168.252.13:80 check\nserver storage02.ocp4.platformengineers.xyz 192.168.252.14:80 check\nserver storage03.ocp4.platformengineers.xyz 192.168.252.15:80 check\n\n\n# ---------------------------------------------------------------------\n# Exposing HAProxy Statistic Page\n# ---------------------------------------------------------------------\n#listen stats\n#bind :32700\n#stats enable\n#stats uri /\n#stats hide-version\n#stats auth admin:RedH@t322\n</code></pre> </li> <li> <p>Check the configuration file:</p> [root@bastion ~]<pre><code>haproxy -c -f /etc/haproxy/haproxy.cfg\n</code></pre> Output<pre><code>Configuration file is valid\n</code></pre> </li> <li> <p>Enable the service:</p> [root@bastion ~]<pre><code>systemctl enable haproxy\n</code></pre> Output<pre><code>Created symlink /etc/systemd/system/multi-user.target.wants/haproxy.service \u2192 /usr/lib/systemd/system/haproxy.service.\n</code></pre> </li> <li> <p>Start the service:</p> [root@bastion ~]<pre><code>systemctl start haproxy\n</code></pre> </li> </ol>"},{"location":"airgapped/orig/#26-install-the-http-server-in-the-offline-bastion","title":"26. Install the HTTP server in the offline bastion","text":"<p>For the provisioning of the ignition files to configure the OpenShift cluster nodes, an HTTP server is required. In this installation, that server resides in the offline bastion. In order to install an Apache HTTPD web server, follow the steps below.</p> <ol> <li> <p>Check the http package is installed:</p> [root@bastion ~]<pre><code>yum list installed | grep http\n</code></pre> Output<pre><code>httpd.x86_64     2.4.37-56.module+el8.8.0+19808+379766d6.7     @rhel-8-for-x86_64-appstream-rpms\n</code></pre> </li> <li> <p>Configure the httpd server to boot on port <code>8080</code> by configuring the file in the path <code>/etc/httpd/conf/httpd.conf</code> and changing the default port <code>80</code> to <code>8080</code>.</p> [root@bastion ~]<pre><code>vi /etc/httpd/conf/httpd.conf\n</code></pre> Output<pre><code>[...]\n#\n#Listen 12.34.56.78:80\nListen 8080\n\n#\n[...]\n</code></pre> </li> <li> <p>Start the service:</p> [root@bastion ~]<pre><code>systemctl enable httpd\n</code></pre> Output<pre><code>Created symlink /etc/systemd/system/multi-user.target.wants/httpd.service \u2192 /usr/lib/systemd/system/httpd.service.\n</code></pre> [root@bastion ~]<pre><code>systemctl start httpd\n</code></pre> </li> <li> <p>Create the following directory to host the images and the ignition files: </p> [root@bastion ~]<pre><code>mkdir /var/www/html/ign\n</code></pre> </li> </ol>"},{"location":"airgapped/orig/#27-generate-an-ssh-key-in-the-offline-bastion","title":"27. Generate an SSH key in the offline bastion","text":"<p>In order to be able to log on to the OpenShift cluster nodes from the bastion server using SSH after the installation has been done, a public ssh key needs to be added to the file <code>install-config.yaml</code>.</p> <ol> <li> <p>Generate an SSH key</p> [root@bastion ~]<pre><code>ssh-keygen -t ed25519 -N '' -f ~/.ssh/id_ed25519\n</code></pre> Output<pre><code>Generating public/private ed25519 key pair.\nYour identification has been saved in /root/.ssh/id_ed25519.\nYour public key has been saved in /root/.ssh/id_ed25519.pub.\nThe key fingerprint is:\nSHA256:bldGYez9jXDnXs+6luiy7dDnEFVIoKqVtJpTyaMg/wU root@bastion.ocp4.platformengineers.xyz\nThe key's randomart image is:\n+--[ED25519 256]--+\n|           .+o...|\n|           o.... |\n|        . .....  |\n|       o = .o.o .|\n|  . . E S   +o =.|\n|   o . X . + .. =|\n|    . * + o o..+o|\n|     . + ..o.+o +|\n|      .   .=+.+o |\n+----[SHA256]-----+\n</code></pre> </li> <li> <p>Display the public key:</p> [root@bastion ~]<pre><code>cat /root/.ssh/id_ed25519.pub\n</code></pre> Output<pre><code>ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIET7OJRP4wD7ibIvcQxA9/9B80qUp0IPog0M9E7zofED root@bastion.ocp4.platformengineers.xyz\n</code></pre> </li> <li> <p>Start the SSH agent</p> [root@bastion ~]<pre><code>eval \"$(ssh-agent -s)\"\n</code></pre> Output<pre><code>Agent pid 7233\n</code></pre> </li> <li> <p>Add the key to the SSH agent</p> [root@bastion ~]<pre><code>ssh-add ~/.ssh/id_ed25519\n</code></pre> Output<pre><code>Identity added: /root/.ssh/id_ed25519 (root@bastion.ocp4.platformengineers.xyz)\n</code></pre> </li> </ol>"},{"location":"airgapped/orig/#28-configure-the-coreos-iso-maker-in-the-online-bastion","title":"28. Configure the CoreOS ISO Maker in the online bastion","text":"<ol> <li> <p>Exit the offline bastion:</p> [root@bastion ~]<pre><code>exit\n</code></pre> </li> <li> <p>Access the online bastion:</p> [student laptop]<pre><code>ssh root@192.168.252.22\n</code></pre> </li> <li> <p>Change to the route where the CoreOS ISO Maker is</p> [root@bastiononline ~]<pre><code>cd /root/Coreos-iso-maker/coreos-iso-maker/\n</code></pre> </li> </ol> <p>In the file <code>group_vars/all.yml</code> the following configuration is defined:</p> Variable Definition Value gateway default IP router 192.168.252.1 netmask default netmask 255.255.255.0 interface NIC device name ens192 dns dns server. This can be done as a list. Don't add more than 3 192.168.252.23 webserver_url webserver that holds the Ignition file bastion.ocp4.platformengineers.xyz webserver_port webserver port for the webserver above 80 webserver_ignition_path Ignition subpath in http server ign install_drive drive to install RHCOS on sda ocp_version Full CoreOS version you are going for 4.16.3 is_checksum sha256 checksum of the ISO. Got it from https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/4.16/4.16.3/sha256sum.txt b4b2bbe4462258e9d30cab2f4a9d94b45960bc03ffa578be3400b9cbcac4912c iso_name Name of the ISO to download. Makes certain assumptions that should be verified rhcos-{{ocp_version }}-x86_64-live.x86_64.iso rhcos_bios Name of the BIOS image to boot from. This is how the file is named on your webserver. Make certain assumptions that should be verified rhcos-{{ocp_version }}-x86_64-metal.x86_64.raw.gz arch The CPU Architecture type. Must be one of x86_64 (default) or ppc64le x86_64 <ol> <li> <p>Open the <code>group_vars/all.yml</code> file:</p> [root@bastiononline core-iso-maker]<pre><code>vi group_vars/all.yml\n</code></pre> </li> <li> <p>Make sure it looks like below:</p> <p>Tip</p> <p>Although you can read <code>ocp_version</code> in the file below, it refers to the RedHat CoreOS version you are getting installed (4.16.3). The OpenShift version that will eventually get installed remains 4.16.9 which is the OpentShift install binaries, CLI and container images version we have downloaded previously in this tutorial.</p> all.yml<pre><code>---\n# If only one network interface\ngateway: 192.168.252.1\nnetmask: 255.255.255.0\n# VMWare default ens192\n# KVM default ens3\n# Libvirt default enp1s0\n# Intel NUC default eno1\ninterface: ens192\n\ndns:\n- 192.168.252.23\n\nwebserver_url: bastion.ocp4.platformengineers.xyz\nwebserver_port: 8080\n# Ignition subpath in http server (optionnal, defaults to nothing)\nwebserver_ignition_path: /ign\n# Path to download master ignition file will be\n# http://192.168.1.20:8080/ignition/master.ign\n\n# Drive to install RHCOS\n# Libvirt - can be vda\ninstall_drive: sda\n\n# Timeout for selection menu during first boot\n# '-1' for infinite timeout. Default '10'\nboot_timeout: -1\n\n# Chose the binary architecture\n# x86_64 or ppc64le\narch: \"x86_64\"\n\nocp_version: 4.16.3\n#iso_checksum: d15bd7ae942573eece34ba9c59e110e360f15608f36e9b83ab9f2372d235bef2\niso_checksum: b4b2bbe4462258e9d30cab2f4a9d94b45960bc03ffa578be3400b9cbcac4912c\n#iso_checksum_ppc64: ff3ef20a0c4c29022f52ad932278b9040739dc48f4062411b5a3255af863c95e\niso_name: rhcos-{{ ocp_version }}-x86_64-live.x86_64.iso\n#iso_name_ppc64: rhcos-{{ ocp_version }}-ppc64le-installer.ppc64le.iso\nrhcos_bios: rhcos-{{ ocp_version }}-x86_64-metal.x86_64.raw.gz\n...\n</code></pre> <p>Tip</p> <p>Dont forget to set <code>boot_timeout: -1</code> so that you have no time limit when asked for the machine type at boot time</p> </li> </ol> <p>The machines of the OpenShift cluster are defined in the <code>inventory.yml</code> file.</p> <ol> <li> <p>Open the <code>inventory.yml</code> file:</p> [root@bastiononline core-iso-maker]<pre><code>vi inventory.yml\n</code></pre> </li> <li> <p>Make sure the file looks like below:</p> inventory.yml<pre><code>--- \nall:\n  children:\n    bootstrap:\n      hosts: \n        bootstrap.ocp4.platformengineers.xyz:\n          dhcp: false\n          ipv4: 192.168.252.3\n    controlplane:\n      hosts:\n        controlplane01.ocp4.platformengineers.xyz: \n          dhcp: false\n          ipv4: 192.168.252.4\n        controlplane02.ocp4.platformengineers.xyz: \n          dhcp: false\n          ipv4: 192.168.252.5\n        controlplane03.ocp4.platformengineers.xyz: \n          dhcp: false\n          ipv4: 192.168.252.6\n    compute:\n      hosts:\n        infra01.ocp4.platformengineers.xyz: \n          dhcp: false\n          ipv4: 192.168.252.7\n        infra02.ocp4.platformengineers.xyz: \n          dhcp: false\n          ipv4: 192.168.252.8\n        infra03.ocp4.platformengineers.xyz: \n          dhcp: false\n          ipv4: 192.168.252.9\n        compute01.ocp4.platformengineers.xyz: \n          dhcp: false\n          ipv4: 192.168.252.10                \n        compute02.ocp4.platformengineers.xyz: \n          dhcp: false\n          ipv4: 192.168.252.11\n        compute03.ocp4.platformengineers.xyz: \n          dhcp: false\n          ipv4: 192.168.252.12\n        storage01.ocp4.platformengineers.xyz: \n          dhcp: false\n          ipv4: 192.168.252.13                \n        storage02.ocp4.platformengineers.xyz: \n          dhcp: false\n          ipv4: 192.168.252.14\n        storage03.ocp4.platformengineers.xyz: \n          dhcp: false\n          ipv4: 192.168.252.15 \n...\n</code></pre> </li> <li> <p>Copy the RHCOS ISO live image and the metal raw file for the OpenShift version we are going to install to the temporary directory:</p> [root@bastiononline core-iso-maker]<pre><code>cp /root/registry/downloads/images/rhcos-4.16.3-x86_64-live.x86_64.iso /tmp\ncp /root/registry/downloads/images/rhcos-4.16.3-x86_64-metal.x86_64.raw.gz /tmp\n</code></pre> </li> <li> <p>Run the following Ansible playbook to create the ISO image:</p> [root@bastiononline core-iso-maker]<pre><code>ansible-playbook playbook-single.yml\n</code></pre> Output<pre><code>[...]\nPLAY RECAP *********************************************************************************************************************\nbootstrap.ocp4.platformengineers.xyz : ok=4    changed=2    unreachable=0    failed=0    skipped=2    rescued=0    ignored=0\ninfra01.ocp4.platformengineers.xyz : ok=4    changed=1    unreachable=0    failed=0    skipped=2    rescued=0    ignored=0\ninfra02.ocp4.platformengineers.xyz : ok=4    changed=1    unreachable=0    failed=0    skipped=2    rescued=0    ignored=0\ninfra03.ocp4.platformengineers.xyz : ok=4    changed=1    unreachable=0    failed=0    skipped=2    rescued=0    ignored=0\nlocalhost                  : ok=7    changed=4    unreachable=0    failed=0    skipped=4    rescued=0    ignored=0\ncontrolplane01.ocp4.platformengineers.xyz : ok=4    changed=2    unreachable=0    failed=0    skipped=2    rescued=0    ignored=0\ncontrolplane02.ocp4.platformengineers.xyz : ok=4    changed=1    unreachable=0    failed=0    skipped=2    rescued=0    ignored=0\ncontrolplane03.ocp4.platformengineers.xyz : ok=4    changed=1    unreachable=0    failed=0    skipped=2    rescued=0    ignored=0\ncompute01.ocp4.platformengineers.xyz : ok=4    changed=1    unreachable=0    failed=0    skipped=2    rescued=0    ignored=0\ncompute02.ocp4.platformengineers.xyz : ok=4    changed=1    unreachable=0    failed=0    skipped=2    rescued=0    ignored=0\ncompute03.ocp4.platformengineers.xyz : ok=4    changed=1    unreachable=0    failed=0    skipped=2    rescued=0    ignored=0\nstorage01.ocp4.platformengineers.xyz : ok=4    changed=1    unreachable=0    failed=0    skipped=2    rescued=0    ignored=0\nstorage02.ocp4.platformengineers.xyz : ok=4    changed=1    unreachable=0    failed=0    skipped=2    rescued=0    ignored=0\nstorage03.ocp4.platformengineers.xyz : ok=4    changed=1    unreachable=0    failed=0    skipped=2    rescued=0    ignored=0\n</code></pre> </li> </ol> <p>Copy the resulting ISO files to the image directory <code>/root/registry/downloads/images/</code> and upload the <code>rhcos-install-cluster.iso</code> file to the vCenter datastore to be used as an ISO to install the OpenShift cluster master and worker nodes.</p> <ol> <li> <p>List the ISO files in the <code>/tmp</code> folder</p> [root@bastiononline core-iso-maker]<pre><code>ls -lart /tmp/*iso\n</code></pre> Output<pre><code>-rw-r--r-- 1 root root 1220542464 Oct 27 11:58 /tmp/rhcos-4.16.3-x86_64-live.x86_64.iso\n-rw-r--r-- 1 root root 1220235264 Oct 27 11:59 /tmp/rhcos-install-cluster.iso\n</code></pre> </li> <li> <p>Move original ISO files to their own folder:</p> [root@bastiononline core-iso-maker]<pre><code>mkdir /root/registry/downloads/images/orig\nmv /root/registry/downloads/images/rhcos-4.16.3-x86_64-* /root/registry/downloads/images/orig/\ncp /tmp/*.iso /root/registry/downloads/images/\nchmod 777 /tmp/rhcos-install-cluster.iso\n</code></pre> </li> <li> <p>SCP the final ISO file into the Guacamole VM.</p> <p>Tip</p> <p>The following command is executed from within the Guacamole VM. That is, you need to connect to the vCenter, launch the web console for the Guacamole VM, open a terminal window in there and execute the command below. To upload ISO files, as we did in the section 7. Guacamole VM at the begining of the course for uploading the RHEL 8.7 OS ISO file for the online bastion and offline bastion, we must do it from the Guacamole VM because the VPN bandwidth is very limited.</p> <p>Extra tip for free ;-)</p> <p>To get the <code>@</code> symbol typed on the terminal, because the layout of the keyboard set up in the Guacamole VM might very well be different than your laptops keyboard layout, we strongly recommned you open the brower and google \"at symbol\". From the results displayed by Google, copy the <code>@</code> symbol and paste it in your terminal</p> [Guacamole VM]<pre><code>scp root@192.168.252.22:/tmp/rhcos-install-cluster.iso ./\n</code></pre> </li> </ol> <p>Once you have copied the ISO file from the online bastion, you must add the file to vCenter into a datastore:</p> <p></p>"},{"location":"airgapped/orig/#29-create-the-openshift-vms-in-the-vcenter","title":"29. Create the OpenShift VMs in the vCenter","text":"<p>We need to create the following virtual machines in vCenter according to the following specs:</p> <p>| Type | Number of servers | vCPU | RAM Memory | Storage (system) | Storage (data) | |------|-------------------|------|------------|------------------|----------------|-----| |Control Plane         |3|8 vCPU|32 GB|300 GB|0 GB| |Infra          |3|8 vCPU|32 GB|300 GB|0 GB| |Compute    |3|16 vCPU|64 GB|300 GB|0 GB| |Storage    |3|16 vCPU|64 GB|300 GB|500 GB| |Bootstrap      |1|4 vCPU|16 GB|320 GB|0 GB|</p> <p>And with the following nomenclature:</p> Type FQDN hostname Bootstrap bootstrap.ocp4.platformengineers.xyz Control Plane controlplane01.ocp4.platformengineers.xyz controlplane02.ocp4.platformengineers.xyz controlplane03.ocp4.platformengineers.xyz Infra infra01.ocp4.platformengineers.xyz infra02.ocp4.platformengineers.xyz infra03.ocp4.platformengineers.xyz Compute compute01.ocp4.platformengineers.xyz compute02.ocp4.platformengineers.xyz compute03.ocp4.platformengineers.xyz Storage storage01.ocp4.platformengineers.xyz storage02.ocp4.platformengineers.xyz storage03.ocp4.platformengineers.xyz <p>You can create folders for the different type of VMs if you wish.</p> <p></p> <p>You can review the process for creating the VMs in the section Create the online bastion.</p> <p></p> <p>You need to configure the machines to boot from the ISO that was created in the previous chapter Configure CoreOS ISO Maker in \"online\" bastion</p> <p></p> <p></p> <p>At the end, you should have the following VMs created in your vCenter</p> <p></p>"},{"location":"airgapped/orig/#30-create-the-install-configyaml-file","title":"30. Create the install-config.yaml file","text":"<ol> <li> <p>Exit the online bastion:</p> [root@bastiononline coreos-iso-maker]<pre><code>exit\n</code></pre> </li> <li> <p>Access the offline bastion:</p> [student laptop]<pre><code>ssh root@192.168.252.23\n</code></pre> </li> <li> <p>Create the directory where the <code>install-config.yaml</code> file will be placed:</p> [root@bastion ~]<pre><code>cd /root/registry/downloads/tools/\n</code></pre> [root@bastion tools]<pre><code>mkdir ocp4\ncd ocp4\n</code></pre> </li> </ol> <p>You need to cat the following files to create the <code>install-config.yaml</code> in the next step:</p> <ol> <li> <p>Quay private image registry CA</p> [root@bastion ocp4]<pre><code>cat /root/registry/quay-rootCA/rootCA.pem\n</code></pre> rootCA.pem<pre><code>-----BEGIN CERTIFICATE-----\nMIID3jCCAsagAwIBAgIUHj/Ux66Fpoktc/qtzoP6wgWZ2aIwDQYJKoZIhvcNAQEL\nBQAwdzELMAkGA1UEBhMCVVMxCzAJBgNVBAgMAlZBMREwDwYDVQQHDAhOZXcgWW9y\nazENMAsGA1UECgwEUXVheTERMA8GA1UECwwIRGl2aXNpb24xJjAkBgNVBAMMHWJh\nc3Rpb24ub2NwNC5jc20tc3BnaS5hY21lLmVzMB4XDTIzMDkyMTE4NDAzM1oXDTI2\nMDcxMTE4NDAzM1owdzELMAkGA1UEBhMCVVMxCzAJBgNVBAgMAlZBMREwDwYDVQQH\nDAhOZXcgWW9yazENMAsGA1UECgwEUXVheTERMA8GA1UECwwIRGl2aXNpb24xJjAk\nBgNVBAMMHWJhc3Rpb24ub2NwNC5jc20tc3BnaS5hY21lLmVzMIIBIjANBgkqhkiG\n9w0BAQEFAAOCAQ8AMIIBCgKCAQEAv3eDBDh1GH6i+hptdlbMPYztWDsHCihyjRta\nWgD3SEyHobuWM/lSepwMC2mmyejZKsv/x4PYFsNrWBwt1oVicP3+sGNuNfKheFgn\nLAiXAgt35MLPR7oXZKCb82rb5jFUYWU6Ro2xHv8qxLAAv+BRzq5fDYczvsps3pq0\nrvdNLfCORBmtQnoRuKXTmOreQgZqHZkhb1g8P4jE+BJugSAQT4gy1cnP5HXFqpdC\nqaehQeQEi+k3yHVdYnMyVC2Ya609SUUeSUxuLrlO2EQTSwLzSZ02sMsIfBKbOgmu\nTJPGgd5jqrSu1LDem8YKws8u0MVEhkl3LkuUuOYBbt4kPxoTIwIDAQABo2IwYDAL\nBgNVHQ8EBAMCAuQwEwYDVR0lBAwwCgYIKwYBBQUHAwEwKAYDVR0RBCEwH4IdYmFz\ndGlvbi5vY3A0LmNzbS1zcGdpLmFjbWUuZXMwEgYDVR0TAQH/BAgwBgEB/wIBATAN\nBgkqhkiG9w0BAQsFAAOCAQEAEIanwOhmODUlshlmpXUMeAa7g8S/cEUy19tpo932\n/sHoxVJogMVdxBY2V1EjNUPr/6PImesUq0IA1piYXqdxw23kiTgAM01THn6xq9Bn\nef7jKPuD7dVY1V8xMmt9EF1Bh4aTObCotF/6XOnBA6NaxWNq5jJ7dfRVU7Vqh5xa\nhyGRglgJw1LmrhwXTHLi6XBZXuwVlAw5HZFEsKsqD/6YqhmTMV7DA3Er/g3Qv1iJ\nNmnM5Ftr1huVtwXuETzm0VHIteSWXBV8/c4+COTlQpwVH2mQRcAZQsW8FibQIjP5\nk/26D0f5XWdf0u6cLowCmWRWhHrTe0ttUUm5ZcP7xeZNjA==\n-----END CERTIFICATE-----\n</code></pre> <p>Info</p> <p>The content of this file will be pasted under <code>additionalTrustBundle</code> tag in the <code>install-config.yaml</code> file.</p> </li> <li> <p>The Image Content Source Policy for the mirrored images</p> [root@bastion ocp4]<pre><code>cat /root/oc-mirror-workspace/results-XXXXXX/imageContentSourcePolicy.yaml\n</code></pre> imageContentSourcePolicy.yaml<pre><code>---\napiVersion: operator.openshift.io/v1alpha1\nkind: ImageContentSourcePolicy\nmetadata:\nname: generic-0\nspec:\nrepositoryDigestMirrors:\n- mirrors:\n    - bastion.ocp4.platformengineers.xyz:8443/ubi8\n    source: registry.redhat.io/ubi8\n---\napiVersion: operator.openshift.io/v1alpha1\nkind: ImageContentSourcePolicy\nmetadata:\nlabels:\n    operators.openshift.org/catalog: \"true\"\nname: operator-0\nspec:\nrepositoryDigestMirrors:\n- mirrors:\n    - bastion.ocp4.platformengineers.xyz:8443/openshift-logging\n    source: registry.redhat.io/openshift-logging\n- mirrors:\n    - bastion.ocp4.platformengineers.xyz:8443/openshift-pipelines\n    source: registry.redhat.io/openshift-pipelines\n- mirrors:\n    - bastion.ocp4.platformengineers.xyz:8443/rhel8\n    source: registry.redhat.io/rhel8\n- mirrors:\n    - bastion.ocp4.platformengineers.xyz:8443/ubi8\n    source: registry.redhat.io/ubi8\n- mirrors:\n    - bastion.ocp4.platformengineers.xyz:8443/redhat\n    source: registry.redhat.io/redhat\n- mirrors:\n    - bastion.ocp4.platformengineers.xyz:8443/openshift-serverless-1\n    source: registry.redhat.io/openshift-serverless-1\n- mirrors:\n    - bastion.ocp4.platformengineers.xyz:8443/ocp-tools-4-tech-preview\n    source: registry.redhat.io/ocp-tools-4-tech-preview\n- mirrors:\n    - bastion.ocp4.platformengineers.xyz:8443/openshift4\n    source: registry.redhat.io/openshift4\n---\napiVersion: operator.openshift.io/v1alpha1\nkind: ImageContentSourcePolicy\nmetadata:\nname: release-0\nspec:\nrepositoryDigestMirrors:\n- mirrors:\n    - bastion.ocp4.platformengineers.xyz:8443/openshift/release\n    source: quay.io/openshift-release-dev/ocp-v4.0-art-dev\n- mirrors:\n    - bastion.ocp4.platformengineers.xyz:8443/openshift/release-images\n    source: quay.io/openshift-release-dev/ocp-release\n</code></pre> <p>Info</p> <p>The highlighted content will be pasted under <code>imageContentSources</code> tag in the <code>install-config.yaml</code> file.</p> </li> <li> <p>Create the <code>install-config.yaml</code> file:</p> [root@bastion ocp4]<pre><code>vi install-config.yaml\n</code></pre> </li> <li> <p>Make sure it looks like below:</p> install-config.yaml<pre><code>apiVersion: v1\nbaseDomain: platformengineers.xyz\ncompute:\n- hyperthreading: Enabled\n  name: worker\n  replicas: 9\ncontrolPlane:\n  hyperthreading: Enabled\n  name: master\n  replicas: 3\nmetadata:\n  name: ocp4\nnetworking:\n  clusterNetworks:\n  - cidr: 9.248.0.0/14\n    hostPrefix: 24\n  machineNetwork:\n  - cidr: 192.168.252.0/24\n  networkType: OVNKubernetes\n  serviceNetwork:\n  - 9.31.0.0/16\nplatform:\n  none: {}\nfips: false\npullSecret: '{\"auths\": {\"bastion.ocp4.platformengineers.xyz:8443\": { \"auth\": \"aW5pdDpwYXNzdzByZA==\", \"email\": \"you@example.com\"}}}'\nsshKey: 'ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAICIl3vZvi6eh1SkTE9EWKnZZwG6bVvZZzZzBHuKgbrVW root@bastion.ocp4.platformengineers.xyz'\nadditionalTrustBundle: |\n    -----BEGIN CERTIFICATE-----\n    MIID3jCCAsagAwIBAgIUHj/Ux66Fpoktc/qtzoP6wgWZ2aIwDQYJKoZIhvcNAQEL\n    BQAwdzELMAkGA1UEBhMCVVMxCzAJBgNVBAgMAlZBMREwDwYDVQQHDAhOZXcgWW9y\n    azENMAsGA1UECgwEUXVheTERMA8GA1UECwwIRGl2aXNpb24xJjAkBgNVBAMMHWJh\n    c3Rpb24ub2NwNC5jc20tc3BnaS5hY21lLmVzMB4XDTIzMDkyMTE4NDAzM1oXDTI2\n    MDcxMTE4NDAzM1owdzELMAkGA1UEBhMCVVMxCzAJBgNVBAgMAlZBMREwDwYDVQQH\n    DAhOZXcgWW9yazENMAsGA1UECgwEUXVheTERMA8GA1UECwwIRGl2aXNpb24xJjAk\n    BgNVBAMMHWJhc3Rpb24ub2NwNC5jc20tc3BnaS5hY21lLmVzMIIBIjANBgkqhkiG\n    9w0BAQEFAAOCAQ8AMIIBCgKCAQEAv3eDBDh1GH6i+hptdlbMPYztWDsHCihyjRta\n    WgD3SEyHobuWM/lSepwMC2mmyejZKsv/x4PYFsNrWBwt1oVicP3+sGNuNfKheFgn\n    LAiXAgt35MLPR7oXZKCb82rb5jFUYWU6Ro2xHv8qxLAAv+BRzq5fDYczvsps3pq0\n    rvdNLfCORBmtQnoRuKXTmOreQgZqHZkhb1g8P4jE+BJugSAQT4gy1cnP5HXFqpdC\n    qaehQeQEi+k3yHVdYnMyVC2Ya609SUUeSUxuLrlO2EQTSwLzSZ02sMsIfBKbOgmu\n    TJPGgd5jqrSu1LDem8YKws8u0MVEhkl3LkuUuOYBbt4kPxoTIwIDAQABo2IwYDAL\n    BgNVHQ8EBAMCAuQwEwYDVR0lBAwwCgYIKwYBBQUHAwEwKAYDVR0RBCEwH4IdYmFz\n    dGlvbi5vY3A0LmNzbS1zcGdpLmFjbWUuZXMwEgYDVR0TAQH/BAgwBgEB/wIBATAN\n    BgkqhkiG9w0BAQsFAAOCAQEAEIanwOhmODUlshlmpXUMeAa7g8S/cEUy19tpo932\n    /sHoxVJogMVdxBY2V1EjNUPr/6PImesUq0IA1piYXqdxw23kiTgAM01THn6xq9Bn\n    ef7jKPuD7dVY1V8xMmt9EF1Bh4aTObCotF/6XOnBA6NaxWNq5jJ7dfRVU7Vqh5xa\n    hyGRglgJw1LmrhwXTHLi6XBZXuwVlAw5HZFEsKsqD/6YqhmTMV7DA3Er/g3Qv1iJ\n    NmnM5Ftr1huVtwXuETzm0VHIteSWXBV8/c4+COTlQpwVH2mQRcAZQsW8FibQIjP5\n    k/26D0f5XWdf0u6cLowCmWRWhHrTe0ttUUm5ZcP7xeZNjA==\n    -----END CERTIFICATE-----\nimageContentSources:\n- mirrors:\n  - bastion.ocp4.platformengineers.xyz:8443/openshift/release\n  source: quay.io/openshift-release-dev/ocp-v4.0-art-dev\n- mirrors:\n  - bastion.ocp4.platformengineers.xyz:8443/openshift/release-images\n  source: quay.io/openshift-release-dev/ocp-release\n</code></pre> <p>Important</p> <p>Remember the <code>additionalTrusBundle</code> and <code>imageContentSources</code> sections come from the data output in the steps above.</p> <p>The highlighted content on the <code>pullSecret</code> tag is the base64-encoded username and password obtained on the Create the private image registry in the offline bastion section when we ran the <code>mirror-registry install</code> command.</p> [root@bastion ocp4]<pre><code>echo -n 'init:passw0rd' | base64 -w0\n</code></pre> Output<pre><code>aW5pdDpwYXNzdzByZA==\n</code></pre> <p>Tip</p> <p>If you used the default <code>passw0rd</code> when installing your Quay private image registry, the <code>pullSecret</code> value in the <code>instal-config.yaml</code> above will be exactly the same value you need to use.</p> <p>The content for the <code>sshKey</code> tag is the output of:</p> [root@bastion ocp4]<pre><code>cat /root/.ssh/id_ed25519.pub\n</code></pre> Output<pre><code>ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAICIl3vZvi6eh1SkTE9EWKnZZwG6bVvZZzZzBHuKgbrVW root@bastion.ocp4.platformengineers.xyz\n</code></pre> </li> <li> <p>Create a copy of the <code>install-config.yaml</code> file in another directory, as it is automatically deleted in the following steps:</p> [root@bastion ocp4]<pre><code>cp install-config.yaml ../install-config.yaml\n</code></pre> </li> </ol>"},{"location":"airgapped/orig/#31-create-the-openshift-manifests-and-ignition-files","title":"31. Create the OpenShift Manifests and Ignition files","text":"<ol> <li> <p>Create the OpenShift manifest files</p> [root@bastion ocp4]<pre><code>openshift-install create manifests --dir /root/registry/downloads/tools/ocp4\n</code></pre> Output<pre><code>INFO Consuming Install Config from target directory\nINFO Manifests created in: /root/registry/downloads/tools/ocp4/manifests and /root/registry/downloads/tools/ocp4/openshift\n</code></pre> </li> <li> <p>In order to create the ignition files, install butane in the bastion node:</p> [root@bastion ocp4]<pre><code>cd /root/registry/downloads/tools/\n</code></pre> [root@bastion tools]<pre><code>chmod u+x butane\ncp butane /usr/local/bin/butane\nbutane -V\n</code></pre> Output<pre><code>Butane 0.18.0\n</code></pre> </li> </ol> <p>Create two butane files for the master and worker nodes. In these files, you set the chrony configuration for master and worker nodes.</p> <p>99-worker-chrony-conf-override.bu</p> <ol> <li> <p>Create the <code>99-worker-chrony-conf-override.bu</code> file</p> [root@bastion tools]<pre><code>vi 99-worker-chrony-conf-override.bu\n</code></pre> </li> <li> <p>Make sure it looks like below</p> 99-worker-chrony-conf-override.bu<pre><code>variant: openshift\nversion: 4.16.0\nmetadata:\n  name: 99-worker-chrony-conf-override\n  labels:\n    machineconfiguration.openshift.io/role: worker\nstorage:\n  files:\n    - path: /etc/chrony.conf\n      mode: 0644\n      overwrite: true\n      contents:\n        inline: |\n          # The Machine Config Operator manages this file.\n          server controlplane01.ocp4.platformengineers.xyz iburst\n          server controlplane02.ocp4.platformengineers.xyz iburst\n          server controlplane03.ocp4.platformengineers.xyz iburst\n\n          stratumweight 0\n          driftfile /var/lib/chrony/drift\n          rtcsync\n          makestep 10 3\n          bindcmdaddress 127.0.0.1\n          bindcmdaddress ::1\n          keyfile /etc/chrony.keys\n          commandkey 1\n          generatecommandkey\n          noclientlog\n          logchange 0.5\n          logdir /var/log/chrony\n</code></pre> </li> </ol> <p>99-master-chrony-conf-override.bu</p> <ol> <li> <p>Create the <code>99-master-chrony-conf-override.bu</code> file</p> [root@bastion tools]<pre><code>vi 99-master-chrony-conf-override.bu\n</code></pre> </li> <li> <p>Make sure it looks like below</p> 99-master-chrony-conf-override.bu<pre><code>variant: openshift\nversion: 4.16.0\nmetadata:\n  name: 99-master-chrony-conf-override\n  labels:\n    machineconfiguration.openshift.io/role: master\nstorage:\n  files:\n    - path: /etc/chrony.conf\n      mode: 0644\n      overwrite: true\n      contents:\n        inline: |\n          # Use public servers from the pool.ntp.org project.\n          # Please consider joining the pool (https://www.pool.ntp.org/join.html).\n\n          # The Machine Config Operator manages this file\n          server controlplane01.ocp4.platformengineers.xyz iburst\n          server controlplane02.ocp4.platformengineers.xyz iburst\n          server controlplane03.ocp4.platformengineers.xyz iburst\n\n          stratumweight 0\n          driftfile /var/lib/chrony/drift\n          rtcsync\n          makestep 10 3\n          bindcmdaddress 127.0.0.1\n          bindcmdaddress ::1\n          keyfile /etc/chrony.keys\n          commandkey 1\n          generatecommandkey\n          noclientlog\n          logchange 0.5\n          logdir /var/log/chrony\n\n          # Configure the control plane nodes to serve as local NTP servers\n          # for all worker nodes, even if they are not in sync with an\n          # upstream NTP server.\n\n          # Allow NTP client access from the local network.\n          allow all\n          # Serve time even if not synchronized to a time source.\n          local stratum 3 orphan\n</code></pre> </li> <li> <p>Generate the yaml files with butane:</p> [root@bastion tools]<pre><code>butane 99-worker-chrony-conf-override.bu -o 99-worker-chrony-conf-override.yaml\n</code></pre> [root@bastion tools]<pre><code>butane 99-master-chrony-conf-override.bu -o 99-master-chrony-conf-override.yaml\n</code></pre> </li> <li> <p>Copy them to the installation directory:</p> [root@bastion tools]<pre><code>cp 99-*.yaml /root/registry/downloads/tools/ocp4/manifests/\n</code></pre> </li> <li> <p>Make a copy of the installation directory:</p> [root@bastion tools]<pre><code>cp -r ocp4 ocp4_backup\n</code></pre> </li> <li> <p>Create the ignition files:</p> [root@bastion tools]<pre><code>openshift-install create ignition-configs --dir /root/registry/downloads/tools/ocp4/\n</code></pre> Output<pre><code>INFO Consuming Common Manifests from target directory\nINFO Consuming OpenShift Install (Manifests) from target directory\nINFO Consuming Master Machines from target directory\nINFO Consuming Openshift Manifests from target directory\nINFO Consuming Worker Machines from target directory\nINFO Ignition-Configs created in: /root/registry/downloads/tools/ocp4 and /root/registry/downloads/tools/ocp4/auth\n</code></pre> </li> <li> <p>Provide read and execute permissions for other users and groups to the ignition files:</p> [root@bastion tools]<pre><code>chmod 755 /root/registry/downloads/tools/ocp4/*.ign\n</code></pre> </li> <li> <p>Copy the ignition files to the serving path of the HTTP Server we installed on the Install the http server in the offline bastion section before.</p> [root@bastion tools]<pre><code>cp /root/registry/downloads/tools/ocp4/*.ign /var/www/html/ign/\nll /var/www/html/ign/\n</code></pre> Output<pre><code>total 288\n-rwxr-xr-x 1 root root 284065 Sep 14 05:19 bootstrap.ign\n-rwxr-xr-x 1 root root   1723 Sep 14 05:19 master.ign\n-rwxr-xr-x 1 root root   1723 Sep 14 05:19 worker.ign\n</code></pre> </li> <li> <p>It is recommended to back the directory <code>ocp4/auth</code> up, because it contains the certificates and key of the initial user and administrator of OpenShift: the user kubeadmin.</p> [root@bastion tools]<pre><code>cp -r /root/registry/downloads/tools/ocp4/auth/ /root/registry/downloads/tools/ocp4_auth/\n</code></pre> </li> </ol>"},{"location":"airgapped/orig/#32-deploy-the-openshift-cluster","title":"32. Deploy the OpenShift cluster","text":"<p>In order to get our OpenShift cluster deployed, we first need to start up the bootstrap node that will operate as a temporary control plane for the Control Plane nodes to join later.</p>"},{"location":"airgapped/orig/#321-control-plane-nodes","title":"32.1. Control Plane Nodes","text":"<ol> <li> <p>In your vCenter, start up the bootstrap node. Open the web console for this node in the vCenter so that you can interact with the ISO bootup menu. Remember, this is the bootup menu we have baked into the ISO with the ISO maker from the previous section. Select the bootstrap option from the bootup menu:</p> <p></p> <p>Once the specific bootstrap configuration has been loaded on the machine, you should see the following login screen:</p> <p></p> </li> </ol> <p>At this point, that temporary control plane node for the rest of the Contol Plane nodes to join has been created and started so that you can you can start the control machines.</p> <p>Important</p> <p>Remember to select the appropriate bootup option for the machine you will be starting up on the following steps.</p> <ol> <li> <p>Start the controlplane01 machine:</p> <p></p> <p>Make sure it starts up properly:</p> <p></p> </li> <li> <p>Start the controlplane02 machine:</p> <p></p> <p>Make sure it starts up properly:</p> <p></p> </li> <li> <p>Start the controlplane03 machine:</p> <p></p> <p>Make sure it starts up properly:</p> <p></p> </li> <li> <p>Access the offline bastion:</p> [root@bastion ~]<pre><code>ssh root@192.168.252.23\n</code></pre> </li> <li> <p>Go to <code>/root/registry/downloads/</code> to kick off the wait for the bootstrap to complete command:</p> [root@bastion ~]<pre><code>cd /root/registry/downloads/\n</code></pre> </li> <li> <p>Kick off the wait for the bootstrap to complete command:</p> [root@bastion downloads]<pre><code>openshift-install wait-for bootstrap-complete --dir=/root/registry/downloads/tools/ocp4 --log-level=debug\n</code></pre> <p>If the bootstrap of the control plane nodes for your OpenShift cluster has completed correctly, you should see the following ouput:</p> Output<pre><code>DEBUG OpenShift Installer 4.16.9\nDEBUG Built from commit 194f806390b2b20668f5ac32d7918f9773b29a5e\nINFO Waiting up to 20m0s (until 9:53AM) for the Kubernetes API at https://api.ocp4.platformengineers.xyz:6443...\nDEBUG Loading Agent Config...\nINFO API v1.25.11+1485cc9 up\nDEBUG Loading Install Config...\nDEBUG   Loading SSH Key...\nDEBUG   Loading Base Domain...\nDEBUG     Loading Platform...\nDEBUG   Loading Cluster Name...\nDEBUG     Loading Base Domain...\nDEBUG     Loading Platform...\nDEBUG   Loading Networking...\nDEBUG     Loading Platform...\nDEBUG   Loading Pull Secret...\nDEBUG   Loading Platform...\nDEBUG Using Install Config loaded from state file\nINFO Waiting up to 30m0s (until 10:03AM) for bootstrapping to complete...\nDEBUG Bootstrap status: complete\nINFO It is now safe to remove the bootstrap resources\nDEBUG Time elapsed per stage:\nDEBUG Bootstrap Complete: 1m48s\nINFO Time elapsed: 1m48s\n</code></pre> <p>Tip</p> <p>If the previous command fails or takes too long, you can do the following to troubleshoot/debug:</p> [root@bastion ~]<pre><code>ssh core@bootstrap.ocp4.platformengineers.xyz\n</code></pre> <p>Once inside the bootstrap machine, execute the following command to see if there are any errors that ocurred during the installation.</p> [core@bootstrap ~]<pre><code>journalctl -f\n</code></pre> </li> <li> <p>At this point, we can stop the bootstrap machine and delete it from the vCenter as it will no longer be used during installation.</p> </li> <li> <p>Execute the following command to be able to use oc commands as an administrator against the control plane of your OpenShift cluster:</p> [root@bastion downloads]<pre><code>export KUBECONFIG=/root/registry/downloads/tools/ocp4/auth/kubeconfig\n</code></pre> </li> <li> <p>Execute the following command that should output what user are you authenticated as by using the above <code>kubeconfig</code> credentials file.</p> [root@bastion downloads]<pre><code>oc whoami\n</code></pre> Output<pre><code>system:admin\n</code></pre> </li> <li> <p>Get the nodes of your OpenShift cluster</p> [root@bastion downloads]<pre><code>oc get nodes\n</code></pre> Output<pre><code>NAME                             STATUS   ROLES                  AGE     VERSION\ncontrolplane01.ocp4.platformengineers.xyz   Ready    control-plane,master   25m     v1.29.7+4510e9c\ncontrolplane02.ocp4.platformengineers.xyz   Ready    control-plane,master   19m     v1.29.7+4510e9c\ncontrolplane03.ocp4.platformengineers.xyz   Ready    control-plane,master   7m41s   v1.29.7+4510e9c\n</code></pre> <p>Warning</p> <p>Control Plane nodes must appear in Ready state.</p> </li> </ol>"},{"location":"airgapped/orig/#322-compute-nodes","title":"32.2. Compute Nodes","text":"<p>Once the control plane nodes are ready, we can start all the compute machines, including the infra and storage machines, and to do this, again, make sure you select the appropriate option on the bootup menu:</p> <ol> <li> <p>Start the compute01 machine:</p> <p></p> </li> <li> <p>Start the compute02 machine:</p> <p></p> </li> <li> <p>Start the compute03 machine:</p> <p></p> </li> <li> <p>Start the infra01 machine:</p> <p></p> </li> <li> <p>Start the infra02 machine:</p> <p></p> </li> <li> <p>Start the infra03 machine:</p> <p></p> </li> <li> <p>Start the storage01 machine:</p> <p></p> </li> <li> <p>Start the storage02 machine:</p> <p></p> </li> <li> <p>Start the storage03 machine:</p> <p></p> </li> </ol> <p>Once the compute nodes have successfully started, we should see that the certificate signing request to join the OpenShift cluster should be pending. In order to authorize these workers to join the OpenShift cluster, we need to approve those certificate signing requests.</p> <p>Tip</p> <p>Give around 10mins for the machines to start up so that you can see all the CSR at once. You should have one per machine and you will need to approve a CSR per machine twice.</p> <ol> <li> <p>Get the certificate signing requests (csr):</p> [root@bastion downloads]<pre><code>oc get csr\n</code></pre> Output<pre><code>NAME        AGE   SIGNERNAME                                    REQUESTOR                                                                   REQUESTEDDURATION   CONDITION\ncsr-dncq9   96s   kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   &lt;none&gt;              Pending\ncsr-fxhxg   62s   kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   &lt;none&gt;              Pending\ncsr-kgkgs   6s    kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   &lt;none&gt;              Pending\ncsr-ntmtw   12s   kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   &lt;none&gt;              Pending\ncsr-vzbgt   76s   kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   &lt;none&gt;              Pending\ncsr-zrr6n   41s   kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   &lt;none&gt;              Pending\n</code></pre> </li> <li> <p>Approve any pending certificate signing request:</p> [root@bastion downloads]<pre><code>oc get csr -o go-template='{{range .items}}{{if not .status}}{{.metadata.name}}{{\"\\n\"}}{{end}}{{end}}' | xargs oc adm certificate approve\n</code></pre> Output<pre><code>certificatesigningrequest.certificates.k8s.io/csr-dncq9 approved\ncertificatesigningrequest.certificates.k8s.io/csr-fxhxg approved\ncertificatesigningrequest.certificates.k8s.io/csr-kgkgs approved\ncertificatesigningrequest.certificates.k8s.io/csr-ntmtw approved\ncertificatesigningrequest.certificates.k8s.io/csr-vzbgt approved\ncertificatesigningrequest.certificates.k8s.io/csr-zrr6n approved\n</code></pre> </li> </ol> <p>Tip</p> <p>You may need to get and approve all pending certificate signing request few times as these will appear as machines come up. Also, to get a node to join the OpenShift cluster, two cerficate signing requests must be approved. Therefore, repeat the previous two steps few times until there is no pending certificate signing request.</p> <p>After a reasonable amount of time (15 minutes aprox), confirm that the cluster operators are available and there is none in degraded state if no pending certificate signing request is listed. </p> <ol> <li> <p>Get the not ready cluster operators</p> [root@bastion downloads]<pre><code>oc get co | grep -e \"4.16.9 False\" -e \"False True\" -e \"True          False\" -e \"NAME\"\n</code></pre> Output<pre><code>NAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE\nauthentication                             4.16.9    False       True          False      7h40m   WellKnownAvailable: The well-known endpoint is not yet available: kube-apiserver oauth endpoint https://192.168.252.5:6443/.well-known/oauth-authorization-server is not yet served and authentication operator keeps waiting (check kube-apiserver operator, and check that instances roll out successfully, which can take several minutes per instance)\nkube-apiserver                             4.16.9   True        True          False      7h36m   NodeInstallerProgressing: 2 nodes are at revision 6; 1 nodes are at revision 7\n</code></pre> <p>Warning</p> <p>Repeat the command until there is no cluster operator listed. That will mean all cluster operators are healthy</p> </li> <li> <p>Get all the cluster operators</p> [root@bastion downloads]<pre><code>oc get co\n</code></pre> Output<pre><code>NAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE\nauthentication                             4.16.9    True        False         False      2m22s\nbaremetal                                  4.16.9    True        False         False      47m\ncloud-controller-manager                   4.16.9    True        False         False      50m\ncloud-credential                           4.16.9    True        False         False      55m\ncluster-autoscaler                         4.16.9    True        False         False      47m\nconfig-operator                            4.16.9    True        False         False      48m\nconsole                                    4.16.9    True        False         False      7m59s\ncontrol-plane-machine-set                  4.16.9    True        False         False      48m\ncsi-snapshot-controller                    4.16.9    True        False         False      48m\ndns                                        4.16.9    True        False         False      47m\netcd                                       4.16.9    True        False         False      46m\nimage-registry                             4.16.9    True        False         False      23m\ningress                                    4.16.9    True        False         False      10m\ninsights                                   4.16.9    True        False         False      35m\nkube-apiserver                             4.16.9    True        False         False      34m\nkube-controller-manager                    4.16.9    True        False         False      45m\nkube-scheduler                             4.16.9    True        False         False      45m\nkube-storage-version-migrator              4.16.9    True        False         False      48m\nmachine-api                                4.16.9    True        False         False      47m\nmachine-approver                           4.16.9    True        False         False      47m\nmachine-config                             4.16.9    True        False         False      47m\nmarketplace                                4.16.9    True        False         False      47m\nmonitoring                                 4.16.9    True        False         False      9m37s\nnetwork                                    4.16.9    True        False         False      40m\nnode-tuning                                4.16.9    True        False         False      13m\nopenshift-apiserver                        4.16.9    True        False         False      30m\nopenshift-controller-manager               4.16.9    True        False         False      40m\nopenshift-samples                          4.16.9    True        False         False      25m\noperator-lifecycle-manager                 4.16.9    True        False         False      47m\noperator-lifecycle-manager-catalog         4.16.9    True        False         False      47m\noperator-lifecycle-manager-packageserver   4.16.9    True        False         False      33m\nservice-ca                                 4.16.9    True        False         False      48m\nstorage                                    4.16.9    True        False         False      48m\n</code></pre> </li> <li> <p>Execute the wait for install to complete command</p> [root@bastion downloads]<pre><code>cd /root/registry/downloads/tools/ocp4\n</code></pre> [root@bastion ocp4]<pre><code>openshift-install wait-for install-complete\n</code></pre> Output<pre><code>INFO Waiting up to 40m0s (until 4:28PM) for the cluster at https://api.ocp4.platformengineers.xyz:6443 to initialize...\nINFO Checking to see if there is a route at openshift-console/console...\nINFO Install complete!\nINFO To access the cluster as the system:admin user when using 'oc', run 'export KUBECONFIG=/root/registry/downloads/tools/ocp4/auth/kubeconfig'\nINFO Access the OpenShift web-console here: https://console-openshift-console.apps.ocp4.platformengineers.xyz\nINFO Login to the console with user: \"kubeadmin\", and password: \"o8o6u-zgz8h-CobwL-pWb9i\"\nINFO Time elapsed: 0s\n</code></pre> <p>Tip</p> <p>Alternatively, the same credentials can be found at <code>/root/registry/downloads/tools/ocp4/auth</code></p> </li> </ol> <p>Let's now open up our OpenShift cluster web console in our laptop's web browser.</p> <ol> <li> <p>Add the following urls to your laptop hosts file with the IP address of the load balancer so that you can access them.</p> [student laptop]<pre><code>vi /etc/hosts\n</code></pre> <p>Add the following entry to your hosts file:</p> /etc/hosts<pre><code>192.168.252.24 console-openshift-console.apps.ocp4.platformengineers.xyz oauth-openshift.apps.ocp4.platformengineers.xyz\n</code></pre> </li> <li> <p>Now, you can connect via a web browser to the OpenShift web console using the credentials and url from the OpenShift wait for install to complete command:</p> <p>https://console-openshift-console.apps.ocp4.platformengineers.xyz</p> <p></p> </li> </ol>"},{"location":"airgapped/orig/#33-move-ingress-routing-and-internal-registry-loads-to-infrastructure-nodes","title":"33. Move ingress routing and internal registry loads to infrastructure nodes","text":"<p>This operation is performed to move OpenShift infrastructure components from the compute nodes to the infra nodes. The infra nodes do not consume OpenShift licenses.</p> <ol> <li> <p>Create the labels for the infra nodes</p> [root@bastion ocp4]<pre><code>oc label node infra01.ocp4.platformengineers.xyz node-role.kubernetes.io/infra=\n</code></pre> Output<pre><code>node/infra01.ocp4.platformengineers.xyz labeled\n</code></pre> [root@bastion ocp4]<pre><code>oc label node infra02.ocp4.platformengineers.xyz node-role.kubernetes.io/infra=\n</code></pre> Output<pre><code>node/infra02.ocp4.platformengineers.xyz labeled\n</code></pre> [root@bastion ocp4]<pre><code>oc label node infra03.ocp4.platformengineers.xyz node-role.kubernetes.io/infra=\n</code></pre> Output<pre><code>node/infra03.ocp4.platformengineers.xyz labeled\n</code></pre> </li> <li> <p>Taint the infra nodes to prevent them from receiving other workloads</p> [root@bastion ocp4]<pre><code>oc adm taint nodes -l node-role.kubernetes.io/infra infra=reserved:NoSchedule infra=reserved:NoExecute\n</code></pre> Output<pre><code>node/infra01.ocp4.platformengineers.xyz tainted\nnode/infra02.ocp4.platformengineers.xyz tainted\nnode/infra03.ocp4.platformengineers.xyz tainted\n</code></pre> </li> <li> <p>Move the OpenShift cluster ingress controller to the infra nodes.</p> [root@bastion ocp4]<pre><code>oc edit ingresscontroller default -n openshift-ingress-operator\n</code></pre> </li> <li> <p>Copy the highlighted lines into your default Ingress Controller specification.</p> deault<pre><code># Please edit the object below. Lines beginning with a '#' will be ignored,\n# and an empty file will abort the edit. If an error occurs while saving this file will be\n# reopened with the relevant failures.\n#\napiVersion: operator.openshift.io/v1\nkind: IngressController\nmetadata:\ncreationTimestamp: \"2023-09-14T13:30:51Z\"\nfinalizers:\n- ingresscontroller.operator.openshift.io/finalizer-ingresscontroller\ngeneration: 2\nname: default\nnamespace: openshift-ingress-operator\nresourceVersion: \"2658200\"\nuid: 40d5648f-98c3-46ac-9223-773e52b65f6a\nspec:\nclientTLS:\n    clientCA:\n    name: \"\"\n    clientCertificatePolicy: \"\"\nhttpCompression: {}\nhttpEmptyRequestsPolicy: Respond\nhttpErrorCodePages:\n    name: \"\"\nnodePlacement:\n  nodeSelector:\n    matchLabels:\n      node-role.kubernetes.io/infra: \"\"\n  tolerations:\n  - effect: NoSchedule\n    key: infra\n    value: reserved\n  - effect: NoExecute\n    key: infra\n    value: reserved\nreplicas: 2\ntuningOptions:\n    reloadInterval: 0s\n[...]\n</code></pre> </li> <li> <p>Move the internal OpenShift image registry to the infra nodes</p> [root@bastion ocp4]<pre><code>oc patch configs.imageregistry.operator.openshift.io/cluster --type=merge -p '{\"spec\":{\"nodeSelector\": {\"node-role.kubernetes.io/infra\": \"\"},\"tolerations\": [{\"effect\":\"NoSchedule\",\"key\": \"infra\",\"value\": \"reserved\"},{\"effect\":\"NoExecute\",\"key\": \"infra\",\"value\": \"reserved\"}]}}'\n</code></pre> Output<pre><code>config.imageregistry.operator.openshift.io/cluster patched\n</code></pre> </li> </ol>"},{"location":"airgapped/orig/#34-deploy-openshift-data-foundation-on-storage-nodes","title":"34. Deploy OpenShift Data Foundation on Storage nodes","text":"<p>This operation is performed to provide OpenShift with Block, Object and File persistent storage. Storage nodes do not consume OpenShift Licences.</p> <ol> <li> <p>Create the labels for the infra nodes</p> [root@bastion ocp4]<pre><code>oc label node storage01.ocp4.platformengineers.xyz node-role.kubernetes.io/infra=\n</code></pre> Output<pre><code>node/storage01.ocp4.platformengineers.xyz labeled\n</code></pre> </li> </ol>"},{"location":"airgapped/orig/#35-create-the-catalog-sources-to-access-the-redhat-marketplace-operators","title":"35. Create the catalog sources to access the RedHat Marketplace operators","text":"<p>The OpenShift cluster does not have internet access. Therefore the operator catalog for available operators has to be configured to point to the image mirrored to the Quay private image registry installed on the offline bastion.</p> <p>To do this, we will use the <code>catalogSource-redhat-operator-index.yaml</code> file previously created during installation and available in <code>/root/oc-mirror-workspace/results-XXXXXX</code>.</p> <p>Before applying the catalog source, the default catalogs that have been installed with Openshift cluster need to be deactivated:</p> [root@bastion ~]<pre><code>oc patch OperatorHub cluster --type json -p '[{\"op\": \"add\", \"path\": \"/spec/disableAllDefaultSources\", \"value\": true}]'\n</code></pre> Output<pre><code>operatorhub.config.openshift.io/cluster patched\n</code></pre> <p>We should see no operator available in the OperatorHub of our OpenShift cluster:</p> <p></p> <p>Now, to create that custom catalog source that points to our Quay private image registry, execute the following commands:</p> [root@bastion ~]<pre><code>cd /root/oc-mirror-workspace/results-XXXXX/\n</code></pre> [root@bastion results-XXXXX]<pre><code>oc apply -f catalogSource-cs-redhat-operator-index.yaml\n</code></pre> Output<pre><code>catalogsource.operators.coreos.com/redhat-operator-index created\n</code></pre> <p>Check now if the operators are listed:</p> <p></p>"},{"location":"airgapped/orig/#35-deactivate-the-cluster-update-channel","title":"35. Deactivate the cluster update channel","text":"<p>Openshift defines an update channel to search for and install updates. Since this cluster does not have any internet access, it is recommended to turn off the update channel.</p> <p>Run the following command to modify the update channel and set it to null, leaving the cluster without an update channel.</p> [root@bastion transfer-files]<pre><code>oc adm upgrade channel --allow-explicit-channel\n</code></pre> Output<pre><code>warning: Clearing channel \"stable-4.16\"; cluster will no longer request available update recommendations.\n</code></pre> <p>Congratulations!</p> <p>You have successfully installed an airgapped OpenShift 4.16 cluster. Well done!</p>"},{"location":"airgapped/success/","title":"Success!","text":"<p>Congratulations!</p> <p>You have successfully installed an air-gapped OpenShift Cluster. Well done!</p>"},{"location":"ama-app-mod/","title":"tWAS Application Modernization with AMA","text":""},{"location":"ama-app-mod/#twas-application-modernization-with-ama","title":"tWAS Application Modernization with AMA","text":""},{"location":"ama-app-mod/#prerequisites","title":"Prerequisites","text":"<p>The procedure within this exercise is based upon an IBM TechZone environment that will contain both a traditional WAS environment to be modernized and the tooling to perform the exercise.  Deploy the Modernize WebSphere apps to Liberty using watsonx code assistant - PoT &amp; MoRE / AMA demo environment - Q2 release environment.  </p> <p>This demo provides fundamental hands-on experience with modernizing existing Java applications to WebSphere Liberty, deployed into a container platform, such as Red Hat OpenShift.  The focus of this document is on the practical aspects of how to use the deployment artifacts created by AMA / TA  to speed up the process to deploy a Java app to Liberty running on OpenShift to achieve the objective for operation modernization.</p> <p>Note on TA and AMA</p> <p>Application Modernization Accelerator (AMA) is the next generation of Transformation Advisor (TA).  AMA is provided as part of the EAR JSphere Suite offering.  TA has been widely available for many years and most recently as part of Cloud Pak for Applications.</p>"},{"location":"ama-app-mod/#mod-resorts-application","title":"Mod Resorts Application","text":"<p>Mod Resorts app is a WebSphere application showing the weather in various locations. This app is initially developed for WebSphere traditional. In the demo it will be moved to Liberty in a container and deployed to OCP.  You can view the running version of this application from your bootcamp environment by navigating to localhost:9080/resorts</p> <p></p>"},{"location":"ama-app-mod/ama-install/","title":"AMA Installation","text":""},{"location":"ama-app-mod/ama-install/#install-ama","title":"Install AMA","text":""},{"location":"ama-app-mod/ama-install/#overview","title":"Overview","text":"<p>This guide provides step-by-step instructions for installing the Application Modernization Accelerator (AMA) on supported platforms. For comprehensive details, refer to the IBM Documentation: AMA Installation. AMA can be deployed within your TechZone environment or locally using Podman.</p> <p>Podman and Local AMA</p> <p>If you don't have Podman installed, download Podman Desktop to get started.  Installing AMA on your Mac still may take some tweaking and is not covered in this lab.  We will be running the installation as if the RHEL VM in Tech Zone is your the workstation.</p> <p>The AMA code itself is available from the IBM Registration and Download site where you can obtain the AMA Local install script.  We will eventually perform this from the lab TechZone system.</p>"},{"location":"ama-app-mod/ama-install/#log-in-to-your-tech-zone-rhel-server","title":"Log in to Your Tech Zone RHEL Server","text":"<p>This environment likely has a version of AMA / TA previously installed.  We will walk through the process of installing the latest version of AMA.  This is the process you will need to follow in the field.</p> <p>Cutting and Pasting to the RHEL Techzone VM</p> <p> From your local laptop copy and paste are not directly supported.  Use the VNC panel on the left side to facilitate this.  Paste the text into this panel and from within the noVNC use <code>control V</code> to complete the paste.  You can also improve the view experience by changing the settings within this noVNC control panel.</p> <ol> <li> <p>Access Your TechZone Environment for Managed Liberty MoRE-AMA-WCA</p> <ul> <li>Locate your reservation and click the noVNC link to connect to your server. </li> </ul> </li> <li> <p>The username is automatically set for the login  <code>techzone</code>.  You will be prompted for the Password: <code>IBMDem0s!</code> and choose Send Credentials.</p> </li> <li> <p>Prepare the Installation Directory </p> <ul> <li>Open a Terminal session on the VM and create a directory (e.g., <code>ama_local</code>) to store AMA files.</li> <li>Open a browser on the VM and navigate to the Application Modernization Accelerator Trial found at <code>https://www.ibm.com/account/reg/us-en/signup?formid=urx-53705</code></li> <li> <p>You are directed through the Trial form.</p> <p></p> <p>Complete this form as yourself and download <code>.zip</code> file into your <code>ama_local</code> directory.</p> <p></p> <p>Finally extract its contents.</p> <p></p> </li> </ul> </li> </ol>"},{"location":"ama-app-mod/ama-install/#run-the-installer","title":"Run the Installer","text":"<ol> <li> <p>Start the Installer </p> <ul> <li>Navigate into the AMA bundle you just extracted and run the <code>launch</code> shell script:     <pre><code>./launch.sh\n</code></pre> </li> </ul> </li> <li> <p>Follow the Prompts </p> <ul> <li>Select <code>1</code> to accept the trial.</li> <li>Select <code>1</code> to accept the license terms.</li> <li> <p>Select <code>1</code> to install Application Modernization Accelerator.</p> <p></p> <p>The install takes a few minutes to complete.</p> </li> </ul> </li> <li> <p>Access AMA </p> <ul> <li> <p>After installation, AMA will be available locally at the URLs provided by the installer:</p> <ul> <li>Linux: <code>https://&lt;host name&gt;:443</code></li> <li>MacOS: <code>https://&lt;IP Address&gt;:443</code></li> </ul> <p></p> </li> </ul> </li> </ol> <p>Select Create a Workspace, select your workspace and follow the prompts to download the Transformation Advisor CLI (aka AMA CLI Discovery Tool):</p> <p></p> <p>Follow instructions to download the discovery tools package:</p> <p>Move the <code>.tgz</code> file to your <code>ama_local</code> directory and extract it:</p> <pre><code>gunzip DiscoveryTool-Linux_TestWorkspace.tgz\ntar -xvf DiscoveryTool-Linux_TestWorkspace.tar\n</code></pre> <p>This creates a <code>transformationadvisor</code> folder inside your <code>ama_local</code> directory.</p> <p>Verify the transformationadvisor CLI is available:</p> <pre><code>~/ama_local/transformationadvisor-4.x/bin/transformationadvisor \u2013help\n</code></pre> <p>Copy and Paste within the Tech Zone VM</p> <p>To copy and paste the URL from the terminal into the VM simply use <code>right click copy</code> and <code>right click paste</code>. </p> <p>Congratulations! AMA is now running \"locally\" within your Tech Zone machine.</p> <p>Ports</p> <p>Ensure your system allows inbound and outbound traffic on ports 3000 and 2220 for AMA to function correctly.</p> <p>Select your workspace and follow the prompts to download the Transformation Advisor CLI (aka AMA CLI Discovery Tool): </p> <p>Follow instructions to download the discovery tools package: </p> <p>Move the <code>.tgz</code> file to your <code>ama_local</code> directory and extract it:</p> <pre><code>gunzip DiscoveryTool-Linux_TestWorkspace.tgz\ntar -xvf DiscoveryTool-Linux_TestWorkspace.tar\n</code></pre> <p>This creates a <code>transformationadvisor</code> folder inside your <code>ama_local</code> directory. </p> <p>Verify the transformationadvisor CLI is available:</p> <pre><code>~/ama_local/transformationadvisor-4.x/bin/transformationadvisor --help\n</code></pre>"},{"location":"ama-app-mod/ama-run-scan/","title":"Scanning","text":""},{"location":"ama-app-mod/ama-run-scan/#running-the-ama-scan","title":"Running the AMA Scan","text":"<p>You successfully installed AMA.  The next step is scanning WebSphere and generating migration artifacts to assist with modernization.</p> <p>A Quick Glossary of WebSphere Terms</p> <p>A WebSphere application is a Java-based application designed to run within the WebSphere Application Server (WAS) environment. WAS is a software framework and middleware that provides a platform for developing, deploying, and managing Java enterprise applications.</p> <p>In IBM WebSphere Application Server, a node is a logical grouping of one or more application servers on a physical machine. It represents a server or a group of servers within a cell. Nodes are managed by a node agent, which handles tasks like starting and stopping servers, deploying applications, and managing configuration. </p> <p>In WebSphere, a cell is a logical grouping of one or more nodes, forming a managed domain for application servers. It's the largest unit of organization in WebSphere.</p> <p>A WebSphere profile is a distinct runtime environment for WebSphere Application Server, containing its own set of files, including scripts, properties, and logs, while sharing core product files with other profiles on the same installation. It allows multiple application servers on a single machine using the same binaries. Profiles are used to isolate different environments (e.g., development, testing, production) and simplify administration.</p> <p>A WebSphere Deployment Manager is a special WebSphere Application Server instance that acts as the central administrative point for a cell, which is a logical grouping of WebSphere servers. It allows for centralized management of multiple WebSphere nodes and their application servers. Essentially, it provides a single point of entry for administering a domain of WebSphere servers.</p> <p>In WebSphere Application Server, EAR (Enterprise Archive) and WAR (Web Application Archive) files are used to package and deploy Java EE (Jakarta EE) applications. A WAR file contains a single web application, including JSPs, servlets, and other web resources. An EAR file can contain multiple WAR files, as well as other modules like Enterprise JavaBeans (EJBs) and application clients, making it suitable for larger, more complex applications. </p>"},{"location":"ama-app-mod/ama-run-scan/#locate-application-profiles","title":"Locate Application Profiles","text":"<p>The lab system you are using from Tech Zone contains a WAS application deployment. Identify where your application server profiles and deployed EAR/WAR files are stored. The location may vary depending on the environment you are scanning.</p> <p>Typically, you can find the profiles in an environment by searching for that directory.  As an example, the Tech Zone environment profiles can be accessed via the following:</p> <pre><code>ls -l /home/techzone/IBM/WebSphere/AppServer/profiles/\n</code></pre> <p> </p>"},{"location":"ama-app-mod/ama-run-scan/#run-ama-discovery-transformation-advisor","title":"Run AMA Discovery (Transformation Advisor)","text":"<p>Next, invoke the discovery phase using the Transformation Advisor CLI (aka AMA CLI). You previously identified and tested the CLI thus you know the CLI location (e.g., <code>transformationadvisor-4.x/bin/</code>).  This is referred to as the Live-Profile Collect.</p> <pre><code>./transformationadvisor \\\n    -w /home/techzone/IBM/WebSphere/AppServer \\\n    -p AppSrv01\n</code></pre> <p>What is this command accessing?</p> <p>The switches for this command focus the scan to the domain you are interested in.</p> <ul> <li><code>-w &lt;was-install-root&gt;</code> tells the scanner the path to find your WebSphere installation (contains <code>profiles/</code>, <code>bin/</code>, etc.).</li> <li><code>-p &lt;profileName&gt;</code> specifies the profile to scan (e.g., <code>AppSrv01</code>).</li> </ul> <p>By default, the scanner collects all configuration and deployed applications under profile, bundles them, and creates a <code>migrationBundle.zip</code> for analysis.</p>"},{"location":"ama-app-mod/ama-run-scan/#review-discovery-results","title":"Review Discovery Results","text":"<p>After running the discovery, you receive a migration bundle containing:</p> <ul> <li>Server configuration</li> <li>Deployed EAR/WAR files</li> <li>A zipped bundle for further analysis</li> </ul> <p>Depending on the AMA Version</p> <p>Upload this bundle to the AMA UI for web-based analysis.  If you are curious, you can also unzip this archive to examine the results manually via the raw files.</p> <p>Create a workspace in the AMA Web UI.</p> <p></p> <p>During the creation upload the results of your scan:</p> <p></p> <p></p> <p>Navigate to the scan results you just created:</p> <p></p> <p>Perform the Upload:</p> <p></p> <p>Looking at the Visualization of from the scan report quickly shows two Java Applications and a Database.  If you ran this against a different system you will likely have different results.  Toggling to the Assessment tab shows you some additional information about the implementation.</p> <p></p> <p>Look through the report and attempt to understand some of the findings.  We will provide additional analysis later.</p> <p></p> <p>You will see the assessment report for the applications, including migration time estimates. Click on modresorts-1_0-java8_war.ear to view application details.</p> <p></p> <p>View the migration plan for the application.</p> <p></p> <p>Download the migration plan for the application.</p> <p></p> <p>Once you have downloaded the plan, you can chase the links at the bottom to assist you with potential next steps in your journey including  how to deploy your application on Red Hat OpenShift using the WebSphere Liberty operator.</p> <p>Note</p> <p>This first scan you ran above does not perform code-level analysis (Java 8 \u2192 Java 21, WAS \u2192 Liberty). For that, the Binary Application Scanner is used.  We will do that in next step.</p>"},{"location":"ama-app-mod/ama-run-scan/#run-binary-application-scanner","title":"Run Binary Application Scanner","text":"<p>To analyze application binaries and generate migration reports, run the Binary Application Scanner:</p> <pre><code>java -jar /home/techzone/IBM/WebSphere/AppServer/bin/migration/wamt/binaryAppScanner.jar \\\n/home/techzone/IBM/WebSphere/AppServer/profiles/AppSrv01/config/cells/\\\nrhel9-baseCell01/applications/modresorts-1_0-java8_war.ear \\\n--all \\\n--sourceAppServer=was90 \\\n--sourceJava=ibm8 \\\n--targetAppServer=liberty \\\n--targetJava=java21 \\\n--targetCloud=containers \\\n--output=/home/techzone/ama-output/modresorts-liberty-java21\n</code></pre> <p>Explaining the Command</p> <ul> <li> <p><code>java -jar \u2026/binaryAppScanner.jar</code> You\u2019re launching the Migration Toolkit for Application Binaries\u2019 scanner tool. This JAR contains the logic to inspect Java EE archives and produce:</p> <ul> <li>An Application Migration Report (inventory, detailed analysis, technology evaluation)</li> <li>OpenRewrite recipes for code refactoring</li> <li>Configuration recipes for target servers -Containerization artifacts (Dockerfile + Kubernetes YAML)</li> </ul> </li> <li> <p>Positional argument: your .ear path <code>/home/techzone/IBM/WebSphere/AppServer/profiles/AppSrv01/config/cells/rhel9-baseCell01/applications/modresorts-1_0-java8_war.ear</code>. This tells the scanner exactly which binary to examine. It will:</p> <ul> <li>Unpack the EAR</li> <li>Read its bytecode to analyze API usage</li> <li>Look up any matching server-side configuration under the same profile (for features, resources, etc.)</li> </ul> </li> <li> <p><code>--all (Action)</code> This single flag instructs the tool to run every built-in report in one pass:</p> <ul> <li>Inventory (what\u2019s in the archive?)</li> <li>Technology Evaluation (what programming models are used?)</li> <li>Detailed Migration Analysis (what rules get flagged?)</li> <li>Configuration (what server.xml or wsadmin scripts to generate?)</li> <li>Using --all is the easiest way to get the full migration bundle.</li> </ul> </li> <li> <p>Source flags</p> <ul> <li><code>--sourceAppServer=was90</code> Indicates your application currently runs on WebSphere Application Server 9.0. The scanner will apply WAS-specific rules and know how to locate any deployed-server artifacts.</li> <li><code>--sourceJava=ibm8</code> Tells it \u201cthe code is compiled for IBM Java 8,\u201d so it can detect deprecated or removed APIs.</li> </ul> </li> <li> <p>Target flags</p> <ul> <li><code>--targetAppServer=liberty</code> You want the recommendations and configuration snippets targeted at Liberty, not a traditional WAS profile.</li> <li><code>--targetJava=java21</code> You\u2019re migrating your code to run on Java 21, so the tool will surface any Java\u20138\u219221 incompatibilities and generate OpenRewrite recipes to fix them.</li> <li><code>--targetCloud=containers</code> Ask the scanner to include containerization recipes\u2014it will drop in a Dockerfile and Kubernetes YAML under a containerization/ folder in your output.</li> </ul> </li> <li> <p><code>--output=\u2026</code> Specifies the directory where all of the scanner\u2019s outputs will go. In this case: <code>/home/techzone/ama-output/modresorts-liberty-java21/</code> You\u2019ll find:</p> <ul> <li>A modresorts-\u2026_migrationBundle.zip with JSON/XML and the raw EAR</li> <li>modresorts-\u2026_MigrationReport.html (the human-readable analysis)</li> <li>server.xml pre-populated with the Liberty features your app needs</li> <li>Possibly sensitivedata.xml and mimetypes.xml (inventory/config snippets)</li> <li>A containerization/ folder containing your Dockerfile and Kubernetes YAML</li> </ul> </li> </ul> <p>We have successfully invoked two complementary scans.  Lets go over what we have completed and summarize the steps and commands in the next section. </p> <p>Now you have successfully run the AMA scan and generated the migration artifacts for your application. You can use these artifacts to plan and execute your application modernization journey.  The section will give you additional information about these scans.</p>"},{"location":"ama-app-mod/ama-scan-results/","title":"Scan Results","text":""},{"location":"ama-app-mod/ama-scan-results/#understanding-the-scans","title":"Understanding the Scans","text":""},{"location":"ama-app-mod/ama-scan-results/#live-profile-collect","title":"Live-Profile \"Collect\"","text":"<p>Live-profile \u201ccollect\u201d with <code>transformationadvisor</code>:</p> <pre><code>./transformationadvisor \\\n  -w /path/to/your/WAS/install \\\n  -p YourProfileName\n</code></pre> <ul> <li>When to use: you have shell access to a running WAS profile that already has your EAR/WAR deployed.</li> <li>What it does: automatically packages up the entire profile\u2014server.xml, resources, JMS, deployed archives\u2014and uploads (or prepares for upload) to the AMA UI.</li> <li>Why it\u2019s valuable:<ul> <li>You get a full picture of how the app is configured at runtime (data sources, security, clustering, custom JVM args\u2026).</li> <li>No need to manually locate your EAR/WAR.</li> <li>You can review container recipes right in the UI, side-by-side with your live-server config.</li> </ul> </li> </ul>"},{"location":"ama-app-mod/ama-scan-results/#binary-scan","title":"Binary Scan","text":"<p>Binary scan with <code>binaryAppScanner.jar</code>:</p> <pre><code>java -jar binaryAppScanner.jar \\\n  /path/to/yourApp.ear \\\n  --all \\\n  --sourceAppServer=was90 \\\n  --sourceJava=ibm8 \\\n  --targetAppServer=liberty \\\n  --targetJava=java21 \\\n  --targetCloud=containers \\\n  --output=/some/output/dir\n</code></pre> <ul> <li>When to use: you only have the packaged archive(s) \u2014 EAR, WAR or an exploded directory \u2014 or you want to drive specific migrations flags (Java-8\u219221, Liberty feature list, container manifests).</li> <li>What it does:<ul> <li>Runs the Detailed Migration Analysis (inventory, technology evaluation, API-change rules)</li> <li>Generates OpenRewrite recipes for Java-8\u219221 upgrades</li> <li>Builds a Liberty server.xml snippet and a containerization folder with Dockerfile + Kubernetes YAML</li> </ul> </li> <li>Why it\u2019s valuable:<ul> <li>Fine-grained control over source-and-target flags</li> <li>Ideal for embedding in a CI/CD pipeline (you point at the built artifact, not the live server)</li> <li>You can prove \u201czero flagged rules\u201d after you\u2019ve refactored and rebuilt.</li> </ul> </li> <li>When to use each scan:<ul> <li>Full Modernization: start with the collect step to feed the AMA UI and get a holistic, server-side view. Then run the binary scan against the same EAR to drill into Java-21 migration and container recipes.</li> <li>Pure code-migration pipelines: skip straight to the binary scan. You only care about API-changes and container output, and you want it fully automatable without touching production servers.</li> </ul> </li> </ul> <p>Where do I scan?</p> <p>How do I use the scanners?  When do I use which scanner?</p> <ul> <li>Have a running profile? Do the <code>transformationadvisor -w\u2026 -p\u2026 collect</code> first.</li> <li>Have only artifacts or want JDK migration flags? Run <code>binaryAppScanner.jar</code> with your <code>--source\u2026 and --target\u2026</code> options</li> </ul> <p>Using both in tandem gives you the best of both worlds: real-world config context plus precise, flag-driven migration guidance.</p>"},{"location":"ama-app-mod/ama-scan-results/#next-steps","title":"Next Steps","text":"<p>Although you could use the artifacts from these reports to quickly deploy the application, the bootcamp experience continues using watsonx Code Assistant to finish the modernization experience.</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/#blog","title":"Blog","text":""},{"location":"cn/","title":"Cloud Native Introduction","text":""},{"location":"cn/#cloud-native-introduction","title":"Cloud Native Introduction","text":"<p>Cloud is everywhere. Today, many companies want to migrate their applications on to cloud. For this migration to be done, the applications must be re-architected in a way that they fully utilize the advantages of the cloud.</p> <p>Containerization is another foundation for our adoption of Cloud Native.  Cloud, Hybrid Cloud and Cloud Native are intertwined concepts that we will introduce together in this section.</p>"},{"location":"cn/app-dev/","title":"Cloud Native Application Development","text":""},{"location":"cn/app-dev/#cloud-native-application-development","title":"Cloud Native Application Development","text":""},{"location":"cn/app-dev/#introduction","title":"Introduction","text":"<p>Cloud native is all about the concepts that are used for building and deploying your applications on any cloud platform. It involves many things like adopting microservices architecture, containerization, orchestration etc.</p> <p>Cloud native applications will be managed by the infrastructure which in turn is managed by applications. In this installment, let us see what you need to include in your cloud native applications and how to run them on the cloud infrastructure.</p>"},{"location":"cn/app-dev/#application-design","title":"Application Design","text":"<p>An application is called cloud native if it is designed in a way such that it takes advantage of most of the benefits of the cloud. So, these applications are all managed by software like mentioned before.</p> <p>Let's say we have an application packaged in a container and running on Kubernetes. This application does not accept any runtime configuration. There is no logging defined. Some of the configurations like database IP, credentials etc are hardcoded.</p> <p>What happens if the application stops working ? How are you going to debug it ?</p> <p>Will you call such an application cloud-native ? Definitely not.</p> <p>Containers and Kubernetes help you to run the applications smoothly on cloud infrastructure. Along with this, you also need to know how to effectively build and manage these applications.</p>"},{"location":"cn/app-dev/#cloud-native-capabilities","title":"Cloud native Capabilities","text":"<p>While developing cloud native applications, some of the capabilities that we include in the applications are as follows.</p> <ul> <li>Configuration</li> <li>Health Checks</li> <li>Logging</li> <li>Metrics</li> <li>Resiliency</li> <li>Service discovery</li> </ul> <p>Usually, you can implement them in your applications in two ways.</p> <ul> <li>Using the language libraries</li> <li>Using sidecar</li> </ul> <p>To implement these capabilities, you can import the language libraries into your application which will automatically get you most of these capabilities with out defining any extra code. This is easy to do and is well suitable if you have few languages in your applications.</p> <p>But if your application is a polyglot with many different languages, it is difficult to manage all the libraries. In such cases, you can use sidecar which is implemented as separate service including things like Health end point monitoring health of the application, configuration watching changes in the configs and reloading the application when required, registrator for service discovery, envoy proxy to handle resiliency and metrics etc.</p>"},{"location":"cn/app-dev/#application-lifecycle","title":"Application Lifecycle","text":"<p>Deploy</p> <p>Deploying your cloud native application is not just taking the existing code and running it on cloud. Cloud native applications are defined in a way such the software manage them. For this, make sure you have the below embedded in your application.</p> <ul> <li>Continuous integration</li> <li>Continuous deployment</li> <li>Health checks</li> </ul> <p>Deployments for your application should be automated, tested and verified. If you are introducing new features to your applications, you should be able to deploy them dynamically without restarting your applications. Also, when you are planning on a new feature or a new version to be deployed, make sure you have traffic control mechanisms in place which allows you to route the traffic towards or away from the application as per your requirements to reduce the outage impact.</p> <p>Run</p> <p>Running your application is one of the most important phases in the application lifecycle. While running the application, two most important aspects to keep in mind are</p> <ul> <li>Observability</li> <li>Operability</li> </ul> <p>While running your application, you need to understand the what the application is doing which is observability and also you you should be able to change the application as needed which is operability.</p> <p>When your application is not meeting the SLO or is broken, what do you do ? In a cloud native application, we follow the below steps to see where the problem resides.</p> <ol> <li>Verify infrastructure tests</li> <li>Application debugging - This can be done by using application performance monitoring (APM), distributed tracing etc.</li> <li>More verbose Logging</li> </ol> <p>In today's world, as the business keeps increasing, the application grows and you need to make sure that you defined a proper debugging strategy for your application which makes it easy to dynamically debug the applications similar to how we dynamically deploy them.</p> <p>One more important things to remember is that it is always easy to push new applications but the converse is not true. Though that is the case, it is still very important to retire the old applications that are not in use.</p> <p>Retire</p> <p>In cloud-native applications, all the new services are deployed automatically. Also, the services are monitored automatically using the monitoring mechanisms in place.</p> <p>Don't you think the services should be retired in the same way too ?</p> <p>If you keep deploying new services without cleaning up the old ones which are in no longer use accrues a lot of technical debt. So, make sure your application includes a telemetry mechanism which helps you to identify if a service is being used. If not, the decision should be made by the business to keep it or retire it.</p>"},{"location":"cn/app-dev/#twelve-factor-design-methodology","title":"Twelve factor design methodology","text":"<ul> <li>Code base - One code base tracked in revision control, many deploys.</li> <li>Dependencies - Explicitly declare and isolate dependencies.</li> <li>Config - Store config in the environment.</li> <li>Backing services - Treat backing services as attached resources.</li> <li>Build, release, run - Strictly separate build and run stages.</li> <li>Processes - Execute the app as one (or more) stateless process(es).</li> <li>Port binding - Export services through port binding.</li> <li>Concurrency - Scale-out through the process model.</li> <li>Disposability - Maximize robustness with fast startup and graceful shutdown.</li> <li>Dev/prod parity - Keep development, staging, and production as similar as possible.</li> <li>Logs - Treat logs as event streams.</li> <li>Admin processes - Run admin/management tasks as one-off processes.</li> </ul>"},{"location":"cn/app-dev/#application-requirements","title":"Application Requirements","text":"<p>Runtime and Isolation</p> <p>Your applications must be isolated from the operating system. You should be able to run them any where. This allows you to run multiple applications on same server and also allows to control their dependencies and resources.</p> <p>One way to achieve this is containerization. Among the different container options, Podman is popular. Container is nothing but a way to package your application and run it in an isolated environment. While developing the applications, also make sure all the dependencies are declared in your application before packaging it.</p> <p>Resource Allocation and Scheduling</p> <p>Your applications must include dynamic scheduling. This helps you to figure out where the application must run and this decisions are automatically taken for you by the scheduler. This scheduler collects all the informations of resources for different system and chooses the right place to run the application. Operator can override the decisions of the scheduler if he wants to.</p> <p>Environment isolation</p> <p>You need a proper environment isolation to differentiate dev, test, stage, production etc. based on your requirements. With out the complete duplication of your cluster, the infrastructure should be able to separate the dependencies through different application environments.</p> <p>These environments should include all of the resources like databases, network resources etc. needed by the application. Cloud native infrastructure can create environments with very low overhead.</p> <p>Service discovery</p> <p>In your application, there may be multiple services. These services may depend on one another. How will they find each other if one service needs to communicate with other ? For this, the infrastructure should provide a way for services to find each other.</p> <p>This may be in different ways. It can be using API calls or using DNS or with network proxies. There should be a service discovery mechanism in place and how you do this does not matter.</p> <p>Usually cloud native applications make use their infrastructure for service discovery to identify the dependent services. Some of them are cloud metadata services, DNS, etcd and consul etc.</p> <p>State Management</p> <p>While defining your cloud native application, you should provide a mechanism to check the status of the application. This can be done by an API or hook that checks the current state of the application like if it is submitted, Scheduled, ready, healthy, unhealthy, terminating etc.</p> <p>We usually have such capabilities in any of the orchestration platform we use. For example, if you consider Kubernetes, you can do this using events, probes and hooks. When the application is submitted, scheduled, or scaled, the event is triggered. Readiness probe checks if the application is ready and liveness probes checks if the application is healthy. Hooks are used for events that need to happen before or after processes start.</p> <p>Monitoring and logging</p> <p>Monitoring and logging should be a part of the cloud-native application. Dynamically monitoring all the services of the application is important. It keeps checking the entire application and is used for debugging purposes when required. Also, make sure your logging system is capable of collecting and consolidating all logs together based on application, environments, tags etc.</p> <p>Metrics</p> <p>Cloud-native applications must include metrics as a part of their code. All the telemetry data needed will be provided by the metrics. This helps you to know whether your application is meeting the service-level objectives.</p> <p>Metrics are collected at instance level and later aggregated together to provide the complete view of the application. Once the application provides the metrics, underlying infrastructure will scrape them out and use them for analysis.</p> <p>Debugging and tracing</p> <p>When an application is deployed and problem occurs, we refer to logging system. But if that does not resolve the issue, we need distributed tracing. Distributed tracing helps us to understand what is happening in the application. They will us to debug problems by providing us an interface to visualize which is different from the details we get from logging. Also, it provides shorter feedback loops which helps you to debug distributed systems easily.</p> <p>Application tracing is always important and make sure it is a part of your cloud-native application. If in case you cannot include it in the application, you can also enable it at infrastructure level using proxies or traffic analysis.</p>"},{"location":"cn/app-dev/#conclusion","title":"Conclusion","text":"<p>We discussed the cloud-native application design, implementations of cloud native patterns, and application life cycle. We also saw how we can design our cloud native applications using the twelve factor methodology. Along with this, we also explored what we need to include in our cloud naive application while building it.</p> <p>References</p> <ul> <li>Justin Garrison, Kris Nova, (2018). Managing cloud native applications. Publisher: O'Reilly Media, Inc.</li> <li>Piyum Zonooz, Erik Farr, Kamal Arora, Tom Laszewski, 2018. Cloud Native Architectures. Publisher: Packt Publishing</li> <li>12factor.net</li> </ul>"},{"location":"cn/cloud-native/","title":"Cloud Native","text":""},{"location":"cn/cloud-native/#cloud-native","title":"Cloud Native","text":"<p>We begin with this Cloud-Native Presentation </p>"},{"location":"cn/cloud-native/#what-is-cloud-native","title":"What is Cloud-Native?","text":"<p>Cloud-native is about how we build and run applications taking full advantage of cloud computing rather than worrying about where we deploy it.</p> <p>Cloud-native refers less to where an application resides and more to how it is built and deployed.</p> <ul> <li> <p>A cloud-native application consists of discrete, reusable components     known as microservices that are designed to integrate into any cloud     environment.</p> </li> <li> <p>These microservices act as building blocks and are often packaged in     containers.</p> </li> <li> <p>Microservices work together as a whole to comprise an application,     yet each can be independently scaled, continuously improved, and     quickly iterated through automation and orchestration processes.</p> </li> <li> <p>The flexibility of each microservice adds to the agility and     continuous improvement of cloud-native applications.</p> </li> </ul> <p>CNCF Cloud Native Definition</p> <p>Cloud native technologies empower organizations to build and run scalable applications in modern, dynamic environments such as public, private, and hybrid clouds. Containers, service meshes, microservices, immutable infrastructure, and declarative APIs exemplify this approach.</p> <p>These techniques enable loosely coupled systems that are resilient, manageable, and observable. Combined with robust automation, they allow engineers to make high-impact changes frequently and predictably with minimal toil.</p> <p>The Cloud Native Computing Foundation seeks to drive adoption of this paradigm by fostering and sustaining an ecosystem of open source, vendor-neutral projects. We democratize state-of-the-art patterns to make these innovations accessible for everyone.</p>"},{"location":"cn/cloud-native/#why-cloud-native","title":"Why Cloud-Native?","text":"<p>Cloud-native applications are different from the traditional applications that run in your data centres. The applications that are designed in the traditional way are not built keeping cloud compatibility in mind. They may have strong ties with the internal systems. Also, they cannot take advantage of all the benefits of the cloud.</p> <p>So, we need a new architecture for our applications to utilize the benefits of cloud. There is a need to design the applications keeping cloud in mind and take advantage of several cloud services like storage, queuing, caching etc.</p> <ul> <li> <p>Speed, safety, and scalability comes with cloud-native applications.</p> </li> <li> <p>Helps you to quickly deliver the advancements.</p> </li> <li> <p>Allows you to have loose ties into the corporate IT where it most     certainly would destabilize legacy architectures.</p> </li> <li> <p>Helps you to continuously deliver your applications with zero     downtime.</p> </li> <li> <p>Infrastructure is less predictable.</p> </li> <li> <p>Service instances are all disposable.</p> </li> <li> <p>Deployments are immutable.</p> </li> <li> <p>To meet the expectations of the today\u2019s world customers, these     systems are architected for elastic scalability.</p> </li> </ul>"},{"location":"cn/cloud-native/#cloud-native-concepts","title":"Cloud-native concepts","text":"<p>Some of the important characteristics of cloud-native applications are as follows.</p> <ul> <li> <p>Disposable Infrastructure</p> </li> <li> <p>Isolation</p> </li> <li> <p>Scalability</p> </li> <li> <p>Disposable architecture</p> </li> <li> <p>Value added cloud services</p> </li> <li> <p>Polyglot cloud</p> </li> <li> <p>Self-sufficient, full-stack teams</p> </li> <li> <p>Cultural Change</p> </li> </ul> <p>Disposable Infrastructure</p> <p>While creating applications on cloud, you need several cloud resources as part of it. We often hear how easy it is to create all these resources. But did you ever think how easy is it to dispose them. It is definitely not that easy to dispose them and that is why you don\u2019t hear a lot about it.</p> <p>In traditional or legacy applications, we have all these resources residing on machines. If these go down, we need to redo them again and most of this is handled by the operations team manually. So, when we are creating applications on cloud, we bring those resources like load balancers, databases, gateways, etc on to cloud as well along with machine images and containers.</p> <p>While creating these applications, you should always keep in mind that if you are creating a resource when required, you should also be able to destroy it when not required. Without this, we cannot achieve the factors speed, safety and scalability. If you want this to happen, we need automation.</p> <p>Automation allows you to</p> <ul> <li> <p>Deliver new features at any time.</p> </li> <li> <p>Deliver patches faster.</p> </li> <li> <p>Improves the system quality.</p> </li> <li> <p>Facilitates team scale and efficiency.</p> </li> </ul> <p>Now you know what we are talking about. Disposable infrastructure is nothing but <code>Infrastructure as Code</code>.</p> <p>Infrastructure as Code</p> <p>Here, you develop the code for automation exactly as same as the you do for the rest of the application using agile methodologies.</p> <ul> <li> <p>Automation code is driven by a story.</p> </li> <li> <p>Versioned in the same repository as rest of the code.</p> </li> <li> <p>Continuously tested as part of CI/CD pipeline.</p> </li> <li> <p>Test environments are created and destroyed along with test runs.</p> </li> </ul> <p>Thus, disposable infrastructure lays the ground work for scalability and elasticity.</p> <p>Isolation</p> <p>In traditional or legacy applications, the applications are monoliths. So, when there is bug or error in the application, you need to fix it. Once you changed the code, the entire application should be redeployed. Also, there may be side effects which you can never predict. New changes may break any components in the application as they are all inter related.</p> <p>In cloud-native applications, to avoid the above scenario, the system is decomposed into bounded isolated components. Each service will be defined as one component and they are all independent of each other. So, in this case, when there is a bug or error in the application, you know which component to fix and this also avoids any side effects as the components are all unrelated pieces of code.</p> <p>Thus, cloud-native systems must be resilient to man made errors. To achieve this we need isolation and this avoids a problem in one component affecting the entire system. Also, it helps you to introduce changes quickly in the application with confidence.</p> <p>Scalability</p> <p>Simply deploying your application on cloud does not make it cloud-native. To be cloud native it should be able to take full benefits of the cloud. One of the key features is Scalability.</p> <p>In today\u2019s world, once your business starts growing, the number of users keep increasing and they may be from different locations. Your application should be able to support more number of devices and it should also be able to maintain its responsiveness. Moreover, this should be efficient and cost-effective.</p> <p>To achieve this, cloud native application runs in multiple runtimes spread across multiple hosts. The applications should be designed and architected in a way that they support multi regional, active-active deployments. This helps you to increase the availability and avoids single point of failures.</p> <p>Disposable architecture</p> <p>Leveraging the disposable infrastructure and scaling isolated components is important for cloud native applications. Disposable architecture is based on this and it takes the idea of disposability and replacement to the next level.</p> <p>Most of us think in a monolithic way because we got used to traditional or legacy applications a lot. This may lead us to take decisions in monolithic way rather than in cloud native way. In monoliths, we tend to be safe and don\u2019t do a lot of experimentation. But Disposable architecture is exactly opposite to monolithic thinking. In this approach, we develop small pieces of the component and keep experimenting with it to find an optimal solution.</p> <p>When there is a breakthrough in the application, you can\u2019t simply take decisions based on the available information which may be incomplete or inaccurate. So, with disposable architecture, you start with small increments, and invest time to find the optimal solution. Sometimes, there may be a need to completely replace the component, but that initial work was just the cost of getting the information that caused the breakthrough. This helps you to minimize waste allowing you to use your resources on controlled experiments efficiently and get good value out of it in the end.</p> <p>Value added cloud services</p> <p>When you are defining an application, there are many things you need to care of. Each and every service will be associated with many things like databases, storage, redundancy, monitoring, etc. For your application, along with your components, you also need to scale the data. You can reduce the operational risk and also get all such things at greater velocity by leveraging the value-added services that are available on cloud. Sometimes, you may need third party services if they are not available on your cloud. You can externally hook them up with your application as needed.</p> <p>By using the value added services provided by your cloud provider, you will get to know all the available options on your cloud and you can also learn about all the new services. This will help you to take good long-termed decisions. You can definitely exit the service if you find something more suitable for your component and hook that up with your application based on the requirements.</p> <p>Polyglot cloud</p> <p>Most of you are familiar with Polyglot programming. For your application, based on the component, you can choose the programming languages that best suits it. You need not stick to a single programming language for the entire application. If you consider Polyglot persistence, the idea is choose the storage mechanism that suits better on a component by component basis. It allows a better global scale.</p> <p>Similarly, the next thing will be Polyglot cloud. Like above, here you choose a cloud provider that better suits on a component by component basis. For majority of your components, you may have a go to cloud provider. But, this does not stop you from choosing a different one if it suits well for any of your application components. So, you can run different components of your cloud native system on different cloud providers based on your requirements.</p> <p>Self-sufficient, full-stack teams</p> <p>In a traditional set up, many organizations have teams based on skill set like backend, user interface, database, operations etc. Such a structure will not allow you to build cloud native systems.</p> <p>In cloud native systems, the system is composed of bounded isolated components. They have their own resources. Each of such component must be owned by self-sufficient, full stack team. That team is entirely responsible for all the resources that belong to that particular component. In this set up, team tends to build quality up front in as they are the ones who deploy it and they will be taking care of it if the component is broken. It is more like you build it and then you run it. So, the team can continuously deliver advancements to the components at their own pace. Also, they are completely responsible for delivering it safely.</p> <p>Cultural Change</p> <p>Cloud native is different way of thinking. We need to first make up our minds, not just the systems, to utilize the full benefits of cloud. Compared to the traditional systems, there will be lots of things we do differently in cloud-native systems.</p> <p>To make that happen, cultural change is really important. To change the thinking at high level, we just to first prove that the low level practices can truly deliver and encourage lean thinking. With this practice, you can conduct experimentation. Based on the feedback from business, you can quickly and safely deliver your applications that can scale.</p>"},{"location":"cn/cloud-native/#cloud-native-roadmap","title":"Cloud-native Roadmap","text":"<p>You can define your cloud native road map in many ways. You can get there by choosing different paths. Let us see the trail map defined by CNCF.</p> <p>CNCF defined the Cloud Native Trail Map providing an overview for enterprises starting their cloud native journey as follows.</p> <p>This cloud map gives us various steps that an engineering team may use while considering the cloud native technologies and exploring them. The most common ones among them are Containerization, CI/CD, and Orchestration. Next crucial pieces will be Observability &amp; Analysis and Service Mesh. And later comes the rest of them like Networking, Distributed Database, Messaging, Container runtime, and software distribution based on your requirements.</p> <p></p> <ul> <li> <p>With out Containerization, you cannot build cloud native     applications. This helps your application to run in any computing     environment. Basically, all your code and dependencies are packaged     up together in to a single unit here. Among different container     platforms available, Podman is a preferred one.</p> </li> <li> <p>To bring all the changes in the code to container automatically, it     is nice to set up a CI/CD pipeline which does that. There are many     tools available like jenkins, travis, etc.</p> </li> <li> <p>Since we have containers, we need container orchestration to manage     the container lifecycles. Currently, Kubernetes is one solution     which is popular.</p> </li> <li> <p>Monitoring and Observability plays a very important role. It is good     to set up some of them like logging, tracing, metrics etc.</p> </li> <li> <p>To enable more complex operational requirements, you can use a     service mesh. It helps you out with several things like service     discovery, health, routing, A/B testing etc. Istio is one of the     examples of service mesh.</p> </li> <li> <p>Networking plays a crucial role. You should define flexible     networking layers based on your requirements. For this, you can use     Calico, Weave Net etc.</p> </li> <li> <p>Sometimes, you may need distributed databases. Based on your     requirements, if you need more scalability and resiliency, these are     required.</p> </li> <li> <p>Messaging may be required sometimes too. Go with different messaging     queues like Kafka, RabbitMQ etc available when you need them.</p> </li> <li> <p>Container Registry helps you to store all your containers. You can     also enable image scanning and signing if required.</p> </li> <li> <p>As a part of your application, sometimes you may need a secure     software distribution.</p> </li> </ul> <p>Also, if you want to see the cloud native landscape, check it out here.</p>"},{"location":"cn/cloud-native/#summary","title":"Summary","text":"<p>In this, we covered the fundamentals of cloud native systems. You now know what cloud native is, why we need it and how it is important. Cloud native is not just deploying your application on cloud but it is more of taking full advantages of cloud. Also, from cloud-native roadmap, you will get an idea on how to design and architect your cloud-native system. You can also get the idea of different tools, frameworks, platforms etc from the cloud-native landscapes.</p> <p>Also, if you are interesting in knowing more, we have Cloud-Native: A Complete Guide. Feel free to check this out.</p> <p>References</p> <ul> <li>Learn Cloud-native</li> <li>John Gilbert, (2018). Cloud Native Development Patterns and Best Practices. Publisher: Packt Publishing</li> <li>CNCF Landscape</li> <li>CNCF Definition</li> </ul>"},{"location":"cn/containers/","title":"Containers","text":""},{"location":"cn/containers/#containers","title":"Containers","text":"<p>You wanted to run your application on different computing environments. It may be your laptop, test environment, staging environment or production environment.</p> <ul> <li>When you run it on these different environments, will your application work reliably?</li> <li>What happens if something changes with the underlying software?</li> <li>What if the security policies are different? or something else changes?</li> </ul> <p>To solve this problems, we need Containers.</p> <p>Containers are a standard way to package an application and all its dependencies so that it can be moved between environments and run without change. They work by hiding the differences between applications inside the container so that everything outside the container can be standardized.</p> <p>For example, Docker created standard way to create images for Linux Containers.</p>"},{"location":"cn/containers/#presentations","title":"Presentations","text":"<p>Container Basics </p>"},{"location":"cn/containers/#why-containers","title":"Why containers ?","text":"<ul> <li>We can run them anywhere.</li> <li>Containers are lightweight.</li> <li>Isolate your application from others.</li> </ul>"},{"location":"cn/containers/#different-container-standards","title":"Different Container Standards","text":"<p>There are many different container standards available today. Some of them are as follows.</p> <p>Docker - The most common standard, made Linux containers usable by the masses.</p> <p>Rocket<sup>1</sup> (rkt) - An emerging container standard from CoreOS, the company that developed etcd.</p> <p>Garden - The format Cloud Foundry builds using buildpacks.</p> <p>Among them, Docker was one of the most popular mainstream container software tools.</p> <p>Open Container Initiative (OCI)</p> <p>A Linux Foundation project developing a governed container standard. Docker and Rocket<sup>1</sup> are OCI-compliant. But, Garden is not.</p>"},{"location":"cn/containers/#benefits","title":"Benefits","text":"<ul> <li>Lightweight</li> <li>Scalable</li> <li>Efficient</li> <li>Portable</li> <li>Supports agile development</li> </ul> <p>To know more about Containerization, we have couple of guides. Feel free to check them out.</p> <ul> <li>Containerization: A Complete Guide</li> <li>Containers: A Complete Guide</li> </ul>"},{"location":"cn/containers/#docker","title":"Docker","text":"<p>Docker is one of the most popular Containerization platforms which allows you to develop, deploy, and run application inside containers.</p> <ul> <li>It is an open source project.</li> <li>Can run it anywhere.</li> </ul> <p>An installation of Docker includes an engine. This comes with a daemon, REST APIs, and CLI. Users can use CLI to interact with the docker using commands. These commands are sent to the daemon which listens for the Docker Rest APIs which in turn manages images and containers. The engine runs a container by retrieving its image from the local system or registry. A running container starts one or more processes in the Linux kernel.</p>"},{"location":"cn/containers/#docker-image","title":"Docker Image","text":"<p>A read-only snapshot of a container that is stored in Docker Hub or in private repository. You use an image as a template for building containers.</p> <p>These images are build from the <code>Dockerfile</code>.</p> <p>Dockerfile</p> <ul> <li>It is a text document that contains all the instructions that are necessary to build a docker image.</li> <li>It is written in an easy-to-understand syntax.</li> <li>It specifies the operating system.</li> <li>It also includes things like environmental variables, ports, file locations etc.</li> </ul> <p>If you want to try building docker images, try this course on O'Reilly (Interactive Learning Platform){target=\"_blank\"}.</p> <ul> <li>Building Container Images -  Estimated Time: 12 minutes.</li> </ul>"},{"location":"cn/containers/#docker-container","title":"Docker Container","text":"<p>The standard unit where the application service is located or transported. It packages up all code and its dependencies so that the application runs quickly and reliably from one computing environment to another.</p> <p>If you want to try deploying a docker container, try this course on O'Reilly (Interactive Learning Platform).</p>"},{"location":"cn/containers/#docker-engine","title":"Docker Engine","text":"<p>Docker Engine is a program that creates, ships, and runs application containers. The engine runs on any physical or virtual machine or server locally, in private or public cloud. The client communicates with the engine to run commands.</p> <p>If you want to learn more about docker engines, try this course on O'Reilly</p>"},{"location":"cn/containers/#docker-registry","title":"Docker Registry","text":"<p>The registry stores, distributes, and shares container images. It is available in software as a service (SaaS) or in an enterprise to deploy anywhere you that you choose.</p> <p>Docker Hub is a popular registry. It is a registry which allows you to download docker images which are built by different communities. You can also store your own images there. You can check out various images available on docker hub here.</p> <p>References</p> <ul> <li>Docker resources</li> <li>Docker tutorial</li> <li>The Evolution of Linux Containers and Their Future</li> <li>Open Container Initiative (OCI)</li> <li>Cloud Native Computing Foundation (CNCF)</li> <li>Demystifying the Open Container Initiative (OCI) Specifications</li> </ul> <ol> <li> <p>CoreOS and its Rocket technology was acquired by Red Hat.  Press Release, this acquisition facilitated the creation of Red Hat CoreOS which is the Linux operating system that powers Red Hat OpenShift.\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"cn/imageregistry/","title":"Lab CN 1 - Image Registries","text":""},{"location":"cn/imageregistry/#lab-cn-1-image-registries","title":"Lab CN 1 - Image Registries","text":"<p>A registry is a repository used to store and access container images. Container registries can support container-based application development, often as part of DevOps processes.</p> <p>Associate your Quay.io account with your Red Hat account!</p> <p>If you haven't used Quay.io in a while and already have an account, you may need to associate Quay.io and Red Hat logins.  See this article for help.</p> <p>Container registries save developers valuable time in the creation and delivery of cloud-native applications, acting as the intermediary for sharing container images between systems. They essentially act as a place for developers to store container images and share them out via a process of uploading (pushing) to the registry and downloading (pulling) into another system, like a Kubernetes cluster.</p> <p>Learn More </p> Tutorial <p>Make sure you have Podman Desktop installed and up and running. You may need to first visit Quay.io and set up a username. Use the Red Hat account you created earlier.</p> Login to Quay<pre><code>podman login quay.io\nUsername: your_username\nPassword: your_password\n</code></pre> <p>First we'll create a container with a single new file based off of the busybox base image:    </p>Create a new container<pre><code>podman run busybox echo \"fun\" &gt; newfile\n</code></pre>   The container will immediately terminate, so we'll use the command below to list it:   <pre><code>podman ps -a\n</code></pre>   The next step is to commit the container to an image and then tag that image with a relevant name so it can be saved to a repository.<p></p> <p>In the below command you must replace:</p> <ul> <li> with your container id from the previous command</li> <li> with a namespace of your choice. For a personal account this will be: name_surname_ibm</li> <li> with a name for the repository</li> </ul> Create a new image<pre><code>podman commit &lt;container_id&gt; quay.io/&lt;namespace&gt;/&lt;repository_name&gt;\n</code></pre> <p>Now that we've tagged our image with a repository name, we can push the repository to Quay Container Registry:   </p>Push the image to Quay<pre><code>podman push quay.io/&lt;namespace&gt;/&lt;repository_name&gt;\n</code></pre>   Your repository has now been pushed to Quay Container Registry!<p></p> <p>To view your repository, click on the button below:</p> <p>Repositories</p>"},{"location":"cn/podman/","title":"Lab CN 2 - Podman Containers","text":""},{"location":"cn/podman/#lab-cn-2-podman-containers","title":"Lab CN 2 - Podman Containers","text":"<p>In this lab, you will learn about how to use podman and how to run applications using podman. This lab will not explicitly give you the commands to progress through these exercises, but will show you a similar expected output.</p> <p>It's your goal to create the commands needed (shown as &lt; command &gt; at each step) to complete the lab.</p>"},{"location":"cn/podman/#prerequisites","title":"Prerequisites","text":"<ul> <li>Create a Quay account. This account is needed to push images to a container registry. Follow the tutorial to get familiar with interacting with Quay</li> <li>You need to install Podman Desktop in your environment. Follow the instructions here to install it on MacOS and here to install it on Windows.</li> </ul>"},{"location":"cn/podman/#working-with-podman","title":"Working with podman","text":"<p>Before proceeding, make sure podman is properly installed on your system.</p> <ol> <li>Please verify your Podman by looking up the version.</li> </ol> <p>If it is installed, you will see a version number something similar to below.</p> <pre><code>$ &lt;command&gt;\nClient:       Podman Engine\nVersion:      5.3.0\nAPI Version:  5.3.0\nGo Version:   go1.23.3\nGit Commit:   874bf2c301ecf0ba645f1bb45f81966cc755b7da\nBuilt:        Wed Nov 13 03:10:17 2024\nOS/Arch:      darwin/amd64\n\nServer:       Podman Engine\nVersion:      5.2.2\nAPI Version:  5.2.2\nGo Version:   go1.22.6\nBuilt:        Wed Aug 21 10:00:00 2024\nOS/Arch:      linux/amd64\n</code></pre> <p>Running a hello-world container</p> <p>Let us start with a <code>hello-world</code> container.</p> <ol> <li>run a <code>hello-world</code> container.</li> </ol> <p>If it is successfully run, you will see something like below.</p> <pre><code>$ &lt;command&gt;\nTrying to pull quay.io/podman/hello:latest...\nGetting image source signatures\nCopying blob sha256:81df7ff16254ed9756e27c8de9ceb02a9568228fccadbf080f41cc5eb5118a44\nCopying config sha256:5dd467fce50b56951185da365b5feee75409968cbab5767b9b59e325fb2ecbc0\nWriting manifest to image destination\n!... Hello Podman World ...!\n\n         .--\"--.           \n       / -     - \\         \n      / (O)   (O) \\        \n   ~~~| -=(,Y,)=- |         \n    .---. /`  \\   |~~      \n ~/  o  o \\~~~~.----. ~~   \n  | =(X)= |~  / (O (O) \\   \n   ~~~~~~~  ~| =(Y_)=-  |   \n  ~~~~    ~~~|   U      |~~ \n\nProject:   https://github.com/containers/podman\nWebsite:   https://podman.io\nDesktop:   https://podman-desktop.io\nDocuments: https://docs.podman.io\nYouTube:   https://youtube.com/@Podman\nX/Twitter: @Podman_io\nMastodon:  @Podman_io@fosstodon.org\n</code></pre> <p>Since, <code>hello-world</code> image is not existing locally, it is pulled from <code>podman/hello</code>. But if it is already existing, podman will not pull it every time but rather use the existing one.</p> <p>This image is pulled from https://quay.io/repository/podman/hello?. Quay.io is a repository used to store container images. Similarly, you can use your own registries to store images.</p> <p>Verifying the hello-world image</p> <ol> <li>Now verify if an image is existing in your system locally.</li> </ol> <p>You will then see something like below.</p> <pre><code>$ &lt;command&gt;\nREPOSITORY                              TAG         IMAGE ID      CREATED       SIZE\nquay.io/podman/hello                    latest      5dd467fce50b  6 months ago  787 kB\n</code></pre>"},{"location":"cn/podman/#get-the-sample-application","title":"Get the sample application","text":"<p>To get the sample application, you will need to clone it from github.</p> <pre><code># Clone the sample app\ngit clone https://github.com/ibm-cloud-architecture/cloudnative_sample_app.git\n\n# Go to the project's root folder\ncd cloudnative_sample_app/\n</code></pre>"},{"location":"cn/podman/#run-the-application-on-podman","title":"Run the application on Podman","text":"<p> If you are looking to build this application on Apple Silicon, these instructions will not work! Look out for this symbol -  - in the below instructions if you get stuck!</p>"},{"location":"cn/podman/#build-the-container-image","title":"Build the container image","text":"<p>Let's take look at the Containerfile, sometimes referred to as a Dockerfile before building it.</p> <pre><code>FROM maven:3.3-jdk-8 as builder\n\nCOPY . .\nRUN mvn clean install\n\nFROM openliberty/open-liberty:springBoot2-ubi-min as staging\n\nCOPY --chown=1001:0 --from=builder /target/cloudnativesampleapp-1.0-SNAPSHOT.jar /config/app.jar\nRUN springBootUtility thin \\\n    --sourceAppPath=/config/app.jar \\\n    --targetThinAppPath=/config/dropins/spring/thinClinic.jar \\\n    --targetLibCachePath=/opt/ol/wlp/usr/shared/resources/lib.index.cache\n</code></pre> <ul> <li>Using the <code>FROM</code> instruction, we provide the name and tag of an image that should be used as our base. This must always be the first instruction in the Containerfile.</li> <li>Using <code>COPY</code> instruction, we copy new contents from the source filesystem to the container filesystem.</li> <li><code>RUN</code> instruction executes the commands.</li> </ul> <p>This Containerfile leverages multi-stage builds, which lets you create multiple stages in your Dockerfile to do certain tasks.</p> <p>In our case, we have two stages.</p> <ul> <li>The first one uses <code>maven:3.3-jdk-8</code> as its base image to download and build the project and its dependencies using Maven.</li> <li>The second stage uses <code>openliberty/open-liberty:springBoot2-ubi-min</code> as its base image to run the compiled code from the previous stage.</li> </ul> <p>The advantage of using the multi-stage builds approach is that the resulting image only uses the base image of the last stage. Meaning that in our case, we will only end up with the <code>openliberty/open-liberty:springBoot2-ubi-min</code> as our base image, which is much tinier than having an image that has both Maven and the JRE.</p> <p>By using the multi-stage builds approach when it makes sense to use it, you will end up with much lighter and easier to maintain images, which can save you space on your Container Registry. Also, having tinier images usually means less resource consumption on your worker nodes, which can result cost-savings.</p> <p>Once, you have the Containerfile ready, the next step is to build it. The <code>build</code> command allows you to build a image which you can later run as a container.</p> <ol> <li>Build the Containerfile with the <code>image_name</code> of <code>greeting</code> and give it a <code>image_tag</code> of <code>v1.0.0</code> and build it using the current context.</li> </ol> <p>You will see something like below:</p> <pre><code>    $ &lt;command&gt;\n    Step 1/6 : FROM maven:3.3-jdk-8 as builder\n    ---&gt; 9997d8483b2f\n    Step 2/6 : COPY . .\n    ---&gt; c198e3e54023\n    Step 3/6 : RUN mvn clean install\n    ---&gt; Running in 24378df7f87b\n    [INFO] Scanning for projects...\n    .\n    .\n    .\n    [INFO] Installing /target/cloudnativesampleapp-1.0-SNAPSHOT.jar to /root/.m2/repository/projects/cloudnativesampleapp/1.0-SNAPSHOT/cloudnativesampleapp-1.0-SNAPSHOT.jar\n    [INFO] Installing /pom.xml to /root/.m2/repository/projects/cloudnativesampleapp/1.0-SNAPSHOT/cloudnativesampleapp-1.0-SNAPSHOT.pom\n    [INFO] ------------------------------------------------------------------------\n    [INFO] BUILD SUCCESS\n    [INFO] ------------------------------------------------------------------------\n    [INFO] Total time: 44.619 s\n    [INFO] Finished at: 2020-04-06T16:07:04+00:00\n    [INFO] Final Memory: 38M/385M\n    [INFO] ------------------------------------------------------------------------\n    Removing intermediate container 24378df7f87b\n    ---&gt; cc5620334e1b\n    Step 4/6 : FROM openliberty/open-liberty:springBoot2-ubi-min as staging\n    ---&gt; 021530b0b3cb\n    Step 5/6 : COPY --chown=1001:0 --from=builder /target/cloudnativesampleapp-1.0-SNAPSHOT.jar /config/app.jar\n    ---&gt; dbc81e5f4691\n    Step 6/6 : RUN springBootUtility thin     --sourceAppPath=/config/app.jar     --targetThinAppPath=/config/dropins/spring/thinClinic.jar     --targetLibCachePath=/opt/ol/wlp/usr/shared/resources/lib.index.cache\n    ---&gt; Running in 8ea80b5863cb\n    Creating a thin application from: /config/app.jar\n    Library cache: /opt/ol/wlp/usr/shared/resources/lib.index.cache\n    Thin application: /config/dropins/spring/thinClinic.jar\n    Removing intermediate container 8ea80b5863cb\n    ---&gt; a935a129dcb2\n    Successfully built a935a129dcb2\n    Successfully tagged greeting:v1.0.0\n</code></pre> <p> (Apple Silicon only) My build failed!</p> <p>If running on M1, you will encounter the following error(s) while building this container image: </p><pre><code>[1/2] STEP 1/3: FROM maven:3.3-jdk-8 AS builder\nWARNING: image platform (linux/amd64) does not match the expected platform (linux/arm64)\n\n&lt;output continues...&gt;\n\n[2/2] STEP 1/3: FROM openliberty/open-liberty:springBoot2-ubi-min AS staging\nResolving \"openliberty/open-liberty\" using unqualified-search registries (/etc/containers/registries.conf.d/999-podman-machine.conf)\nTrying to pull docker.io/openliberty/open-liberty:springBoot2-ubi-min...\nError: creating build container: choosing an image from manifest list docker://openliberty/open-liberty:springBoot2-ubi-min: no image found in manifest list for architecture \"arm64\", variant \"v8\", OS \"linux\"\n</code></pre> In the output above, podman is telling us that the build fails as the base image used by this Containerfile does not support arm64 architectures. <p></p> <p>Try to fix the error yourself. Here are a couple of tips: 1. Base images quickly become outdated and stale. Try searching <code>Dockerhub</code> 2. If you are unfamiliar with running a Java <code>.jar</code> file (lucky you!), have a look at this blog: https://spring.io/guides/gs/spring-boot-docker</p> <p>If you are really stuck, here is repository with an updated Dockerfile.</p> <p>Apple Silicon only END</p> <ol> <li>Next, verify your newly built image</li> </ol> <p>The output will be as follows.</p> <pre><code>$ &lt;command&gt;\nREPOSITORY                              TAG                  IMAGE ID      CREATED         SIZE\nquay.io/benswinney-ibm/greeting         v1.0.0               7674fa09c091  8 seconds ago   542 MB\nquay.io/podman/hello                    latest               5dd467fce50b  6 months ago    787 kB\ndocker.io/openliberty/open-liberty      springBoot2-ubi-min  021530b0b3cb  5 years ago     486 MB\ndocker.io/library/maven                 3.3-jdk-8            9997d8483b2f  7 years ago     669 MB\n</code></pre>"},{"location":"cn/podman/#run-the-podman-container","title":"Run the podman container","text":"<p>Now let's try running the podman container. Run it with the following parameters:</p> <ol> <li>Expose port <code>9080</code>. Run it in the background in detached mode. Give the container the name of <code>greeting</code>.</li> </ol> <p>Once done, you will have something like below.</p> <pre><code>$ &lt;command&gt;\nbc2dc95a6bd1f51a226b291999da9031f4443096c1462cb3fead3df36613b753\n</code></pre> <p>Also, podman cannot create two containers with the same name. If you try to run the same container having the same name again, you will see something like below.</p> <pre><code>$ &lt;command&gt;\npodman: Error response from daemon: Conflict. The container name \"/greeting\" is already in use by container \"a74b91789b29af6e7be92b30d0e68eef852bfb24336a44ef1485bb58becbd664\". You have to remove (or rename) that container to be able to reuse that name.\nSee 'podman run --help'.\n</code></pre> <p>It is a good practice to name your containers. Naming helps you to discover your service easily.</p> <ol> <li>List all the running containers.</li> </ol> <p>You will see something like below.</p> <pre><code>$ &lt;command&gt;\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                              NAMES\nbc2dc95a6bd1        greeting:v1.0.0     \"/opt/ol/helpers/run\u2026\"   18 minutes ago      Up 18 minutes       0.0.0.0:9080-&gt;9080/tcp, 9443/tcp   greeting\n</code></pre> <ol> <li>Let's inspect the running container.</li> </ol> <p>By inspecting the container, you can access detailed information about the container. By using this command, you get to know the details about network settings, volumes, configs, state etc.</p> <p>If we consider our container, it is as follows. You can see lot of information about the <code>greeting</code> container.</p> <pre><code>$ &lt;command&gt;\n[\n     {\n          \"Id\": \"7674fa09c09194abc3641e913b8ef9127658921e01a07cccbd26e74031f30784\",\n          \"Digest\": \"sha256:ef292f1f1e573e46b140cce57a4da269c231f7bba4352a4c066bc6313edabb7c\",\n          \"RepoTags\": [\n               \"quay.io/benswinney-ibm/greeting:v1.0.0\"\n          ],\n          \"RepoDigests\": [\n               \"quay.io/benswinney-ibm/greeting@sha256:ef292f1f1e573e46b140cce57a4da269c231f7bba4352a4c066bc6313edabb7c\"\n          ],\n          \"Parent\": \"08b3dcd861a5b3a854cf9361120f6f68f0608ef855027108c8f67e2b7db06d09\",\n          \"Comment\": \"Imported from -\",\n          \"Created\": \"2024-11-25T05:31:04.216084457Z\",\n          \"Config\": {\n               \"User\": \"1001\",\n               \"ExposedPorts\": {\n                    \"9080/tcp\": {},\n                    \"9443/tcp\": {}\n               },\n               \"Env\": [\n                    \"PATH=/opt/ol/wlp/bin:/opt/ol/docker/:/opt/ol/helpers/build:/opt/ibm/java/jre/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\",\n                    \"container=oci\",\n                    \"JAVA_VERSION=1.8.0_sr5fp40\",\n                    \"JAVA_HOME=/opt/ibm/java/jre\",\n                    \"IBM_JAVA_OPTIONS=-Xshareclasses:name=liberty,nonfatal,cacheDir=/output/.classCache/ -XX:+UseContainerSupport\",\n                    \"LOG_DIR=/logs\",\n                    \"WLP_OUTPUT_DIR=/opt/ol/wlp/output\",\n                    \"WLP_SKIP_MAXPERMSIZE=true\",\n                    \"RANDFILE=/tmp/.rnd\"\n               ],\n        ..........\n        ..........\n        ..........\n    }\n]\n</code></pre> <ol> <li>Get the logs of the <code>greeting</code> container.</li> </ol> <p>It helps you to access the logs of your container. It allows you to debug the container if it fails. It also lets you to know what is happening with your application.</p> <p>At the end, you will see something like below.</p> <pre><code>$ &lt;command&gt;\n.   ____          _            __ _ _\n/\\\\ / ___'_ __ _ _(_)_ __  __ _ \\ \\ \\ \\\n( ( )\\___ | '_ | '_| | '_ \\/ _` | \\ \\ \\ \\\n\\\\/  ___)| |_)| | | | | || (_| |  ) ) ) )\n'  |____| .__|_| |_|_| |_\\__, | / / / /\n=========|_|==============|___/=/_/_/_/\n:: Spring Boot ::        (v2.1.7.RELEASE)\n2019-08-30 16:57:01.494  INFO 1 --- [ecutor-thread-5] application.SBApplication                : Starting SBApplication on bc2dc95a6bd1 with PID 1 (/opt/ol/wlp/usr/servers/defaultServer/dropins/spring/thinClinic.jar started by default in /opt/ol/wlp/output/defaultServer)\n2019-08-30 16:57:01.601  INFO 1 --- [ecutor-thread-5] application.SBApplication                : No active profile set, falling back to default profiles: default\n[AUDIT   ] CWWKT0016I: Web application available (default_host): http://bc2dc95a6bd1:9080/\n2019-08-30 16:57:09.641  INFO 1 --- [cutor-thread-25] o.s.web.context.ContextLoader            : Root WebApplicationContext: initialization completed in 7672 ms\n2019-08-30 16:57:12.279  INFO 1 --- [ecutor-thread-5] o.s.b.a.e.web.EndpointLinksResolver      : Exposing 15 endpoint(s) beneath base path '/actuator'\n2019-08-30 16:57:12.974  INFO 1 --- [ecutor-thread-5] o.s.s.concurrent.ThreadPoolTaskExecutor  : Initializing ExecutorService 'applicationTaskExecutor'\n2019-08-30 16:57:13.860  INFO 1 --- [ecutor-thread-5] d.s.w.p.DocumentationPluginsBootstrapper : Context refreshed\n2019-08-30 16:57:13.961  INFO 1 --- [ecutor-thread-5] d.s.w.p.DocumentationPluginsBootstrapper : Found 1 custom documentation plugin(s)\n2019-08-30 16:57:14.020  INFO 1 --- [ecutor-thread-5] s.d.s.w.s.ApiListingReferenceScanner     : Scanning for api listing references\n2019-08-30 16:57:14.504  INFO 1 --- [ecutor-thread-5] application.SBApplication                : Started SBApplication in 17.584 seconds (JVM running for 33.368)\n[AUDIT   ] CWWKZ0001I: Application thinClinic started in 21.090 seconds.\n[AUDIT   ] CWWKF0012I: The server installed the following features: [el-3.0, jsp-2.3, servlet-4.0, springBoot-2.0, ssl-1.0, transportSecurity-1.0, websocket-1.1].\n[AUDIT   ] CWWKF0011I: The defaultServer server is ready to run a smarter planet. The defaultServer server started in 33.103 seconds.\n</code></pre> <p>This shows that the Spring Boot application is successfully started.</p>"},{"location":"cn/podman/#access-the-application","title":"Access the application","text":"<ul> <li>To access the application, open the browser and access http://localhost:9080/greeting?name=John.</li> </ul> <p> (Apple silicon only) My application isn't working!</p> <p>If you used the Apple Silicon Dockerfile provided, your application is not running on port 9080. Look at application logs and expose the right port, or...</p> Cheat and get the answer now Port 8080  <p>Apple Silicon only END</p> <p>You will see something like below.</p> <pre><code>{\"id\":2,\"content\":\"Welcome to Cloudnative bootcamp !!! Hello, John :)\"}\n</code></pre> <p>Container Image Registry</p> <p>Container Image Registry is a place where you can store the container images. They can be public or private registries. They can be hosted by third party as well. In this lab, we are using Quay.</p>"},{"location":"cn/podman/#pushing-an-image-to-a-registry","title":"Pushing an image to a Registry","text":"<p>Let us now push the image to the Quay registry. Before pushing the image to the registry, one needs to login.</p> <ol> <li>Login to Quay using your credentials.</li> </ol> <p>Once logged in we need to take the image for the registry.</p> <ol> <li> <p>Tag your image for the image registry using the <code>same name and tag from before</code>. Be sure to include the host name of the target image registry in the destination tag (e.g. quay.io). NOTE: the tag command has both the source tag and repository destination tag in it.</p> </li> <li> <p>Now push the image to the registry. This allows you to share images to a registry.</p> </li> </ol> <p>If everything goes fine, you will see something like below.</p> <pre><code>$ &lt;command&gt;\nThe push refers to repository [quay.io/&lt;repository_name&gt;/greeting]\n2e4d09cd03a2: Pushed\nd862b7819235: Pushed\na9212239031e: Pushed\n4be784548734: Pushed\na43c287826a1: Mounted from library/ibmjava\ne936f9f1df3e: Mounted from library/ibmjava\n92d3f22d44f3: Mounted from library/ibmjava\n10e46f329a25: Mounted from library/ibmjava\n24ab7de5faec: Mounted from library/ibmjava\n1ea5a27b0484: Mounted from library/ibmjava\nv1.0.0: digest: sha256:21c2034646a31a18b053546df00d9ce2e0871bafcdf764f872a318a54562e6b4 size: 2415\n</code></pre> <p>Once the push is successful, your image will be residing in the registry.</p>"},{"location":"cn/podman/#clean-up","title":"Clean Up","text":"<ol> <li> <p>Stop the <code>greeting</code> container.</p> </li> <li> <p>Remove the container.</p> </li> <li> <p>Remove the image. (NOTE: You will need the image_id to remove it.)</p> </li> </ol>"},{"location":"cn/podman/#pulling-an-image-from-the-registry","title":"Pulling an image from the registry","text":"<p>Sometimes, you may need the images that are residing on your registry. Or you may want to use some public images out there. Then, we need to pull the image from the registry.</p> <ol> <li>Pull the image <code>greeting</code> from the registry,</li> </ol> <p>If it successfully got pulled, we will see something like below.</p> <pre><code>ddcb5f219ce2: Pull complete\ne3371bbd24a0: Pull complete\n49d2efb3c01b: Pull complete\nDigest: sha256:21c2034646a31a18b053546df00d9ce2e0871bafcdf764f872a318a54562e6b4\nStatus: Downloaded newer image for &lt;repository_name&gt;/greeting:v1.0.0\ndocker.io/&lt;repository_name&gt;/greeting:v1.0.0\n</code></pre>"},{"location":"cn/podman/#conclusion","title":"Conclusion","text":"<p>You have successfully completed this lab! Let's take a look at what you learned and did today:</p> <ul> <li>Learned about the Containerfile.</li> <li>Learned about podman images.</li> <li>Learned about podman containers.</li> <li>Learned about multi-stage podman builds.</li> <li>Ran the Greetings service on Podman.</li> </ul>"},{"location":"cn/reference/","title":"Container References","text":""},{"location":"cn/reference/#container-references","title":"Container References","text":"<p>Containers are a standard way to package an application and all its dependencies so that it can be moved between environments and run without change. They work by hiding the differences between applications inside the container so that everything outside the container can be standardized.</p> <p>For example, Podman created standard way to create images for Linux Containers.</p>"},{"location":"cn/reference/#basic-podman-commands","title":"Basic Podman Commands","text":"Action Command Get Podman version <code>podman version</code> Run <code>hello-world</code> Container <code>podman run hello-world</code> List Running Containers <code>podman ps</code> Stop a container <code>podman stop &lt;container-name/container-id&gt;</code> List container Images <code>podman images</code> Login into registry <code>podman login</code> Build an image <code>podman build -t &lt;image_name&gt;:&lt;tag&gt; .</code> Inspect a container object <code>podman inspect &lt;name/id&gt;</code> Inspect a container image <code>podman inspect image &lt;name/id&gt;</code> Pull an image <code>podman pull &lt;image_name&gt;:&lt;tag&gt;</code> Push an Image <code>podman push &lt;image_name&gt;:&lt;tag&gt;</code> Remove a container <code>podman rm &lt;container-name/container-id&gt;</code>"},{"location":"cn/reference/#running-podman","title":"Running Podman","text":"Local Podman <ol> <li> <p>Install Podman Desktop</p> </li> <li> <p>Test it out</p> </li> </ol> <p>Related Lab Activities</p> <p>In the next portion of the bootcamp, you will complete labs directly related to the above learning</p> <ul> <li>Using the IBM Container Registry in the Build and Deploy Run using IBM Container Registry</li> <li>Running a Sample Application on Docker in the Docker Lab</li> </ul> <p>Once you have completed these tasks, you should have a base understanding of containers and how to use Docker.</p>"},{"location":"cn/running-containers/","title":"Lab CN 3 - Running a Container Locally","text":""},{"location":"cn/running-containers/#lab-cn-3-running-a-container-locally","title":"Lab CN 3 - Running a Container Locally","text":"<p>In this lab we are going to build on what you completed in Lab 2 and deploy the greeting container image on either OpenShift Local or MiniKube (depending on your setup)</p>"},{"location":"cn/running-containers/#prerequisites","title":"Prerequisites","text":"<ul> <li>Completed Cloud Native Lab 1 and 2</li> <li>OpenShift Local or MiniKube installed on your machine</li> <li>The Openshift Client CLI (<code>oc</code>) or <code>kubectl</code> installed on your machine</li> <li>Greeting container image pushed to your personal Quay registry.</li> </ul>"},{"location":"cn/running-containers/#start-your-local-cluster","title":"Start Your Local Cluster","text":"OpenShift LocalMiniKube <ol> <li>Refer to the OpenShift Local docs to ensure OpenShift Local is running on your local machine.</li> </ol> <p> OpenShift local requires 10GB of memory to run. Make sure that you have that amount of memory available on the system as you start the install.</p> <ol> <li>Once <code>OpenShift Local</code> has started, follow the instructions printed to the terminal to log in via the cli. Then check the node is running happily:</li> </ol> <pre><code>oc get nodes\n</code></pre> <p>Expected output:</p> <pre><code>NAME   STATUS   ROLES                         AGE   VERSION\ncrc    Ready    control-plane,master,worker   21d   v1.30.7\n</code></pre> <p>Refer to the MiniKube docs to start MiniKube. </p> <ol> <li>Once MiniKube has started, check the node is running and the right context is set in <code>kubectl</code>:</li> </ol> <pre><code>kubectl get nodes\n</code></pre> <p>Expected output:</p> <pre><code>NAME       STATUS   ROLES           AGE   VERSION\nminikube   Ready    control-plane   60s   v1.32.0\n</code></pre>"},{"location":"cn/running-containers/#run-the-application-as-a-pod","title":"Run the application as a pod","text":"<p>This lab will not be covering Kubernetes in detail, but we are going to cover a very simple deployment. </p> <p> You can use <code>oc</code> and <code>kubectl</code> interchangeably for this section of the lab </p> <ol> <li>A pod is the smallest deployable unit in Kubernetes. You can deploy the greeting application from lab 2 with the following command:</li> </ol> <pre><code>kubectl run greeting --image quay.io/&lt;repository_name&gt;/greeting:&lt;tag&gt;\n</code></pre> <ol> <li>Check the pod is up and running:</li> </ol> <pre><code>kubectl get pods\n</code></pre> <p>Expected output: </p> <pre><code>NAME       READY   STATUS    RESTARTS   AGE\ngreeting   1/1     Running   0          106s\n</code></pre> <ol> <li>You can find more information on the pod using the <code>describe</code> command:</li> </ol> <p></p><pre><code>kubectl describe pod greeting\n</code></pre> Expected ouptut:<p></p> <pre><code>Name:             greeting\nNamespace:        default\nPriority:         0\nService Account:  default\nNode:             minikube/192.168.49.2\nStart Time:       Wed, 29 Jan 2025 13:27:27 +1100\nLabels:           run=greeting\nAnnotations:      &lt;none&gt;\nStatus:           Running\nIP:               10.244.0.4\nIPs:\n  IP:  10.244.0.4\nContainers:\n  greeting:\n    Container ID:   docker://866d0711b4c08b461421f9a22aa9702f9c37c33c3e185a68d806b4158577c094\n    Image:          quay.io/samuele_chinellato_ibm/greeting:v0.0.1\n    Image ID:       docker-pullable://quay.io/samuele_chinellato_ibm/greeting@sha256:e540f501125ae9030bcadfbc20a7b2d7d18b766112e76fd5b08f94fe7c56798e\n    Port:           &lt;none&gt;\n    Host Port:      &lt;none&gt;\n    State:          Running\n      Started:      Wed, 29 Jan 2025 13:28:02 +1100\n    Ready:          True\n    Restart Count:  0\n    Environment:    &lt;none&gt;\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8xqq6 (ro)\nConditions:\n  Type                        Status\n  PodReadyToStartContainers   True\n  Initialized                 True\n  Ready                       True\n  ContainersReady             True\n  PodScheduled                True\nVolumes:\n  kube-api-access-8xqq6:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       &lt;nil&gt;\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              &lt;none&gt;\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age    From               Message\n  ----    ------     ----   ----               -------\n  Normal  Scheduled  6m11s  default-scheduler  Successfully assigned default/greeting to minikube\n  Normal  Pulling    6m11s  kubelet            Pulling image \"quay.io/samuele_chinellato_ibm/greeting:v0.0.1\"\n  Normal  Pulled     5m37s  kubelet            Successfully pulled image \"quay.io/samuele_chinellato_ibm/greeting:v0.0.1\" in 33.663s (33.663s including waiting). Image size: 146552052 bytes.\n  Normal  Created    5m37s  kubelet            Created container: greeting\n  Normal  Started    5m37s  kubelet            Started container greeting\n</code></pre> <ol> <li>Once your pod enters a <code>running</code> state, run the following command to access the application from your machine:</li> </ol> <pre><code>kubectl port-forward pod/greeting 9080:9080\n</code></pre> <p>Expected output:</p> <pre><code>Forwarding from 127.0.0.1:9080 -&gt; 9080\nForwarding from [::1]:9080 -&gt; 9080\n</code></pre> <ol> <li>Open http://localhost:9080/greeting?name=world in your browser. You should see a message from your greeting app!</li> </ol> <p> Apple Silicon Only replace 9080 with 8080 in the above commands</p> <p>This lab concludes the Cloud Native section of the Platform Engineering BootCamp. You are now ready to move on to Kubernetes and OpenShift</p>"},{"location":"cn-challenge/","title":"Cloud Native Challenge","text":""},{"location":"cn-challenge/#cloud-native-challenge","title":"Cloud Native Challenge","text":""},{"location":"cn-challenge/#phase-1-local-develop","title":"Phase 1 - Local Develop","text":"<ul> <li>Start by creating a Github Repo for your application.</li> <li>Choose <code>NodeJS</code>, <code>Python</code>, or <code>React</code>.</li> <li>Site about one of the following:<ul> <li>Yourself</li> <li>Hobby</li> <li>Place you live</li> </ul> </li> <li>Must be able to run locally</li> </ul>"},{"location":"cn-challenge/#application-requirements","title":"Application Requirements","text":"<ul> <li>Minimum of 3 webpages</li> <li>Minimum of 1 GET and POST method each.</li> <li>SwaggerUI Configured for API Testing.</li> <li>API's exposed through Swagger</li> <li>Custom CSS files for added formatting.</li> </ul>"},{"location":"cn-challenge/#testing","title":"Testing","text":"<p>Setup each of the following tests that apply:</p> <ul> <li>Page tests</li> <li>API tests</li> <li>Connection Tests</li> </ul>"},{"location":"cn-challenge/#phase-2-application-enhancements","title":"Phase 2 - Application Enhancements","text":""},{"location":"cn-challenge/#database-connectivity-and-functionality","title":"Database Connectivity and Functionality","text":"<ul> <li>Add local or cloud DB to use for data collection.</li> <li>Use 3<sup>rd</sup> party API calls to get data.<ul> <li>Post Data to DB via API Call</li> <li>Retrieve Data from DB via API Call</li> <li>Delete Data from DB via API Call</li> </ul> </li> </ul>"},{"location":"cn-challenge/#phase-3-containerize","title":"Phase 3 - Containerize","text":""},{"location":"cn-challenge/#container-image","title":"Container Image","text":"<ul> <li>Create a DockerFile</li> <li>Build your docker image from the dockerfile</li> <li>Run it locally via Docker Desktop or another docker engine.</li> </ul>"},{"location":"cn-challenge/#image-registries","title":"Image Registries","text":"<ul> <li>Once validation of working docker image, push the image up to a registry.</li> <li>Use one of the following registries:<ul> <li>Docker</li> <li>Quay.io</li> <li>IBM Container</li> </ul> </li> <li>Push the image up with the following name: <code>{DockerRegistry}/{yourusername}/techdemos-cn:v1</code></li> </ul>"},{"location":"cn-challenge/#phase-4-kubernetes-ready","title":"Phase 4 - Kubernetes Ready","text":""},{"location":"cn-challenge/#create-pod-and-deployment-files","title":"Create Pod and Deployment files","text":"<ul> <li>Create a <code>Pod</code> YAML to validate your image.</li> <li>Next, create a <code>deployment</code> yaml file with the setting of 3 replicas.</li> <li>Verify starting of deployment</li> <li>Push all YAML files to Github</li> </ul>"},{"location":"cn-challenge/#application-exposing","title":"Application Exposing","text":"<ul> <li>Create a <code>Service</code> and <code>Route</code> yaml</li> <li>Save <code>Service</code> and <code>Route</code> yamls in Github</li> </ul>"},{"location":"cn-challenge/#configuration-setup","title":"Configuration Setup","text":"<ul> <li>Create a <code>ConfigMap</code> for all site configuration.</li> <li>Setup <code>Secrets</code> for API keys or Passwords to 3<sup>rd</sup> parties.</li> <li>Add storage where needed to deployment.</li> </ul>"},{"location":"cn-challenge/#phase-5-devopsgitops","title":"Phase 5 - Devops/Gitops","text":""},{"location":"cn-challenge/#tekton-pipeline-setup","title":"Tekton Pipeline Setup","text":"<ul> <li>Create a Tekton pipeline to do the following:<ul> <li>Setup</li> <li>Test</li> <li>Build and Push Image</li> <li>GitOps Version Update</li> </ul> </li> <li>Make each of the above their own task.</li> <li>Setup triggers to respond to Github commits and PR's</li> </ul>"},{"location":"cn-challenge/#gitsops-configuration","title":"GitsOps Configuration","text":"<ul> <li>Use ArgoCD to setup Deployment.</li> <li>Test your ArgoCD deployment</li> <li>Make a change to site and push them.</li> <li>Validate new image version.</li> </ul>"},{"location":"cn-challenge/#extras","title":"Extras","text":""},{"location":"cn-challenge/#chatbot-functions","title":"Chatbot Functions","text":"<ul> <li>Watson Assistant Integration</li> <li>Conversation about your sites topic.</li> <li>Have Chat window or page.</li> <li>Integrate Watson Assistant Actions.</li> </ul>"},{"location":"concert/","title":"IBM Concert","text":""},{"location":"concert/#ibm-concert","title":"IBM Concert","text":""},{"location":"concert/#lab-introduction-compliance-and-resilience-with-ibm-concert","title":"\ud83d\udd27 Lab Introduction: Compliance and Resilience with IBM Concert","text":"<p>This lab is built for engineers, architects, and technical practitioners who are responsible for maintaining compliance, managing risk, and ensuring operational resilience across hybrid and multicloud environments.</p> <p>IBM Concert is designed to embed compliance and resilience directly into your operational workflows. Through policy-as-code and continuous posture management, Concert helps teams shift from reactive compliance checks to proactive, automated governance.</p> <p>In this lab, you\u2019ll work through practical scenarios that cover the core capabilities of IBM Concert, including:</p> <ul> <li> <p> Installation and Configuration \u2013 Deploy IBM Concert into your environment and connect it to your infrastructure.</p> </li> <li> <p> Certificate Lifecycle Management \u2013 Ingest, validate, and renew certificates to prevent outages and maintain secure communications.</p> </li> <li> <p> Software Composition Analysis (SCA) \u2013 Generate and manage SBOMs, and detect supply chain risks tied to open-source dependencies.</p> </li> <li> <p> Policy Management \u2013 Write, deploy, and test policies as code to monitor for compliance and resilience requirements.</p> </li> <li> <p> Posture Monitoring and Remediation \u2013 Detect non-compliance, configuration drift, or resiliency gaps and trigger automated remediation workflows.</p> </li> </ul> <p>\ud83d\udd25 What You'll Walk Away With:</p> <ul> <li> <p>A working knowledge of how to apply IBM Concert to manage compliance and resilience posture in real-time.</p> </li> <li> <p>Hands-on experience defining policies that continuously validate against internal controls and external standards (like CIS, NIST, or company-specific policies).</p> </li> <li> <p>Practical skills in automating certificate operations and managing software supply chain risks as part of your security posture.</p> </li> <li> <p>This lab is designed to be hands-on, outcome-driven, and directly applicable to real-world operational challenges. Whether you're responsible for platform engineering, site reliability, DevSecOps, or governance, this lab will give you the tools to embed compliance and resilience into the way your infrastructure runs.</p> </li> </ul>"},{"location":"concert/concert-comp-workflow/","title":"Managing Compliance","text":""},{"location":"concert/concert-comp-workflow/#managing-compliance","title":"Managing Compliance","text":""},{"location":"concert/concert-comp-workflow/#objective","title":"Objective","text":"<p>In order to use Compliance dimension, an organisation need to ingests compliance assessment data using Concert workflow. This will deliver a holistic view of the compliance posture of its application environments.</p> <p>In this lab, you will use concert workflow to ingest compliance data from a rhel vm in IBM Concert.  We will use your VM concert as the target of the compliance job.</p>"},{"location":"concert/concert-comp-workflow/#prerequisite","title":"Prerequisite","text":"<ul> <li>IBM Concert must be installed</li> <li>Concert workflow must be installed</li> </ul>"},{"location":"concert/concert-comp-workflow/#content","title":"Content","text":"<ul> <li>Managing Compliance</li> <li>Objective</li> <li>Prerequisite</li> <li>Content</li> <li>Run compliance workflow<ul> <li>Create an environment</li> <li>Create a compliance catalog and a compliance profile</li> <li>Install the workflow in IBM Concert</li> <li>Create an Authentication to ssh the Concert VM</li> <li>Run manually the workflow</li> <li>Run the workflow from an ingestion job</li> <li>Scheduling the workflow job</li> </ul> </li> <li>Compliance Management</li> </ul>"},{"location":"concert/concert-comp-workflow/#run-compliance-workflow","title":"Run compliance workflow","text":"<p>You are going to use a workflow called CIS RHEL9 OpensSCAP Compliance Scan provided by IBM Concert to ingest compliance data in Concert. This workflow automates the CIS compliance scan for RHEL 9 using OpenSCAP. </p> <p>Official documentation is here</p>"},{"location":"concert/concert-comp-workflow/#create-an-environment","title":"Create an environment","text":"<p>In order to be able to ingest compliance data in IBM Concert, you must have an environment defined in IBM Concert with the hostname of the machine that will be scanned.</p> <p>To create this environment, follow these steps:</p> <ol> <li> <p>From your VM Rhel 9 reservation page, get the VM hostname (If you are connected in ssh on your vm, you can also get your hostname with the <code>hostname</code>command)   </p> </li> <li> <p>From the arena view on your concert UI, select Define and upload-&gt;Define Environment-&gt;From resources </p> </li> <li> <p>In the Define an environment screen, enter following informations:</p> </li> <li> <p>name: your VM hostname</p> </li> <li>type: other</li> <li>purpose: what you want</li> </ol> <p>Then click next, next and Create</p>"},{"location":"concert/concert-comp-workflow/#create-a-compliance-catalog-and-a-compliance-profile","title":"Create a compliance catalog and a compliance profile","text":"<p>Official documentation is here</p> <ul> <li>A compliance catalog serves as the single source of truth for an client organization's compliance-related policies, procedures, and standards. Concert supports compliance catalogs based on the NIST 800-53 (Rev4 for OCP and Rev5), PCI, or a custom standard.</li> </ul> <p>To create a catalog for our  CIS RHEL9 OpensSCAP Compliance Scan workflow, follow these steps:</p> <ol> <li>Navigate to Dimensions-&gt;Compliance</li> <li>Select Catalog tab and click Add catalog-&gt;From Standards Library button</li> <li> <p>Select CIS Controls entry and click Add</p> </li> <li> <p>A compliance profile represents a subset of controls from a compliance catalog. Each profile specifies a set of rules the scan results will use to assess the overall compliance of the application environments.</p> </li> </ol> <p>To create a profile for our CIS RHEL9 OpensSCAP Compliance Scan workflow, follow these steps:</p> <ol> <li>Select Profile tab, click Create profile button and select From resources</li> <li> <p>Enter following values and click Create button</p> </li> <li> <p>Name: profile_cis</p> </li> <li>Select one or more catalogs: select CIS Controls catalog</li> <li>Select controls: Click Select all (or choose specific compliance rules)</li> </ol>"},{"location":"concert/concert-comp-workflow/#install-the-workflow-in-ibm-concert","title":"Install the workflow in IBM Concert","text":"<ol> <li> <p>Download the CIS RHEL9 OpensSCAP Compliance Scan worflow from the Automation Library</p> </li> <li> <p>Upload the workflow in concert </p> </li> <li> <p>On concert UI, select Workflows-&gt;Manage menu</p> </li> <li>Select Import button</li> <li>And choose the zip file corresponding to the workflow you downloaded in step1 (name CIS_RHEL9_OpenSCAP_Compliance_Scan.zip)</li> </ol>"},{"location":"concert/concert-comp-workflow/#create-an-authentication-to-ssh-the-concert-vm","title":"Create an Authentication to ssh the Concert VM","text":"<p>CIS_RHEL9_OpenSCAP_Compliance_Scan workflow will do an ssh connection to the target machine to run an OpenSCAP compliance scan. In our case, the target machine is our Concert VM.  Follow these step to create an SSH Authentication:</p> <ol> <li>On concert UI, select Workflows-&gt;Authentications menu </li> <li> <p>Click the Create authentication button and enter following informations:</p> </li> <li> <p>name: concert-vm-ssh</p> </li> <li>service: SSH</li> <li>Host: your VM Ip public address (from your reservation page)</li> <li>Port: 2223</li> <li>Username: itzuser</li> <li>RSA Private Key: the content of your VM pem key (downloaded from your reservation page)</li> </ol> <p></p>"},{"location":"concert/concert-comp-workflow/#run-manually-the-workflow","title":"Run manually the workflow","text":"<ol> <li>On concert UI, select Workflows-&gt;Manage menu </li> <li>Select CIS_RHEL9_OpenSCAP_Compliance_Scan workflow</li> <li>Edit the RHEL_OSCAP_Scan step</li> </ol> <ol> <li>Add a sudo before each commands and save your modifications</li> </ol> <ol> <li> <p>Execute the worflow</p> </li> <li> <p>Select Run-&gt;Run with Custom Inputs, select the authentication entries and click run</p> </li> </ol> <p> </p> <p>Note: You can also run the workflow in debug mode. In this case you must give the authentication values in the workflow Start box </p> <p>The execution of the flow take at least 5 minutes, be patient.</p> <ol> <li> <p>Check the ingested data</p> </li> <li> <p>When the workflow is finished, navigate to Administration-&gt;Event log menu and check that the compliance file upload is successfull   </p> </li> <li> <p>Navigate to Dimensions-&gt;Compliance menu and consult compliance data for your concert VM.</p> </li> </ol>"},{"location":"concert/concert-comp-workflow/#run-the-workflow-from-an-ingestion-job","title":"Run the workflow from an ingestion job","text":"<p>You can create an ingestion job to run the compliance scan</p> <ol> <li>Navigate to the Administration-&gt;Integrations menu</li> <li>Click Create ingestion job button</li> <li> <p>Enter following values and click Create</p> </li> <li> <p>Name: Concert VM Compliance</p> </li> <li>Connection type: Concert Workflows</li> <li>Connection: CONCERT_WORKFLOWS</li> <li>Workflow reference: /User/CIS_RHEL9_OpenSCAP_Compliance_Scan</li> <li>Concert auth: ibmconcert@0000-0000-0000-0000/ConcertAPIKey</li> <li> <p>Ssh authentication: ibmconcert@0000-0000-0000-0000/concert-vm-ssh</p> </li> <li> <p>Then you can launch the job</p> </li> </ol> <p></p> <p>Note: you need to reload the page to see if the job is finished </p>"},{"location":"concert/concert-comp-workflow/#scheduling-the-workflow-job","title":"Scheduling the workflow job","text":"<p>Alternatively, you can also schedule a workflow job for ingestion of compliance scans into Concert if needed.</p> <ol> <li>Navigate to Workflows-&gt;Schedule menu</li> <li>Select Create job button</li> <li>Populate the values and click Create</li> </ol> <p></p>"},{"location":"concert/concert-comp-workflow/#compliance-management","title":"Compliance Management","text":"<p>Walkthrough the uploaded compliance assessment:</p> <ul> <li>Home page - Compliance dimension</li> <li>Compliance Dimension</li> <li>Select an entry</li> <li>Sort by results</li> <li>Expand one</li> <li>Open ticket</li> </ul>"},{"location":"concert/concert-ops-certs/","title":"Managing Operations","text":""},{"location":"concert/concert-ops-certs/#managing-operations","title":"Managing Operations","text":""},{"location":"concert/concert-ops-certs/#objective","title":"Objective","text":"<p>In order to use Operation dimension, an organisation need to ingest its application certificates or environment certificates in IBM Concert using either Concert toolkit or workflows. Then in Concert we can view the list of certificates and their validity status. Then we can create tickets in third-party ticketing system to renew or replace expiring certificates.</p> <p>In this lab, you will manually use the concert toolkit to upload our micro-services certificates in IBM Concert.</p>"},{"location":"concert/concert-ops-certs/#prerequisite","title":"Prerequisite","text":"<ul> <li> IBM Concert must be installed</li> <li> You have run the manual data ingestion</li> </ul>"},{"location":"concert/concert-ops-certs/#certificates-data-ingestion","title":"Certificates Data Ingestion","text":"<p>Certificates can be ingested into Concert following several way:</p> <ul> <li>Using the built-in Concert Workflow Certificate Ingestion from a kubeadm (in Administration-&gt;Integration-&gt;Create Ingestion job)</li> </ul> <p></p> <ul> <li>Using the concert-toolkit for application certificates during the CI/CD process</li> </ul> <p>We will use Concert toolkit to ingest a sample certificate in Concert. Here are the manual steps to follow:</p> <ol> <li> <p>Connect on the machine you have provisioned on Techzone in Lab0</p> <pre><code>ssh itzuser@&lt;VM ip address&gt; -p 2223 -i /path/to/concert/sshkey/pem_ibmcloudvsi_download.pem\numask 022\n</code></pre> </li> <li> <p>Take a look at the certificate template provided by Concert Toolkit</p> <pre><code>cd $HOME/concert-bootcamp/SBOMs-ingestion/templates\nvi cert-sbom-values.yaml.template\n</code></pre> <p>In this file you can see that it is possible to generate certificate SBOMs using 3 ways:</p> <ul> <li>Line 22: Providing an URL (we will use this method)</li> <li>Line 26: Reading certificate files</li> <li>Line 33: Providing certificates details manually</li> </ul> <p>As you have not deployed the application components in these labs, we use the https://www.ibm.com url to get the certificate. In a real deployment environment, you will replace this URL with your application deployment URLs. </p> </li> <li> <p>Source the ingestion job environment variables for hr-app component</p> <pre><code>cd $HOME/concert-bootcamp/SBOMs-ingestion\nsource app-common-variables.variables\nsource hr-app.variables\n</code></pre> </li> <li> <p>Create a concert toolkit config file from the certificate template</p> <pre><code>envsubst &lt; templates/cert-sbom-values.yaml.template &gt; $HOME/concert-bootcamp/SBOMs-ingestion/concert_data/${COMPONENT_NAME}/cert-sbom-values.yaml\n</code></pre> </li> <li> <p>Generate the certificate SBOM using Concert toolkit</p> <pre><code>APP_COMMAND=\"cert-inventory --cert-config /app/sample/cert-sbom-values.yaml\"\nSRC_PATH=\"$HOME/concert-bootcamp/SBOMs-ingestion/concert_data/${COMPONENT_NAME}\"\nOUTPUTDIR=\"$HOME/concert-bootcamp/SBOMs-ingestion/concert_data/${COMPONENT_NAME}\"\npodman run -v \"${SRC_PATH}\":/app/sample  -v \"${OUTPUTDIR}\":/toolkit-data --rm ${CONCERT_TOOLKIT_IMAGE} /bin/bash -c \"${APP_COMMAND}\"\n</code></pre> </li> <li> <p>Patch the sbom generated</p> <p>At the time we write this lab (May 2025), there is 1 issue in the sbom generated. Follow this steps to correct it:</p> <pre><code>sudo chmod 666 $HOME/concert-bootcamp/SBOMs-ingestion/concert_data/${COMPONENT_NAME}/certificates-hr-app.json\nvi $HOME/concert-bootcamp/SBOMs-ingestion/concert_data/${COMPONENT_NAME}/certificates-hr-app.json\n</code></pre> <ul> <li> <p>Reduce the number of dns_names entries </p> </li> <li> <p>Save the file (:wq)</p> </li> </ul> </li> <li> <p>Upload the certificate file in Concert using Concert API</p> <pre><code>curl -k -X \"POST\" -H \"accept: application/json\" -H \"InstanceID: ${CONCERT_INSTANCE_ID}\" -H \"Authorization: C_API_KEY ${CONCERT_APIKEY}\" -H \"Content-Type: multipart/form-data\" -F \"data_type=certificate\" -F \"filename=@${OUTPUTDIR}/certificates-hr-app.json\" \"https://${CONCERT_HOST}:${CONCERT_PORT}/ingestion/api/v1/upload_files\"\n</code></pre> </li> <li> <p>Check your certificate upload in IBM Concert UI</p> <p>After logging in your IBM Concert UI, you can see your ingested certificate from the Operation dimension.   </p> <ul> <li> <p>First, you can check that the upload is successfull by looking at menu Administration-&gt;Event log. Here you can see the status of all the files ingested</p> <p></p> </li> <li> <p>Then, you can navigate the the menu Dimensions-&gt;Operation to see your certificate</p> <p></p> </li> </ul> </li> </ol>"},{"location":"concert/concert-ops-certs/#certificates-management","title":"Certificates management","text":"<p>Walkthrough the uploaded certificates:</p> <ul> <li>Home page - Operations dimension</li> <li>Operation Dimension</li> <li>Select a certificate</li> <li>Renewal if expired</li> </ul>"},{"location":"concert/concert-resilience/","title":"Managing Resilience","text":""},{"location":"concert/concert-resilience/#managing-resilience","title":"Managing Resilience","text":""},{"location":"concert/concert-resilience/#objective","title":"Objective","text":"<p>In order to use Resilience dimension, an organisation need to determine the non-functional requirements (NFRs) that apply to its applications, as well as the target values that must be achieved to meet contractual obligations or otherwise be considered resilient. Also, relevant data must be collected from the applications and their environment components in order to import them to Concert on a regular basis.</p> <p>In this lab, you will use and create a concert workflow to ingest in IBM Concert resilience data concerning the quality of docker images. We will use the 2 images that you have build in lab4.</p>"},{"location":"concert/concert-resilience/#prerequisite","title":"Prerequisite","text":"<ul> <li>IBM Concert must be installed</li> <li>Concert workflow must be installed</li> </ul>"},{"location":"concert/concert-resilience/#content","title":"Content","text":"<ul> <li>Managing Resilience</li> <li>Objective</li> <li>Prerequisite</li> <li>Content</li> <li>Activate the Resilience</li> <li>Import Resilience data using a workflow<ul> <li>Resilience library and a Resilience profile</li> <li>Import a resilience workflow</li> <li>Build your own sub workflow</li> <li>Complete the resilience workflow previously imported</li> <li>Run the workflow to populate you application resilience posture</li> </ul> </li> <li>Resilience Management</li> </ul>"},{"location":"concert/concert-resilience/#activate-the-resilience","title":"Activate the Resilience","text":"<ol> <li>From Concert UI, navigate to Administration-&gt;Settings</li> <li>Select Miscellaneous tab</li> <li>Enable resilience</li> </ol> <p>You must also enable resilience for your application:</p> <ol> <li>From Concert UI, navigate to Inventory-&gt;Applications</li> <li>Select your hr-application</li> <li>Select Settings tab</li> <li>In Resilience menu, enable resilience</li> </ol> <p></p>"},{"location":"concert/concert-resilience/#import-resilience-data-using-a-workflow","title":"Import Resilience data using a workflow","text":""},{"location":"concert/concert-resilience/#resilience-library-and-a-resilience-profile","title":"Resilience library and a Resilience profile","text":"<p>For this lab, we will use part of a provided library called Container Build Integrity Library. These library NFRs are used to assess security, efficiency and maintainability of container image quality. We will calculate these metrics:</p> <ul> <li>Average image size</li> <li>Max image size</li> <li>Percentage of images with excessive layers</li> </ul> <p>To discover this library, follow these steps</p> <ol> <li>From Concert UI, navigate to Dimensions-&gt;Resilience</li> <li>Select Libraries tab</li> <li>Click Container Build Integrity Library link</li> <li>Expand Image Layers and Image Size</li> </ol> <p>And take a look at how the scores are calculated. The next steps consists in creating a workflow to get images metrics and upload an assessment in IBM Concert.</p>"},{"location":"concert/concert-resilience/#import-a-resilience-workflow","title":"Import a resilience workflow","text":"<p>You will start to import a pre-defined workflow available here.   </p> <ol> <li>From Concert UI, navigate to Workflows-&gt;Manage</li> <li>Click the Import button (top right of the window) and select the concert_v110_absolute_Resilience.zip workflow from your laptop</li> </ol> <p>This workflow retrieves, from your concert VM, the hr-application images you have build in Lab4.   Then, for each images it will do a <code>podman inspect</code> command and calculate metrics based on the <code>podman inspect</code> command result </p> <p>To be able to ssh your concert VM, you need to define an SSH Authentication:</p> <ol> <li>Navigate to Workflows-&gt;Authentications</li> <li>Click the Create authentication button (top right of the window)</li> <li>Call it dev-vm</li> <li>In Service, select SSH</li> <li> <p>Then enter these values:</p> </li> <li> <p>Host: your Concert VM IP</p> </li> <li>Port: 2223</li> <li>Username: itzuser</li> <li>RSA Private Key: the content of the pem file you download from your reservation page</li> </ol> <p></p>"},{"location":"concert/concert-resilience/#build-your-own-sub-workflow","title":"Build your own sub workflow","text":"<p>You will now create a workflow that will be used as a sub-worflow of docker_images_metrics workflow in order to define a new metric: the percentage of big images. The aim of this workflow is to extract the image size from a json object that have the format of the result of the <code>podman inspect</code> command</p> <ol> <li>From Concert UI, navigate to Workflows-&gt;Manage</li> <li>Navigate in Resilience folder</li> <li>Click the button Create workflow (top right of the window)</li> <li>Call it docker_images_size and click Create button</li> <li>Define your variables:</li> </ol> Name Type Default Value Selected box json_inspect Array [{\"Architecture\": \"amd64\", \"Os\": \"linux\", \"Size\": 1378729490}] in / required image_size Number 0 out / log <p>Then you are going to use a \"jq\" node in order to extract the size from the json_inspect input variable:</p> <ol> <li>From the palette that is at the left pane of your window, navigate in Common-&gt;Json</li> <li> <p>Select the jq box and drag and drop it before the Assign_1 box      </p> </li> <li> <p>From the Object Editor that is in the right pane of your window, click on OneOf&gt; </p> </li> <li> <p>Select Array, Click OK and Click Cancel </p> </li> <li> <p>Then select the pencil to set the JSON variable that jq will use as input     </p> </li> <li> <p>Enter value: $json_inspect</p> </li> <li>For the Filter variable put the value: \".[].Size\" (don't forget the quotes)     </li> </ol> <p>You just need now to assign the result of the jq node in the image_size output variable of your flow</p> <ol> <li>Select the Assign node that is under the jq node</li> <li> <p>From the Object Editor that is in the right pane of your window, enter following values:</p> </li> <li> <p>variable: $image_size</p> </li> <li> <p>value: $jq_1.result</p> <p></p> </li> </ol> <p>You can now test your workflow:</p> <ol> <li> <p>Click the Run button     </p> </li> <li> <p>At the bottom of your window, you should have the image size corresponding to the value of your json_inspect input variable displayed</p> <p></p> </li> </ol>"},{"location":"concert/concert-resilience/#complete-the-resilience-workflow-previously-imported","title":"Complete the resilience workflow previously imported","text":"<p>You are going to complete the empty branch in the main workflow in order to calculate images metrics</p> <ol> <li>From Concert UI, navigate to Workflows-&gt;Manage</li> <li>Navigate in Resilience folder</li> <li>Open the docker_images_metrics workflow</li> <li> <p>Scroll the Split_1 node where you will see an empty  Branch-2</p> <p></p> </li> <li> <p>From the palette that is at the left pane of your window, navigate in User-&gt;Resilience</p> </li> <li>Drag and drop the docker_image_size node (it is the sub-flow you just create before) in the empty branch </li> <li>Name the node get_image_size_flow</li> <li> <p>From the Object Editor that is in the right pane of your window, enter following values:</p> </li> <li> <p>json_inspect: $ssh_inspect_image.result (the result of the node named ssh_inspect_image is the input of your sub-flow)</p> </li> <li> <p>Then complete your branch as shown in following image</p> </li> </ol> <p>TIPS: most common nodes can be added by clicking the + that are in the flow where you want to add your node. </p> <p></p>"},{"location":"concert/concert-resilience/#run-the-workflow-to-populate-you-application-resilience-posture","title":"Run the workflow to populate you application resilience posture","text":"<p>Before running your flow, you need to modify the value of these variables:</p> Name Value concert_host The IP address of your Concert VM concert_api_key The API key of your concert installation (can be retrieved from your env.sh file create in lab1) <p>Now, you can now run your flow.  The best to begin is to run it in debug mode and put a breakpoint for example on node Upload_to_concert and take a look at the value of your concert_data variable</p> <p> </p> <p>You should see these values for concert_data variable:</p> <p></p> <p>Then you can finish the flow by clicking the Resume debug button</p> <p></p> <p>If the flow is successfull, you see that in the logs</p> <p></p> <p>You can now navigate to Dimensions-&gt;Resilience and select the components_images_posture</p>"},{"location":"concert/concert-resilience/#resilience-management","title":"Resilience Management","text":"<p>Walkthrough the uploaded assessment:</p> <ul> <li>Home page - Resilience dimension</li> <li>Resilience Dimension</li> <li>Select \"component_images_posture\"<ul> <li>Open a change request on an assessment</li> <li>Select an assessment</li> <li>In assessment summary tab, Sort by Assessed score</li> <li>In Actions tab, sort by Assessed score</li> </ul> </li> </ul> <p></p>"},{"location":"concert/concert-sbom/","title":"Concert sbom","text":"<p>This lab targets Concert v1.0.5.1</p> <p>This lab exercise was written and tested against a Concert v1.0.5.1 instance. Please ensure you are running the right version.</p>"},{"location":"concert/concert-sbom/#utilities","title":"Utilities","text":"syft <p><code>syft</code> is a command line utility for generating a Software Bill of Materials (SBOM) from container images and filesystems.</p> Linux (RHEL)MacWindows <pre><code>curl -sSfL https://raw.githubusercontent.com/anchore/syft/main/install.sh | sudo sh -s -- -b /usr/local/bin\n</code></pre> <p>You can also obtain the syft CLI following the official syft github repository install instructions.</p> <p>The best way to install syft for your Mac is using Homebrew</p> <pre><code>brew install syft\n</code></pre> <p>You can also obtain the syft CLI following the official syft github repository install instructions.</p> <p>You can obtain the syft CLI following the official syft github repository install instructions.</p> grype <p><code>grype</code> is a vulnerability scanner for container images and filesystems.</p> Linux (RHEL)MacWindows <pre><code>curl -sSfL https://raw.githubusercontent.com/anchore/grype/main/install.sh | sudo sh -s -- -b /usr/local/bin\n</code></pre> <p>You can also obtain the <code>grype</code> CLI following the official grype github repository install instructions.</p> <p>The best way to install <code>grype</code> for your Mac is using Homebrew</p> <pre><code>brew install grype\n</code></pre> <p>You can obtain the <code>grype</code> CLI following the official grype github repository install instructions.</p> <p>You can obtain the <code>grype</code> CLI following the official grype github repository install instructions.</p> trivy <p><code>trivy</code> is a vulnerability scanner for container images and filesystems.</p> Linux (RHEL)MacWindows <pre><code>cat &lt;&lt; EOF | sudo tee -a /etc/yum.repos.d/trivy.repo\n[trivy]\nname=Trivy repository\nbaseurl=https://aquasecurity.github.io/trivy-repo/rpm/releases/\\$basearch/\ngpgcheck=1\nenabled=1\ngpgkey=https://aquasecurity.github.io/trivy-repo/rpm/public.key\nEOF\nsudo yum -y update\nsudo yum -y install trivy\n</code></pre> <p>You can also obtain the <code>trivy</code> CLI following the official trivy github repository install instructions.</p> <p>The best way to install <code>trivy</code> for your Mac is using Homebrew</p> <pre><code>brew install trivy\n</code></pre> <p>You can obtain the <code>trivy</code> CLI following the official trivy github repository install instructions.</p> <p>You can obtain the <code>trivy</code> CLI following the official trivy github repository install instructions.</p> cdxgen <p><code>cdxgen</code> is a CLI tool to create a valid and compliant CycloneDX Bill of Materials (BOM) containing an aggregate of all project dependencies in JSON format.</p> Linux (RHEL)MacWindows <pre><code>sudo dnf module enable nodejs:18\nsudo dnf install -y npm\ncurl -s \"https://raw.githubusercontent.com/alexdelorenzo/npm-user/main/npm-user.sh\" | bash\nnpm install -g @cyclonedx/cdxgen\n</code></pre> <p>We add the npm-user script so that npm packages can be installed and run as non-root users.</p> <p>You can also obtain the <code>cdxgen</code> CLI following the official cdxgen github repository install instructions.</p> <p>The best way to install <code>cdxgen</code> for your Mac is using Homebrew</p> <pre><code>brew install cdxgen\n</code></pre> <p>You can also obtain the <code>cdxgen</code> CLI following the official cdxgen github repository install instructions.</p> <p>You can obtain the <code>cdxgen</code> CLI following the official cdxgen github repository install instructions.</p> git Linux (RHEL)MacWindows <pre><code>sudo dnf install -y git\n</code></pre> <p>You can also obtain the git CLI following the official Git Documentation.</p> <p>The best way to install the git utility for your Mac is using Homebrew</p> <pre><code>brew install git\n</code></pre> <p>You can also obtain the git CLI following the official Git Documentation.</p> <p>You can obtain the git CLI following the official GitHub Documentation.</p> jq Linux (RHEL)MacWindows <pre><code>sudo dnf install -y jq\n</code></pre> <p>You can also obtain the pre-compiled binaries in the jq Documentation.</p> <p>The best way to install the jq utility for your Mac is using Homebrew</p> <pre><code>brew install jq\n</code></pre> <p>You can also obtain the pre-compiled binaries in the jq Documentation.</p> <p>You can obtain the pre-compiled binaries in the jq Documentation.</p>"},{"location":"concert/concert-sbom/#generating-sboms-from-command-line","title":"Generating SBOMs from command line","text":"<p>Run commands from Linux or MacOS</p> <p>This lab exercise uses commands that are meant to be run from a Linux or MacOS command line.  If you're on a Windows workstation, please ssh to your IBM Concert VM or a bastion host and run these steps from there.</p>"},{"location":"concert/concert-sbom/#concert-utils","title":"concert-utils","text":"<p>concert-utils is a series of scripts that can aid you in integrating IBM Concert into your existing automation processes.</p> <p>Create a sample environment variable file to simulate a pipeline environment.  Create a <code>common_variables.sh</code> script from the sample common_variables.sh below, save it, and source it.</p> sample common_variables.sh <pre><code>####\n# Concert Toolkit Image\n####\nexport CONCERT_TOOLKIT_IMAGE=icr.io/cpopen/ibm-concert-toolkit:latest\n\n####\n# Concert details\n####\nexport BASE_URL=\"https://&lt;YOUR_CONCERT_VM_PUBLIC_IP&gt;:12443\"\nexport CONCERT_USERNAME=\"&lt;YOUR_CONCERT_USERNAME&gt;\"\nexport CONCERT_PASSWORD=\"&lt;YOUR_CONCERT_PASSWORD&gt;\"\n# If running Concert on OpenShift\n# export BASE_URL=\"https://&lt;CONCERT_OPENSHIFT_ROUTE&gt;/ibm/concert\"\n# export TOKEN=\"ZenApiKey &lt;YOUR_ZENAPIKEY&gt;\"\n\n####\n# cyclonedx generation utility for source code\n####\nexport OUTPUTDIR=~/dev/ibm-concert/lab\nexport SRC_PATH=./qotd-web\n\n######\n# Application Variables\n###\n\nexport APP_NAME=qotd-app\nexport APP_VERSION=1.0.0\nexport APP_CRITICALITY=5\n\nexport COMPONENT_NAME=qotd-web\nexport COMPONENT_VERSION=4.1.0\n\nexport ENVIRONMANET_NAME_1=qa\nexport ENVIRONMANET_NAME_2=stage\nexport ENVIRONMANET_NAME_3=prod\n\n###\n# inventory specific vaiable\n###\nexport REPO_NAME=qotd-web\nexport REPO_URL=https://github.ibm.com/cs-tel-ibm-concert-qotd/qotd-web\nexport REPO_BRANCH=main\nexport REPO_COMMIT_SHA=30f2b263723dc480c6d506155aafa05ab4461f40\nexport BUILD_NUMBER=1\nexport IMAGE_NAME=imgrepo.aws.ibm-gtmaa.dev/concert-apps/cs-tel-ibm-concert-qotd/qotd-web\nexport IMAGE_TAG=latest\nexport IMAGE_DIGEST=\"sha256:728ca85a2116f001335881d5024f099d50142bc17ff054c5311c4a6ba7956cd2\"\n\n###\n# deploy/release specific vaiable\n###\n\nexport ENV_TARGET=production\nexport DEPLOYMENT_REPO_NAME=https://github.ibm.com/cs-tel-ibm-concert-qotd/qotd-web/\nexport K8_PLATFORM=ocp\nexport CLUSTER_ENV_PLATFORM=aws\nexport CLUSTER_ID=ncolon-hhdtl\nexport CLUSTER_REGION=us-east-2\nexport CLUSTER_NAME=ncolon\nexport CLUSTER_NAMESPACE=qotd-app\nexport APP_URL=${COMPONENT_NAME}-qotd-app.apps.concert.aws.ibm-gmtaa.dev\nexport ENVIRONMENT_TARGET=production\n\nexport BUSINESS_NAME=\"IBM\"\nexport BUSINESS_UNIT=\"IBM Technology\"\nexport BUSINESS_EMAIL=\"ncolon@us.ibm.com\"\nexport BUSINESS_PHONE=\"123-456-7890\"\n\nexport ACCESS_POINT_NAME=\"qotd-web\"\nexport APP_ENDPOINT=\"/qotd-web\"\nexport APP_ENDPOINT_EXPOSURE=\"public\"\n\nexport K8S_NAME=concert-aws-prod-us-east-2\nexport K8S_PLATFORM=ocp\nexport K8S_ENV_PLATFORM=aws\nexport K8S_CLUSTER_ID=concert-nv4pj\nexport K8S_CLUSTER_REGION=us-east-2\nexport K8S_CLUSTER_NAME=concert\nexport K8S_NAMESPACE=qotd\n\nexport CONTAINER_COMMAND=\"podman run\"\nexport OPTIONS=\"--platform linux/amd64 -it --rm -u 0\"\n</code></pre> <p>Warning</p> <p>If at any point during this training exercise you log off from your IBM Concert VM, or the SSH connection breaks, you need to set your environment variables again with</p> <pre><code>source ./common_variables.sh\n</code></pre> <p>The best way to obtain concert-utils is by pulling them down from GitHub.  Open a terminal window in your workstation, and run the following command</p> <pre><code>source ./common_variables.sh\ntest -e ${OUTPUTDIR} || mkdir -p ${OUTPUTDIR}\ncp ./common_variables.sh ${OUTPUTDIR}\ncd ${OUTPUTDIR}\n</code></pre> <pre><code>git clone https://github.com/IBM/Concert.git concert-public\nmv concert-public/toolkit-enablement/concert-utils .\nrm -rf concert-public\n</code></pre> output <pre><code>Cloning into 'concert-public'...\nremote: Enumerating objects: 646, done.\nremote: Counting objects: 100% (179/179), done.\nremote: Compressing objects: 100% (133/133), done.\nremote: Total 646 (delta 53), reused 127 (delta 37), pack-reused 467 (from 2)\nReceiving objects: 100% (646/646), 6.91 MiB | 25.56 MiB/s, done.\nResolving deltas: 100% (199/199), done.\n</code></pre>"},{"location":"concert/concert-sbom/#codescan-sbom","title":"codescan SBOM","text":"<p>Warning</p> <p>Before completing this step head to https://github.ibm.com/settings/tokens/new and generate a personal access token. Make sure to tick the <code>repo    Full control of private repositories</code> checkbox.</p> <p>We first need to pull down the microservice application code from GitHub.  We will use the <code>qotd-web</code> sample component for this module. Open the <code>qotd-web</code> in your browser, click on the green <code>&lt;&gt; Code</code> dropdown button, click on the HTTP tab, and copy the repository reference.</p> <pre><code>git clone git@github.ibm.com:cs-tel-ibm-concert-qotd/qotd-web.git\ncd qotd-web\nexport REPO_COMMIT_SHA=$(git rev-parse HEAD) \ncd -\n</code></pre> <ol> <li>Get the latest repository commit hash</li> </ol> output <pre><code>\u279c  lab $ git clone git@github.ibm.com:cs-tel-ibm-concert-qotd/qotd-web.git\nCloning into 'qotd-web'...\nremote: Enumerating objects: 143, done.\nremote: Counting objects: 100% (49/49), done.\nremote: Compressing objects: 100% (24/24), done.\nremote: Total 143 (delta 42), reused 31 (delta 25), pack-reused 94\nReceiving objects: 100% (143/143), 855.99 KiB | 3.01 MiB/s, done.\nResolving deltas: 100% (69/69), done.\n\u279c  lab $ cd qotd-web\n\u279c  lab $ export REPO_COMMIT_SHA=$(git rev-parse HEAD)\n\u279c  lab $ cd ..\n</code></pre> using concert-utils <p>Run the <code>create-code-cyclondex-sbom.sh</code> helper script against the code directory just downloaded.  The helper script will call <code>cdxgen</code>, download all the associated packages needed to run your microservice, and scan them.  The script takes two parameter, <code>--outputfile &lt;filename&gt;</code> and <code>--cdxgen-args \"--spec-version 1.5\"</code>.</p> <pre><code>source ./common_variables.sh\n./concert-utils/helpers/create-code-cyclonedx-sbom.sh \\\n  --outputfile codescan-cyclonedx.json --cdxgen-args \"--spec-version 1.5\"\n</code></pre> output <pre><code>\u279c  lab $ source ./common_variables.sh\n\u279c  lab $ ./concert-utils/helpers/create-code-cyclonedx-sbom.sh \\\n  --outputfile codescan-cyclonedx.json --cdxgen-args \"--spec-version 1.5\"\npodman run --platform linux/amd64 -it --rm -u 0 -v ./qotd-web:/concert-sample -v /Users/ncolon/dev/ibm-concert/lab:/toolkit-data icr.io/cpopen/ibm-concert-toolkit:v1.0.3.1 bash -c code-scan --src /concert-sample --output-file codescan-cyclonedx.json --cdxgen-args \"--spec-version 1.5\"\n\nRunning command:\ncdxgen -o /toolkit-data/codescan-cyclonedx.json --spec-version 1.5\nSBOM has been generated successfully at /toolkit-data/codescan-cyclonedx.json\n\u279c  lab $ ls -la\ntotal 392\ndrwxr-xr-x   7 ncolon  staff     224 Aug 21 23:49 ./\ndrwxr-xr-x   5 ncolon  staff     160 Aug 13 12:00 ../\n-rw-r--r--   1 ncolon  staff      86 Aug 21 23:49 codescan-cyclonedx-metadata.json\n-rw-r--r--   1 ncolon  staff  189286 Aug 21 23:49 codescan-cyclonedx.json # (1)!\n-rwxr-xr-x   1 ncolon  staff    2207 Aug 21 23:49 common_variables.sh*\ndrwxr-xr-x   7 ncolon  staff     224 Aug 21 23:44 concert-utils/\ndrwxr-xr-x  20 ncolon  staff     640 Aug 21 23:49 qotd-web/\n\u279c  lab $\n</code></pre> <ol> <li>codescan CycloneDX file generated via concert-utils</li> </ol> using cdxgen <p>Run <code>cdxgen</code> inside the source code directory just downloaded.  cdxgen will download all the associated packages needed to run your microservice, and scan them.</p> <pre><code>pushd qotd-web\ncdxgen -r -o ../codescan-cyclonedx-sbom.json --validate --spec-version 1.5\npopd\n</code></pre> <pre><code>\u279c  lab $ pushd qotd-web\n\u279c  qotd-web git:(main) $ cdxgen -r -o ../codescan-cyclonedx-sbom.json --validate\nExecuting 'npm install' in .\n\u279c  qotd-web git:(main) $ popd\n\u279c  lab $ ls -al\ntotal 408\ndrwxr-xr-x   5 ncolon  staff     160 Jul 31 20:11 ./\ndrwxr-xr-x   8 ncolon  staff     256 Jul 31 20:01 ../\n-rw-r--r--   1 ncolon  staff  205564 Jul 31 20:09 codescan-cyclonedx-sbom.json # (1)!\ndrwxr-xr-x   8 ncolon  staff     256 Jul 31 19:51 concert-utils/\ndrwxr-xr-x  18 ncolon  staff     576 Jul 31 20:11 qotd-web/\n</code></pre> <ol> <li>CycloneDX codescan SBOM just generated</li> </ol>"},{"location":"concert/concert-sbom/#imagescan-sbom","title":"imagescan SBOM","text":"<p>We will use the <code>qotd-web</code> sample component for this module.</p> using concert-utils <p>Run the <code>create-image-cyclonedx-sbom.sh</code> helper script against the code directory just downloaded.  The helper script will scan the image defined in using <code>${IMAGE}:${IMAGE_TAG}</code> as the full image name.</p> <pre><code>source ./common_variables.sh\n./concert-utils/helpers/create-image-cyclonedx-sbom.sh\n</code></pre> output <pre><code>\u279c  lab $ source ./common_variables.sh\n\u279c  lab $ ./concert-utils/helpers/create-image-cyclonedx-sbom.sh\npodman run --platform linux/amd64 -it --rm -u 0 -v ./qotd-web:/concert-sample-src -v /Users/ncolon/dev/ibm-concert/lab:/toolkit-data icr.io/cpopen/ibm-concert-toolkit:v1.0.3.1 bash -c image-scan --images imgrepo.aws.ibm-gtmaa.dev/concert-apps/cs-tel-ibm-concert-qotd/qotd-web:latest\n\nScanning image: imgrepo.aws.ibm-gtmaa.dev/concert-apps/cs-tel-ibm-concert-qotd/qotd-web:latest # (1)!\n\u2714 Parsed image                                sha256:3eedb7a51bea6981ef2710ba4a806f98a4ee6eeade666a478e8ae45d89b62412\n\u2714 Cataloged contents                                 04977bf8b7a808957f1b2583bdd2e634ea8853363865fd1f13521115e07af0b4\n  \u251c\u2500\u2500 \u2714 Packages                        [674 packages]\n  \u251c\u2500\u2500 \u2714 File digests                    [64 files]\n  \u251c\u2500\u2500 \u2714 File metadata                   [64 locations]\n  \u2514\u2500\u2500 \u2714 Executables                     [33 executables]\nSBOM has been generated successfully at /toolkit-data/imgrepo.aws.ibm-gtmaa.dev_concert-apps_cs-tel-ibm-concert-qotd_qotd-web:latest-imagescan-cyclonedx-sbom.json\n\u279c  lab $ ls -la\ntotal 4752\ndrwxr-xr-x   9 ncolon  staff      288 Aug 21 23:51 ./\ndrwxr-xr-x   5 ncolon  staff      160 Aug 13 12:00 ../\n-rw-r--r--   1 ncolon  staff       86 Aug 21 23:49 codescan-cyclonedx-metadata.json\n-rw-r--r--   1 ncolon  staff   189286 Aug 21 23:49 codescan-cyclonedx.json\n-rwxr-xr-x   1 ncolon  staff     2207 Aug 21 23:49 common_variables.sh*\ndrwxr-xr-x   7 ncolon  staff      224 Aug 21 23:44 concert-utils/\n-rw-r--r--   1 ncolon  staff       86 Aug 21 23:52 imgrepo.aws.ibm-gtmaa.dev_concert-apps_cs-tel-ibm-concert-qotd_qotd-web:latest-imagescan-cyclonedx-sbom-metadata.json\n-rw-r--r--   1 ncolon  staff  1483824 Aug 21 23:52 imgrepo.aws.ibm-gtmaa.dev_concert-apps_cs-tel-ibm-concert-qotd_qotd-web:latest-imagescan-cyclonedx-sbom.json  # (2)!\ndrwxr-xr-x  20 ncolon  staff      640 Aug 21 23:49 qotd-web/\n\u279c  lab $\n</code></pre> <ol> <li>The image full name, from <code>${IMAGE_NAME}:${IMAGE_TAG}</code></li> <li>The filename generated by the concert-utils script.</li> </ol> using syft <p>To generate the image scan SBOM, we will use <code>syft</code>. </p><pre><code>syft scan registry:quay.io/hollisc/qotd-web:1.0.0 -o cyclonedx-json@1.5 &gt; imagescan-cyclonedx-sbom.json\n</code></pre><p></p> <p>As of 1.0.0 GA code, IBM Concert only understands the 1.5 version of the CycloneDX spec.  With 1.0.1 and later, the more common CycloneDX 1.6 format is available as well.  For now, we will pass the <code>-o cyclonedx-json@1.5</code> parameter to syft so that the output is generated following the CycloneDX 1.5 spec.</p> <pre><code>\u279c  lab $ syft scan registry:quay.io/hollisc/qotd-web:1.0.0 -o cyclonedx-json@1.5 &gt; imagescan-cyclonedx-sbom.json\n\u2714 Parsed image                          sha256:c8be3211c2f55692ec876486203925c6e490106577229a55bd91e2ec0934b0d7\n\u2714 Cataloged contents                           b98993977e263ecd5debf9936ac4c1624e66691a3e125d239718df65323bf589\n  \u251c\u2500\u2500 \u2714 Packages                        [671 packages]\n  \u251c\u2500\u2500 \u2714 File digests                    [63 files]\n  \u251c\u2500\u2500 \u2714 File metadata                   [63 locations]\n  \u2514\u2500\u2500 \u2714 Executables                     [21 executables]\n\u279c  lab $ ls -la\ntotal 2352\ndrwxr-xr-x   6 ncolon  staff     192 Jul 31 20:28 ./\ndrwxr-xr-x   8 ncolon  staff     256 Jul 31 20:01 ../\ndrwxr-xr-x   8 ncolon  staff     256 Jul 31 19:51 concert-utils/\n-rw-r--r--   1 ncolon  staff  993092 Jul 31 20:30 imagescan-cyclonedx-sbom.json # (1)!\ndrwxr-xr-x  14 ncolon  staff     448 Jul 31 20:25 qotd-web/\n\u279c  lab $\n</code></pre> <ol> <li>CycloneDX imagescan SBOM just generated with <code>syft</code></li> </ol>"},{"location":"concert/concert-sbom/#cve-scan","title":"CVE scan","text":"<p>To generate the CVE vulnerability scan CSV file, we will use <code>trivy</code> from Aquasec.  We will use the <code>qotd-web</code> sample component for this module. As of 1.0.1 GA code, IBM Concert can consume vulnerability scans in CycloneDX format.</p> using trivy <p>To generate the CVE file, use the following command:</p> <pre><code>source ./common_variables.sh\ntrivy image $IMAGE_NAME:$IMAGE_TAG --format cyclonedx --scanners vuln -o cve-scan.json\n</code></pre> output <pre><code>\u279c  lab $ source ./common_variables.sh\n\u279c  lab $ trivy image $IMAGE_NAME:$IMAGE_TAG --format cyclonedx --scanners vuln -o cve-scan.json\n2024-08-21T23:55:57-04:00   INFO    [vuln] Vulnerability scanning is enabled\n2024-08-21T23:55:57-04:00   INFO    Detected OS family=\"alpine\" version=\"3.11.10\"\n2024-08-21T23:55:57-04:00   INFO    [alpine] Detecting vulnerabilities...   os_version=\"3.11\" repository=\"3.11\" pkg_num=16\n2024-08-21T23:55:57-04:00   INFO    Number of language-specific files   num=1\n2024-08-21T23:55:57-04:00   INFO    [node-pkg] Detecting vulnerabilities...\n2024-08-21T23:55:57-04:00   WARN    This OS version is no longer supported by the distribution  family=\"alpine\" version=\"3.11.10\"\n2024-08-21T23:55:57-04:00   WARN    The vulnerability detection may be insufficient because security updates are not provided\n\u279c  lab $ ls -la\ntotal 6560\ndrwxr-xr-x  10 ncolon  staff      320 Aug 21 23:55 ./\ndrwxr-xr-x   5 ncolon  staff      160 Aug 13 12:00 ../\n-rw-r--r--   1 ncolon  staff       86 Aug 21 23:49 codescan-cyclonedx-metadata.json\n-rw-r--r--   1 ncolon  staff   189286 Aug 21 23:49 codescan-cyclonedx.json\n-rwxr-xr-x   1 ncolon  staff     2207 Aug 21 23:49 common_variables.sh*\ndrwxr-xr-x   7 ncolon  staff      224 Aug 21 23:44 concert-utils/\n-rw-r--r--   1 ncolon  staff   922179 Aug 21 23:55 cve-scan.json # (1)!\n-rw-r--r--   1 ncolon  staff       86 Aug 21 23:52 imgrepo.aws.ibm-gtmaa.dev_concert-apps_cs-tel-ibm-concert-qotd_qotd-web:latest-imagescan-cyclonedx-sbom-metadata.json\n-rw-r--r--   1 ncolon  staff  1483824 Aug 21 23:52 imgrepo.aws.ibm-gtmaa.dev_concert-apps_cs-tel-ibm-concert-qotd_qotd-web:latest-imagescan-cyclonedx-sbom.json\ndrwxr-xr-x  20 ncolon  staff      640 Aug 21 23:49 qotd-web/\n</code></pre> <ol> <li>CVE vulnerability file created just now.</li> </ol>"},{"location":"concert/concert-sbom/#application-sbom","title":"application SBOM","text":"<p>To generate an application SBOM, we use the <code>create-application-sbom.sh</code> helper script.  The script requires a config file that we can generate dynamically from variables in <code>common_variables.sh</code>.  You can view a more comprehensive application config file in the IBM Concert Toolkit sample tempalte.</p> using concert-utils <pre><code>source ./common_variables.sh\ncat &lt;&lt; EOF | envsubst &gt; app-config.yaml\nspec_version: \"1.0.2\"\nconcert:\n  application:\n    output_file: \"${APP_NAME}-${COMPONENT_NAME}-app-definition-sbom.json\"\n    app_name: \"${APP_NAME}\"\n    version: \"${APP_VERSION}\"\n    business:\n      name: \"${BUSINESS_NAME}\"\n      units:\n      - name: \"${BUSINESS_UNIT}\"\n        email: \"${BUSINESS_EMAIL}\"\n        phone: \"${BUSINESS_PHONE}\"\n    properties:\n      application_criticality: \"${APP_CRITICALITY}\"\n    components:\n    - component_name: \"${COMPONENT_NAME}\"\n      version: \"${APP_VERSION}\"\n      repositories:\n      - name: \"${REPO_NAME}\"\n        url: \"${REPO_URL}\"\n      image:\n        name: \"${IMAGE_NAME}\"\n    environment_targets:\n    - name: \"${ENV_TARGET}\"\n    services:\n    - name: \"${ACCESS_POINT_NAME}\"\n      type: \"app_end_point\"\n      endpoints:\n      - \"${APP_ENDPOINT}\"\n      properties:\n        network_exposure: \"${APP_ENDPOINT_EXPOSURE}\"\n      reliant_by:\n      - \"${COMPONENT_NAME}\"\nEOF\n./concert-utils/helpers/create-application-sbom.sh --outputdir ${OUTPUTDIR} --configfile app-config.yaml\n</code></pre> output <pre><code>\u279c  lab $ source ./common_variables.sh\n\u279c  lab $ ./concert-utils/helpers/create-application-sbom.sh --outputdir ${OUTPUTDIR} --configfile app-config.yaml\npodman run--platform linux/amd64 -it --rm -u 0 -v /Users/ncolon/dev/ibm-concert/lab:/toolkit-data icr.io/cpopen/ibm-concert-toolkit:v1.0.3.1 bash -c app-sbom --app-config /toolkit-data/app-config.yaml\n\n2024-08-22 03:58:02,619 - INFO - Read configuration file: /toolkit-data/app-config.yaml\n2024-08-22 03:58:02,625 - INFO - Rendered template variables\n2024-08-22 03:58:02,626 - INFO - Added additional data to BOM file\n2024-08-22 03:58:02,631 - INFO - Processed application \"qotd-app\" to BOM file: /toolkit-data/qotd-app-qotd-web-app-definition-sbom.json\n\u279c  lab $ ls -la\ntotal 6576\ndrwxr-xr-x  12 ncolon  staff      384 Aug 21 23:57 ./\ndrwxr-xr-x   5 ncolon  staff      160 Aug 13 12:00 ../\n-rw-r--r--   1 ncolon  staff      826 Aug 21 23:57 app-config.yaml # (1)!\n-rw-r--r--   1 ncolon  staff       86 Aug 21 23:49 codescan-cyclonedx-metadata.json\n-rw-r--r--   1 ncolon  staff   189286 Aug 21 23:49 codescan-cyclonedx.json\n-rwxr-xr-x   1 ncolon  staff     2207 Aug 21 23:49 common_variables.sh*\ndrwxr-xr-x   7 ncolon  staff      224 Aug 21 23:44 concert-utils/\n-rw-r--r--   1 ncolon  staff   922179 Aug 21 23:55 cve-scan.json\n-rw-r--r--   1 ncolon  staff       86 Aug 21 23:52 imgrepo.aws.ibm-gtmaa.dev_concert-apps_cs-tel-ibm-concert-qotd_qotd-web:latest-imagescan-cyclonedx-sbom-metadata.json\n-rw-r--r--   1 ncolon  staff  1483824 Aug 21 23:52 imgrepo.aws.ibm-gtmaa.dev_concert-apps_cs-tel-ibm-concert-qotd_qotd-web:latest-imagescan-cyclonedx-sbom.json\n-rw-r--r--   1 ncolon  staff     2096 Aug 21 23:58 qotd-app-qotd-web-app-definition-sbom.json # (2)!\ndrwxr-xr-x  20 ncolon  staff      640 Aug 21 23:49 qotd-web/\n\u279c  lab $\n</code></pre> <ol> <li>Application SBOM configuration file</li> <li>Concert Application SBOM generated by the concert-utils script.</li> </ol>"},{"location":"concert/concert-sbom/#build-sbom","title":"Build SBOM","text":"<p>To generate a build SBOM, we use the <code>create-build-sbom.sh</code> helper script. The script requires a config file that we can generate dynamically from variables in <code>common_variables.sh</code>. You can view a more comprehensive application config file in the IBM Concert Toolkit sample template.</p> using concert-utils <pre><code>source ./common_variables.sh\nIMAGESCAN_FILE=$(echo ${IMAGE_NAME}|tr \"/\" \"_\"):${IMAGE_TAG}-imagescan-cyclonedx-sbom.json\nCODESCAN_URN=$(jq .serialNumber codescan-cyclonedx.json)\nIMAGESCAN_URN=$(jq .serialNumber ${IMAGESCAN_FILE})\n\ncat &lt;&lt; EOF | envsubst &gt; build-config.yaml\nspec_version: \"1.0.2\"\nconcert:\n  builds:\n  - component_name: \"${COMPONENT_NAME}\"\n    output_file: \"${APP_NAME}-${COMPONENT_NAME}-build-sbom.json\"\n    app_name: \"${APP_NAME}\"\n    number: 1\n    version: \"${APP_VERSION}\"\n    repositories:\n    - name: \"${REPO_NAME}\"\n      url: \"${REPO_URL}\"\n      branch: \"${REPO_BRANCH}\"\n      commit_sha: \"${REPO_COMMIT_SHA}\"\n      cyclonedx_bom_link:\n        file: \"codescan-cyclonedx.json\"\n        data:\n          serial_number: ${CODESCAN_URN}\n          version: \"1\"\n    image:\n      name: \"${IMAGE_NAME}\"\n      tag: \"${IMAGE_TAG}\"\n      digest: \"${IMAGE_DIGEST}\"\n      cyclonedx_bom_link:\n        file: \"${IMAGESCAN_FILE}\"\n        data:\n          serial_number: ${IMAGESCAN_URN}\n          version: \"1\"\nEOF\n./concert-utils/helpers/create-build-sbom.sh --outputdir ${OUTPUTDIR} --configfile build-config.yaml\n</code></pre> output <pre><code>\u279c  lab $ source ./common_variables.sh\n\u279c  lab $ ./concert-utils/helpers/create-build-sbom.sh --outputdir ${OUTPUTDIR} --configfile build-config.yaml\npodman run run --platform linux/amd64 -it --rm -u 0 -v /Users/ncolon/dev/ibm-concert/lab:/toolkit-data icr.io/cpopen/ibm-concert-toolkit:v1.0.3.1 bash -c build-sbom --build-config /toolkit-data/build-config.yaml\n\n2024-08-22 04:00:38,189 - INFO - Read configuration file: /toolkit-data/build-config.yaml\n2024-08-22 04:00:38,196 - INFO - Extracted values for component: qotd-web\n2024-08-22 04:00:38,197 - INFO - Rendered template variables\n2024-08-22 04:00:38,199 - INFO - Processed component \"qotd-web\" to BOM file: /toolkit-data/qotd-app-qotd-web-build-sbom.json\n\u279c  lab $ ls -al\ntotal 6592\ndrwxr-xr-x  14 ncolon  staff      448 Aug 22 00:00 ./\ndrwxr-xr-x   5 ncolon  staff      160 Aug 13 12:00 ../\n-rw-r--r--   1 ncolon  staff      826 Aug 21 23:57 app-config.yaml\n-rw-r--r--   1 ncolon  staff      558 Aug 22 00:00 build-config.yaml # (1)!\n-rw-r--r--   1 ncolon  staff       86 Aug 21 23:49 codescan-cyclonedx-metadata.json\n-rw-r--r--   1 ncolon  staff   189286 Aug 21 23:49 codescan-cyclonedx.json\n-rwxr-xr-x   1 ncolon  staff     2207 Aug 21 23:49 common_variables.sh*\ndrwxr-xr-x   7 ncolon  staff      224 Aug 21 23:44 concert-utils/\n-rw-r--r--   1 ncolon  staff   922179 Aug 21 23:55 cve-scan.json\n-rw-r--r--   1 ncolon  staff       86 Aug 21 23:52 imgrepo.aws.ibm-gtmaa.dev_concert-apps_cs-tel-ibm-concert-qotd_qotd-web:latest-imagescan-cyclonedx-sbom-metadata.json\n-rw-r--r--   1 ncolon  staff  1483824 Aug 21 23:52 imgrepo.aws.ibm-gtmaa.dev_concert-apps_cs-tel-ibm-concert-qotd_qotd-web:latest-imagescan-cyclonedx-sbom.json\n-rw-r--r--   1 ncolon  staff     2096 Aug 21 23:58 qotd-app-qotd-web-app-definition-sbom.json\n-rw-r--r--   1 ncolon  staff     1956 Aug 22 00:00 qotd-app-qotd-web-build-sbom.json # (2)!\ndrwxr-xr-x  20 ncolon  staff      640 Aug 21 23:49 qotd-web/\n\u279c  lab $\n</code></pre> <ol> <li>Build SBOM configuration file</li> <li>Concert Build SBOM generated by the concert-utils script.</li> </ol>"},{"location":"concert/concert-sbom/#deploy-sbom","title":"Deploy SBOM","text":"<p>To generate a deploy SBOM, we use the <code>create-deploy-sbom.sh</code> helper script. The script requires a config file that we can generate dynamically from variables in <code>common_variables.sh</code>. You can view a more comprehensive application config file in the IBM Concert Toolkit sample template.</p> using concert-utils <pre><code>source ./common_variables.sh\ncat &lt;&lt; EOF | envsubst &gt; deploy-config.yaml\nspec_version: \"1.0.2\"\nconcert:\n  deployments:\n  - output_file: \"${APP_NAME}-${COMPONENT_NAME}-deploy-sbom.json\"\n    metadata:\n      component_name: \"${COMPONENT_NAME}\"\n      number: \"1\"\n      version: \"${APP_VERSION}\"\n    environment_target: \"${ENV_TARGET}\"\n    repositories:\n    - name: \"${REPO_NAME}\"\n      url: \"${REPO_URL}\"\n      branch: \"${REPO_BRANCH}\"\n      commit_sha: \"${REPO_COMMIT_SHA}\"\n    services:\n    - name: \"${ACCESS_POINT_NAME}\"\n      type: \"app_access_point\"\n      properties:\n        base_url: \"${APP_URL}\"\n    runtime:\n    - name: \"${K8S_NAME}\"\n      type: \"kubernetes\"\n      depends_on:\n      - \"${COMPONENT_NAME}\"\n      properties:\n        platform: \"${K8S_PLATFORM}\"\n        cluster_platform: \"${K8S_ENV_PLATFORM}\"\n        cluster_id: \"${K8S_CLUSTER_ID}\"\n        cluster_region: \"${K8S_CLUSTER_REGION}\"\n        cluster_name: \"${K8S_CLUSTER_NAME}\"\n      api-server: ${K8S_APISERVER}\n      namespaces:\n      - name: \"${K8S_NAMESPACE}\"\n        images:\n        - name: \"${IMAGE_NAME}\"\n          tag: \"${IMAGE_TAG}\"\n          digest: \"${IMAGE_DIGEST}\"\nEOF\n./concert-utils/helpers/create-deploy-sbom.sh --outputdir ${OUTPUTDIR} --configfile deploy-config.yaml\n</code></pre> output <pre><code>\u279c  lab $ source ./common_variables.sh\n\u279c  lab $ ./concert-utils/helpers/create-deploy-sbom.sh --outputdir ${OUTPUTDIR} --configfile deploy-config.yaml\npodman run --platform linux/amd64 -it --rm -u 0 -v /Users/ncolon/dev/ibm-concert/lab:/toolkit-data icr.io/cpopen/ibm-concert-toolkit:v1.0.3.1 bash -c deploy-sbom --deploy-config /toolkit-data/deploy-config.yaml\n\n2024-08-22 04:03:40,807 - INFO - Read configuration file: /toolkit-data/deploy-config.yaml\n2024-08-22 04:03:40,812 - INFO - Extracted values for component: qotd-web\n2024-08-22 04:03:40,813 - INFO - Rendered template variables\n2024-08-22 04:03:40,813 - INFO - Added additional data for component: qotd-web\n2024-08-22 04:03:40,814 - INFO - Processed component \"qotd-web\" to BOM file: /toolkit-data/qotd-app-qotd-web-deploy-sbom.json\n\u279c  lab $ ls -la\ntotal 6608\ndrwxr-xr-x  16 ncolon  staff      512 Aug 22 00:03 ./\ndrwxr-xr-x   5 ncolon  staff      160 Aug 13 12:00 ../\n-rw-r--r--   1 ncolon  staff      826 Aug 21 23:57 app-config.yaml\n-rw-r--r--   1 ncolon  staff      558 Aug 22 00:00 build-config.yaml\n-rw-r--r--   1 ncolon  staff       86 Aug 21 23:49 codescan-cyclonedx-metadata.json\n-rw-r--r--   1 ncolon  staff   189286 Aug 21 23:49 codescan-cyclonedx.json\n-rwxr-xr-x   1 ncolon  staff     2207 Aug 21 23:49 common_variables.sh*\ndrwxr-xr-x   7 ncolon  staff      224 Aug 21 23:44 concert-utils/\n-rw-r--r--   1 ncolon  staff   922179 Aug 21 23:55 cve-scan.json\n-rw-r--r--   1 ncolon  staff      918 Aug 22 00:03 deploy-config.yaml # (1)!\n-rw-r--r--   1 ncolon  staff       86 Aug 21 23:52 imgrepo.aws.ibm-gtmaa.dev_concert-apps_cs-tel-ibm-concert-qotd_qotd-web:latest-imagescan-cyclonedx-sbom-metadata.json\n-rw-r--r--   1 ncolon  staff  1483824 Aug 21 23:52 imgrepo.aws.ibm-gtmaa.dev_concert-apps_cs-tel-ibm-concert-qotd_qotd-web:latest-imagescan-cyclonedx-sbom.json\n-rw-r--r--   1 ncolon  staff     2096 Aug 21 23:58 qotd-app-qotd-web-app-definition-sbom.json\n-rw-r--r--   1 ncolon  staff     1956 Aug 22 00:00 qotd-app-qotd-web-build-sbom.json\n-rw-r--r--   1 ncolon  staff     2927 Aug 22 00:03 qotd-app-qotd-web-deploy-sbom.json # (2)!\ndrwxr-xr-x  20 ncolon  staff      640 Aug 21 23:49 qotd-web/\n</code></pre> <ol> <li>Deploy SBOM configuration file</li> <li>Concert Deploy SBOM generated by the concert-utils script.</li> </ol>"},{"location":"concert/concert-sbom/#upload-sboms","title":"Upload SBOMs","text":"using concert-utils <pre><code>source ./common_variables.sh\ncat &lt;&lt; EOF | envsubst &gt; config.yaml\ngeneral:\n  retries: 3\n  timeout: 30\n  verbose: true\n  insecure: true\napp:\n  base_url: \"${BASE_URL}\"\n  instance_id: \"0000-0000-0000-0000\"\nEOF\n\nif [[ -z ${TOKEN} ]]; then # (1)!\n  cat &lt;&lt; EOF | envsubst &gt;&gt; config.yaml\n  auth:\n    user:\n      username: ${CONCERT_USERNAME}\n      password: ${CONCERT_PASSWORD}\nEOF\nelse\n  cat &lt;&lt; EOF | envsubst &gt;&gt; config.yaml\n  auth:\n    token:\n      token: ${TOKEN}\nEOF\nfi\n\ncat &lt;&lt; EOF | envsubst &gt;&gt; config.yaml\ninput-data:\n  application-def:\n    - file: \"/toolkit-data/${APP_NAME}-${COMPONENT_NAME}-app-definition-sbom.json\"\n  build:\n    - file: \"/toolkit-data/${APP_NAME}-${COMPONENT_NAME}-build-sbom.json\"\n  deploy:\n    - file: \"/toolkit-data/${APP_NAME}-${COMPONENT_NAME}-deploy-sbom.json\"\n  cyclonedx:\n    - file: \"/toolkit-data/codescan-cyclonedx.json\"\n    - file: \"/toolkit-data/$(echo ${IMAGE_NAME}|tr \"/\" \"_\"):${IMAGE_TAG}-imagescan-cyclonedx-sbom.json\"\n  image-scan:\n    - file: \"/toolkit-data/cve-scan.json\"\n      metadata:\n        scanner_name: \"trivy\"\nEOF\n./concert-utils/helpers/concert_upload.sh  --outputdir ${OUTPUTDIR}\n</code></pre> <ol> <li>If the TOKEN variable is set in common_variables.sh, use token authentication, otherwise use username and password.</li> </ol> output <pre><code>\u279c  lab $ source ./common_variables.sh\n\u279c  lab $ ./concert-utils/helpers/concert_upload.sh  --outputdir ${OUTPUTDIR}\npodman run --platform linux/amd64 -it --rm -u 0 -v /Users/ncolon/dev/ibm-concert/lab:/toolkit-data icr.io/cpopen/ibm-concert-toolkit:v1.0.3.1 bash -c upload-concert --upload-config /toolkit-data/config.yaml\n\n2024-08-22 04:05:50,482 - INFO - Read configuration file: /toolkit-data/config.yaml\n2024-08-22 04:05:51,292 - INFO - Upload response for /toolkit-data/qotd-app-qotd-web-build-sbom.json: {'record_paths': ['0000-0000-0000-0000/roja/application_sbom/2024/8/22/51372574_qotd-app-qotd-web-build-sbom.json'], 'job_data': {'message': '200 OK'}}\n2024-08-22 04:05:51,846 - INFO - Upload response for /toolkit-data/qotd-app-qotd-web-deploy-sbom.json: {'record_paths': ['0000-0000-0000-0000/roja/application_sbom/2024/8/22/621108670_qotd-app-qotd-web-deploy-sbom.json'], 'job_data': {'message': '200 OK'}}\n2024-08-22 04:05:52,342 - INFO - Upload response for /toolkit-data/qotd-app-qotd-web-app-definition-sbom.json: {'record_paths': ['0000-0000-0000-0000/roja/application_sbom/2024/8/22/119899727_qotd-app-qotd-web-app-definition-sbom.json'], 'job_data': {'message': '200 OK'}}\n2024-08-22 04:05:53,133 - INFO - Upload response for /toolkit-data/codescan-cyclonedx.json: {'record_paths': ['0000-0000-0000-0000/roja/package_sbom/2024/8/22/896230519_codescan-cyclonedx.json'], 'job_data': {'message': '200 OK'}}\n2024-08-22 04:05:55,638 - INFO - Upload response for /toolkit-data/imgrepo.aws.ibm-gtmaa.dev_concert-apps_cs-tel-ibm-concert-qotd_qotd-web:latest-imagescan-cyclonedx-sbom.json: {'record_paths': ['0000-0000-0000-0000/roja/package_sbom/2024/8/22/352668393_imgrepo.aws.ibm-gtmaa.dev_concert-apps_cs-tel-ibm-concert-qotd_qotd-web:latest-imagescan-cyclonedx-sbom.json'], 'job_data': {'message': '200 OK'}}\n2024-08-22 04:06:04,257 - INFO - Upload response for /toolkit-data/cve-scan.json: {'record_paths': ['0000-0000-0000-0000/roja/image_scan/2024/8/22/993433284_cve-scan.json'], 'job_data': {'message': '200 OK'}}\n\u279c  lab $\n</code></pre>"},{"location":"concert/concert-sbom/#viewing-application-details-on-concert","title":"Viewing Application Details on Concert","text":"<p>Now we have uploaded SBOM data to IBM Concert, let's take a look at what insights into the application it gives us.</p> <p>From the Concert Homepage, we can see an overview of the application vulnerabilities:</p> <p></p> <p>Concert will break down Software Composition into outdated dependencies (Versions Behind Recommended), dependencies with vulnerabilities (Versions with Vulnerabilities), denied licenses and unknown versions.</p> <p>Click on Versions Behind Recommended. Concert will list:</p> <ul> <li>All of the dependencies installed that need updating</li> <li>The installed version in the application</li> <li>The latest version published for the dependency</li> <li>A reliability Score. A high level score based on the OpenSSF Scorecard</li> </ul> <p></p> <p>Back on the Concert Home page, select <code>Vulnerability</code> to reveal identified CVEs:</p> <p></p> <p>Select <code>Total identified CVEs</code>. Concert will show identified CVEs in your application, their priority, a Risk Score Generated by Concert, and a Common Vulnerability Scoring System (CVSS) score:</p> <p></p> <p>Congratulations</p> <p>You have successfully completed the Concert SBOM Lab!</p>"},{"location":"concert/concert-sbom/#acknowledgments","title":"Acknowledgments","text":"<p>This lab was based on: https://pages.github.ibm.com/cs-tel-ibm-concert/training/module2/manual-build/</p>"},{"location":"concert/concert-software-comp/","title":"Managing Software Composition and CVEs","text":""},{"location":"concert/concert-software-comp/#managing-software-composition-and-cves","title":"Managing Software Composition and CVEs","text":""},{"location":"concert/concert-software-comp/#objective","title":"Objective","text":"<p>In order to use the Software Composition dimension, an organization needs to ingest its application description into Concert using a Software Bill of Materials (SBOM). The Software Composition dimension helps to identify and mitigate risks associated with packages and components based on several key indicators of reliability, maintainability, and security.  </p> <p>IBM\u00ae Concert fetches a set of reliability checks and generates an aggregate score measuring the reliability of each package. Based on its assessment, Concert recommends specific actions to address risks, such as those related to outdated or unsupported packages, license compliance issues, and exploitable vulnerabilities.</p> <p>In this lab, you will learn how to ingest application data from micro-services development in IBM Concert. Then you will see the result and the related functionalities of IBM Concert concerning this data.</p>"},{"location":"concert/concert-software-comp/#prerequisites","title":"Prerequisites","text":"<ul> <li> IBM Concert is installed</li> <li> watsonx.ai is integrated with Concert</li> </ul>"},{"location":"concert/concert-software-comp/#data-ingestion","title":"Data Ingestion","text":"<p>Data ingestion in IBM Concert is done using a SBOM (Software Bill of Materials). IBM Concert provides a Concert Toolkit docker image to help in generating SBOMs based on configuration yaml files and scanning products (Syft, CycloneDX, Trivy, etc ....). For more information, the documentation is here</p> <p>We will use this toolkit to generate the SBOMs for a very simple, provided python application.</p>"},{"location":"concert/concert-software-comp/#verify-that-you-have-a-concert-api-key-defined-if-not-create-it","title":"Verify that you have a Concert API Key defined, if not create it","text":"<ol> <li>From a browser go to the IBM Concert URL</li> <li>Login to Concert using your Concert credentials</li> <li> <p>Click the circle at top right of the window and select API Key</p> <p></p> </li> <li> <p>In the API Key window, click Generate API Key</p> <p></p> </li> <li> <p>Copy the API key generated and paste somewhere safe, we will use it to upload SBOM files into IBM Concert</p> </li> </ol>"},{"location":"concert/concert-software-comp/#a-manual-ingestion","title":"a - Manual Ingestion","text":"<p>In the following steps, you are going to clone three GitHub repositories:</p> <ul> <li>The hr-app repository that contain the code of a backend application called hr-app. It is a python script that calls the summarization-svc.</li> <li>The summarization-svc repository that contains the code for a summarization service. It is a python script that does a LLM inference.</li> <li>The SBOMs-ingestion repository that contains a script to generate SBOMs using the Concert Toolkit, it has template files and the required environment variables.</li> </ul> <p>Here is the structure that you will have after the clone of the repositories:</p> <p></p> <p>In the folder SBOMs-ingestion you have:</p> <ol> <li> <p>scripts/generate-sboms.sh: </p> <p>A shell script that take as parameter the component for which we want to generate SBOMs. This script will:</p> <ul> <li>Load the common variables and specific application variables</li> <li>Use the templates of the templates directory to generate the config files used by concert toolkit to generate the application, build and deploy SBOMs.</li> <li>Generate the code scan SBOM with concert toolkit. Concert toolkit use Cyclone DX.</li> <li>Generate the image scan SBOM with concert toolkit. Concert toolkit use Syft.</li> <li>Generate the image CVEs using Trivy</li> </ul> </li> <li> <p>scripts/upload.sh: </p> <p>A shell script that take as parameter the application for which we want to upload the SBOMs generated by generate-sboms.sh. This script use Concert APIs to upload SBOMs in IBM Concert</p> </li> <li> <p>app-common-variables.variables</p> <p>The environment variables used by the scripts whatever the component is.</p> </li> <li> <p>hr-app.variables</p> <p>The environment variables with values specific to hr-app component.</p> </li> <li> <p>summarization-svc.variables</p> <p>The environment variables with values specific to summarization-svc component.</p> <p>In the following steps, you will just clone the repositories and run the script. But in lab5, you will manually generate a certificate SBOM for your application and upload it in IBM Concert. So, you will have the opportunity to manipulate a template file, environment variables, the concert toolkit and IBM Concert Apis.</p> </li> </ol> <p>For this lab, follow these steps to ingest our application data in Concert:</p>"},{"location":"concert/concert-software-comp/#clone-and-build-the-micro-services","title":"Clone and Build the Micro-Services","text":"<p>You will start to clone the 2 micro-services composing the application.</p> <ol> <li> <p>Connect on the machine you have provisioned on Techzone in Lab0</p> <pre><code>ssh itzuser@&lt;VM ip address&gt; -p 2223 -i /path/to/concert/sshkey/pem_ibmcloudvsi_download.pem\n</code></pre> </li> <li> <p>Create a folder named concert-bootcamp</p> <pre><code>umask 022\nmkdir -p $HOME/concert-bootcamp/Applications\n</code></pre> </li> <li> <p>Clone and build the hr-app component</p> <p>NOTE: <code>git clone</code> command will ask you a for username and password. Enter here your github.ibm email and the token you created during the install</p> <pre><code>cd $HOME/concert-bootcamp/Applications\ngit clone https://github.ibm.com/concert-bootcamp/hr-app.git\ncd hr-app\n./buildImage.sh\n</code></pre> </li> <li> <p>Clone and build summarization-svc component</p> <pre><code>cd $HOME/concert-bootcamp/Applications\ngit clone https://github.ibm.com/concert-bootcamp/summarization-svc.git\ncd summarization-svc\n./buildImage.sh\n</code></pre> </li> </ol>"},{"location":"concert/concert-software-comp/#clone-the-sbom-ingestion-scripts","title":"Clone the SBOM ingestion scripts","text":"<ol> <li> <p>Then you will clone a project containing a directory structure and a script to generate SBOMs using the Concert Toolkit and upload them in concert using IBM Concert APIs.</p> <pre><code>cd $HOME/concert-bootcamp\ngit clone https://github.ibm.com/concert-bootcamp/SBOMs-ingestion.git\n</code></pre> </li> <li> <p>Create an output directory for the generated SBOMs</p> <pre><code>mkdir $HOME/concert-bootcamp/SBOMs-ingestion/concert_data\nchmod 777 $HOME/concert-bootcamp/SBOMs-ingestion/concert_data\n</code></pre> </li> <li> </li> </ol>"},{"location":"concert/concert-software-comp/#execute-the-sbom-generation-script","title":"Execute the SBOM generation script","text":"<pre><code>cd $HOME/concert-bootcamp/SBOMs-ingestion/scripts\n./generate-sboms.sh hr-app\n./generate-sboms.sh summarization-svc\n</code></pre> <p>Here you can take a look at the SBOMs generated in folder $HOME/concert-bootcamp/SBOMs-ingestion/concert_data.   One folder as been created for each component (one for hr-app and one for summarization-svc)</p>"},{"location":"concert/concert-software-comp/#upload-sboms-on-ibm-concert","title":"Upload SBOMs on IBM concert","text":"<ol> <li> <p>Update app-common-variables.variables file to specify Concert variables:</p> <pre><code>cd $HOME/concert-bootcamp/SBOMs-ingestion\nvi app-common-variables.variables\n</code></pre> <p>Put your concert values for:</p> <ul> <li>CONCERT_HOST (just the server IP address, in the form aaa.bbb.ccc.ddd)</li> <li>CONCERT_PORT (12443)</li> <li>CONCERT_APIKEY</li> </ul> <p>Save the file (:wq)</p> </li> <li> <p>Run the upload script</p> <pre><code>cd $HOME/concert-bootcamp/SBOMs-ingestion/scripts\n./upload.sh hr-app\n./upload.sh summarization-svc\n</code></pre> </li> <li> <p>Log on concert and look at the Arena view</p> </li> </ol>"},{"location":"concert/concert-software-comp/#b-jenkins-pipeline","title":"b - Jenkins Pipeline","text":"<p>Manual data ingestion in Concert is quite painful. So, the best way to ingest these data and keep concert up to date during the application lifecycle is to generate and ingest SBOMs during the CI/CD process.   </p> <p>In each component code of this lab (hr-app and summarization-svc) you will find a Jenkins file as a pipeline example.  Any other CI/CD tools could be use (teckton, ...)</p> <p>Jenkins demo done by the instructor.</p>"},{"location":"concert/concert-software-comp/#managing-software-composition","title":"Managing Software Composition","text":"<p>Walkthrough the uploaded data:</p> <ul> <li>Home page - Software composition dimension</li> <li>Arena view</li> <li>Application Inventory</li> </ul>"},{"location":"concert/concert-software-comp/#managing-cves","title":"Managing CVEs","text":"<p>Walkthrough the uploaded CVEs:</p> <ul> <li>Home page - Vulnerability dimension</li> <li>Vulnerability Dimension</li> <li>Risk score vs CVSS score</li> <li>Select a CVE</li> <li>Recommended mitigation strategy generated by watsonx</li> <li>Ask watsonx</li> <li>Blast radius</li> <li>Expand and open ticket</li> </ul>"},{"location":"concert/concert-software-comp/#create-a-github-connection-for-ticketing","title":"Create a github connection for ticketing","text":"<p>In many places, you have the possibility to open tickets. IBM Concert allow to open ticket on Github, Jira, ServiceNow or Salesforce.   Before beeing able to open tickets on github, a github connection must be created.  Here are the steps to follow:</p> <ol> <li>Navigate to Administration-&gt;Integrations</li> <li>Select Connections tab</li> <li>Click Create connection button</li> <li>Search for github</li> </ol> <p></p> <ol> <li> <p>Select the card and enter following value:</p> <ul> <li>Name: Github.ibm connection</li> <li>Host: https://github.ibm.com/api/v3</li> <li>Personal access token: your personal access token you created in step III of Lab0</li> <li>Click Validate connection</li> </ul> <p></p> <ul> <li>If Validation status is success, click Create button</li> </ul> </li> </ol>"},{"location":"concert/concert-software-comp/#open-a-ticket-for-a-cve","title":"Open a ticket for a CVE","text":"<p>Here we are going to open a ticket for a CVEs on our hr-app component.</p> <ol> <li>Navigate to Arena View</li> <li>Select a CVE</li> </ol> <p></p> <ol> <li>Click the red circle</li> <li>Take a look at the Blast radius and go back to Overview tab</li> <li>Expand an Open findings entry </li> <li>Click Open ticket+</li> </ol> <p></p> <ol> <li> <p>In the Open Ticket window, select your connection and enter following values:</p> <ul> <li>Organization: concert-bootcamp</li> <li>Repository: hr-app</li> </ul> </li> <li> <p>Click Open</p> </li> </ol> <p></p> <ol> <li>On the CVE page that is displayed now, select your ticket to see it on github</li> </ol> <p> </p>"},{"location":"concert/concert-walkthrough/","title":"Concert Walkthrough","text":""},{"location":"concert/concert-walkthrough/#concert-walkthrough","title":"Concert Walkthrough","text":""},{"location":"concert/concert-walkthrough/#objective","title":"Objective","text":"<p>In this lab, we will load demo data into Concert and walk through IBM Concert functionalities. At the end, we will clean up the demo data in order to have an empty instance to continue the sequence of labs.</p>"},{"location":"concert/concert-walkthrough/#prerequisites","title":"Prerequisites","text":"<ul> <li> IBM Concert must be installed</li> <li> IBM Concert must be integrated with watsonx.ai </li> </ul>"},{"location":"concert/concert-walkthrough/#login-to-concert-and-load-demo-data","title":"Login to Concert and load demo data","text":"<ol> <li>From a browser, enter the URL of your Concert instance (https://YOUR_VM_IP:12443) and login with your Concert username and password.</li> <li>If it is the first time you are logging in, you are invited to upload sample data on the right of the window. </li> <li> <p>Otherwise, you can upload sample data by clicking the question mark at the top right of the window.</p> <p></p> </li> <li> <p>In order to have a first look at IBM Concert functionalities, click either Sample data or \"Load sample data** depending if you are in case 2 or 3.</p> </li> </ol>"},{"location":"concert/concert-walkthrough/#walkthrough","title":"Walkthrough","text":"<p>With sample data uploaded into IBM Concert, you can navigate to the Home page, the Arena view, and the Dimensions page and have a first glance at IBM Concert's capabilities</p>"},{"location":"concert/concert-walkthrough/#reset-concert-data","title":"Reset Concert data","text":"<ul> <li>Navigate to Administration-&gt;Settings</li> <li>Select the Miscellaneous tab</li> <li>Click the Delete sample data button</li> </ul>"},{"location":"concert/concert-workflow-install/","title":"Concert Workflows Installation","text":""},{"location":"concert/concert-workflow-install/#concert-workflows-installation","title":"Concert Workflows Installation","text":""},{"location":"concert/concert-workflow-install/#objective","title":"Objective","text":"<p>Concert Workflows is an embedded version of Rapid Infrastructure Automation in IBM Concert and is available as an add-on workflow automation service for on-premises deployments of Concert. </p> <p>The add-on embeds workflow definition and automation capabilities so you can define, manage, and automate workflows within the Concert UI.</p> <p>The objective is to get data from an organization's environments and applications using low-code flows.  </p> <p>By default, a flow is executed in a worker located on the IBM Concert host. A notion of remote workers exist in Concert Workflow to enable the ingestion of data into IBM Concert from environments that cannot be reached directly by the IBM Concert host. The remote worker is located near the environment, collects the required data, and ingests it into IBM Concert using Concert's APIs.</p> <p>In this lab, you will install IBM Concert Workflows as an add-on to your Concert installation.</p>"},{"location":"concert/concert-workflow-install/#prerequisite","title":"Prerequisite","text":"<ul> <li> IBM Concert must be installed</li> </ul>"},{"location":"concert/concert-workflow-install/#install-ibm-concert-workflows-vm","title":"Install IBM Concert Workflows (VM)","text":"<p>Concert Workflows installation requires k3s and helm.</p>"},{"location":"concert/concert-workflow-install/#install-prerequisites","title":"Install Prerequisites","text":"<ol> <li>Connect to the VM you reserved on TechZone</li> </ol> <pre><code>ssh itzuser@&lt;VM ip address&gt; -p 2223 -i /path/to/concert/sshkey/pem_ibmcloudvsi_download.pem\n</code></pre> <ol> <li>install k3s</li> </ol> <p>Offical documentation k3s installation</p> <pre><code>cd $HOME\ncurl -sfL https://get.k3s.io | sudo INSTALL_K3S_VERSION=v1.29.2+k3s1 sh -s - --write-kubeconfig-mode 644 --disable traefik\n</code></pre> <ol> <li> <p>Install Helm</p> </li> <li> <p>Add /usr/local/bin in your path</p> </li> </ol> <pre><code>echo \"export PATH=$PATH:/usr/local/bin\" &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre> <ul> <li>Download and install helm</li> </ul> <pre><code>curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\nchmod 700 get_helm.sh\n./get_helm.sh\nsudo chmod 777 /usr/local/bin/helm\n</code></pre> <ol> <li>Specify Kubernetes configuration file</li> </ol> <pre><code>echo \"export KUBECONFIG=/etc/rancher/k3s/k3s.yaml\" &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre>"},{"location":"concert/concert-workflow-install/#install-concert-workflow","title":"Install Concert workflow","text":"<p>Official documentation: Concert Worflow installation</p> <ol> <li>Connect to the VM you have created on Techzone in Lab0 and source environment variables</li> </ol> <pre><code>ssh itzuser@&lt;VM ip address&gt; -p 2223 -i /path/to/concert/sshkey/pem_ibmcloudvsi_download.pem\n</code></pre> <ol> <li>Get concert workflow installation files</li> </ol> <pre><code>source $HOME/env.sh\ncd /mnt/concert\nwget https://github.com/IBM/Concert/releases/download/v1.1.0/ibm-concert-std-workflows.tgz\ntar xfz ibm-concert-std-workflows.tgz\n</code></pre> <ol> <li>Update environment variables</li> </ol> <pre><code>vi $HOME/env.sh\n</code></pre> <p>Update the values for the following keys:</p> <ul> <li>IBM_REG_PASS with your entitlement key (same as CONCERT_REGISTRY_PASSWORD value)</li> <li>VM_IP with your VM public IP address (in the format aaa.bbb.ccc.ddd)</li> </ul> <p>Save the file (:wq) and source the $HOME/env.sh file to set environment variables</p> <pre><code>source $HOME/env.sh\n</code></pre> <ol> <li>Navigate to workflows folder</li> </ol> <pre><code>cd /mnt/concert/workflows\n</code></pre> <ol> <li>Launch concert-workflow installation</li> </ol> <pre><code>./bin/deploy-k8s --license-acceptance=y \\\n--instance-address=$VM_IP \\\n--concert-user=$CONCERT_USER \\\n--concert-url=$CONCERT_URL \\\n--c-api-key=$CONCERT_APIKEY\n</code></pre> <p>IMPORTANT: Wait until the end of the installation. Be patient, it can take up to 20 minutes. It's time for a coffee break !</p> <ol> <li>Enable (Python function-as-a-service (FaaS) action blocks)</li> </ol> <p>Insecure access to the internal registry must be enabled and registries.yaml must be defined under /etc/rancher/k3s:</p> <pre><code>sudo vi /etc/rancher/k3s/registries.yaml\n</code></pre> <p>Insert</p> <pre><code>configs:\n  \"YOUR_VM_IP\": \n    \"tls\": \n       insecure_skip_verify: true\n</code></pre> <p>Restart the Kubernetes service</p> <pre><code>sudo systemctl restart k3s\n</code></pre> <ol> <li> <p>Check Concert Workflow installation</p> </li> <li> <p>From a browser, enter the URL of your concert instance (https://YOUR_VM_IP:12443) and log with your concert username and password.</p> </li> <li>You should have now a Workflows menu</li> <li>Navigate to Workflows-&gt;Manage and check that a page is displayed successfully</li> <li>Navigate to Administration-&gt;Integrations and then in Connections tab</li> <li>Check that you have a connection named CONCERT_WORKFLOWS</li> </ol>"},{"location":"concert/concert-workflows/","title":"Intro to Concert Workflows","text":""},{"location":"concert/concert-workflows/#intro-to-concert-workflows","title":"Intro to Concert Workflows","text":"<p>For this lab exercise, we will install Concert Workflows into an existing Concert deployment (VM deployment) and run our first basic workflow. For the latest documentation on how to achieve this process, consult the IBM Concert documentation on Concert Workflows add-on.</p>"},{"location":"concert/concert-workflows/#objective","title":"Objective","text":"<p>Concert Workflows is an embedded version of Rapid Infrastructure Automation in IBM Concert and is available as an add-on workflow automation service for on-premises deployments of Concert. </p> <p>The add-on embeds workflow definition and automation capabilities so you can define, manage, and automate workflows within the Concert UI.</p> <p>The objective is to get data from an organization's environments and applications using low-code flows.  </p> <p>By default, a flow is executed in a worker located on the IBM Concert host. A notion of remote workers exist in Concert Workflow to enable the ingestion of data into IBM Concert from environments that cannot be reached directly by the IBM Concert host. The remote worker is located near the environment, collects the required data, and ingests it into IBM Concert using Concert's APIs.</p> <p>In this lab, you will install IBM Concert Workflows as an add-on to your Concert installation.</p>"},{"location":"concert/concert-workflows/#prerequisite","title":"Prerequisite","text":"<ul> <li> IBM Concert must be installed</li> </ul>"},{"location":"concert/concert-workflows/#installing-concert-workflows-prerequisites","title":"Installing Concert Workflows Prerequisites","text":"<p>Concert Workflows run as a containerized application in a Kubernetes cluster. As such, you will need to install a standalone K3s cluster and Helm on the VM where you installed IBM Concert.</p> <p>Environment</p> <p>These instructions have been tested from the Concert VM. Below is the command to SSH into the VM that was used in the previous labs.</p> <p></p><pre><code>ssh -i ~/Downloads/pem_ibmcloudvsi_download.pem -p 2223 itzuser@&lt;PUBLIC IP&gt;\n</code></pre> Some changes may be required if you want to follow from another machine.<p></p>"},{"location":"concert/concert-workflows/#installing-rancher-k3s","title":"Installing Rancher K3s","text":"<p>Install K3s from the command line.  You will need to disable its default ingress controller (traefik), so that port 443 is not in use.  SSH to your concert VM and execute the following commands:</p> Installing Rancher K3s <pre><code>cd $HOME\ncurl -sfL https://get.k3s.io | sudo INSTALL_K3S_VERSION=v1.29.2+k3s1 sh -s - --write-kubeconfig-mode 644 --disable traefik\n</code></pre>"},{"location":"concert/concert-workflows/#install-helm","title":"Install Helm","text":"Add /usr/local/bin in your path <pre><code>echo \"export PATH=$PATH:/usr/local/bin\" &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre> install Helm <pre><code>curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\nchmod 700 get_helm.sh\n./get_helm.sh\nsudo chmod 755 /usr/local/bin/helm\n</code></pre> output <pre><code>$ curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\n$ chmod 700 get_helm.sh\n$ ./get_helm.sh\n./get_helm.sh: line 138: /usr/local/bin/helm: Permission denied\nHelm v3.17.0 is available. Changing from version .\nDownloading https://get.helm.sh/helm-v3.17.0-linux-amd64.tar.gz\nVerifying checksum... Done.\nPreparing to install helm into /usr/local/bin\nhelm installed into /usr/local/bin/helm\n$ sudo chmod 755 /usr/local/bin/helm\n</code></pre> Specify Kubernetes configuration file <pre><code>echo \"export KUBECONFIG=/etc/rancher/k3s/k3s.yaml\" &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre>"},{"location":"concert/concert-workflows/#installing-concert-workflows-on-a-vm","title":"Installing Concert Workflows on a VM","text":"<p>Download the IBM Concert Workflows installer and authenticate to the IBM Entitlement Registry.</p> <p>Prerequisite</p> <p>You will need an Entitlement key to install Workflows. This will be the same key that you fetched during the install. Fetch your key or create one here.</p> <p>Source your environment variables and change directory to your install folder:</p> <pre><code>source $HOME/env.sh\ncd /mnt/concert\n</code></pre> Downloading Installer <pre><code>wget https://github.com/IBM/Concert/releases/download/v1.1.0/ibm-concert-std-workflows.tgz\ntar xfz ibm-concert-std-workflows.tgz\n</code></pre> output <pre><code>$ wget https://github.com/IBM/Concert/releases/download/v1.1.0/ibm-concert-std-workflows.tgz\n--2025-01-29 14:39:44--  https://github.com/IBM/Concert/releases/download/v1.1.0/ibm-concert-std-workflows.tgz\nResolving github.com (github.com)... 140.82.112.4\nConnecting to github.com (github.com)|140.82.112.4|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://objects.githubusercontent.com/github-production-release-asset-2e65be/815253674/d37113aa-93c4-4635-850a-6d478b44bb77?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=releaseassetproduction%2F20250129%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20250129T143945Z&amp;X-Amz-Expires=300&amp;X-Amz-Signature=88b0e3843a4642a3249d744526bfbb580d50f551240dcd148aded92c384305a6&amp;X-Amz-SignedHeaders=host&amp;response-content-disposition=attachment%3B%20filename%3Dibm-concert-std-workflows.tgz&amp;response-content-type=application%2Foctet-stream [following]\n--2025-01-29 14:39:45--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/815253674/d37113aa-93c4-4635-850a-6d478b44bb77?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=releaseassetproduction%2F20250129%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20250129T143945Z&amp;X-Amz-Expires=300&amp;X-Amz-Signature=88b0e3843a4642a3249d744526bfbb580d50f551240dcd148aded92c384305a6&amp;X-Amz-SignedHeaders=host&amp;response-content-disposition=attachment%3B%20filename%3Dibm-concert-std-workflows.tgz&amp;response-content-type=application%2Foctet-stream\nResolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\nConnecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.110.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 40468 (40K) [application/octet-stream]\nSaving to: \u2018ibm-concert-std-workflows.tgz\u2019\n\nibm-concert-std-workflows.tgz           100%[===============================================================================&gt;]  39.52K  --.-KB/s    in 0.001s\n\n2025-01-29 14:39:45 (41.7 MB/s) - \u2018ibm-concert-std-workflows.tgz\u2019 saved [40468/40468]\n\n$ tar xfz ibm-concert-std-workflows.tgz\n</code></pre> Update environment variables <pre><code>vi $HOME/env.sh\n</code></pre> <p>Update the values for the following keys:</p> <ul> <li>IBM_REG_PASS with your entitlement key (same as CONCERT_REGISTRY_PASSWORD value)</li> <li>VM_IP with your VM public IP address (in the format aaa.bbb.ccc.ddd)</li> </ul> Save the file (:wq) and source the $HOME/env.sh file to set environment variables <pre><code>source $HOME/env.sh\n</code></pre> Navigate to workflows folder <pre><code>cd /mnt/concert/workflows\n</code></pre> Launch Workflows installation <pre><code>./bin/deploy-k8s --license-acceptance=y \\\n--instance-address=$VM_IP \\\n--concert-user=$CONCERT_USER \\\n--concert-url=$CONCERT_URL \\\n--c-api-key=$CONCERT_APIKEY\n</code></pre> <p>IMPORTANT: Wait until the end of the installation. Be patient, it can take up to 20 minutes. It's time for a coffee break!</p> Enable (Python function-as-a-service (FaaS) action blocks) <p>Insecure access to the internal registry must be enabled and registries.yaml must be defined under /etc/rancher/k3s:</p> <pre><code>sudo vi /etc/rancher/k3s/registries.yaml\n</code></pre> <p>Insert these lines into the registries.yaml</p> <pre><code>configs:\n\"YOUR_VM_IP\": \n    \"tls\": \n    insecure_skip_verify: true\n</code></pre> <p>Restart the Kubernetes service</p> <pre><code>sudo systemctl restart k3s\n</code></pre> Check Concert Workflows installation <ul> <li> From a browser, enter the URL of your Concert instance (https://YOUR_VM_IP:12443) and log with your Concert username and password.</li> <li> You should have now a Workflows menu</li> <li> Navigate to Workflows-&gt;Manage and check that a page is displayed successfully</li> <li> Navigate to Administration-&gt;Integrations and then in Connections tab</li> <li> Check that you have a connection named CONCERT_WORKFLOWS</li> </ul>"},{"location":"concert/concert-workflows/#creating-your-first-workflow","title":"Creating your first Workflow","text":"<p>Back in your browser, refresh the Concert home page. You should now see a Workflow tab at the top of your Concert instance. Select <code>Manage</code>:</p> <p></p> <p>Select <code>Create Workflow</code>, give your workflow a name, and click <code>Select Template</code>. Set the template to <code>HTTP Template</code>:</p> <p></p> <p>Create your workflow and run it using the <code>Run</code> button. You should see the workflow make a request to <code>example.com</code>!</p> <p>Workflow Lab Complete</p> <p>Congratulations, you have successfully completed the Workflow install lab.</p>"},{"location":"concert/concert-workflows/#acknowledgements","title":"Acknowledgements","text":"<p>This lab was based on: https://pages.github.ibm.com/cs-tel-ibm-concert/training/module3/concert-workflows/#installing-concert-workflows-on-a-vm and https://github.ibm.com/concert-bootcamp/ibm-concert-lab-guide/blob/main/labs/Lab2-concert-workflow-installation.md</p>"},{"location":"concert/installing/install-openshift/","title":"Installing IBM Concert &gt; OpenShift Install","text":""},{"location":"concert/installing/install-openshift/#installing-ibm-concert-openshift-install","title":"Installing IBM Concert &gt; OpenShift Install","text":"<p>The following guide is derived from the IBM Concert Documentation.  Please refer to the official IBM documentation for the latest up-to-date materials.</p>"},{"location":"concert/installing/install-openshift/#lab-environments","title":"Lab Environments","text":""},{"location":"concert/installing/install-openshift/#sizing","title":"Sizing","text":"<p>The IBM Sales Configurator lists the following configuration for a small t-shirt size Concert cluster.</p> <ul> <li>3 master nodes. Each with 4 CPU 16GB RAM</li> <li>3 worker nodes. Each with 16 CPU 64GB RAM</li> </ul> <p></p>"},{"location":"concert/installing/install-openshift/#techzone","title":"TechZone","text":""},{"location":"concert/installing/install-openshift/#techzone-reservation","title":"TechZone Reservation","text":"<p>Use an OpenShift cluster deplpyed on IBM Cloud.  This lab has been written and tested on the following TechZone Certified Base Image in the VMWare on IBM Cloud Environments collection.  Select the following environment from this collection.</p> <p></p> <p>In the reservation details, pick your Preferred Geography, and use the <code>8 vCPU | 32 GB</code> Worker Node Flavor.</p> <p></p> <ul> <li>OpenShift Cluster (VMware on IBM Cloud) - UPI - Public</li> </ul> <p>Network Connectivity</p> <p>Pay close attention to network connectivity.  Your OpenShift cluster needs to be deployed in a network where it can get to your image registry and your github enterprise server.  More importantly, your github enterprise server needs to be able to reach your OpenShift cluster so that it can inform when a code push occurs on the github repo.</p> <p>When your reservation is ready, you will receive an email from TechZone with a link to your reservation.</p> <p></p> <p>In the reservation details page, you will find important information such as the OpenShift API URL, credentials to log in, and a link to the OpenShift Console.  You can also download a copy of the cluster's kubeconfig file for an alternate means of authentication.</p> <p></p>"},{"location":"concert/installing/install-openshift/#client-workstation-requirements","title":"Client workstation requirements","text":"podman <p>Docker Desktop is no longer allowed for IBM workstations, so we will use podman. The podman and docker commands are interchangeable throughout this guide.</p> Linux (RHEL)MacWindows <pre><code>sudo dnf install -y podman\n</code></pre> <pre><code>brew install podman-desktop\n</code></pre> <p>Refer to the Podman Install Insrtuctions if you havent installed podman in your workstation.</p> jq Linux (RHEL)MacWindows <pre><code>sudo dnf install -y jq\n</code></pre> <p>You can also obtain the pre-compiled binaries in the jq Documentation.</p> <p>The best way to install the jq utility for your Mac is using Homebrew</p> <pre><code>brew install jq\n</code></pre> <p>You can also obtain the pre-compiled binaries in the jq Documentation.</p> <p>You can obtain the pre-compiled binaries in the jq Documentation.</p> OpenShift Client Linux (RHEL)MacWindows <p>You can obtain the OpenShift CLI following the official OpenShift Documentation.</p> <p>Alternately, you can download the latest stable version from https://mirror.openshift.com/pub/openshift-v4/clients/ocp/stable/ that suits your OS and Architecture.  Uncompress the file, and move the <code>oc</code> utility to a directory in your <code>$PATH</code>.</p> <p>For example:</p> <pre><code>curl https://mirror.openshift.com/pub/openshift-v4/clients/ocp/stable/openshift-client-linux-amd64-rhel9-4.17.3.tar.gz -o openshift-client.tgz\ntar xf openshift-client.tgz\nsudo mv oc /usr/local/bin/\n</code></pre> <p>The best way to install the OpenShift Cient utility for your Mac is using Homebrew</p> <pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nbrew install openshift-cli\n</code></pre> <p>You can also obtain the OpenShift CLI following the official OpenShift Documentation.</p> <p>You can obtain the OpenShift CLI following the official OpenShift Documentation.</p>"},{"location":"concert/installing/install-openshift/#obtain-ibm-entitlement-key","title":"Obtain IBM Entitlement Key","text":"<p>To obtain your IBM entitlement API key perform the following steps:</p> <p>Log in to Container software library on My IBM with the IBMid and password that are associated with the entitled software. On the Entitlement keys tab, select Copy to copy the entitlement key to clipboard.</p>"},{"location":"concert/installing/install-openshift/#installing-ibm-concert","title":"Installing IBM Concert","text":"<p>The Concert product documentation lists all the steps required to update the global pull secret for your OpenShift cluster.  Bellow is a quick summary</p>"},{"location":"concert/installing/install-openshift/#log-on-to-openshift","title":"Log on to OpenShift","text":"<p>On the TechZone reservation details page, take note of the API URL, as well as the cluster admin username and password.</p> <p>To log on to your OpenShift cluster with the oc utulity, execute the following step:</p> oc login <pre><code>oc login -u &lt;CLUSTER ADMIN USERNAME&gt; -p &lt;CLUSTER ADMIN PASSWORD&gt; &lt;API_URL&gt;\n</code></pre> <p>In the example reservation details posted above, the login command would be</p> <pre><code>oc login -u kubeadmin -p UEz2r-SgXDp-hBLhB-5dusx https://api.673d53e1b6c6b35d1da0621c.ocp.techzone.ibm.com:6443\n</code></pre>"},{"location":"concert/installing/install-openshift/#update-global-pull-secret","title":"Update global pull secret","text":"update global pull secret <p>Set up your environment variables.</p> <pre><code>export PROD_REGISTRY=cp.icr.io\nexport PROD_USER=cp\nexport IBM_ENTITLEMENT_KEY=\"eY...\"\n</code></pre> <p>The following procedure will extract the global pull secret from your cluster, add the credentials to the IBM Entitlement Registry, and then update the global pull secret in the cluster</p> <pre><code>oc get secret/pull-secret -n openshift-config --template='{{index .data \".dockerconfigjson\" | base64decode}}' &gt; dockerconfigjson\noc registry login --registry=${PROD_REGISTRY} --auth-basic=\"${PROD_USER}:${IBM_ENTITLEMENT_KEY}\" --to=dockerconfigjson\noc set data secret/pull-secret -n openshift-config --from-file=.dockerconfigjson=dockerconfigjson\n</code></pre>"},{"location":"concert/installing/install-openshift/#install-ibm-concert","title":"Install IBM Concert","text":"<p>The IBM Concert install documentation lists all the steps required to deploy IBM Concert into an OpenShift cluster.  Some additional details are described bellow.</p> <p>Concert uses a bash script to install and configure its environment, similar to the cpd-cli utility used by CloudPak for Data.</p> Command <pre><code>mkdir ibm-concert\ncd ibm-concert\ncurl https://raw.githubusercontent.com/IBM/Concert/main/Software/manage/ibm-concert-manage.sh -o ibm-concert-manage.sh\nchmod +x ibm-concert-manage.sh\n</code></pre> <p>set up environemnt variables</p> Command <pre><code>export DOCKER_EXE=podman # (1)!\nexport WORK_DIR=/Users/ncolon/dev/ibm-concert-training/ibm-concert # (2)!\nexport OCP_URL=https://api.concert.aws.ibm-gtmaa.dev:6443 # (3)!\nexport PROJECT_OPERATOR=ibm-concert-operators # (4)!\nexport PROJECT_INSTANCE=ibm-concert # (5)!\nexport STG_CLASS_BLOCK=gp3-csi # (6)!\nexport STG_CLASS_FILE=\"\" # (7)!\nexport IBM_ENTITLEMENT_KEY=\"eyJh...\" # (8)!\n</code></pre> <ol> <li>DOCKER_EXE: docker or podman.  This guide uses podman, but commands are interchangeable.</li> <li>WORK_DIR: Workstation directory where the <code>ibm-concert-manage.sh</code> script runs</li> <li>OCP_URL: Red Hat OpenShift container platform URL.  You can obtain it by running <code>oc whoami --show-server</code></li> <li>PROJECT_OPERATOR: IBM automation application framework operator installation project</li> <li>PROJECT_INSTANCE: Concert installation project</li> <li>STG_CLASS_BLOCK: Block storage class name</li> <li>STG_CLASS_FILE: File storage class name</li> <li>IBM_ENTITLEMENT_KEY: IBM entitlement API key</li> </ol> <p>Initialize the AAF utility.  It runs as a local container in podman.</p> Command <pre><code>./ibm-concert-manage.sh initialize\n</code></pre> initialize output <pre><code>\u279c  lab $ $ ./ibm-concert-manage.sh initialize\nibm-aaf-utils\n===== work_dir is /Users/ncolon/dev/ibm-concert/1.0.3-install/ =====\n2401c7877a46b99dc62317f7513d2d4bc14f3fdbfa792bbce424427b4a1198d6\n/opt/ansible\nUpdating param-aaf file\ncopying file\nto check on things:-\npodman exec -ti ibm-aaf-utils bash\n  %\n</code></pre> <p>Obtain an OpenShift login token from your OpenShift console.  This token expires every 24hrs.  Navigate to your OpenShift console, and clock on the <code>kube:admin</code> dropdown on the top right, and click on <code>Copy login command</code></p> <p></p> <p>On the next screen, click on <code>Display Token</code>. </p> <p>Copy the token, and copy it as the value for <code>OCP_TOKEN</code> below, and then log the installer into your OpenShift instance.</p> Command <pre><code>export OCP_TOKEN=\"sha256~...\"\n./ibm-concert-manage.sh login-to-ocp --token=${OCP_TOKEN} --server=${OCP_URL}\n</code></pre> login-to-ocp output <pre><code>\u279c  lab $ ./ibm-concert-manage.sh login-to-ocp --token=${OCP_TOKEN} --server=${OCP_URL}\nlogin-to-ocp..\nWARNING: Using insecure TLS client config. Setting this option is not supported!\n\nLogged into \"https://api.concert.aws.ibm-gtmaa.dev:6443\" as \"kube:admin\" using the token provided.\n\nYou have access to 69 projects, the list has been suppressed. You can list all projects with 'oc projects'\n\nUsing project \"default\".\nUsing project \"default\" on server \"https://api.concert.aws.ibm-gtmaa.dev:6443\".\n</code></pre> <p>Finally, start the Concert installation.  It should take around 45-60 minutes.</p> Command <pre><code>./ibm-concert-manage.sh concert-setup\n</code></pre> concert-setup output <pre><code>\u279c  lab $ ./ibm-concert-manage.sh concert-setup\nconcert-setup\n\n[...REDACTED FOR BREVITY...]\nPLAY [localhost] ***************************************************************\n\nTASK [include_role : utils] ****************************************************\n\nTASK [utils : include_vars] ****************************************************\nok: [localhost]\n\nTASK [utils : include_vars] ****************************************************\nok: [localhost]\n\n[...REDACTED FOR BREVITY...]\n\nTASK [debug] *******************************************************************\nskipping: [localhost]\n\nPLAY RECAP *********************************************************************\nlocalhost                  : ok=86   changed=9    unreachable=0    failed=0    skipped=118  rescued=0    ignored=0   \n\nrunning pre-validation routines (if any) for post_apply_cr..\nFri Aug  2 01:56:32 UTC 2024\ncommand ran successfully. ['cr-apply', '--license_acceptance=true', '--release=4.8.3', '--components=cpd_platform,concert', '--cpd_operator_ns=ibm-concert-operators', '--cpd_instance_ns=ibm-concert', '--block_storage_class=gp3-csi', '--file_storage_class=', '--preview=false', '@']\n2024-08-02 01:56:32,182 - __main__ - INFO - command ran successfully. ['cr-apply', '--license_acceptance=true', '--release=4.8.3', '--components=cpd_platform,concert', '--cpd_operator_ns=ibm-concert-operators', '--cpd_instance_ns=ibm-concert', '--block_storage_class=gp3-csi', '--file_storage_class=', '--preview=false', '@']\n2024-08-02 01:56:32,184 - __main__ - INFO - deployment succeeded\n</code></pre> Upgrading IBM Concert <p>To upgrade IBM Concert, repeat the steps to set up environment variables, initialize and login to ocp.  Replace the <code>./ibm-concert-manage.sh concert-setup</code> step with</p> <pre><code>./ibm-concert-manage.sh upgrade-concert\n</code></pre>"},{"location":"concert/installing/install-openshift/#validating-install","title":"Validating Install","text":"<p>Upon completion, you can run these following commands to validate that the install completed successfully.</p> <p>Validate the IBM Concert CatalogSources</p> Command <pre><code>oc get catalogsource -n ${PROJECT_OPERATOR}\n</code></pre> validating CatalogSource <pre><code>\u279c  lab $ oc get catalogsource -n ${PROJECT_OPERATOR}\nNAME                              DISPLAY                                 TYPE   PUBLISHER   AGE\ncloud-native-postgresql-catalog   ibm-cloud-native-postgresql-4.18.0      grpc   IBM         18m\ncpd-platform                      ibm-cp-datacore-4.3.0+20240311.101055   grpc   IBM         12m\nibm-aaf-operator-catalog          ibm-aaf-operator-1.0.0-linux-amd64      grpc   IBM         10m\nopencloud-operators               ibm-cp-common-services-4.4.0            grpc   IBM         18m\n</code></pre> <p>Validate the ClusterServiceVersions</p> Command <pre><code>oc get csv -n ${PROJECT_OPERATOR}\n</code></pre> validate csv <pre><code>\u279c  lab $ oc get csv -n ${PROJECT_OPERATOR}\nNAME                                          DISPLAY                                VERSION   REPLACES                          PHASE\ncloud-native-postgresql.v1.18.7               EDB Postgres for Kubernetes            1.18.7    cloud-native-postgresql.v1.18.6   Succeeded\ncpd-platform-operator.v5.3.0                  Cloud Pak for Data Platform Operator   5.3.0                                       Succeeded\nibm-aaf-operator.v1.0.0                       ibm-aaf-operator                       1.0.0                                       Succeeded\nibm-cert-manager-operator.v4.2.2              IBM Cert Manager                       4.2.2                                       Succeeded\nibm-common-service-operator.v4.4.0            IBM Cloud Pak foundational services    4.4.0                                       Succeeded\nibm-commonui-operator.v4.3.1                  Ibm Common UI                          4.3.1                                       Succeeded\nibm-iam-operator.v4.4.0                       IBM IM Operator                        4.4.0                                       Succeeded\nibm-mongodb-operator.v4.2.2                   IBM MongoDB Operator                   4.2.2                                       Succeeded\nibm-namespace-scope-operator.v4.2.2           IBM NamespaceScope Operator            4.2.2                                       Succeeded\nibm-zen-operator.v5.1.1                       IBM Zen Service                        5.1.1                                       Succeeded\noperand-deployment-lifecycle-manager.v4.2.3   Operand Deployment Lifecycle Manager   4.2.3                                       Succeeded\n\u279c  lab $ \n</code></pre> <p>Note</p> <p>You may have more CSVs in your <code>${PROJECT_OPERATOR}</code> namespace depending on what other cluster scoped operators are installed on your cluster.</p> <p>Validate the status of all the CPFS services.  All services should have a Status of <code>Completed</code>.</p> Command <pre><code>./ibm-concert-manage.sh get-cr-status\n</code></pre> get-cr-status <pre><code>\u279c  lab $ ./ibm-concert-manage.sh get-cr-status\nget-cr-status\nExecute script: python3 /opt/ansible/bin/get_cr_status.py --cpd_instance_ns ibm-concert\n\nRunning the get_cr_status.py script. Start of the log.\n================================================================\n\n[...REDACTED FOR BREVITY...]\n\nComponent     CR-kind     CR-name    Namespace    Status     Version    Creationtimestamp     Reconciled-version    Operator-info\n------------  ----------  ---------  -----------  ---------  ---------  --------------------  --------------------  ----------------------------------\nzen           ZenService  lite-cr    ibm-concert  Completed  5.1.1      2024-08-02T01:24:50Z  5.1.1                 zen operator 5.1.1 build 37\ncpd_platform  Ibmcpd      ibmcpd-cr  ibm-concert  Completed  4.8.3      2024-08-02T01:22:20Z  N/A                   cpdPlatform operator 5.3.0 build 5\n\nThe get_cr_status.py script ran successfully. End of the log.\n================================================================\n\n[SUCCESS] The status of custom resources was saved to '/status.csv'.\n</code></pre>"},{"location":"concert/installing/install-openshift/#acessing-ibm-concert-instance","title":"Acessing IBM Concert Instance","text":"<p>Once deployed, you can access your IBM Concert instance by issuing the following command:</p> Command <pre><code>./ibm-concert-manage.sh get-concert-instance-details\n</code></pre> get-concert-instance-details <pre><code>\u279c lab $ ./ibm-concert-manage.sh get-concert-instance-details\nget-concert-instance-details\n# admin_password\nConcert Url: concert-ibm-concert.apps.concert.aws.ibm-gtmaa.dev\nZen Administration URL: concert-ibm-concert.apps.concert.aws.ibm-gtmaa.dev/zen\nConcert Username: cpadmin\nConcert Password: [REDACTED]\n</code></pre>"},{"location":"concert/installing/install-overview/","title":"Installing IBM Concert &gt; Overview","text":""},{"location":"concert/installing/install-overview/#installing-ibm-concert-overview","title":"Installing IBM Concert &gt; Overview","text":"<p>IBM Concert can be deployed in 3 different configurations.</p> <ul> <li>VM Based install: IBM Concert is installed as a series containers. Very limited high availability (HA), but can be deployed in a smaller footprint.</li> <li>OpenShift based install: IBM Concert is installed on top of OpenShift, and leverages the CPFS framework for authentication.  Much more resilient and ready for production deployments.</li> <li>SaaS deployment: IBM Concert is deployed as a services on either IBM Cloud or AWS.</li> </ul>"},{"location":"concert/installing/install-vm-old/","title":"Installing IBM Concert &gt; VM Install","text":""},{"location":"concert/installing/install-vm-old/#installing-ibm-concert-vm-install","title":"Installing IBM Concert &gt; VM Install","text":""},{"location":"concert/installing/install-vm-old/#documentation","title":"Documentation","text":"<p>The following guide is derived from the IBM Concert Documentation.  Please refer to the official IBM documentation for the latest up-to-date materials.</p>"},{"location":"concert/installing/install-vm-old/#lab-environments","title":"Lab Environments","text":""},{"location":"concert/installing/install-vm-old/#sizing","title":"Sizing","text":"<p>The recommended size for your Linux VM is a 16 CPU, 32GB RAM with 512GB for storage.</p>"},{"location":"concert/installing/install-vm-old/#techzone-reservation","title":"TechZone Reservation","text":"<p>Use a RHEL9 VM on IBM Cloud.  This lab has been written and tested on the following TechZone Certified Base Image in the IBM Cloud Environment.  Select this environment from this collection.</p> <p></p> <p>In the reservation details, pick your Preferred Geography, and use the <code>8 vCPU | 32 GiB</code> VM Profile.  Altough this is does not meet the minimum requirements for a customer VM deployment, it will suffice for the purposes of this training exercise.</p> <p></p> Bring Your Own VM <p>You can also bring your own Linux VM from your on-prem or cloud environment.  Ensure access to port 12443 is allowed to VM.  This lab exercise is based on RHEL9 VMs provisioned by the TechZone environment above, there may be changes that need to be accounted for when using other environments, but that is outside of the scope of this guide.</p> <p>When your reservation is ready, you will receive an email from TechZone with a link to your reservation</p> <p></p> <p>On the reservation details, take note of the public IP address of your RHEL9 vm, as well as download the SSH key with the button provided</p> <p></p>"},{"location":"concert/installing/install-vm-old/#obtain-ibm-entitlement-key","title":"Obtain IBM Entitlement Key","text":"<p>To obtain your IBM entitlement API key perform the following steps:</p> <p>Log in to Container software library on My IBM with the IBMid and password that are associated with the entitled software. On the Entitlement keys tab, select Copy to copy the entitlement key to clipboard.</p>"},{"location":"concert/installing/install-vm-old/#install-prerequirements","title":"Install Prerequirements","text":"<p>SSH into the IBM Concert VM with the public IP and SSH key obtained from the TechZone reservation details.  You'll need to change permissions on the SSH key first before you SSH.</p> SSH <pre><code>chmod 600 ~/Downloads/pem_ibmcloudvsi_download.pem\nssh -i ~/Downloads/pem_ibmcloudvsi_download.pem -p 2223 itzuser@&lt;PUBLIC IP&gt;\n</code></pre> <p>Warning</p> <p>The installation steps need to be run on the Linux VM that will act as your IBM Concert server.  See the Before you being section in the IBM Concert documentation for VM requirements</p>"},{"location":"concert/installing/install-vm-old/#prepare-vm-disk","title":"Prepare VM disk","text":"<p>To prepare the 500Gb disk, execute the following commands. Note: if you run into any issues, it may be helpful to execute the commands one at a time.</p> Code <pre><code>sudo -i\nlsblk\nmkfs.ext4 -m 0 -E lazy_itable_init=0,lazy_journal_init=0,discard /dev/vdd\nblkid | grep /dev/vdd\n</code></pre> <p>Copy the UUID value output and store it in a safe place.</p> Code <pre><code>mkdir -p /mnt/concert\ncp /etc/fstab /etc/fstab.orig\nvi /etc/fstab\n</code></pre> <p>Insert this line at the end of the file: </p> Code <pre><code>UUID=YOUR_UUID /mnt/concert ext4 discard,defaults,nofail 0 0\n</code></pre> <p>Replace YOUR_UUID by the one listed by the blkid | grep /dev/vdd command      Save the file and continue with following commands:</p> Code <pre><code>mount -a\nsystemctl daemon-reload\nlsblk\nchmod 777 /mnt/concert\n</code></pre> Required Utilities podman gitwgetjq <p>Docker Desktop is no longer allowed for IBM workstations, so we will use podman. Refer to the Podman Install Instructions if you havent installed podman in your workstation.  The podman and docker commands are interchangeable throughout this guide.</p> <p>You can obtain the git CLI following the official Git Documentation.</p> <p><code>wget</code> should be installed by default on your Linux system.  If its not, follow the install instructions on the official GNU Wget website.</p> <p><code>jq</code> should be installed by default on your Linux system.  If its not, follow the install instructions on the official jq website.</p> <p>This lab is based on a RHEL9 VM in TechZone, to install all prereqs at once there, you can run the following command</p> <pre><code>sudo dnf install -y podman git wget jq\n</code></pre>"},{"location":"concert/installing/install-vm-old/#provision-watsonxai-on-techzone","title":"Provision watsonx.ai on TechZone","text":"<p>Note: Concert require model ibm/granite-3-2-8b-instruct on watsonx.ai, so watsonx.ai must be provision in US </p>"},{"location":"concert/installing/install-vm-old/#watsonxai-provisioning","title":"Watsonx.ai Provisioning","text":"<ol> <li> <p>Navigate to watsonx.ai on IBM Techzone</p> </li> <li> <p>Make a reservation for IBM watsonx.ai and choose AMERICAS as preferred geography (at the time we write this lab, only this region has the model used by IBM Concert inferences)</p> </li> <li> <p>When the reservation is ready, you should receive a mail from IBM Cloud to join an account in IBM Cloud.</p> </li> <li> <p>Open the mail, click the join now link and follow the instructions.</p> <p></p> </li> <li> <p>When you have the IBM Cloud first screen, verify that you have the good account selected on the top bar</p> <p></p> </li> </ol> <p>Note: The IBM Cloud account selected should match the Cloud Account specified in the IBM TechZone reservation.</p> <p></p>"},{"location":"concert/installing/install-vm-old/#create-a-watsonx-project-and-get-project-id","title":"Create a watsonx project and get project ID","text":"<ol> <li> <p>Select watsonx to from the burger menu on the left</p> <p></p> </li> <li> <p>Click Launch in the watsonx.ai tile</p> <p></p> </li> <li> <p>Scroll down in the page that appear and click Create a sandbox project in the Projects tile</p> <p></p> </li> <li> <p>Select the sandbox that have been created (you may need to refresh the page)</p> <p></p> </li> <li> <p>In Manage tab, copy the \"Project ID\" and store it in a safe place</p> <p></p> </li> </ol>"},{"location":"concert/installing/install-vm-old/#get-api-key-and-service-id-information","title":"Get API Key and service ID information","text":"<ul> <li> <p>From your techzone reservation screen, retrieve the APIKey and the service ID and store them in a safe place</p> <p></p> </li> </ul>"},{"location":"concert/installing/install-vm-old/#api-key-import-the-service-id-as-part-of-the-project","title":"API key - import the Service ID as part of the project","text":"<ol> <li> <p>From your watsonx screen, in the manage tab, select Access control in the left menu</p> <p></p> </li> <li> <p>Click the Add Collaborators button and select Add Access Group.</p> <p></p> </li> <li> <p>Enter your Access Group name. The access Group Name is the IBM Cloud Service ID on your TechZone reservation page.</p> <p></p> </li> <li> <p>Select your group, give it admin right and click Add button</p> <p></p> </li> </ol>"},{"location":"concert/installing/install-vm-old/#installing-ibm-concert","title":"Installing IBM Concert","text":"<p>Warning</p> <p>The installation steps need to be run on the Linux VM that will act as your IBM Concert server.  See the Before you being section in the IBM Concert documentation for VM requirements</p>"},{"location":"concert/installing/install-vm-old/#download-installation-artifacts","title":"Download installation artifacts","text":"Code <pre><code>LATEST_TAG=$(curl --silent https://api.github.com/repos/IBM/Concert/releases/latest | jq -r .tag_name)\nwget https://github.com/IBM/Concert/releases/download/${LATEST_TAG}/ibm-concert-std.tgz\ntar -xf ibm-concert-std.tgz\n</code></pre>"},{"location":"concert/installing/install-vm-old/#set-up-environment-variables","title":"Set up environment variables","text":"Code <pre><code>export DOCKER_EXE=podman\nexport CONCERT_REGISTRY=cp.icr.io/cp/concert\nexport CONCERT_REGISTRY_USER=cp\nexport CONCERT_REGISTRY_PASSWORD=\"eyJh...\"  # (1)!\n</code></pre> <ol> <li>Your IBM Entitlement Key</li> </ol>"},{"location":"concert/installing/install-vm-old/#log-in-to-the-ibm-container-registry","title":"Log in to the IBM Container Registry","text":"Notes on Airgap Environments <p>If you're deploying on an AirGap environment, you will first need to mirror the images to your private container image registry.  You will need to install skopeo on a workstation with access to both cp.icr.io and to the private image registry where images will be mirrored.</p> <pre><code>export CONCERT_PUBLIC_REGISTRY=cp.icr.io/cp/concert\nexport CONCERT_PUBLIC_REGISTRY_USER=cp\nexport CONCERT_PUBLIC_REGISTRY_PASSWORD=\"eyJh...\"  ## Your IBM Entitlement Key\nexport CONCERT_REGISTRY=&lt;YOUR PRIVATE REGISTRY&gt;\nexport CONCERT_REGISTRY_USER=&lt;YOUR PRIVATE REGISTRY USERNAME&gt;\nexport CONCERT_REGISTRY_PASSWORD=&lt;YOUR PRIVATE REGISTRY PASSWORD&gt;\nexport BUILD_VERSION=$(cat ibm-concert-std/etc/build_version)\n\nskopeo login ${CONCERT_PUBLIC_REGISTRY} -u ${CONCERT_PUBLIC_REGISTRY_USER} -p ${CONCERT_PUBLIC_REGISTRY_PASSWORD}\nskopeo login ${CONCERT_REGISTRY} -u ${CONCERT_REGISTRY_USER} -p ${CONCERT_REGISTRY_PASSWORD}\nIMAGES=(\n    ibm-roja-sra\n    ibm-roja-portal-gw\n    roja-ui\n    rojacore\n    ibm-roja-compliance\n    ibm-roja-pipeline\n    ibm-roja-ingestion\n    ibm-roja-py-utils\n    ibm-roja-postgres\n    ibm-roja-minio\n)\nfor image in \"${IMAGES[@]}\"; do\n    skopeo copy \\\n        docker://${CONCERT_PUBLIC_REGISTRY}/${image}:${BUILD_VERSION} \\\n        docker://${CONCERT_REGISTRY}/${image}:${BUILD_VERSION}\ndone\n</code></pre> <p>Once the images are mirrored, you can proceed with the installation steps.</p> Code <pre><code>${DOCKER_EXE} login ${CONCERT_REGISTRY} \\\n    --username=${CONCERT_REGISTRY_USER} --password=${CONCERT_REGISTRY_PASSWORD}\n</code></pre>"},{"location":"concert/installing/install-vm-old/#install-ibm-concert","title":"Install IBM Concert","text":"Code <pre><code>ibm-concert-std/bin/setup --license_acceptance=y \\\n    --registry=${CONCERT_REGISTRY} --runtime=${DOCKER_EXE} \\\n    --username=ibmconcert --password\n</code></pre> <p>If you don't specify a password with <code>--password=&lt;somepassword&gt;</code> the installation process will prompt you for one.</p> output <pre><code>$ ibm-concert-std/bin/setup --license_acceptance=y \\\n    --registry=${CONCERT_REGISTRY} --runtime=${DOCKER_EXE} \\\n    --username=ibmconcert --password\nEnter Password:\n</code></pre>"},{"location":"concert/installing/install-vm-old/#validate-ibm-concert-install","title":"Validate IBM Concert Install","text":"<p>Validate the IBM Concert install with the ibm-concert/bin/status script.  Ensure all services are <code>running</code>.</p> Code <pre><code>ibm-concert-std/bin/status\n</code></pre> output <pre><code>$ ibm-concert-std/bin/status\nibm-roja-minio : running\nibm-roja-postgres : running\nibm-roja-py-utils : running\nibm-roja-ingestion : running\nibm-roja-pipeline : running\nibm-roja-compliance : running\nibm-roja-portal-gw : running\nibm-roja-core : running\nibm-roja-ui : running\nibm-roja-mgmt : running\nibm-roja-sra : running\n\nURL:  https://your.server:12443\n</code></pre>"},{"location":"concert/installing/install-vm-old/#updating-ssl-certificate-on-ibm-concert","title":"Updating SSL Certificate on IBM Concert","text":"<p>The customer has installed a piece of vendor software that exposes a https service. IBM Concert generates a self-signed certificate and serves that.</p> <p>When connecting from a client, like a browser:</p> <ul> <li>self-signed certs by unknown issuers will trigger warnings in the browser and even after you accept it, it will continue to mark it as \"connection not secure\"</li> <li>in certain browser side UI situations, caching to local storage is not done when untrusted connections are in use. Hence that also leads to performance problems.</li> </ul> <p>As per security policies, most customers will not permit the use of such self-signed certs - but rather require certs that are signed by an official CA or by their own internal CA. For example, w3.ibm.com cert is issued/signed by Digicert (an official CA), but many other internal IBM sites serve certs issued by \"IBM INTERNAL INTERMEDIATE CA\".</p> <ul> <li>our browsers and command lines trust the internal issues and external issuers - and hence permit access to such websites.</li> </ul> <p>Certificates are issued to a specific host/domain. For example, if a customer installs Concert in a machine called \"my-vm-03.us.example.com\", then the certificate must match that host/domain name. Typically it is a \"wildcard\" cert issued to the domain - such as us.example.com.</p> <ul> <li>these certificate-key pair must be generated by a CA that is trusted by the clients, including browsers.</li> <li>these certificates are renewed and generally maintained by the customer, because only their CA can issue/sign such certs in the first place.</li> </ul> <p>To replace the self-signed certificate for IBM Concert, place a copy of the certificate and private key in the <code>ibm-concert-std/localstorage/volumes/infra/tls/external-tls/</code> folder and restart the <code>ibm-roja-portal-gw</code> service.  In this example, we are creating a self-signed certificate key pair.  For deployments on customer's systems, obtain real certificates signed by your customer's CA for their environment.</p> Code <pre><code>OUT=ibm-concert-std/localstorage/volumes/infra/tls/external-tls\nsudo openssl req -newkey rsa:2048 -noenc -keyout $OUT/tls.key -out $OUT/tls.csr \\\n    -subj \"/C=US/ST=North Carolina/L=Raleigh/O=IBM/OU=WW CS/CN=server.example\"\nsudo openssl x509 -signkey $OUT/tls.key -in $OUT/tls.csr -req -days 365 -out $OUT/tls.crt\nsudo chmod 644 $OUT/tls*\n./ibm-concert-std/bin/start_service ibm-roja-portal-gw\n</code></pre> <p></p>"},{"location":"concert/installing/install-vm-old/#log-in-to-ibm-concert","title":"Log in to IBM Concert","text":"<p>Log in to IBM Concert using <code>https://&lt;VM_PUBLIC_IP&gt;:12443</code> as your URL and the credentials from the previous steps.</p>"},{"location":"concert/installing/install-vm-old/#if-running-podman","title":"If running podman","text":"<p>On some linux distributions, when you log out of your shell, podman pods are destroyed.  You can persist them with the following command:</p> Code <pre><code>loginctl enable-linger $USER\n</code></pre>"},{"location":"concert/installing/install-vm/","title":"Installing IBM Concert &gt; VM Install","text":""},{"location":"concert/installing/install-vm/#installing-ibm-concert-vm-install","title":"Installing IBM Concert &gt; VM Install","text":""},{"location":"concert/installing/install-vm/#documentation","title":"Documentation","text":"<p>The following guide is derived from the IBM Concert Documentation and labs created by Sophie Martin and Gauthier Siri.  Please refer to the official IBM documentation for the latest up-to-date materials.</p>"},{"location":"concert/installing/install-vm/#lab-environments","title":"Lab Environments","text":""},{"location":"concert/installing/install-vm/#sizing","title":"Sizing","text":"<p>The recommended size for your Linux VM is a 16 CPU, 32GB RAM with 512GB for storage.</p>"},{"location":"concert/installing/install-vm/#techzone-reservation","title":"TechZone Reservation","text":"<p>Use a RHEL9 VM on IBM Cloud.  This lab has been written and tested on the following TechZone Certified Base Image in the IBM Cloud Environment.  Select this environment from this collection.</p> <p>In the reservation details, pick your Preferred Geography, and use the <code>16 vCPU | 64 GiB</code> VM Profile.</p> Bring Your Own VM <p>You can also bring your own Linux VM from your on-prem or cloud environment.  Ensure access to port 12443 is allowed to VM.  This lab exercise is based on RHEL9 VMs provisioned by the TechZone environment above, there may be changes that need to be accounted for when using other environments, but that is outside of the scope of this guide.</p> <p>On the reservation details, take note of the public IP address of your RHEL9 vm, as well as download the SSH key with the button provided</p> <p></p>"},{"location":"concert/installing/install-vm/#obtain-ibm-entitlement-key","title":"Obtain IBM Entitlement Key","text":"<p>To obtain your IBM entitlement API key perform the following steps:</p> <p>Log in to Container software library on My IBM with the IBMid and password that are associated with the entitled software. On the Entitlement keys tab, select Copy to copy the entitlement key to clipboard.</p>"},{"location":"concert/installing/install-vm/#install-prerequirements","title":"Install Prerequirements","text":"<p>SSH into the IBM Concert VM with the public IP and SSH key obtained from the TechZone reservation details.  You'll need to change permissions on the SSH key first before you SSH.</p> SSH <pre><code>chmod 600 ~/Downloads/pem_ibmcloudvsi_download.pem\nssh -i ~/Downloads/pem_ibmcloudvsi_download.pem -p 2223 itzuser@&lt;PUBLIC IP&gt;\n</code></pre> <p>Warning</p> <p>The installation steps need to be run on the Linux VM that will act as your IBM Concert server.  See the Before you being section in the IBM Concert documentation for VM requirements</p>"},{"location":"concert/installing/install-vm/#modify-the-techzone-machine-hostname","title":"Modify the TechZone Machine Hostname","text":"<p>In version 2.0.0, Concert can be joined only if the machine has a FQDN known by a DNS. This is not the case of TechZone VMs. You will need to change the VM hostname so that it can be resolvable. Replace YOUR_VM_PUBLIC_IP with the public IP defined in your TechZone reservation.</p> <pre><code>sudo hostnamectl set-hostname &lt;YOUR_VM_PUBLIC_IP&gt;.nip.io\n</code></pre> <p>Reboot the VM</p> <pre><code>sudo reboot\n</code></pre>"},{"location":"concert/installing/install-vm/#prepare-vm-disk","title":"Prepare VM disk","text":"<p>To prepare the 500Gb disk, execute the following commands. Note: if you run into any issues, it may be helpful to execute the commands one at a time.</p> Code <pre><code>sudo -i\nlsblk\nmkfs.ext4 -m 0 -E lazy_itable_init=0,lazy_journal_init=0,discard /dev/vdd\nblkid | grep /dev/vdd\n</code></pre> <p>Copy the UUID value output and store it in a safe place.</p> Code <pre><code>mkdir -p /mnt/concert\ncp /etc/fstab /etc/fstab.orig\nvi /etc/fstab\n</code></pre> <p>Insert this line at the end of the file: </p> Code <pre><code>UUID=YOUR_UUID /mnt/concert ext4 discard,defaults,nofail 0 0\n</code></pre> <p>Replace YOUR_UUID by the one listed by the blkid | grep /dev/vdd command      Save the file and continue with following commands:</p> Code <pre><code>mount -a\nsystemctl daemon-reload\nlsblk\nchmod 777 /mnt/concert\n</code></pre> Required Utilities podman gitwgetjq <p>Docker Desktop is no longer allowed for IBM workstations, so we will use podman. Refer to the Podman Install Instructions if you havent installed podman in your workstation.  The podman and docker commands are interchangeable throughout this guide.</p> <p>You can obtain the git CLI following the official Git Documentation.</p> <p><code>wget</code> should be installed by default on your Linux system.  If its not, follow the install instructions on the official GNU Wget website.</p> <p><code>jq</code> should be installed by default on your Linux system.  If its not, follow the install instructions on the official jq website.</p> <p>This lab is based on a RHEL9 VM in TechZone, to install all prereqs at once there, you can run the following command</p> <pre><code>sudo dnf install -y podman git wget jq\n</code></pre>"},{"location":"concert/installing/install-vm/#provision-watsonxai-on-techzone","title":"Provision watsonx.ai on TechZone","text":"<p>Note: Concert require model ibm/granite-3-2-8b-instruct on watsonx.ai, so watsonx.ai must be provision in US </p>"},{"location":"concert/installing/install-vm/#watsonxai-provisioning","title":"Watsonx.ai Provisioning","text":"<ol> <li> <p>Navigate to watsonx.ai on IBM Techzone</p> </li> <li> <p>Make a reservation for IBM watsonx.ai and choose AMERICAS as preferred geography (at the time we write this lab, only this region has the model used by IBM Concert inferences)</p> </li> <li> <p>When the reservation is ready, you should receive a mail from IBM Cloud to join an account in IBM Cloud.</p> </li> <li> <p>Open the mail, click the join now link and follow the instructions.</p> <p></p> </li> <li> <p>When you have the IBM Cloud first screen, verify that you have the good account selected on the top bar</p> <p></p> </li> </ol> <p>Note: The IBM Cloud account selected should match the Cloud Account specified in the IBM TechZone reservation.</p> <p></p>"},{"location":"concert/installing/install-vm/#create-a-watsonx-project-and-get-project-id","title":"Create a watsonx project and get project ID","text":"<ol> <li> <p>Select watsonx to from the burger menu on the left</p> <p></p> </li> <li> <p>Click Launch in the watsonx.ai tile</p> <p></p> </li> <li> <p>Scroll down in the page that appear and click Create a sandbox project in the Projects tile</p> <p></p> </li> <li> <p>Select the sandbox that have been created (you may need to refresh the page)</p> <p></p> </li> <li> <p>In Manage tab, copy the \"Project ID\" and store it in a safe place</p> <p></p> </li> </ol>"},{"location":"concert/installing/install-vm/#get-api-key-and-service-id-information","title":"Get API Key and service ID information","text":"<ul> <li> <p>From your techzone reservation screen, retrieve the APIKey and the service ID and store them in a safe place</p> <p></p> </li> </ul>"},{"location":"concert/installing/install-vm/#api-key-import-the-service-id-as-part-of-the-project","title":"API key - import the Service ID as part of the project","text":"<ol> <li> <p>From your watsonx screen, in the manage tab, select Access control in the left menu</p> <p></p> </li> <li> <p>Click the Add Collaborators button and select Add Access Group.</p> <p></p> </li> <li> <p>Enter your Access Group name. The access Group Name is the IBM Cloud Service ID on your TechZone reservation page.</p> <p></p> </li> <li> <p>Select your group, give it admin right and click Add button</p> <p></p> </li> </ol>"},{"location":"concert/installing/install-vm/#installing-ibm-concert","title":"Installing IBM Concert","text":"<p>Warning</p> <p>IMPORTANT: if you are already logged on the VM, verify that you are connected as itzuser (not root). You may need to open a new terminal window and re-ssh into the machine. The installation steps need to be run on the itzuser of the Linux VM that will act as your IBM Concert server.  See the Before you being section in the IBM Concert documentation for VM requirements. </p>"},{"location":"concert/installing/install-vm/#download-installation-artifacts","title":"Download installation artifacts","text":"Code <pre><code>loginctl enable-linger itzuser\n\nLATEST_TAG=$(curl --silent https://api.github.com/repos/IBM/Concert/releases/latest | jq -r .tag_name)\ncd /mnt/concert\nwget https://github.com/IBM/Concert/releases/download/${LATEST_TAG}/ibm-concert-std.tgz\ntar -xf ibm-concert-std.tgz\n</code></pre>"},{"location":"concert/installing/install-vm/#set-up-environment-variables","title":"Set up environment variables","text":"Create a $HOME/env.sh file <pre><code>vi $HOME/env.sh\n</code></pre> <p>Copy and paste the content of env.sh in this $HOME/env.sh file Update the values for the following keys (other keys will be updated later):</p> <ul> <li>CONCERT_REGISTRY_PASSWORD: your entitlement key surrounded by double quotes</li> </ul> <p>Save the file (:wq)</p> Source the $HOME/env.sh file to set environment variables <pre><code>source $HOME/env.sh\n</code></pre>"},{"location":"concert/installing/install-vm/#log-in-to-the-ibm-container-registry","title":"Log in to the IBM Container Registry","text":"Notes on Airgap Environments <p>If you're deploying on an AirGap environment, you will first need to mirror the images to your private container image registry.  You will need to install skopeo on a workstation with access to both cp.icr.io and to the private image registry where images will be mirrored.</p> <pre><code>export CONCERT_PUBLIC_REGISTRY=cp.icr.io/cp/concert\nexport CONCERT_PUBLIC_REGISTRY_USER=cp\nexport CONCERT_PUBLIC_REGISTRY_PASSWORD=\"eyJh...\"  ## Your IBM Entitlement Key\nexport CONCERT_REGISTRY=&lt;YOUR PRIVATE REGISTRY&gt;\nexport CONCERT_REGISTRY_USER=&lt;YOUR PRIVATE REGISTRY USERNAME&gt;\nexport CONCERT_REGISTRY_PASSWORD=&lt;YOUR PRIVATE REGISTRY PASSWORD&gt;\nexport BUILD_VERSION=$(cat ibm-concert-std/etc/build_version)\n\nskopeo login ${CONCERT_PUBLIC_REGISTRY} -u ${CONCERT_PUBLIC_REGISTRY_USER} -p ${CONCERT_PUBLIC_REGISTRY_PASSWORD}\nskopeo login ${CONCERT_REGISTRY} -u ${CONCERT_REGISTRY_USER} -p ${CONCERT_REGISTRY_PASSWORD}\nIMAGES=(\n    ibm-roja-sra\n    ibm-roja-portal-gw\n    roja-ui\n    rojacore\n    ibm-roja-compliance\n    ibm-roja-pipeline\n    ibm-roja-ingestion\n    ibm-roja-py-utils\n    ibm-roja-postgres\n    ibm-roja-minio\n)\nfor image in \"${IMAGES[@]}\"; do\n    skopeo copy \\\n        docker://${CONCERT_PUBLIC_REGISTRY}/${image}:${BUILD_VERSION} \\\n        docker://${CONCERT_REGISTRY}/${image}:${BUILD_VERSION}\ndone\n</code></pre> <p>Once the images are mirrored, you can proceed with the installation steps.</p> Code <pre><code>${DOCKER_EXE} login ${CONCERT_REGISTRY} \\\n    --username=${CONCERT_REGISTRY_USER} --password=${CONCERT_REGISTRY_PASSWORD}\n</code></pre>"},{"location":"concert/installing/install-vm/#install-ibm-concert","title":"Install IBM Concert","text":"Code <pre><code>ibm-concert-std/bin/setup --license_acceptance=y \\\n    --registry=${CONCERT_REGISTRY} --runtime=${DOCKER_EXE} \\\n    --username=ibmconcert --password\n</code></pre> <p>If you don't specify a password with <code>--password=&lt;somepassword&gt;</code> the installation process will prompt you for one.</p> output <pre><code>$ ibm-concert-std/bin/setup --license_acceptance=y \\\n    --registry=${CONCERT_REGISTRY} --runtime=${DOCKER_EXE} \\\n    --username=ibmconcert --password\nEnter Password:\n</code></pre>"},{"location":"concert/installing/install-vm/#validate-ibm-concert-install","title":"Validate IBM Concert Install","text":"<p>Validate the IBM Concert install with the ibm-concert/bin/status script.  Ensure all services are <code>running</code>.</p> Code <pre><code>ibm-concert-std/bin/status\n</code></pre> output <pre><code>$ ibm-concert-std/bin/status\nibm-roja-minio : running\nibm-roja-postgres : running\nibm-roja-py-utils : running\nibm-roja-ingestion : running\nibm-roja-pipeline : running\nibm-roja-compliance : running\nibm-roja-portal-gw : running\nibm-roja-core : running\nibm-roja-ui : running\nibm-roja-mgmt : running\nibm-roja-sra : running\n\nURL:  https://your.server:12443\n</code></pre>"},{"location":"concert/installing/install-vm/#updating-ssl-certificate-on-ibm-concert","title":"Updating SSL Certificate on IBM Concert","text":"<p>The customer has installed a piece of vendor software that exposes a https service. IBM Concert generates a self-signed certificate and serves that.</p> <p>When connecting from a client, like a browser:</p> <ul> <li>self-signed certs by unknown issuers will trigger warnings in the browser and even after you accept it, it will continue to mark it as \"connection not secure\"</li> <li>in certain browser side UI situations, caching to local storage is not done when untrusted connections are in use. Hence that also leads to performance problems.</li> </ul> <p>As per security policies, most customers will not permit the use of such self-signed certs - but rather require certs that are signed by an official CA or by their own internal CA. For example, w3.ibm.com cert is issued/signed by Digicert (an official CA), but many other internal IBM sites serve certs issued by \"IBM INTERNAL INTERMEDIATE CA\".</p> <ul> <li>our browsers and command lines trust the internal issues and external issuers - and hence permit access to such websites.</li> </ul> <p>Certificates are issued to a specific host/domain. For example, if a customer installs Concert in a machine called \"my-vm-03.us.example.com\", then the certificate must match that host/domain name. Typically it is a \"wildcard\" cert issued to the domain - such as us.example.com.</p> <ul> <li>these certificate-key pair must be generated by a CA that is trusted by the clients, including browsers.</li> <li>these certificates are renewed and generally maintained by the customer, because only their CA can issue/sign such certs in the first place.</li> </ul> <p>To replace the self-signed certificate for IBM Concert, place a copy of the certificate and private key in the <code>ibm-concert-std/localstorage/volumes/infra/tls/external-tls/</code> folder and restart the <code>ibm-roja-portal-gw</code> service.  In this example, we are creating a self-signed certificate key pair.  For deployments on customer's systems, obtain real certificates signed by your customer's CA for their environment.</p> Code <pre><code>OUT=ibm-concert-std/localstorage/volumes/infra/tls/external-tls\nsudo openssl req -newkey rsa:2048 -noenc -keyout $OUT/tls.key -out $OUT/tls.csr \\\n    -subj \"/C=US/ST=North Carolina/L=Raleigh/O=IBM/OU=WW CS/CN=server.example\"\nsudo openssl x509 -signkey $OUT/tls.key -in $OUT/tls.csr -req -days 365 -out $OUT/tls.crt\nsudo chmod 644 $OUT/tls*\n./ibm-concert-std/bin/start_service ibm-roja-portal-gw\n</code></pre> <p></p>"},{"location":"concert/installing/install-vm/#log-in-to-ibm-concert","title":"Log in to IBM Concert","text":"<p>Log in to IBM Concert using <code>https://&lt;VM_PUBLIC_IP&gt;:12443</code> as your URL and the credentials from the previous steps.</p>"},{"location":"concert/installing/install-vm/#set-up-concert-and-watsonx-integration","title":"Set up Concert and watsonx integration","text":"<p>In the following steps, YOUR_VM_IP is the public IP defined in your Techzone reservation</p> <ol> <li>From a browser go to the Concert URL (https://YOUR_VM_IP:12443)</li> <li>Login to Concert using ibmconcert as the username and the password you created in step 4.</li> <li> <p>Click the circle at top right of the window and select API Key</p> <p></p> </li> <li> <p>In the API Key window, click Generate API Key</p> <p></p> </li> <li> <p>Copy the API key generated and save in a safe place</p> </li> <li> <p>Update environment variables</p> <pre><code>vi $HOME/env.sh\n</code></pre> <p>Update the values for the following keys:</p> <ul> <li>CONCERT_APIKEY with the API Key you created in step 5.</li> <li>CONCERT_URL with https://YOUR_VM_IP:12443 (replace YOUR_VM_IP with your own VM address)</li> <li>WATSONX_API_KEY: use the API key you got in Lab 0 - Get API Key and service ID information, from your TechZone wx.ai reservation page</li> <li>WATSONX_API_PROJECT_ID: use the project ID you got from Lab 0 - Create a watsonx project and get project ID,</li> <li>WATSONX_API_URL: https://us-south.ml.cloud.ibm.com , since the instance is provision in US. Save the file (:wq) and source the $HOME/env.sh file to set environment variables</li> </ul> <pre><code>source $HOME/env.sh\n</code></pre> </li> <li> <p>Apply the watsonx.ai configuration</p> <pre><code>echo \"WATSONX_API_KEY=$WATSONX_API_KEY\" &gt;&gt; ibm-concert-std/etc/local_config.env\necho \"WATSONX_API_PROJECT_ID=$WATSONX_API_PROJECT_ID\" &gt;&gt; ibm-concert-std/etc/local_config.env\necho \"WATSONX_API_URL=$WATSONX_API_URL\" &gt;&gt; ibm-concert-std/etc/local_config.env\n</code></pre> </li> <li> <p>Then you need to start the appropriate service:</p> <pre><code>cd /mnt/concert\nibm-concert-std/bin/start_service ibm-roja-py-utils\n</code></pre> </li> <li> <p>Test the integration </p> <p>To verify that the integration with watsonx.ai is successfull, you can look at the ibm-roja-py-utils pod logs:</p> <pre><code>podman logs ibm-roja-py-utils\n</code></pre> </li> </ol> The 2 first lines of the logs should be: <pre><code>{'timestamp': 2025-04-13:13:45:15, 'logLevel': info, 'callerMethod': client.py:L459, 'message': Client successfully initialized}\n{'timestamp': 2025-04-13:13:45:15, 'logLevel': info, 'callerMethod': genai.py:L129, 'message': Connection to watsonx.ai successful!}\n</code></pre>"},{"location":"concert/installing/install-vm/#create-an-ibm-github-token-to-clone-the-github-repo","title":"Create an IBM Github token to clone the GitHub repo","text":"<p>Create a GitHub Token to clone using cli thru HTTPS:</p> <ul> <li>Login to https://github.ibm.com</li> <li>Click your profile in the top right</li> <li>Settings \u2014&gt; Developer settings \u2014&gt; Personal access tokens</li> <li>Click Generate new token</li> <li>Enter a name you can remember, for example IBM GitHub Clone</li> <li>Check repo, the underlying boxes such as <code>repo:status</code> will also be checked</li> <li>Click Generate token</li> <li>Copy your token and store it in a safe place as you won't be able to retrieve it (but you can generate a new one)</li> </ul>"},{"location":"concert/labsv2/","title":"IBM Concert bootcamp","text":""},{"location":"concert/labsv2/#ibm-concert-bootcamp","title":"IBM Concert bootcamp","text":"<p>These materials were made by Sophie Martin and Gauthier Siri</p> <p>In this bootcamp, based on a simple python application composed of 2 micro services, you will learn to:</p> <ul> <li>Lab0 - Reserve required environment on techzone</li> <li>Lab1 - Install IBM Concert</li> <li>Lab2 - Install Concert Workflow</li> <li>Lab3 - Concert walkthrough</li> <li>Lab4 - Upload applications SBOMS in IBM Concert and manage software composition</li> <li>Lab5 - Upload applications certificates in IBM Concert and manage operations</li> <li>Lab6 - Upload Compliance data in IBM Concert and manage compliance</li> <li>Lab7 - Upload Resilience data in IBM Concert and manage resilience</li> <li>Lab8 - Auto-discovery and Auto-resilience</li> </ul> <p>All the labs are based on Concert Version 2.0.0.    In order to run the labs, you will provision on IBM Techzone some environments.     </p> <p>Here is the architecture schema of your environment:    </p> <p></p>"},{"location":"concert/labsv2/labs/Lab0-setup/","title":"IBM Concert bootcamp - Lab setup","text":""},{"location":"concert/labsv2/labs/Lab0-setup/#ibm-concert-bootcamp-lab-setup","title":"IBM Concert bootcamp - Lab setup","text":""},{"location":"concert/labsv2/labs/Lab0-setup/#objective","title":"Objective","text":"<p>This lab covers the prerequisite to the IBM Concert bootcamp.</p> <p>It will cover: -  booking a VM on Techzone, where IBM Concert will be deployed -  booking a watsonx.ai instance on Techzone -  creating a github token to clone the required files</p>"},{"location":"concert/labsv2/labs/Lab0-setup/#prerequisite","title":"Prerequisite","text":"<ul> <li>You must have:</li> <li>An IBM Cloud user</li> <li> <p>An account on https://github.ibm.com</p> </li> <li> <p>On your laptop, you must have</p> </li> <li>A ssh client</li> <li>A web browser</li> </ul>"},{"location":"concert/labsv2/labs/Lab0-setup/#content","title":"Content","text":"<ul> <li>IBM Concert bootcamp - Lab setup</li> <li>Objective</li> <li>Prerequisite</li> <li>Content</li> <li>I - Provision a vm on techzone<ul> <li>Provision the VM</li> <li>Modify the techzone machine hostname</li> <li>Prepare VM disk</li> </ul> </li> <li>II - Provision a watsonx.ai on techzone<ul> <li>Watsonx.ai Provisioning</li> <li>Create a watsonx project and get project ID</li> <li>Get API Key and service ID information</li> <li>API key - import the Service ID as part of the project</li> </ul> </li> <li>III - Create an IBM Github token to clone the github repo</li> </ul>"},{"location":"concert/labsv2/labs/Lab0-setup/#i-provision-a-vm-on-techzone","title":"I - Provision a vm on techzone","text":""},{"location":"concert/labsv2/labs/Lab0-setup/#provision-the-vm","title":"Provision the VM","text":"<p>Provision your concert VM from this link.  The minimum requirements to install IBM Concert and Concert workflox are: 16 vCPUs/32GB RAM/512GB Disk.    On the provisionned VM, a 500GB disk is present but you need to prepare it.</p> <p>Once your reservation is provisioned, all the connection information are available from the reservation page:</p> <p></p> <p>From your reservation page, download the ssh key to connect to your VM instance. Save it somewhere, you will need to use it to connect to your instance using SSH.</p> <p>Note: On Linux/MacOS, you need to modify the SSH Key permission to be able to use it. </p><pre><code>chmod 600 /path/to/concert/sshkey/pem_ibmcloudvsi_download.pem\n</code></pre><p></p>"},{"location":"concert/labsv2/labs/Lab0-setup/#modify-the-techzone-machine-hostname","title":"Modify the techzone machine hostname","text":"<p>In version 2.0.0, concert can be joined only if the machine has a FQDN known by a DNS. This is not the case of techzone VMs. You will then change the vm hostname so that it can be resolvable. Replace YOUR_VM_PUBLIC_IP with the public IP defined in your Techzone reservation.</p> <pre><code>sudo hostnamectl set-hostname &lt;YOUR_VM_PUBLIC_IP&gt;.nip.io\n</code></pre> <ol> <li>Reboot the VM</li> </ol> <pre><code>sudo reboot\n</code></pre>"},{"location":"concert/labsv2/labs/Lab0-setup/#prepare-vm-disk","title":"Prepare VM disk","text":"<ul> <li>To prepare the 500Gb disk, execute the following commands:</li> </ul> <pre><code>ssh itzuser@&lt;VM ip address&gt; -p 2223 -i /path/to/concert/sshkey/pem_ibmcloudvsi_download.pem\nsudo -i\nlsblk\nmkfs.ext4 -m 0 -E lazy_itable_init=0,lazy_journal_init=0,discard /dev/vdd\nblkid | grep /dev/vdd\n</code></pre> <ul> <li>Copy the UUID value and store it in a safe place</li> </ul> <pre><code>mkdir -p /mnt/concert\n\ncp /etc/fstab /etc/fstab.orig\nvi /etc/fstab\n</code></pre> <ul> <li>Insert this line at the end of the file: </li> </ul> <pre><code>UUID=YOUR_UUID /mnt/concert ext4 discard,defaults,nofail 0 0\n</code></pre> <ul> <li>Replace YOUR_UUID by the one listed by the blkid | grep /dev/vdd command       </li> <li>Save the file and continue with following commands:</li> </ul> <pre><code>mount -a\nsystemctl daemon-reload\nlsblk\nchmod 777 /mnt/concert\n</code></pre>"},{"location":"concert/labsv2/labs/Lab0-setup/#ii-provision-a-watsonxai-on-techzone","title":"II - Provision a watsonx.ai on techzone","text":"<p>Note: Concert require model ibm/granite-3-2-8b-instruct on watsonx.ai, so watsonx.ai must be provision in US </p>"},{"location":"concert/labsv2/labs/Lab0-setup/#watsonxai-provisioning","title":"Watsonx.ai Provisioning","text":"<ol> <li> <p>Navigate to watsonx.ai on IBM Techzone</p> </li> <li> <p>Make a reservation for IBM watsonx.ai and choose AMERICAS as preferred geography (at the time we write this lab, only this region has the model used by IBM Concert inferences)</p> </li> <li> <p>When the reservation is ready, you should receive a mail from IBM Cloud to join an account in IBM Cloud.</p> </li> <li> <p>Open the mail, click the join now link and follow the instructions.</p> </li> </ol> <p></p> <ol> <li>When you have the IBM Cloud first screen, verify that you have the good account selected on the top bar</li> </ol> <p></p> <p>The IBM Cloud account selected should match the Cloud Account specified in the IBM TechZone reservation.</p> <p></p>"},{"location":"concert/labsv2/labs/Lab0-setup/#create-a-watsonx-project-and-get-project-id","title":"Create a watsonx project and get project ID","text":"<ol> <li>Select watsonx to from the burger menu on the left</li> </ol> <ol> <li>Click Launch in the watsonx.ai tile</li> </ol> <ol> <li>Scroll down in the page that appear and click Create a sandbox project in the Projects tile</li> </ol> <ol> <li>Select the sandbox that have been created (you should need to refresh the page)</li> </ol> <ol> <li>In Manage tab, copy the \"Project ID\" and store it in a safe place</li> </ol>"},{"location":"concert/labsv2/labs/Lab0-setup/#get-api-key-and-service-id-information","title":"Get API Key and service ID information","text":"<ol> <li>From your techzone reservation screen, retrieve the APIKey and the service ID and store them in a safe place</li> </ol>"},{"location":"concert/labsv2/labs/Lab0-setup/#api-key-import-the-service-id-as-part-of-the-project","title":"API key - import the Service ID as part of the project","text":"<ol> <li>From your watsonx screen, in the manage tab, select Access control in the left menu</li> </ol> <ol> <li>Click the Add Collaborators button and select Add Access Group.</li> </ol> <ol> <li>Enter your Access Group name. The access Group Name is the IBM Cloud Service ID on your TechZone reservation page.</li> </ol> <ol> <li>Select your group, give it admin right and click Add button</li> </ol>"},{"location":"concert/labsv2/labs/Lab0-setup/#iii-create-an-ibm-github-token-to-clone-the-github-repo","title":"III - Create an IBM Github token to clone the github repo","text":"<p>Create a GitHub Token to clone using cli thru HTTPS:</p> <ul> <li>Login to https://github.ibm.com</li> <li>Click your profile in the right top</li> <li>Settings \u2014&gt; Developer settings \u2014&gt; Personal access tokens</li> <li>Click Generate new token</li> <li>Enter a name you can remember, for example IBM GitHub Clone</li> <li>Check repo, the underlying boxes such as <code>repo:status</code> will also be checked</li> <li>Click Generate token</li> <li>Copy your token and store it in a safe place as you won't be able to retrieve it (but you can generate a new one)</li> </ul>"},{"location":"concert/labsv2/labs/Lab1-concert-installation-vm/","title":"IBM Concert installation on a virtual machine","text":""},{"location":"concert/labsv2/labs/Lab1-concert-installation-vm/#ibm-concert-installation-on-a-virtual-machine","title":"IBM Concert installation on a virtual machine","text":""},{"location":"concert/labsv2/labs/Lab1-concert-installation-vm/#objective","title":"Objective","text":"<p>In this lab, you will install IBM Concert on a standalone server.</p>"},{"location":"concert/labsv2/labs/Lab1-concert-installation-vm/#prerequisite","title":"Prerequisite","text":"<ul> <li>An virtual machine must have been provisionned on Techzone and configured as explained in Lab0</li> </ul>"},{"location":"concert/labsv2/labs/Lab1-concert-installation-vm/#content","title":"Content","text":"<ul> <li>IBM Concert installation on a virtual machine</li> <li>Objective</li> <li>Prerequisite</li> <li>Content</li> <li>I - Installing IBM Concert on a VM</li> <li>II - Watsonx.ai integration<ul> <li>Techzone reservation</li> <li>Configure watsonx.ai in IBM Concert</li> </ul> </li> </ul>"},{"location":"concert/labsv2/labs/Lab1-concert-installation-vm/#i-installing-ibm-concert-on-a-vm","title":"I - Installing IBM Concert on a VM","text":"<p>Official documentation VM installation</p> <ol> <li>Connect on the machine you have provisioned on Techzone in Lab0</li> </ol> <p>IMPORTANT: if you are already logged on the VM, verify that you are connected as itzuser (not root).   </p> <pre><code>ssh itzuser@&lt;VM ip address&gt; -p 2223 -i /path/to/concert/sshkey/pem_ibmcloudvsi_download.pem\n</code></pre> <ol> <li>Change the umask in .bashrc file</li> </ol> <pre><code>echo \"umask 022\" &gt;&gt; $HOME/.bashrc\nsource $HOME/.bashrc\n</code></pre> <ol> <li>Start the installation</li> </ol> <pre><code>loginctl enable-linger itzuser\ncd /mnt/concert\nwget https://github.com/IBM/Concert/releases/download/v2.0.0.1/ibm-concert.tar.gz\ntar xfz ibm-concert.tar.gz\n</code></pre> <ol> <li>Create a $HOME/env.sh file</li> </ol> <pre><code>vi $HOME/env.sh\n</code></pre> <p>Copy Paste the content of env.sh in this $HOME/env.sh file Update the values for the following keys (other keys will be updated later):</p> <ul> <li>CONCERT_REGISTRY_PASSWORD: your entitlement key surrounded by double quotes</li> </ul> <p>Save the file (:wq)</p> <ol> <li>Source the $HOME/env.sh file to set environment variables</li> </ol> <pre><code>source $HOME/env.sh\n</code></pre> <ol> <li>Configure the Concert parameter file</li> </ol> <pre><code>cd $INSTALL_DIR\ncp $INSTALL_DIR/etc/sample-params/concert-vm-quick-start-params.ini $INSTALL_DIR/etc/params.ini\n</code></pre> <ol> <li>Edit the params.ini file with the required parameters</li> </ol> <pre><code>DOCKER_EXE=podman\n\nINSTALL_VM=true\nINSTALL_CONCERT=true\nIMAGE_REGISTRY_PREFIX=cp.icr.io/cp\nHUB_IMAGE_REGISTRY_SUFFIX=/solis-hub\nCONCERT_IMAGE_REGISTRY_SUFFIX=/concert\n</code></pre> <ol> <li>Install concert</li> </ol> <p>Replace CONCERT_USER and CONCERT_PASSWORD by values of your choice</p> <pre><code>${DOCKER_EXE} login ${IBM_REGISTRY} --username=${IBM_REGISTRY_USER} --password=${IBM_REGISTRY_PASSWORD}\n$INSTALL_DIR/bin/setup --license_acceptance=y --username=CONCERT_USER --password=CONCERT_PASSWORD\n</code></pre> <p>The installation take 5 to 7 minutes, be patient.    </p> <ol> <li>Connect on Concert and create an API Key</li> </ol> <p>In the following steps, YOUR_VM_PUBLIC_IP is the public IP defined in your Techzone reservation</p> <ul> <li>From a browser go to the Concert URL (https://YOUR_VM_PUBLIC_IP.nip.io:12443)</li> <li>Log on concert using ibmconcert as user and with the password you have specified in step 4.</li> <li>Click the circle at top right of the window and select API Key </li> <li>In the API Key window, click Generate API Key </li> <li>Copy the API key generated in a safe place</li> </ul>"},{"location":"concert/labsv2/labs/Lab1-concert-installation-vm/#ii-watsonxai-integration","title":"II - Watsonx.ai integration","text":""},{"location":"concert/labsv2/labs/Lab1-concert-installation-vm/#techzone-reservation","title":"Techzone reservation","text":"<p>Be sure to reserve a watsonx.ai instance as explained in Lab 0 - III - Provision a watsonx.ai on techzone.</p>"},{"location":"concert/labsv2/labs/Lab1-concert-installation-vm/#configure-watsonxai-in-ibm-concert","title":"Configure watsonx.ai in IBM Concert","text":"<p>The watsonx.ai integration is simply done through setting some config parameters in the config files of IBM concert.  </p> <ol> <li>You will need to update the $HOME/env.sh file. <pre><code>vim $HOME/env.sh\n</code></pre></li> </ol> <p>Update the following variables: - WATSONX_API_KEY: use the API key you got in Lab 0 - Get API Key and service ID information, from your techzone wx.ai reservation page - WATSONX_API_PROJECT_ID: use the project ID you got from Lab 0 - Create a watsonx project and get project ID, - WATSONX_API_URL: https://us-south.ml.cloud.ibm.com , since the instance is provision in US.</p> <p>Save the file (:wq) and source the $HOME/env.sh file to set environment variables</p> <pre><code>source $HOME/env.sh\n</code></pre> <ol> <li>Apply the watsonx.ai configuration</li> </ol> <pre><code>echo \"WATSONX_API_KEY=$WATSONX_API_KEY\" &gt;&gt; ibm-concert-std/etc/local_config.env\necho \"WATSONX_API_PROJECT_ID=$WATSONX_API_PROJECT_ID\" &gt;&gt; ibm-concert-std/etc/local_config.env\necho \"WATSONX_API_URL=$WATSONX_API_URL\" &gt;&gt; ibm-concert-std/etc/local_config.env\n</code></pre> <ol> <li>Then you need to start the appropriate service:</li> </ol> <pre><code>cd /mnt/concert\nibm-concert-std/bin/start_service ibm-roja-py-utils\n</code></pre> <ol> <li>Test the integration </li> </ol> <p>To verify that the integration with watsonx.ai is successfull, you can look at the ibm-roja-py-utils pod logs:</p> <pre><code>podman logs ibm-roja-py-utils\n</code></pre> <p>The 2 first lines of the logs should be:</p> <pre><code>{'timestamp': 2025-04-13:13:45:15, 'logLevel': info, 'callerMethod': client.py:L459, 'message': Client successfully initialized}\n{'timestamp': 2025-04-13:13:45:15, 'logLevel': info, 'callerMethod': genai.py:L129, 'message': Connection to watsonx.ai successful!}\n</code></pre>"},{"location":"concert/labsv2/labs/Lab2-concert-workflow-installation/","title":"Concert workflow installation","text":""},{"location":"concert/labsv2/labs/Lab2-concert-workflow-installation/#concert-workflow-installation","title":"Concert workflow installation","text":""},{"location":"concert/labsv2/labs/Lab2-concert-workflow-installation/#objective","title":"Objective","text":"<p>Concert workflow is an embedded version of Rapid Infrastructure Automation in IBM Concert and is available as an add-on workflow automation service for on-premises deployments of Concert.  The add-on embeds workflow definition and automation capabilities so you can define, manage, and automate workflows within the Concert UI. The objective is to get data from an organisation environments and applications using flows.  </p> <p>By default a flow is executed in a worker located on IBM Concert host. A notion of remote worker exist in Concert Workflow to enable the ingestion of data in IBM Concert from environments that cannot be reached directly by IBM Concert host. The remote worker is located near from the environment, collect required data and ingest them to IBM Concert using Concert APIs.</p> <p>In this lab, you will install IBM Concert workflow as an add-on of your concert installation.</p>"},{"location":"concert/labsv2/labs/Lab2-concert-workflow-installation/#prerequisite","title":"Prerequisite","text":"<ul> <li>IBM Concert must be installed</li> </ul>"},{"location":"concert/labsv2/labs/Lab2-concert-workflow-installation/#content","title":"Content","text":"<ul> <li>Concert workflow installation</li> <li>Objective</li> <li>Prerequisite</li> <li>Content</li> <li>Install IBM Concert workflow with concert installed on a VM<ul> <li>Install preprequisites</li> <li>Install Concert workflow</li> </ul> </li> </ul>"},{"location":"concert/labsv2/labs/Lab2-concert-workflow-installation/#install-ibm-concert-workflow-with-concert-installed-on-a-vm","title":"Install IBM Concert workflow with concert installed on a VM","text":"<p>Concert workflow installation require k3s and helm.</p>"},{"location":"concert/labsv2/labs/Lab2-concert-workflow-installation/#install-preprequisites","title":"Install preprequisites","text":"<ol> <li>Connect to the VM you have created on Techzone in Lab0</li> </ol> <pre><code>ssh itzuser@&lt;VM ip address&gt; -p 2223 -i /path/to/concert/sshkey/pem_ibmcloudvsi_download.pem\n</code></pre> <ol> <li>install k3s</li> </ol> <p>Offical documentation k3s installation</p> <pre><code>cd $HOME\ncurl -sfL https://get.k3s.io | sudo INSTALL_K3S_VERSION=v1.33.3+k3s1 sh -s - --write-kubeconfig-mode 644 --disable traefik\n</code></pre> <ol> <li>Specify Kubernetes configuration file</li> </ol> <pre><code>echo \"export KUBECONFIG=/etc/rancher/k3s/k3s.yaml\" &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre> <ol> <li>Insecure access to the internal registry must be enabled and registries.yaml must be defined under /etc/rancher/k3s:</li> </ol> <p>Replace YOUR_VM_PUBLIC_IP with the public IP defined in your Techzone reservation.</p> <pre><code>sudo vi /etc/rancher/k3s/registries.yaml\n</code></pre> <p>Insert</p> <pre><code>configs:\n  \"YOUR_VM_PUBLIC_IP.nip.io\": \n    \"tls\": \n       insecure_skip_verify: true\n</code></pre> <p>Restart the Kubernetes service</p> <pre><code>sudo systemctl restart k3s\n</code></pre> <ol> <li> <p>Install Helm</p> </li> <li> <p>Add /usr/local/bin in your path</p> </li> </ol> <pre><code>echo \"export PATH=$PATH:/usr/local/bin\" &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre> <ul> <li>Download and install helm</li> </ul> <pre><code>curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\nchmod 700 get_helm.sh\n./get_helm.sh\n</code></pre>"},{"location":"concert/labsv2/labs/Lab2-concert-workflow-installation/#install-concert-workflow","title":"Install Concert workflow","text":"<p>Official documentation: Concert Worflow installation</p> <ol> <li>Connect to the VM you have created on Techzone in Lab0 and source environment variables</li> </ol> <pre><code>ssh itzuser@&lt;VM ip address&gt; -p 2223 -i /path/to/concert/sshkey/pem_ibmcloudvsi_download.pem\n</code></pre> <ol> <li>Configure the Concert Workflow parameter file</li> </ol> <pre><code>source $HOME/env.sh\ncd $INSTALL_DIR\ncp $INSTALL_DIR/etc/sample-params/workflows-quickstart-vm-params.ini $INSTALL_DIR/etc/params.ini\n</code></pre> <ol> <li>Edit the params.ini file with the required parameters</li> </ol> <p>Replace YOUR_VM_PUBLIC_IP with the public IP defined in your Techzone reservation.</p> <pre><code>DOCKER_EXE=podman\n\nINSTALL_VM=true\nINSTALL_WORKFLOWS=true\n# Registry users\nREG_USER=cp\nIMAGE_REGISTRY_PREFIX=cp.icr.io/cp\nHUB_IMAGE_REGISTRY_SUFFIX=/solis-hub\nWORKFLOWS_IMAGE_REGISTRY_SUFFIX=/concert         \nWORKFLOWS_INSTANCE_ADDRESS=YOUR_VM_PUBLIC_IP.nip.io\n</code></pre> <ol> <li>Install Concert Workflow</li> </ol> <p>Replace  and  by your own values</p> <pre><code>${DOCKER_EXE} login ${IBM_REGISTRY} --username=${IBM_REGISTRY_USER} --password=${IBM_REGISTRY_PASSWORD}\n$INSTALL_DIR/bin/setup --license_acceptance=y --username=&lt;user&gt; --password=&lt;password&gt; --registry_password=${IBM_REGISTRY_PASSWORD}\n</code></pre> <p>IMPORTANT: Wait until the end of the installation. Be patient, it can take up to 20 minutes. It's time for a coffee break !</p> <ol> <li> <p>Check Concert Workflow installation</p> </li> <li> <p>From a browser, enter the URL of your concert instance (https://YOUR_VM_PUBLIC_IP.nip.io/workflows) and log with your concert username and password.</p> </li> <li>You should have now a Workflows menu on the left burger meny</li> <li>Navigate to Workflows-&gt;Workflows and check that a page is displayed successfully</li> </ol>"},{"location":"concert/labsv2/labs/Lab3-concert-walkthrough/","title":"Concert Walkthrough","text":""},{"location":"concert/labsv2/labs/Lab3-concert-walkthrough/#concert-walkthrough","title":"Concert Walkthrough","text":""},{"location":"concert/labsv2/labs/Lab3-concert-walkthrough/#objective","title":"Objective","text":"<p>In this lab, we will load demo data on concert and walkthrough IBM Concert functionnalities. At the end, we will clean these demo data in order to have an empty instance to execute following labs.</p>"},{"location":"concert/labsv2/labs/Lab3-concert-walkthrough/#prerequisite","title":"Prerequisite","text":"<ul> <li>IBM Concert must be installed</li> <li>IBM Concert must be integrated with Watsonx.ai </li> </ul>"},{"location":"concert/labsv2/labs/Lab3-concert-walkthrough/#content","title":"Content","text":"<ul> <li>Concert Walkthrough</li> <li>Objective</li> <li>Prerequisite</li> <li>Content</li> <li>I - Log on Concert and load demo data</li> <li>II - Walkthrough</li> <li>III - Reset concert data</li> </ul>"},{"location":"concert/labsv2/labs/Lab3-concert-walkthrough/#i-log-on-concert-and-load-demo-data","title":"I - Log on Concert and load demo data","text":"<ol> <li>From a browser, enter the URL of your concert instance (https://YOUR_VM_IP:12443) and log with your concert username and password.</li> <li>If it is the first time you log in, you are invited to upload sample data on the right of the window. </li> <li>Otherwise, you can upload sample data by clicking the question mark at the top right of the window.</li> </ol> <ol> <li>In order to have a first view of IBM Concert functionnalities, click either Sample data or \"Load sample data** depending if you are in case 2. ou 3.</li> </ol>"},{"location":"concert/labsv2/labs/Lab3-concert-walkthrough/#ii-walkthrough","title":"II - Walkthrough","text":"<p>With sample data uploaded in IBM Concert, you can navigate in the Home page, the Arena view and the Dimensions and have a first idea of IBM Concert possibilities</p>"},{"location":"concert/labsv2/labs/Lab3-concert-walkthrough/#iii-reset-concert-data","title":"III - Reset concert data","text":"<ul> <li>Navigate in Administration-&gt;Settings</li> <li>Select the Miscellaneous tab</li> <li>Click the Delete Sample data button</li> </ul>"},{"location":"concert/labsv2/labs/Lab4-managing-software-composition-and-cves/","title":"Managing Software Composition and CVEs","text":""},{"location":"concert/labsv2/labs/Lab4-managing-software-composition-and-cves/#managing-software-composition-and-cves","title":"Managing Software Composition and CVEs","text":""},{"location":"concert/labsv2/labs/Lab4-managing-software-composition-and-cves/#objective","title":"Objective","text":"<p>In order to use Software composition dimension, an organisation need to ingest its application description in Concert using Software Bill of Materials (SBOMs). Then the Software composition dimension helps to identify and mitigate risks associated with packages and components based on several key indicators of reliability, maintainability, and security.  </p> <p>IBM\u00ae Concert fetches a set of reliability checks and generates an aggregate score measuring the reliability of each package. Based on its assessment, Concert recommends specific actions to address risks, such as those related to outdated or unsupported packages, licenses compliance issues, and exploitable vulnerabilities.</p> <p>In this lab, you will learn how to ingest application data from micro-services development in IBM Concert. Then you will see the result and the related functionnalities of IBM Concert concerning these data.</p>"},{"location":"concert/labsv2/labs/Lab4-managing-software-composition-and-cves/#prerequisite","title":"Prerequisite","text":"<p>You must have installed IBM Concert and integrate IBM Concert with watsonx.ai (Lab0 and Lab1)</p>"},{"location":"concert/labsv2/labs/Lab4-managing-software-composition-and-cves/#content","title":"Content","text":"<ul> <li>Managing Software Composition and CVEs</li> <li>Objective</li> <li>Prerequisite</li> <li>Content</li> <li>I - Data ingestion<ul> <li>Verify that you have a Concert API Key defined, if not create it</li> <li>a - Manual ingestion</li> <li>Clone and build the micro-services</li> <li>Clone the SBOM ingestion scripts</li> <li>Execute the SBOM generation script</li> <li>Upload SBOMs on IBM concert</li> <li>b - Jenkins Pipeline</li> </ul> </li> <li>II - Managing Software Composition</li> <li>III - Managing CVEs<ul> <li>Create a github connection for ticketing</li> <li>Open a ticket for a CVE</li> </ul> </li> </ul>"},{"location":"concert/labsv2/labs/Lab4-managing-software-composition-and-cves/#i-data-ingestion","title":"I - Data ingestion","text":"<p>Data ingestion in IBM Concert is done using SBOM (Software Bill of Materials). IBM Concert provides a Concert Toolkit docker image to help to generate SBOMs based on configuration yaml files and scanning products (Syft, CycloneDX, Trivy, etc ....). For more information, the documentation is here</p> <p>We will use this toolkit to generate the SBOMs of a very simple provided python application.</p>"},{"location":"concert/labsv2/labs/Lab4-managing-software-composition-and-cves/#verify-that-you-have-a-concert-api-key-defined-if-not-create-it","title":"Verify that you have a Concert API Key defined, if not create it","text":"<ul> <li>From a browser go to the IBM Concert</li> <li>Log on concert using your concert credentials</li> <li>Click the circle at top right of the window and select API Key </li> <li>In the API Key window, click Generate API Key </li> <li>Copy the API key generated somewhere, we will use it to upload SBOMs files in IBM Concert</li> </ul>"},{"location":"concert/labsv2/labs/Lab4-managing-software-composition-and-cves/#a-manual-ingestion","title":"a - Manual ingestion","text":"<p>In the following steps, you are going to clone three github repositories:</p> <ul> <li>the hr-app repository that contain the code of a backend application called hr-app. It is a python script that just call the summarization-svc.</li> <li>the summarization-svc repository that contain the code of a summarization service. It is a python script that do a LLM inference.</li> <li>the SBOMs-ingestion repository that contain a script to generate SBOMs using the concert toolkit, its templates files and the required environment variables.</li> </ul> <p>Here is the structure that you will have after the clone of the repositories:</p> <p></p> <p>In the folder SBOMs-ingestion you have:</p> <ol> <li>scripts/generate-sboms.sh: </li> </ol> <p>A shell script that take as parameter the component for which we want to generate SBOMs. This script will: - Load the common variables and specific application variables - Use the templates of the templates directory to generate the config files used by concert toolkit to generate the application, build and deploy SBOMs. - Generate the code scan SBOM with concert toolkit. Concert toolkit use Cyclone DX. - Generate the image scan SBOM with concert toolkit. Concert toolkit use Syft. - Generate the image CVEs using Trivy</p> <ol> <li>scripts/upload.sh: </li> </ol> <p>A shell script that take as parameter the application for which we want to upload the SBOMs generated by generate-sboms.sh. This script use Concert APIs to upload SBOMs in IBM Concert</p> <ol> <li>app-common-variables.variables</li> </ol> <p>The environment variables used by the scripts whatever the component is.</p> <ol> <li>hr-app.variables</li> </ol> <p>The environment variables with values specific to hr-app component.</p> <ol> <li>summarization-svc.variables</li> </ol> <p>The environment variables with values specific to summarization-svc component.</p> <p>In the following steps, you will just clone the repositories and run the script. But in lab5, you will manually generate a certificate SBOM for your application and upload it in IBM Concert. So, you will have the opportunity to manipulate a template file, environment variables, the concert toolkit and IBM Concert Apis.</p> <p>For this lab, follow these steps to ingest our application data in Concert:</p>"},{"location":"concert/labsv2/labs/Lab4-managing-software-composition-and-cves/#clone-and-build-the-micro-services","title":"Clone and build the micro-services","text":"<p>You will start to clone the 2 micro-services composing the application.</p> <ol> <li>Connect on the machine you have provisioned on Techzone in Lab0</li> </ol> <pre><code>ssh itzuser@&lt;VM ip address&gt; -p 2223 -i /path/to/concert/sshkey/pem_ibmcloudvsi_download.pem\n</code></pre> <ol> <li>Create a folder named concert-bootcamp</li> </ol> <pre><code>umask 022\nmkdir -p $HOME/concert-bootcamp/Applications\n</code></pre> <ol> <li>Clone and build the hr-app component</li> </ol> <p>NOTE: <code>git clone</code> command will ask you a for username and password. Enter here your github.ibm email and the token you create in Part III of lab0</p> <pre><code>cd $HOME/concert-bootcamp/Applications\ngit clone https://github.ibm.com/concert-bootcamp/hr-app.git\ncd hr-app\n./buildImage.sh\n</code></pre> <ol> <li>Clone and build summarization-svc component</li> </ol> <pre><code>cd $HOME/concert-bootcamp/Applications\ngit clone https://github.ibm.com/concert-bootcamp/summarization-svc.git\ncd summarization-svc\n./buildImage.sh\n</code></pre>"},{"location":"concert/labsv2/labs/Lab4-managing-software-composition-and-cves/#clone-the-sbom-ingestion-scripts","title":"Clone the SBOM ingestion scripts","text":"<p>Then you will clone a project containing a directory structure and a script to generate SBOMs using the Concert Toolkit and upload them in concert using IBM Concert APIs.</p> <pre><code>cd $HOME/concert-bootcamp\ngit clone -b concert-2.0.0 https://github.ibm.com/concert-bootcamp/SBOMs-ingestion.git\n</code></pre> <p>Create an output directory for the generated SBOMs</p> <pre><code>mkdir $HOME/concert-bootcamp/SBOMs-ingestion/concert_data\nchmod 777 $HOME/concert-bootcamp/SBOMs-ingestion/concert_data\n</code></pre>"},{"location":"concert/labsv2/labs/Lab4-managing-software-composition-and-cves/#execute-the-sbom-generation-script","title":"Execute the SBOM generation script","text":"<pre><code>cd $HOME/concert-bootcamp/SBOMs-ingestion/scripts\n./generate-sboms.sh hr-app\n./generate-sboms.sh summarization-svc\n</code></pre> <p>Here you can take a look at the SBOMs generated in folder $HOME/concert-bootcamp/SBOMs-ingestion/concert_data.   One folder as been created for each component (one for hr-app and one for summarization-svc)</p>"},{"location":"concert/labsv2/labs/Lab4-managing-software-composition-and-cves/#upload-sboms-on-ibm-concert","title":"Upload SBOMs on IBM concert","text":"<ol> <li>Update app-common-variables.variables file to specify Concert variables:</li> </ol> <pre><code>cd $HOME/concert-bootcamp/SBOMs-ingestion\nvi app-common-variables.variables\n</code></pre> <p>Put your concert values for:</p> <ul> <li>CONCERT_HOST (just the server IP address, in the form aaa.bbb.ccc.ddd)</li> <li>CONCERT_PORT (12443)</li> <li>CONCERT_APIKEY</li> </ul> <p>Save the file (:wq)</p> <ol> <li>Run the upload script</li> </ol> <p></p><pre><code>cd $HOME/concert-bootcamp/SBOMs-ingestion/scripts\n./upload.sh hr-app\n./upload.sh summarization-svc\n</code></pre> 3. Log on concert and look at the Arena view<p></p>"},{"location":"concert/labsv2/labs/Lab4-managing-software-composition-and-cves/#b-jenkins-pipeline","title":"b - Jenkins Pipeline","text":"<p>Manual data ingestion in Concert is quite painful. So, the best way to ingest these data and keep concert up to date during the application lifecycle is to generate and ingest SBOMs during the CI/CD process.   </p> <p>In each component code of this lab (hr-app and summarization-svc) you will find a Jenkins file as a pipeline example.  Any other CI/CD tools could be use (teckton, ...)</p> <p>Jenkins demo done by the instructor.</p>"},{"location":"concert/labsv2/labs/Lab4-managing-software-composition-and-cves/#ii-managing-software-composition","title":"II - Managing Software Composition","text":"<p>Walkthrough the uploaded data:</p> <ul> <li>Home page - Software composition dimension</li> <li>Arena view</li> <li>Application Inventory</li> </ul>"},{"location":"concert/labsv2/labs/Lab4-managing-software-composition-and-cves/#iii-managing-cves","title":"III - Managing CVEs","text":"<p>Walkthrough the uploaded CVEs:</p> <ul> <li>Home page - Vulnerability dimension</li> <li>Vulnerability Dimension</li> <li>Risk score vs CVSS score</li> <li>Select a CVE</li> <li>Recommended mitigation strategy generated by watsonx</li> <li>Ask watsonx</li> <li>Blast radius</li> <li>Expand and open ticket</li> </ul>"},{"location":"concert/labsv2/labs/Lab4-managing-software-composition-and-cves/#create-a-github-connection-for-ticketing","title":"Create a github connection for ticketing","text":"<p>In many places, you have the possibility to open tickets. IBM Concert allow to open ticket on Github, Jira, ServiceNow or Salesforce.   Before beeing able to open tickets on github, a github connection must be created.  Here are the steps to follow:</p> <ol> <li>Navigate to Administration-&gt;Integrations</li> <li>Select Connections tab</li> <li>Click Create connection button</li> <li>Search for github</li> </ol> <p></p> <ol> <li> <p>Select the card and enter following value:</p> </li> <li> <p>Name: Github.ibm connection</p> </li> <li>Host: https://github.ibm.com/api/v3</li> <li>Personal access token: your personal access token you created in step III of Lab0</li> <li>Click Validate connection</li> </ol> <p></p> <ul> <li>If Validation status is success, click Create button</li> </ul>"},{"location":"concert/labsv2/labs/Lab4-managing-software-composition-and-cves/#open-a-ticket-for-a-cve","title":"Open a ticket for a CVE","text":"<p>Here we are going to open a ticket for a CVEs on our hr-app component.</p> <ol> <li>Navigate to Arena View</li> <li>Select a CVE</li> </ol> <p></p> <ol> <li>Click the red circle</li> <li>Take a look at the Blast radius and go back to Overview tab</li> <li>Expand an Open findings entry </li> <li>Click Open ticket+</li> </ol> <p></p> <ol> <li> <p>In the Open Ticket window, select your connection and enter following values:</p> </li> <li> <p>Organization: concert-bootcamp</p> </li> <li> <p>Repository: hr-app</p> </li> <li> <p>Click Open</p> </li> </ol> <p></p> <ol> <li>On the CVE page that is displayed now, select your ticket to see it on github</li> </ol> <p> </p>"},{"location":"concert/labsv2/labs/Lab5-managing-operations-certificates/","title":"Managing Operations","text":""},{"location":"concert/labsv2/labs/Lab5-managing-operations-certificates/#managing-operations","title":"Managing Operations","text":""},{"location":"concert/labsv2/labs/Lab5-managing-operations-certificates/#objective","title":"Objective","text":"<p>In order to use Operation dimension, an organisation need to ingest its application certificates or environment certificates in IBM Concert using either Concert toolkit or workflows. Then in Concert we can view the list of certificates and their validity status. Then we can create tickets in third-party ticketing system to renew or replace expiring certificates.</p> <p>In this lab, you will manually use the concert toolkit to upload our micro-services certificates in IBM Concert.</p>"},{"location":"concert/labsv2/labs/Lab5-managing-operations-certificates/#prerequisite","title":"Prerequisite","text":"<ul> <li>IBM Concert must be installed</li> <li>You have run the manual data ingestion script </li> </ul>"},{"location":"concert/labsv2/labs/Lab5-managing-operations-certificates/#content","title":"Content","text":"<ul> <li>Managing Operations</li> <li>Objective</li> <li>Prerequisite</li> <li>Content</li> <li>Certificates data ingestion</li> <li>Certificates management</li> </ul>"},{"location":"concert/labsv2/labs/Lab5-managing-operations-certificates/#certificates-data-ingestion","title":"Certificates data ingestion","text":"<p>Certificates can be ingested in Concert following several way:</p> <ul> <li>using built-in concert workflow certificate ingestion from a kubeadm (in Administration-&gt;Integration-&gt;Create Ingestion job) </li> <li>using concert-toolkit for applications certificates during the CI/CD process</li> </ul> <p>We will use Concert toolkit to ingest a sample certificate in Concert. Here are the manual steps to follow:</p> <ol> <li>Connect on the machine you have provisioned on Techzone in Lab0</li> </ol> <pre><code>ssh itzuser@&lt;VM ip address&gt; -p 2223 -i /path/to/concert/sshkey/pem_ibmcloudvsi_download.pem\numask 022\n</code></pre> <ol> <li>Take a look at the certificate template provided by Concert Toolkit</li> </ol> <pre><code>cd $HOME/concert-bootcamp/SBOMs-ingestion/templates\nvi cert-sbom-values.yaml.template\n</code></pre> <p>In this file you can see that it is possible to generate certificate SBOMs using 3 ways:</p> <ul> <li>Line 22: Providing an URL (it is what you are going to do)</li> <li>Line 26: By reading certificate files</li> <li>Line 33: Providing manually certificates details</li> </ul> <p>As you have not deployed the application components in these labs, we use https://www.ibm.com url to get the certificate. But in a real deployment environment, you will replace this URL with your application deployment URLs.</p> <ol> <li>Source the ingestion job environment variables for hr-app component</li> </ol> <pre><code>cd $HOME/concert-bootcamp/SBOMs-ingestion\nsource app-common-variables.variables\nsource hr-app.variables\n</code></pre> <ol> <li>Create a concert toolkit config file from the certificate template</li> </ol> <pre><code>envsubst &lt; templates/cert-sbom-values.yaml.template &gt; $HOME/concert-bootcamp/SBOMs-ingestion/concert_data/${COMPONENT_NAME}/cert-sbom-values.yaml\n</code></pre> <ol> <li>Generate the certificate SBOM using Concert toolkit</li> </ol> <pre><code>APP_COMMAND=\"cert-inventory --cert-config /app/sample/cert-sbom-values.yaml\"\nSRC_PATH=\"$HOME/concert-bootcamp/SBOMs-ingestion/concert_data/${COMPONENT_NAME}\"\nOUTPUTDIR=\"$HOME/concert-bootcamp/SBOMs-ingestion/concert_data/${COMPONENT_NAME}\"\npodman run -v \"${SRC_PATH}\":/app/sample  -v \"${OUTPUTDIR}\":/toolkit-data --rm ${CONCERT_TOOLKIT_IMAGE} /bin/bash -c \"${APP_COMMAND}\"\n</code></pre> <ol> <li>Patch the sbom generated</li> </ol> <p>At the time we write this lab (May 2025), there is 1 issue in the sbom generated. Follow this steps to correct it:</p> <pre><code>sudo chmod 666 $HOME/concert-bootcamp/SBOMs-ingestion/concert_data/${COMPONENT_NAME}/certificates-hr-app.json\nvi $HOME/concert-bootcamp/SBOMs-ingestion/concert_data/${COMPONENT_NAME}/certificates-hr-app.json\n</code></pre> <ul> <li> <p>Reduce the number of dns_names entries </p> </li> <li> <p>Save the file (:wq)</p> </li> <li> <p>Upload the certificate file in Concert using Concert API</p> </li> </ul> <pre><code>curl -k -X \"POST\" -H \"accept: application/json\" -H \"InstanceID: ${CONCERT_INSTANCE_ID}\" -H \"Authorization: C_API_KEY ${CONCERT_APIKEY}\" -H \"Content-Type: multipart/form-data\" -F \"data_type=certificate\" -F \"filename=@${OUTPUTDIR}/certificates-hr-app.json\" \"https://${CONCERT_HOST}:${CONCERT_PORT}/ingestion/api/v1/upload_files\"\n</code></pre> <ol> <li>Check your certificate upload in IBM Concert UI</li> </ol> <p>After logging in your IBM Concert UI, you can see your ingested certificate from the Operation dimension.   </p> <ul> <li> <p>First, you can check that the upload is successfull by looking at menu Administration-&gt;Event log. Here you can see the status of all the files ingested </p> </li> <li> <p>Then, you can navigate the the menu Dimensions-&gt;Operation to see your certificate </p> </li> </ul>"},{"location":"concert/labsv2/labs/Lab5-managing-operations-certificates/#certificates-management","title":"Certificates management","text":"<p>Walkthrough the uploaded certificates:</p> <ul> <li>Home page - Operations dimension</li> <li>Operation Dimension</li> <li>Select a certificate</li> <li>Renewal if expired</li> </ul>"},{"location":"concert/labsv2/labs/Lab6-managing-compliance-using-concert-workflow/","title":"Managing Compliance","text":""},{"location":"concert/labsv2/labs/Lab6-managing-compliance-using-concert-workflow/#managing-compliance","title":"Managing Compliance","text":""},{"location":"concert/labsv2/labs/Lab6-managing-compliance-using-concert-workflow/#objective","title":"Objective","text":"<p>In order to use Compliance dimension, an organisation need to ingests compliance assessment data using Concert workflow. This will deliver a holistic view of the compliance posture of its application environments.</p> <p>In this lab, you will use concert workflow to ingest compliance data from a rhel vm in IBM Concert.  We will use your VM concert as the target of the compliance job.</p>"},{"location":"concert/labsv2/labs/Lab6-managing-compliance-using-concert-workflow/#prerequisite","title":"Prerequisite","text":"<ul> <li>IBM Concert must be installed</li> <li>Concert workflow must be installed</li> </ul>"},{"location":"concert/labsv2/labs/Lab6-managing-compliance-using-concert-workflow/#content","title":"Content","text":"<ul> <li>Managing Compliance</li> <li>Objective</li> <li>Prerequisite</li> <li>Content</li> <li>Run compliance workflow<ul> <li>Create an environment</li> <li>Create a compliance catalog and a compliance profile</li> <li>Install the workflow in IBM Concert</li> <li>Create an Authentication to ssh the Concert VM</li> <li>Create an Authentication for concert</li> <li>Run manually the workflow</li> <li>Scheduling the workflow job</li> </ul> </li> <li>Compliance Management</li> </ul>"},{"location":"concert/labsv2/labs/Lab6-managing-compliance-using-concert-workflow/#run-compliance-workflow","title":"Run compliance workflow","text":"<p>You are going to use a workflow called CIS RHEL9 OpensSCAP Compliance Scan provided by IBM Concert to ingest compliance data in Concert. This workflow automates the CIS compliance scan for RHEL 9 using OpenSCAP. </p> <p>Official documentation is here</p>"},{"location":"concert/labsv2/labs/Lab6-managing-compliance-using-concert-workflow/#create-an-environment","title":"Create an environment","text":"<p>In order to be able to ingest compliance data in IBM Concert, you must have an environment defined in IBM Concert with the hostname of the machine that will be scanned.</p> <p>To create this environment, follow these steps:</p> <ol> <li> <p>Get your hostname with the <code>hostname</code>command</p> </li> <li> <p>From the arena view on your concert UI, select Define and upload-&gt;Define Environment-&gt;From resources </p> </li> <li> <p>In the Define an environment screen, enter following informations:</p> </li> <li> <p>name: your VM hostname</p> </li> <li>type: other</li> <li>purpose: what you want</li> </ol> <p>Then click next, next and Create</p>"},{"location":"concert/labsv2/labs/Lab6-managing-compliance-using-concert-workflow/#create-a-compliance-catalog-and-a-compliance-profile","title":"Create a compliance catalog and a compliance profile","text":"<p>Official documentation is here</p> <ul> <li>A compliance catalog serves as the single source of truth for an client organization's compliance-related policies, procedures, and standards. Concert supports compliance catalogs based on the NIST 800-53 (Rev4 for OCP and Rev5), PCI, or a custom standard.</li> </ul> <p>To create a catalog for our  CIS RHEL9 OpensSCAP Compliance Scan workflow, follow these steps:</p> <ol> <li>Navigate to Dimensions-&gt;Compliance</li> <li>Select Catalog tab and click Add catalog-&gt;From Standards Library button</li> <li> <p>Select CIS Controls entry and click Add</p> </li> <li> <p>A compliance profile represents a subset of controls from a compliance catalog. Each profile specifies a set of rules the scan results will use to assess the overall compliance of the application environments.</p> </li> </ol> <p>To create a profile for our CIS RHEL9 OpensSCAP Compliance Scan workflow, follow these steps:</p> <ol> <li>Select Profile tab, click Create profile button and select From resources</li> <li> <p>Enter following values and click Create button</p> </li> <li> <p>Name: profile_cis</p> </li> <li>Select one or more catalogs: select CIS Controls catalog</li> <li>Select controls: Click Select all (or choose specific compliance rules)</li> </ol>"},{"location":"concert/labsv2/labs/Lab6-managing-compliance-using-concert-workflow/#install-the-workflow-in-ibm-concert","title":"Install the workflow in IBM Concert","text":"<ol> <li> <p>Download the CIS RHEL9 OpensSCAP Compliance Scan worflow from the Automation Library</p> </li> <li> <p>Upload the workflow in concert </p> </li> <li> <p>From the burger menu, select Workflows-&gt;Worflows menu</p> </li> <li>Select Import button</li> <li>And choose the zip file corresponding to the workflow you downloaded in step1 (name CIS_RHEL9_OpenSCAP_Compliance_Scan.zip)</li> </ol>"},{"location":"concert/labsv2/labs/Lab6-managing-compliance-using-concert-workflow/#create-an-authentication-to-ssh-the-concert-vm","title":"Create an Authentication to ssh the Concert VM","text":"<p>CIS_RHEL9_OpenSCAP_Compliance_Scan workflow will do an ssh connection to the target machine to run an OpenSCAP compliance scan. In our case, the target machine is our Concert VM.  Follow these step to create an SSH Authentication:</p> <ol> <li>From the burger menu, select Workflows-&gt;Authentications menu </li> <li> <p>Click the Create authentication button and enter following informations:</p> </li> <li> <p>name: concert-vm-ssh</p> </li> <li>service: SSH</li> <li>Host: your VM Ip public address (from your reservation page)</li> <li>Port: 2223</li> <li>Username: itzuser</li> <li>RSA Private Key: the content of your VM pem key (downloaded from your reservation page)</li> </ol> <p></p>"},{"location":"concert/labsv2/labs/Lab6-managing-compliance-using-concert-workflow/#create-an-authentication-for-concert","title":"Create an Authentication for concert","text":"<p>CIS_RHEL9_OpenSCAP_Compliance_Scan workflow will upload the scan result to IBM Concert. Follow these step to create an IBM Hub - Self Authentication:</p> <ol> <li>From the burger menu, select Workflows-&gt;Authentications menu </li> <li> <p>Click the Create authentication button and enter following informations:</p> </li> <li> <p>name: concert-auth</p> </li> <li>service: IBM Hub - Self</li> </ol> <p></p>"},{"location":"concert/labsv2/labs/Lab6-managing-compliance-using-concert-workflow/#run-manually-the-workflow","title":"Run manually the workflow","text":"<ol> <li>From the burger menu, select Workflows-&gt;Workflows menu </li> <li>Select CIS_RHEL9_OpenSCAP_Compliance_Scan workflow</li> <li>Edit the RHEL_OSCAP_Scan step</li> </ol> <ol> <li>Add a sudo before each commands and save your modifications</li> </ol> <ol> <li> <p>In the Start box, update the value of following variables (keep the \"):</p> </li> <li> <p>concert_auth: \"concert-auth\"</p> </li> <li>ssh_authentication: \"concert-vm-ssh\"</li> </ol> <p></p> <ol> <li> <p>Execute the worflow</p> </li> <li> <p>Click the Run button</p> </li> </ol> <p></p> <p>Note: You can also run the workflow in debug mode. </p> <p>The execution of the flow take at least 5 minutes, be patient.</p> <ol> <li> <p>Check the ingested data</p> </li> <li> <p>When the workflow is finished, navigate to Concert-&gt;Administration-&gt;Event log menu and check that the compliance file upload is successfull   </p> </li> <li> <p>Navigate to Dimensions-&gt;Compliance menu and consult compliance data for your concert VM.</p> </li> </ol>"},{"location":"concert/labsv2/labs/Lab6-managing-compliance-using-concert-workflow/#scheduling-the-workflow-job","title":"Scheduling the workflow job","text":"<p>You can also schedule a workflow job for ingestion of compliance scans into Concert if needed.</p> <ol> <li>Navigate to Workflows-&gt;Job menu</li> <li>Select Create job button</li> <li>Populate the values and click Create</li> </ol> <p></p>"},{"location":"concert/labsv2/labs/Lab6-managing-compliance-using-concert-workflow/#compliance-management","title":"Compliance Management","text":"<p>Walkthrough the uploaded compliance assessment:</p> <ul> <li>Home page - Compliance dimension</li> <li>Compliance Dimension</li> <li>Select an entry</li> <li>Sort by results</li> <li>Expand one</li> <li>Open ticket</li> </ul>"},{"location":"concert/labsv2/labs/Lab7-managing-resilience-using-concert-workflow/","title":"Managing Resilience","text":""},{"location":"concert/labsv2/labs/Lab7-managing-resilience-using-concert-workflow/#managing-resilience","title":"Managing Resilience","text":""},{"location":"concert/labsv2/labs/Lab7-managing-resilience-using-concert-workflow/#objective","title":"Objective","text":"<p>In order to use Resilience dimension, an organisation need to determine the non-functional requirements (NFRs) that apply to its applications, as well as the target values that must be achieved to meet contractual obligations or otherwise be considered resilient. Also, relevant data must be collected from the applications and their environment components in order to import them to Concert on a regular basis.</p> <p>In this lab, you will use and create a concert workflow to ingest in IBM Concert resilience data concerning the quality of docker images. We will use the 2 images that you have build in lab4.</p>"},{"location":"concert/labsv2/labs/Lab7-managing-resilience-using-concert-workflow/#prerequisite","title":"Prerequisite","text":"<ul> <li>IBM Concert must be installed</li> <li>Concert workflow must be installed</li> </ul>"},{"location":"concert/labsv2/labs/Lab7-managing-resilience-using-concert-workflow/#content","title":"Content","text":"<ul> <li>Managing Resilience</li> <li>Objective</li> <li>Prerequisite</li> <li>Content</li> <li>Import Resilience data using a workflow<ul> <li>Resilience library and a Resilience profile</li> <li>Import a resilience workflow</li> <li>Build your own sub workflow</li> <li>Complete the resilience workflow previously imported</li> <li>Run the workflow to populate you application resilience posture</li> </ul> </li> <li>Resilience Management</li> </ul>"},{"location":"concert/labsv2/labs/Lab7-managing-resilience-using-concert-workflow/#import-resilience-data-using-a-workflow","title":"Import Resilience data using a workflow","text":""},{"location":"concert/labsv2/labs/Lab7-managing-resilience-using-concert-workflow/#resilience-library-and-a-resilience-profile","title":"Resilience library and a Resilience profile","text":"<p>For this lab, we will use part of a provided library called Container Build Integrity Library. These library NFRs are used to assess security, efficiency and maintainability of container image quality. We will calculate these metrics:</p> <ul> <li>Average image size</li> <li>Max image size</li> <li>Percentage of images with excessive layers</li> </ul> <p>To discover this library, follow these steps</p> <ol> <li>From Concert UI, navigate to Dimensions-&gt;Resilience</li> <li>Select Libraries tab</li> <li>Click Container Build Integrity Library link</li> <li>Expand Image Layers and Image Size</li> </ol> <p>And take a look at how the scores are calculated. The next steps consists in creating a workflow to get images metrics and upload an assessment in IBM Concert.</p>"},{"location":"concert/labsv2/labs/Lab7-managing-resilience-using-concert-workflow/#import-a-resilience-workflow","title":"Import a resilience workflow","text":"<p>You will start to import a pre-defined workflow available here.   </p> <ol> <li>From burger menu, navigate to Workflows-&gt;Workflow</li> <li>Click the Import button (top right of the window) and select the concert_v2001_absolute_Resilience workflow from your laptop</li> </ol> <p>This workflow retrieves, from your concert VM, the hr-application images you have build in Lab4.   Then, for each images it will do a <code>podman inspect</code> command and calculate metrics based on the <code>podman inspect</code> command result </p> <p>To be able to ssh your concert VM, you need to define an SSH Authentication:</p> <ol> <li>Navigate to Workflows-&gt;Authentications</li> <li>Click the Create authentication button (top right of the window)</li> <li>Call it dev-vm</li> <li>In Service, select SSH</li> <li> <p>Then enter these values:</p> </li> <li> <p>Host: your Concert VM IP</p> </li> <li>Port: 2223</li> <li>Username: itzuser</li> <li>RSA Private Key: the content of the pem file you download from your reservation page</li> </ol> <p></p>"},{"location":"concert/labsv2/labs/Lab7-managing-resilience-using-concert-workflow/#build-your-own-sub-workflow","title":"Build your own sub workflow","text":"<p>You will now create a workflow that will be used as a sub-worflow of docker_images_metrics workflow in order to define a new metric: the percentage of big images. The aim of this workflow is to extract the image size from a json object that have the format of the result of the <code>podman inspect</code> command</p> <ol> <li>From burger menu, navigate to Workflows-&gt;Workflow</li> <li>Click the button Create workflow (top right of the window)</li> <li>Call it docker_images_size and click Create button</li> <li>Define your variables:</li> </ol> Name Type Default Value Selected box json_inspect Array [{\"Architecture\": \"amd64\", \"Os\": \"linux\", \"Size\": 1378729490}] in / required image_size Number 0 out / log <p>Then you are going to use a \"jq\" node in order to extract the size from the json_inspect input variable:</p> <ol> <li>From the palette that is at the left pane of your window, navigate in Common-&gt;Json</li> <li> <p>Select the jq box and drag and drop it before the Assign_1 box      </p> </li> <li> <p>From the Object Editor that is in the right pane of your window, click on OneOf&gt; </p> </li> <li> <p>Select Object, Click Save </p> </li> <li> <p>Then select the pencil to set the JSON variable that jq will use as input     </p> </li> <li> <p>Enter value: $json_inspect</p> </li> <li>For the Filter variable put the value: \".[].Size\" (don't forget the quotes)     </li> </ol> <p>You just need now to assign the result of the jq node in the image_size output variable of your flow</p> <ol> <li>Select the Assign node that is under the jq node</li> <li> <p>From the Object Editor that is in the right pane of your window, enter following values:</p> </li> <li> <p>variable: $image_size</p> </li> <li> <p>value: $jq_1.result</p> <p></p> </li> </ol> <p>You can now test your workflow:</p> <ol> <li> <p>Click the Run button     </p> </li> <li> <p>At the bottom of your window, you should have the image size corresponding to the value of your json_inspect input variable displayed</p> <p></p> </li> </ol>"},{"location":"concert/labsv2/labs/Lab7-managing-resilience-using-concert-workflow/#complete-the-resilience-workflow-previously-imported","title":"Complete the resilience workflow previously imported","text":"<p>You are going to complete the empty branch in the main workflow in order to calculate images metrics</p> <ol> <li>From burger menu, navigate to Workflows-&gt;Workflow</li> <li>Navigate in Resilience folder</li> <li>Open the docker_images_metrics workflow</li> <li> <p>Scroll the Split_1 node where you will see an empty  Branch-2</p> <p></p> </li> <li> <p>From the palette that is at the left pane of your window, navigate in User-&gt;Resilience</p> </li> <li>Drag and drop the docker_image_size node (it is the sub-flow you just create before) in the empty branch </li> <li>Name the node get_image_size_flow</li> <li> <p>From the Object Editor that is in the right pane of your window, enter following values:</p> </li> <li> <p>json_inspect: $ssh_inspect_image.result (the result of the node named ssh_inspect_image is the input of your sub-flow)</p> </li> <li> <p>Then complete your branch as shown in following image</p> </li> </ol> <p>TIPS: most common nodes can be added by clicking the + that are in the flow where you want to add your node. </p> <p></p>"},{"location":"concert/labsv2/labs/Lab7-managing-resilience-using-concert-workflow/#run-the-workflow-to-populate-you-application-resilience-posture","title":"Run the workflow to populate you application resilience posture","text":"<p>Before running your flow, you need to modify the value of these variables:</p> Name Value concert_host The IP address of your Concert VM concert_api_key The API key of your concert installation <p>Now, you can now run your flow.  The best to begin is to run it in debug mode and put a breakpoint for example on node Upload_to_concert and take a look at the value of your concert_data variable</p> <p> </p> <p>You should see these values for concert_data variable:</p> <p></p> <p>Then you can finish the flow by clicking the Resume debug button</p> <p></p> <p>If the flow is successfull, you see that in the logs</p> <p></p> <p>You can now navigate to Dimensions-&gt;Resilience and select the components_images_posture</p>"},{"location":"concert/labsv2/labs/Lab7-managing-resilience-using-concert-workflow/#resilience-management","title":"Resilience Management","text":"<p>Walkthrough the uploaded assessment:</p> <ul> <li>Home page - Resilience dimension</li> <li>Resilience Dimension</li> <li>Select \"component_images_posture\"<ul> <li>Open a change request on an assessment</li> <li>Select an assessment</li> <li>In assessment summary tab, Sort by Assessed score</li> <li>In Actions tab, sort by Assessed score</li> </ul> </li> </ul> <p></p>"},{"location":"concert/labsv2/labs/Lab8-auto-discovery-auto-resilience/","title":"Auto-discovery and Auto-resilience","text":""},{"location":"concert/labsv2/labs/Lab8-auto-discovery-auto-resilience/#auto-discovery-and-auto-resilience","title":"Auto-discovery and Auto-resilience","text":""},{"location":"concert/labsv2/labs/Lab8-auto-discovery-auto-resilience/#objective","title":"Objective","text":"<p>In concert 2.0.0, a new feature named Discover your data is available. In this lab, you will use this Auto-discovery feature to discover the hr-application deployed on an Openshift environment. This feature also do Auto-resilience, generating a resilience assessment and Actions in the Action center.  You will analyze the result of the resilience assessment and try to solve the application deployment quality as a remediation to the proposed actions.</p>"},{"location":"concert/labsv2/labs/Lab8-auto-discovery-auto-resilience/#prerequisite","title":"Prerequisite","text":"<ul> <li>IBM Concert must be installed</li> <li>IBM Concert is linked to a watsonx.ai instance</li> <li>An openshift deployment environment has been created for you </li> <li>A namespace named hca-genai-apps-XX has been created for you (XX is a number the instructor will give to you)</li> <li>Your instructor provided you with:</li> <li>Your student number: ex: XX</li> <li>Openshift endpoint: ex: https://api.itz-ow5hkm.infra01-lb.fra02.techzone.ibm.com:6443</li> <li>Openshift Token: ex: sha256~piQ_gJT81JdV1q9zzh2ZqkMAoWoHNaR-Y4VHbYiXt-A</li> <li>Cluster Name: ex: itz-ow5hkm</li> </ul>"},{"location":"concert/labsv2/labs/Lab8-auto-discovery-auto-resilience/#content","title":"Content","text":"<ul> <li>Auto-discovery and Auto-resilience</li> <li>Objective</li> <li>Prerequisite</li> <li>Content</li> <li>Clear the previously imported data</li> <li>Deploy the application on openshift</li> <li>Run Discover your data</li> <li>Explore the actions generated in the Action Center and the created resilience assessment</li> <li>Remediation: Improve the quality of the application deployment</li> <li>Run again the integration job to generate a new resilience assessment</li> </ul>"},{"location":"concert/labsv2/labs/Lab8-auto-discovery-auto-resilience/#clear-the-previously-imported-data","title":"Clear the previously imported data","text":"<p>In this lab, we use the same application you have imported manually in lab 4. Then you will start to clear these data.</p> <ul> <li>From the burger menu, navigate to Concert-&gt;Inventory-&gt;Application Inventory</li> <li>Go in the Build artifacts tab</li> <li>For each build artifact listed, at the end of the line click the 3 points, select Delete, check the 'Do you want ...' checkbox and click Delete</li> <li>Do the same for the Repository tab</li> <li>Do the same for the Application tab</li> <li>Navigate in the Arena View and check that all as been deleted except the environment data</li> </ul>"},{"location":"concert/labsv2/labs/Lab8-auto-discovery-auto-resilience/#deploy-the-application-on-openshift","title":"Deploy the application on openshift","text":"<ol> <li>From your techzone concert VM or your laptop, connect to openshift</li> </ol> <pre><code>oc login --token=&lt;Openshift Token&gt; --server=&lt;Openshift endpoint&gt;\n</code></pre> <ol> <li>Navigate in the application folder and deploy the application in your namespace</li> </ol> <pre><code>oc project hca-genai-apps-XX (replace XX by your student number)\n</code></pre> <ol> <li>Download these deployment files:</li> <li>hr-app</li> <li> <p>summarization-svc</p> </li> <li> <p>Deploy the application</p> </li> </ol> <pre><code>oc apply -f ocp-deploy-HR-app.yaml\noc apply -f ocp-deploy-summarization-svc.yaml\n</code></pre> <ol> <li>Verify the deployment and wait until the pods are in running state</li> </ol> <pre><code>oc get pods\n</code></pre>"},{"location":"concert/labsv2/labs/Lab8-auto-discovery-auto-resilience/#run-discover-your-data","title":"Run Discover your data","text":"<ul> <li>From the burger menu, navigate to Concert-&gt;Home</li> <li>On the top right of the screen, click the Discover Your Data button</li> <li>Select Kubernetes</li> <li>Choose Openshift integration</li> <li>Enter following information:</li> <li>Endpoint: </li> <li>Token: </li> <li>Cluster Name: </li> <li>Validate the connexion from the Revalidate label</li> <li>Click Next</li> <li>Select the namespace named hca-genai-apps-XX (XX is your student number)</li> <li>Select Next</li> </ul> <p>Then Concert discover the content of the namespace and run a Resilience Assessment. You can see the result on the left of your screen.</p>"},{"location":"concert/labsv2/labs/Lab8-auto-discovery-auto-resilience/#explore-the-actions-generated-in-the-action-center-and-the-created-resilience-assessment","title":"Explore the actions generated in the Action Center and the created resilience assessment","text":"<p>Thanks to watsonx.ai, some actions have been created in the Actions center. Take the time to explore them.</p> <p></p> <p>You can also navigate to the Resilience assessment: Concert-&gt;Dimension-&gt;Resilience. Take the time to explore this assessment.</p> <p></p>"},{"location":"concert/labsv2/labs/Lab8-auto-discovery-auto-resilience/#remediation-improve-the-quality-of-the-application-deployment","title":"Remediation: Improve the quality of the application deployment","text":"<p>You are going to focus on these actions listed in the Actions Center: - Assign Dedicated Service Accounts - Configure InitialDelaySeconds - Define CPU Limits</p> <p>For that, you will deploy again the application using a new yaml file</p> <p>Note: you can take a look at the 2 yamls files (before and after improvment) to understand the improvments that have been done</p> <ol> <li>From your techzone concert VM or your laptop, connect to openshift</li> </ol> <pre><code>oc login --token=&lt;Openshift Token&gt; --server=&lt;Openshift endpoint&gt;\n</code></pre> <ol> <li>Navigate in the application folder and deploy the application in your namespace</li> </ol> <pre><code>oc project hca-genai-apps-XX (replace XX by your student number)\n</code></pre> <ol> <li>Download these deployment files:</li> <li>hr-app</li> <li> <p>summarization-svc</p> </li> <li> <p>Deploy the application</p> </li> </ol> <pre><code>oc apply -f ocp-deploy-HR-app-mem-limit-sa-readinessprobe.yaml\noc apply -f ocp-deploy-summarization-svc-mem-limit-sa-readinessprobe.yaml\n</code></pre> <ol> <li>Verify the deployment and wait until the pods are in running state</li> </ol> <pre><code>oc get pods\n</code></pre> <p>You can see that the pods are restarting and take more time to become running.</p>"},{"location":"concert/labsv2/labs/Lab8-auto-discovery-auto-resilience/#run-again-the-integration-job-to-generate-a-new-resilience-assessment","title":"Run again the integration job to generate a new resilience assessment","text":"<ul> <li>From the burger menu, navigate to Concert-&gt;Administration-&gt;Integrations</li> <li>Select the more recent discovery job, click the 3 points at the end of the line and select Run now</li> <li>When finished, </li> <li>go back in the Actions center, you will see that some actions are now in Success mode</li> </ul> <ul> <li>Navigate also in the Resilience dimension, you will see that NFRs have a better score</li> </ul>"},{"location":"concert/old%20labs/concert-workflows-old/","title":"Intro to Concert Workflows","text":""},{"location":"concert/old%20labs/concert-workflows-old/#intro-to-concert-workflows","title":"Intro to Concert Workflows","text":"<p>For this lab exercise, we will install Concert Workflows into an existing Concert deployment (VM deployment) and run our first basic workflow. For the latest documentation on how to achieve this process, consult the IBM Concert documentation on Concert Workflows add-on.</p>"},{"location":"concert/old%20labs/concert-workflows-old/#installing-concert-workflows","title":"Installing Concert Workflows","text":"<p>Concert Workflows run as a containerized application in a Kubernetes cluster. As such, you will need to install a standalone K3s cluster on the VM where you installed IBM Concert.</p> <p>Environment</p> <p>These instructions have been tested from the Concert VM. Review how to connect to it in the SBOM lab Some changes may be required if you want to follow from another machine.</p>"},{"location":"concert/old%20labs/concert-workflows-old/#installing-rancher-k3s","title":"Installing Rancher K3s","text":"<p>Install K3s from the command line.  You will need to disable its default ingress controller (traefik), so that port 443 is not in use.  SSH to your concert VM and execute the following commands:</p> Installing Rancher K3s <pre><code>curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\"--disable traefik\" sh -s - --write-kubeconfig-mode 644\nexport KUBECONFIG=/etc/rancher/k3s/k3s.yaml \nkubectl get nodes\n</code></pre> output <pre><code>$ curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\"--disable traefik\" sh -s - --write-kubeconfig-mode 644\n[INFO]  Finding release for channel stable\n[INFO]  Using v1.31.4+k3s1 as release\n[INFO]  Downloading hash https://github.com/k3s-io/k3s/releases/download/v1.31.4+k3s1/sha256sum-amd64.txt\n[INFO]  Downloading binary https://github.com/k3s-io/k3s/releases/download/v1.31.4+k3s1/k3s\n[INFO]  Verifying binary download\n[INFO]  Installing k3s to /usr/local/bin/k3s\n[INFO]  Finding available k3s-selinux versions\nUpdating Subscription Management repositories.\nRancher K3s Common (stable)                                                                                                     7.3 kB/s | 1.5 kB     00:00\nRed Hat Enterprise Linux 9 for x86_64 - Supplementary (RPMs)                                                                     82 kB/s | 3.7 kB     00:00\nRed Hat Enterprise Linux 9 for x86_64 - AppStream (RPMs)                                                                        104 kB/s | 4.5 kB     00:00\nRed Hat Enterprise Linux 9 for x86_64 - AppStream - Extended Update Support (RPMs)                                               75 kB/s | 4.5 kB     00:00\nRed Hat Enterprise Linux 9 for x86_64 - BaseOS - Extended Update Support (RPMs)                                                 104 kB/s | 4.1 kB     00:00\nRed Hat Enterprise Linux 9 for x86_64 - Supplementary - Extended Update Support (RPMs)                                           72 kB/s | 3.7 kB     00:00\nRed Hat Enterprise Linux 9 for x86_64 - BaseOS (RPMs)                                                                            96 kB/s | 4.1 kB     00:00\nDependencies resolved.\n================================================================================================================================================================\nPackage                             Architecture             Version                              Repository                                              Size\n================================================================================================================================================================\nInstalling:\nk3s-selinux                         noarch                   1.6-1.el9                            rancher-k3s-common-stable                               22 k\nInstalling dependencies:\ncontainer-selinux                   noarch                   3:2.229.0-1.el9_2                    rhel-9-for-x86_64-appstream-eus-rpms                    55 k\n\nTransaction Summary\n================================================================================================================================================================\nInstall  2 Packages\n\nTotal download size: 77 k\nInstalled size: 162 k\nDownloading Packages:\n(1/2): container-selinux-2.229.0-1.el9_2.noarch.rpm                                                                             657 kB/s |  55 kB     00:00\n(2/2): k3s-selinux-1.6-1.el9.noarch.rpm                                                                                         180 kB/s |  22 kB     00:00\n----------------------------------------------------------------------------------------------------------------------------------------------------------------\nTotal                                                                                                                           626 kB/s |  77 kB     00:00\nRancher K3s Common (stable)                                                                                                      38 kB/s | 2.4 kB     00:00\nImporting GPG key 0xE257814A:\nUserid     : \"Rancher (CI) &lt;ci@rancher.com&gt;\"\nFingerprint: C8CF F216 4551 26E9 B9C9 18BE 925E A29A E257 814A\nFrom       : https://rpm.rancher.io/public.key\nKey imported successfully\nRunning transaction check\nTransaction check succeeded.\nRunning transaction test\nTransaction test succeeded.\nRunning transaction\n  Preparing        :                                                                                                                                        1/1\n  Running scriptlet: container-selinux-3:2.229.0-1.el9_2.noarch                                                                                             1/2\n  Installing       : container-selinux-3:2.229.0-1.el9_2.noarch                                                                                             1/2\n  Running scriptlet: container-selinux-3:2.229.0-1.el9_2.noarch                                                                                             1/2\n  Running scriptlet: k3s-selinux-1.6-1.el9.noarch                                                                                                           2/2\n  Installing       : k3s-selinux-1.6-1.el9.noarch                                                                                                           2/2\n  Running scriptlet: k3s-selinux-1.6-1.el9.noarch                                                                                                           2/2\n  Running scriptlet: container-selinux-3:2.229.0-1.el9_2.noarch                                                                                             2/2\n  Running scriptlet: k3s-selinux-1.6-1.el9.noarch                                                                                                           2/2\n  Verifying        : k3s-selinux-1.6-1.el9.noarch                                                                                                           1/2\n  Verifying        : container-selinux-3:2.229.0-1.el9_2.noarch                                                                                             2/2\nInstalled products updated.\n\nInstalled:\n  container-selinux-3:2.229.0-1.el9_2.noarch                                            k3s-selinux-1.6-1.el9.noarch\n\nComplete!\n[INFO]  Creating /usr/local/bin/kubectl symlink to k3s\n[INFO]  Creating /usr/local/bin/crictl symlink to k3s\n[INFO]  Creating /usr/local/bin/ctr symlink to k3s\n[INFO]  Creating killall script /usr/local/bin/k3s-killall.sh\n[INFO]  Creating uninstall script /usr/local/bin/k3s-uninstall.sh\n[INFO]  env: Creating environment file /etc/systemd/system/k3s.service.env\n[INFO]  systemd: Creating service file /etc/systemd/system/k3s.service\n[INFO]  systemd: Enabling k3s unit\nCreated symlink /etc/systemd/system/multi-user.target.wants/k3s.service \u2192 /etc/systemd/system/k3s.service.\n[INFO]  Host iptables-save/iptables-restore tools not found\n[INFO]  Host ip6tables-save/ip6tables-restore tools not found\n[INFO]  systemd: Starting k3s\n$ export KUBECONFIG=/etc/rancher/k3s/k3s.yaml\n$ kubectl get nodes\nNAME                         STATUS   ROLES                  AGE   VERSION\nitzvsi-1100007b1r-c9nslidq   Ready    control-plane,master   15s   v1.31.4+k3s1\n</code></pre>"},{"location":"concert/old%20labs/concert-workflows-old/#installing-concert-workflows-on-a-vm","title":"Installing Concert Workflows on a VM","text":"<p>Download the IBM Concert Workflows installer and authenticate to the IBM Entitlement Registry.</p> <p>Prerequisite</p> <p>You will need an Entitlement key to install Workflows. Fetch your key or create one here.</p> <p>Create a workspace for the lab:</p> <pre><code>mkdir -p ~/workflow-lab &amp;&amp; \\\ncd  ~/workflow-lab\n</code></pre> Downloading installer <pre><code>export DOCKER_EXE=podman\nexport CONCERT_REGISTRY=cp.icr.io/cp/concert\nexport CONCERT_REGISTRY_USER=cp\nexport CONCERT_REGISTRY_PASSWORD=\"eyJh...\" # (1)!\nwget https://github.com/IBM/Concert/releases/download/v1.0.5.1/ibm-concert-std-workflows.tgz\ntar xfz ibm-concert-std-workflows.tgz\n${DOCKER_EXE} login ${CONCERT_REGISTRY} --username=${CONCERT_REGISTRY_USER} --password=${CONCERT_REGISTRY_PASSWORD}\n</code></pre> <ol> <li>Your IBM Entitlement Key</li> </ol> output <pre><code>$ export DOCKER_EXE=podman\n$ export CONCERT_REGISTRY=cp.icr.io/cp/concert\n$ export CONCERT_REGISTRY_USER=cp\n$ export CONCERT_REGISTRY_PASSWORD=[REDACTED]\n$ wget https://github.com/IBM/Concert/releases/download/v1.0.4.1/ibm-concert-std-workflows.tgz\n--2025-01-29 14:39:44--  https://github.com/IBM/Concert/releases/download/v1.0.4.1/ibm-concert-std-workflows.tgz\nResolving github.com (github.com)... 140.82.112.4\nConnecting to github.com (github.com)|140.82.112.4|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://objects.githubusercontent.com/github-production-release-asset-2e65be/815253674/d37113aa-93c4-4635-850a-6d478b44bb77?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=releaseassetproduction%2F20250129%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20250129T143945Z&amp;X-Amz-Expires=300&amp;X-Amz-Signature=88b0e3843a4642a3249d744526bfbb580d50f551240dcd148aded92c384305a6&amp;X-Amz-SignedHeaders=host&amp;response-content-disposition=attachment%3B%20filename%3Dibm-concert-std-workflows.tgz&amp;response-content-type=application%2Foctet-stream [following]\n--2025-01-29 14:39:45--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/815253674/d37113aa-93c4-4635-850a-6d478b44bb77?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=releaseassetproduction%2F20250129%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20250129T143945Z&amp;X-Amz-Expires=300&amp;X-Amz-Signature=88b0e3843a4642a3249d744526bfbb580d50f551240dcd148aded92c384305a6&amp;X-Amz-SignedHeaders=host&amp;response-content-disposition=attachment%3B%20filename%3Dibm-concert-std-workflows.tgz&amp;response-content-type=application%2Foctet-stream\nResolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\nConnecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.110.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 40468 (40K) [application/octet-stream]\nSaving to: \u2018ibm-concert-std-workflows.tgz\u2019\n\nibm-concert-std-workflows.tgz           100%[===============================================================================&gt;]  39.52K  --.-KB/s    in 0.001s\n\n2025-01-29 14:39:45 (41.7 MB/s) - \u2018ibm-concert-std-workflows.tgz\u2019 saved [40468/40468]\n\n$ tar xfz ibm-concert-std-workflows.tgz\n$ ${DOCKER_EXE} login ${CONCERT_REGISTRY} --username=${CONCERT_REGISTRY_USER} --password=${CONCERT_REGISTRY_PASSWORD}\nLogin Succeeded!\n</code></pre> <p>Obtain the CONCERT_HUB_KEY for your IBM Concert instance.  You will need the URL for your concert instance (eg: <code>https://YOUR_VM_PUBLIC_IP:12443</code>) and a Concert API Key.  To obtain your Concert API Key:</p> <ul> <li>Log on to your IBM Concert VM instance, and click on your user avatar on the top right corner, and click on API Key:</li> </ul> <p></p> <ul> <li> <p>Click <code>Generate API key</code>, Note down your API key, you will not see it again!</p> </li> <li> <p>Run the following to fetch your Concert API Key:</p> </li> </ul> get concert hub key <pre><code>export CONCERT_URL=\"https://YOUR_VM_PUBLIC_IP:12443\"\nexport CONCERT_API_KEY=\"a...\" # (1)!\n./workflows/bin/tethering/get_concert_info.sh --concert-url=${CONCERT_URL} --c-api-key=${CONCERT_API_KEY}\n</code></pre> output <pre><code>$ export CONCERT_URL=\"https://52.116.134.155:12443\"\n$ export CONCERT_API_KEY=[REDACTED]\n$ ./workflows/bin/tethering/get_concert_info.sh --concert-url=${CONCERT_URL} --c-api-key=${CONCERT_API_KEY}\nParsing option: '--concert-url', value: 'https://52.116.134.155:12443'\nParsing option: '--c-api-key', value: '[REDACTED]'\n----------\nAPIKEY is XXXX_KEY\n----------\nPARAMETERS\nAPIKEY: ([XXXXXXXXXXXXXXXXXX])\nget_info\nGOT VALUE CONCERT_HUB_KEY \"XXXXXXXXXXXXXXX\"\nGOT VALUE CONCERT_HUB_URL \"https://ibm-roja-portal-gw-svc.roja.svc:12443\"\nGOT VALUE WORKFLOW_APIKEY \"XXXXXXXXXXXXX\"\n</code></pre> <ul> <li>Export the concert hub key from the output of this command:</li> </ul> <p></p><pre><code>export CONCERT_HUB_KEY=\"&lt;from-output&gt;\"\n</code></pre> Modify <code>workflows/bin/concert-workflows-values.yaml</code> with the following changes:<p></p> <ul> <li>Update the <code>imageRegistry</code> parameter to <code>cp.icr.io/cp/concert</code></li> <li>Update the <code>rna.instance.address</code> parameter to <code>https://YOUR_VM_PUBLIC_IP</code> (do not include port 12443)</li> <li>Update the <code>rna.instance.CONCERT_HUB_URL</code> parameter to <code>https://YOUR_VM_PUBLIC_IP:12443</code></li> <li>Update the <code>rna.instance.CONCERT_HUB_KEY</code> parameter to the CONCERT_HUB_KEY obtained above.</li> </ul> <p>The file should look like this</p> concert-workflows-values.yaml <pre><code>imageRegistry:  \"cp.icr.io/cp/concert\"\nimagePullSecretName: ibm-entitlement-key\nrna:\n  instance:\n    address: 'https://52.118.189.193'\n    installation_mode: concert\n    CONCERT_HUB_URL: \"https://52.118.189.193:12443\"\n    CONCERT_HUB_KEY: \"77c2...\"\n  faas:\n    faas_namespace: \"faas\"\n    python:\n      pip_registry: \"https://pypi.org/pypi\"\n      pip_ignore_ssl_errors: true\n</code></pre> <p>Create a namespace in your k3s cluster.</p> Installing Concert Workflows on a VM <pre><code>export CW_NAMESPACE=concert\nkubectl create ns ${CW_NAMESPACE}\n</code></pre> output <pre><code>$ export CW_NAMESPACE=concert\n$ kubectl create ns ${CW_NAMESPACE}\nnamespace/concert created\n</code></pre> <p>Create a secret called ibm-entitlement-key in the same namespace. For example:</p> ibm-entitlement-key secret <pre><code>kubectl create secret docker-registry ibm-entitlement-key \\\n  --docker-server=cp.icr.io \\\n  --docker-username=${CONCERT_REGISTRY_USER} \\\n  --docker-password=${CONCERT_REGISTRY_PASSWORD} \\\n  --namespace=\"${CW_NAMESPACE}\"\n</code></pre> output <pre><code>$ kubectl create secret docker-registry ibm-entitlement-key \\\n  --docker-server=cp.icr.io \\\n  --docker-username=${CONCERT_REGISTRY_USER} \\\n  --docker-password=${CONCERT_REGISTRY_PASSWORD} \\\n  --namespace=\"${CW_NAMESPACE}\"\nsecret/ibm-entitlement-key created\n</code></pre> <p>The Concert Workflow installer relies on Helm. Install it on the VM:</p> install Helm <pre><code>curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\nchmod 700 get_helm.sh\n./get_helm.sh\nsudo chmod 755 /usr/local/bin/helm\n</code></pre> output <pre><code>$ curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\n$ chmod 700 get_helm.sh\n$ ./get_helm.sh\n./get_helm.sh: line 138: /usr/local/bin/helm: Permission denied\nHelm v3.17.0 is available. Changing from version .\nDownloading https://get.helm.sh/helm-v3.17.0-linux-amd64.tar.gz\nVerifying checksum... Done.\nPreparing to install helm into /usr/local/bin\nhelm installed into /usr/local/bin/helm\n$ sudo chmod 755 /usr/local/bin/helm\n</code></pre> <p>Install Concert Workspaces with the following command.  The process takes around 10-15 minutes:</p> install Concert Workspaces <pre><code>./workflows/bin/setup --namespace=\"${CW_NAMESPACE}\"\n</code></pre> output <pre><code>$ ./workflows/bin/setup --namespace=\"${CW_NAMESPACE}\"\n[...]\nThu Mar 27 03:45:20 UTC 2025 =========== ...\nThu Mar 27 03:45:23 UTC 2025 =========== ...\nThu Mar 27 03:45:27 UTC 2025 =========== ...\nThu Mar 27 03:45:27 UTC 2025 =========== 'COMPLETED' IBM Concert Workflows installation\nThu Mar 27 03:45:27 UTC 2025 =========== INFO: IBM Concert Workflows 1.1.5 core installation is successful.\nThu Mar 27 03:45:27 UTC 2025 =========== 'COMPLETED' IBM Concert Workflows installation\nThu Mar 27 03:45:27 UTC 2025 =========== 'STARTED' IBM Concert Workflows registration\n</code></pre> <p>Bug</p> <p>Due to some automation issues with the installer, allow 5-10 minutes after the above command is completed to avoid failure in the next steps.</p> <p>To validate the install, check the status of the pods with <code>kubectl get pods -n \"${CW_NAMESPACE}\"</code>. Ensure sure all containers in all pods are <code>Running</code> or <code>Completed</code>:</p> <pre><code>kubectl get pods -n ${CW_NAMESPACE}\n</code></pre> output <pre><code>NAME                                                    READY   STATUS      RESTARTS        AGE\nrna-core-addon-ansible-7d74494644-th9tx                 1/1     Running     0               9m44s\nrna-core-addon-faas-f46f9db87-ljl8c                     1/1     Running     0               9m43s\nrna-core-addon-mqws-86474c4dc4-n66qm                    1/1     Running     0               9m44s\nrna-core-addon-napalm-78cf7f6944-s2jmm                  1/1     Running     0               9m44s\nrna-core-addon-pdf-f5f4bd7c9-pqvwm                      1/1     Running     0               9m43s\nrna-core-addon-textfsm-6f47c8959d-ndpmr                 1/1     Running     0               9m42s\nrna-core-addon-themes-85748bfdc4-xnxhn                  1/1     Running     0               9m43s\nrna-core-configure-admin-job-kaalikbj-j95t5             0/1     Completed   0               9m44s\nrna-core-configure-ui-themes-job-cet85fru-s8z2x         0/1     Completed   0               9m44s\nrna-core-install-integrations-job-bqt8innt-lqjf6        0/1     Completed   0               9m44s\nrna-core-mysqldb-0                                      1/1     Running     0               9m44s\nrna-core-object-storage-0                               1/1     Running     0               9m44s\nrna-core-pliant-api-dc7f55555-4nl4q                     2/2     Running     1 (9m24s ago)   9m42s\nrna-core-pliant-api-dc7f55555-pf2tl                     2/2     Running     1 (9m25s ago)   9m42s\nrna-core-pliant-app-gateway-6d4f65fc79-l6snm            1/1     Running     5 (8m4s ago)    9m42s\nrna-core-pliant-compiler-69dc77cdf7-g8s8x               1/1     Running     0               9m44s\nrna-core-pliant-db-migration-c4877b8c8-9csbg            1/1     Running     1 (9m22s ago)   9m44s\nrna-core-pliant-flow-converter-5996cd4d9f-jqzzt         1/1     Running     1 (9m41s ago)   9m44s\nrna-core-pliant-front-747ff9b84c-7xlhs                  1/1     Running     0               9m44s\nrna-core-pliant-front-747ff9b84c-j6fm4                  1/1     Running     0               9m44s\nrna-core-pliant-image-registry-0                        1/1     Running     0               9m44s\nrna-core-pliant-kv-store-5775cf8757-w4td6               1/1     Running     0               9m43s\nrna-core-pliant-proxy-6bfc44f9b6-z9fbc                  1/1     Running     0               9m44s\nrna-core-pliant-scheduler-84897d8975-8rcvv              1/1     Running     5 (7m52s ago)   9m42s\nrna-core-pliant-stats-0                                 1/1     Running     1 (9m38s ago)   9m44s\nrna-core-pliant-worker-f4f95d6b6-pwmrx                  1/1     Running     6 (4m30s ago)   9m44s\nrna-core-pliant-worker-f4f95d6b6-tfw46                  1/1     Running     6 (4m42s ago)   9m44s\nrna-core-pliant-worker-nodejs-config-6c4f6cb568-262c8   1/1     Running     0               9m43s\nrna-core-rabbitmq-0                                     1/1     Running     0               9m44s\n</code></pre>"},{"location":"concert/old%20labs/concert-workflows-old/#register-concert-workflows-with-concert","title":"Register Concert Workflows with Concert","text":"<p>To register Concert Workflows as an add-on to your Concert instance, follow this process</p> register workflows <pre><code>export CONCERT_HUB_URL=https://YOUR_VM_PUBLIC_IP:12443\nexport CONCERT_HUB_KEY=[GET FROM STEPS ABOVE]\nexport EXTNS_DIR=$PWD/workflows/extns/\nexport ADDON_NAME=concert_workflows\nexport EXT_URL=https://YOUR_PRIVATE_VM_IP # (1)!\n./workflows/bin/tethering/tether-to-hub.sh \\\n  --concert-hub-url=\"$CONCERT_HUB_URL\" \\\n  --concert-hub-key=\"$CONCERT_HUB_KEY\" \\\n  --extn-dir=\"$EXTNS_DIR\" \\\n  --provider=\"$ADDON_NAME\" \\\n  --external-url=\"$EXT_URL\"\n</code></pre> <ol> <li>This will show as 'Private IP' in your Techzone reservation</li> </ol> output <pre><code>$ export CONCERT_HUB_URL=https://52.116.134.155:12443\n$ export CONCERT_HUB_KEY=c843513c9380f101d6aee6e2\n$ export EXTNS_DIR=$PWD/workflows/extns/\n$ export ADDON_NAME=concert_workflows\n$ export EXT_URL=https://52.116.134.155\n$ ./workflows/bin/tethering/tether-to-hub.sh \\\n      --concert-hub-url=\"$CONCERT_HUB_URL\" \\\n      --concert-hub-key=\"$CONCERT_HUB_KEY\" \\\n      --extn-dir=\"$EXTNS_DIR\" \\\n      --provider=\"$ADDON_NAME\" \\\n      --external-url=\"$EXT_URL\"\nParsing option: '--concert-hub-url', value: 'https://52.116.134.155:12443'\nParsing option: '--concert-hub-key', value: 'c843513c9380f101d6aee6e2'\nParsing option: '--extn-dir', value: '/home/itzuser/workflows/extns/'\nParsing option: '--provider', value: 'concert_workflows'\nParsing option: '--external-url', value: 'https://52.116.134.155'\ntarget_ns (optional) empty\n----------\nENV VARS\nCONCERT_HUB_URL https://52.116.134.155:12443\nCONCERT_HUB_KEY c843513c9380f101d6aee6e2\n----------\nPARAMETERS\nCONCERT_HUB_URL: (https://52.116.134.155:12443)\nCONCERT_HUB_KEY: (c843513c9380f101d6aee6e2)\nEXNT_DIR: (/home/itzuser/workflows/extns/)\nPROVIDER: (concert_workflows)\nTARGET_NS: ()\nEXTERNAL_URL: (https://52.116.134.155)\n/home/itzuser/workflows/bin/tethering\nDifferent NS or Cluster. Need to add ConfigMap app-cfg-cm.\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                Dload  Upload   Total   Spent    Left  Speed\n100  230k  100  230k    0     0  4700k      0 --:--:-- --:--:-- --:--:-- 4700k\nGOT VALUE ROJA_AUTH \"native\"\nGOT VALUE ROJA_CONTEXT \"sw_dev\"\nGOT VALUE CONCERT_HUB_URL https://52.116.134.155\n===&gt; APPLY Config Maps app-cfg-cm\nconfigmap/app-cfg-cm serverside-applied\nGOT VALUE CONCERT_HUB_KEY \"c843513c9380f101d6aee6e2\"\n===&gt; APPLY Secret app-cfg-secret\nsecret/app-cfg-secret serverside-applied\nGOT VALUE tls.key \"-----BEGIN PRIVATE KEY-----\\n\nGOT VALUE tls.crt \"-----BEGIN CERTIFICATE-----\\n\nGOT VALUE /tmp/tls_ca_bundle.pem \"-----BEGIN CERTIFICATE-----\\n\n===&gt; APPLY Secret app-cfg-internal-tls\nsecret/app-cfg-internal-tls serverside-applied\nParsing option: '--extn-dir', value: '/home/itzuser/workflows/extns/'\nParsing option: '--provider', value: 'concert_workflows'\nParsing option: '--external-url', value: 'https://52.116.134.155'\ntarget_ns (optional) empty\n----------\nENV VARS\nCONCERT_HUB_URL https://52.116.134.155:12443\nCONCERT_HUB_KEY c843513c9380f101d6aee6e2\n----------\nPARAMETERS\nEXTN_DIR: (/home/itzuser/workflows/extns/)\nPROVIDER: (concert_workflows)\nTARGET_NS: ()\nEXTERNAL_URL: (https://52.116.134.155)\nCheck for mandatory fields ...\n\nParsing: /home/itzuser/workflows/extns/workflows.extn.jsonpost_extensions\n{\"extensions\":[{\"id\":\"dc59768f-5dda-4857-b137-c3970471d1e2\",\"provider\":\"concert_workflows\",\"name\":\"workflows.registry\",\"display_name\":null,\"extends\":\"portal.route\",\"flag\":null,\"applies_to\":null,\"applies_when\":null,\"data\":\"{\\\"route_type\\\": \\\"pass_thru\\\", \\\"location\\\": \\\"/v2/\\\", \\\"local_svc_url\\\": \\\"https://pliant-proxy\\\", \\\"additional\\\": {\\\"proxy_set_header\\\": [\\\"Upgrade $http_upgrade\\\", \\\"Connection $connection_upgrade\\\"]}, \\\"proxy_url\\\": \\\"https://52.116.134.155\\\"}\",\"nginx_conf\":\"location ~ /v2/([/\\\\?].*|$)\\n{\\n    proxy_pass https://52.116.134.155$1$is_args$args;\\n\\n    proxy_set_header Upgrade $http_upgrade;\\n    proxy_set_header Connection $connection_upgrade;\\n}\"},{\"id\":\"dd25566a-0a37-42c3-8bea-290efe3204ea\",\"provider\":\"concert_workflows\",\"name\":\"workflows.exchange.pass_thru\",\"display_name\":null,\"extends\":\"portal.route\",\"flag\":null,\"applies_to\":null,\"applies_when\":null,\"data\":\"{\\\"route_type\\\": \\\"pass_thru\\\", \\\"location\\\": \\\"/cw_internal/api/concert/token\\\", \\\"local_svc_url\\\": \\\"https://pliant-proxy\\\", \\\"additional\\\": {\\\"rewrite\\\": \\\"^/cw_internal(.*)$ $1 break\\\"}, \\\"proxy_url\\\": \\\"https://52.116.134.155\\\"}\",\"nginx_conf\":\"location ~ /cw_internal/api/concert/token([/\\\\?].*|$)\\n{\\n    proxy_pass https://52.116.134.155;\\n\\n    rewrite ^/cw_internal(.*)$ $1 break;\\n}\"},{\"id\":\"25f12200-bc12-423a-a0a7-9e6f5ed03b6c\",\"provider\":\"concert_workflows\",\"name\":\"workflows.exchange.internal\",\"display_name\":null,\"extends\":\"portal.route\",\"flag\":null,\"applies_to\":null,\"applies_when\":null,\"data\":\"{\\\"route_type\\\": \\\"pass_thru\\\", \\\"location\\\": \\\"/a/\\\", \\\"local_svc_url\\\": \\\"https://pliant-proxy\\\", \\\"additional\\\": {\\\"proxy_http_version\\\": \\\"1.1\\\", \\\"proxy_set_header\\\": [\\\"Host $host\\\", \\\"X-Forwarded-Port $server_port\\\", \\\"X-Forwarded-For $proxy_add_x_forwarded_for\\\", \\\"Connection \\\\\\\"\\\\\\\"\\\"]}, \\\"proxy_url\\\": \\\"https://52.116.134.155\\\"}\",\"nginx_conf\":\"location ~ /a/([/\\\\?].*|$)\\n{\\n    proxy_pass https://52.116.134.155$1$is_args$args;\\n\\n    proxy_http_version 1.1;\\n    proxy_set_header Host $host;\\n    proxy_set_header X-Forwarded-Port $server_port;\\n    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\\n    proxy_set_header Connection \\\"\\\";\\n}\"},{\"id\":\"ea36c713-2837-4bc7-9c5e-6ff6d998aec2\",\"provider\":\"concert_workflows\",\"name\":\"workflows.exchange.api-docs\",\"display_name\":null,\"extends\":\"portal.route\",\"flag\":null,\"applies_to\":null,\"applies_when\":null,\"data\":\"{\\\"route_type\\\": \\\"pass_thru\\\", \\\"location\\\": \\\"/api-docs/\\\", \\\"local_svc_url\\\": \\\"https://pliant-proxy\\\", \\\"additional\\\": {\\\"proxy_http_version\\\": \\\"1.1\\\", \\\"proxy_set_header\\\": [\\\"Host $host\\\", \\\"X-Forwarded-Port $server_port\\\", \\\"X-Forwarded-For $proxy_add_x_forwarded_for\\\", \\\"Connection \\\\\\\"\\\\\\\"\\\"]}, \\\"proxy_url\\\": \\\"https://52.116.134.155\\\"}\",\"nginx_conf\":\"location ~ /api-docs/([/\\\\?].*|$)\\n{\\n    proxy_pass https://52.116.134.155$1$is_args$args;\\n\\n    proxy_http_version 1.1;\\n    proxy_set_header Host $host;\\n    proxy_set_header X-Forwarded-Port $server_port;\\n    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\\n    proxy_set_header Connection \\\"\\\";\\n}\"},{\"id\":\"8ac39e33-6557-4127-b34f-30779d3f8372\",\"provider\":\"concert_workflows\",\"name\":\"workflows.exchange\",\"display_name\":null,\"extends\":\"portal.route\",\"flag\":null,\"applies_to\":null,\"applies_when\":null,\"data\":\"{\\\"route_type\\\": \\\"auth\\\", \\\"xchg_endpoint\\\": \\\"/cw_internal/api/concert/token\\\", \\\"location\\\": \\\"/cw\\\", \\\"local_svc_url\\\": \\\"https://pliant-proxy\\\", \\\"additional\\\": {\\\"limit_req\\\": \\\"zone=uilimit burst=100 nodelay\\\", \\\"proxy_set_header\\\": [\\\"X-Forwarded-For $proxy_add_x_forwarded_for\\\", \\\"Host $host\\\"], \\\"proxy_read_timeout\\\": \\\"300s\\\", \\\"proxy_ssl_server_name\\\": \\\"on\\\", \\\"proxy_hide_header\\\": \\\"Content-Security-Policy\\\", \\\"add_header\\\": \\\"Content-Security-Policy \\\\\\\"default-src 'self'; object-src 'none'; style-src \\\\\\\\* 'unsafe-inline'; font-src \\\\\\\\* data:; img-src 'self' data:;\\\\\\\" always\\\", \\\"proxy_redirect\\\": [\\\"http://$host/api https://$host/cw/api\\\", \\\"https://$host/api https://$host/cw/api\\\"]}, \\\"proxy_url\\\": \\\"https://52.116.134.155\\\"}\",\"nginx_conf\":\"location ~ /cw([/\\\\?].*|$)\\n{\\n    set_by_lua $xchg_endpoint ' return \\\"/cw_internal/api/concert/token\\\" ';\\n    lua_ssl_verify_depth 2;\\n    lua_ssl_trusted_certificate /app/tmp/self-signed-ssl/tls.crt;\\n    lua_ssl_certificate_key /app/tmp/self-signed-ssl/tls.key;\\n    access_by_lua_file /nginx_data/lib/auth.lua;\\n\\n    proxy_pass https://52.116.134.155$1$is_args$args;\\n\\n    limit_req zone=uilimit burst=100 nodelay;\\n    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\\n    proxy_set_header Host $host;\\n    proxy_read_timeout 300s;\\n    proxy_ssl_server_name on;\\n    proxy_hide_header Content-Security-Policy;\\n    add_header Content-Security-Policy \\\"default-src 'self'; object-src 'none'; style-src \\\\* 'unsafe-inline'; font-src \\\\* data:; img-src 'self' data:;\\\" always;\\n    proxy_redirect http://$host/api https://$host/cw/api;\\n    proxy_redirect https://$host/api https://$host/cw/api;\\n}\"},{\"id\":\"fb183db5-ba77-4f9d-b53f-d9af22af2c89\",\"provider\":\"concert_workflows\",\"name\":\"workflows.root\",\"display_name\":\"Workflows\",\"extends\":\"concert.topmenu\",\"flag\":null,\"applies_to\":null,\"applies_when\":null,\"data\":\"{\\\"description\\\": \\\"i18.workflows.root.desc\\\", \\\"root\\\": \\\"true\\\"}\",\"nginx_conf\":\"\"},{\"id\":\"bfd3ab22-25da-4d63-9931-14e85d946af8\",\"provider\":\"concert_workflows\",\"name\":\"workflows_manage\",\"display_name\":\"Manage\",\"extends\":\"concert.topmenu\",\"flag\":null,\"applies_to\":null,\"applies_when\":null,\"data\":\"{\\\"description\\\": \\\"i18n.workflows.manage.desc\\\", \\\"parent\\\": \\\"workflows.root\\\", \\\"url\\\": \\\"/cw/flows\\\"}\",\"nginx_conf\":\"\"},{\"id\":\"354de962-8a53-40d8-9011-b2798160b001\",\"provider\":\"concert_workflows\",\"name\":\"workflows_authentications\",\"display_name\":\"Authentications\",\"extends\":\"concert.topmenu\",\"flag\":null,\"applies_to\":null,\"applies_when\":null,\"data\":\"{\\\"description\\\": \\\"i18n.workflows.authentications.desc\\\", \\\"parent\\\": \\\"workflows.root\\\", \\\"url\\\": \\\"/cw/authentications\\\"}\",\"nginx_conf\":\"\"},{\"id\":\"89b82315-33d4-4914-b54f-06e9df4c4382\",\"provider\":\"concert_workflows\",\"name\":\"workflows_jobs\",\"display_name\":\"Schedule\",\"extends\":\"concert.topmenu\",\"flag\":null,\"applies_to\":null,\"applies_when\":null,\"data\":\"{\\\"description\\\": \\\"i18n.workflows.schedule.desc\\\", \\\"parent\\\": \\\"workflows.root\\\", \\\"url\\\": \\\"/cw/jobs\\\"}\",\"nginx_conf\":\"\"},{\"id\":\"f810ea09-6ca5-498d-a227-f8b59adfbff7\",\"provider\":\"concert_workflows\",\"name\":\"workflows_logs\",\"display_name\":\"History\",\"extends\":\"concert.topmenu\",\"flag\":null,\"applies_to\":null,\"applies_when\":null,\"data\":\"{\\\"description\\\": \\\"i18n.workflows.history.desc\\\", \\\"parent\\\": \\\"workflows.root\\\", \\\"url\\\": \\\"/cw/logs\\\"}\",\"nginx_conf\":\"\"}]}\nCleaning things up ...\n</code></pre> <p>Make the <code>enable_concert_workflows.sh</code> script executable:</p> <pre><code>chmod +x ./workflows/bin/tethering/enable_concert_workflows.sh\n</code></pre> <p>Run the following command to fetch the <code>VALUE WORKFLOW_APIKEY</code>:</p> <pre><code>export CONCERT_URL=\"https://YOUR_VM_PUBLIC_IP:12443\"\nexport CONCERT_API_KEY=\"a...\" \n./workflows/bin/tethering/get_concert_info.sh --concert-url=${CONCERT_URL} --c-api-key=${CONCERT_API_KEY}\n</code></pre> output <pre><code>[...]\nGOT VALUE WORKFLOW_APIKEY \"3f26c...\"\n</code></pre> <p>Set the required environment variables:</p> <pre><code>export CONCERT_API_KEY=&lt;valid API key generated in the Concert&gt;\nexport CONCERT_USER=ibmconcert\nexport WORKFLOW_APIKEY=\"from-previous-step\"\n</code></pre> <pre><code>./workflows/bin/tethering/enable_concert_workflows.sh --concert-url=\"$CONCERT_HUB_URL\" --c-api-key=\"$CONCERT_API_KEY\" --c-user=\"$CONCERT_USER\" --workflow-apikey=\"$WORKFLOW_APIKEY\"\n</code></pre>"},{"location":"concert/old%20labs/concert-workflows-old/#creating-your-first-workflow","title":"Creating your first Workflow","text":"<p>Back in your browser, refresh the Concert home page. You should now see a Workflow tab at the top of your Concert instance. Select <code>Manage</code>:</p> <p></p> <p>Select <code>Create Workflow</code>, give your workflow a name, and click <code>Select Template</code>. Set the template to <code>HTTP Template</code>:</p> <p></p> <p>Create your workflow and run it using the <code>Run</code> button. You should see the workflow make a request to <code>example.com</code>!</p> <p>Workflow Lab Complete</p> <p>Congratulations, you have successfully completed the Workflow install lab.</p>"},{"location":"concert/old%20labs/concert-workflows-old/#acknowledgements","title":"Acknowledgements","text":"<p>This lab was based on: https://pages.github.ibm.com/cs-tel-ibm-concert/training/module3/concert-workflows/#installing-concert-workflows-on-a-vm</p>"},{"location":"concert/old%20labs/index-old/","title":"IBM Concert","text":""},{"location":"concert/old%20labs/index-old/#ibm-concert","title":"IBM Concert","text":"<p>For the IBM Concert hands-on Lab we will be following certain sections from the CS &amp; TEL IBM Concert Training GitHub Pages site.</p> <p>IBM Concert provides capabilities for tackling five main use cases:</p> <ul> <li>Prioritization and mitigation of CVEs: IBM Concert computes a custom CVE score that aims to facilitate the priorization process by answering the question \"which CVEs should be remediated first?\". Concert also provides recommendations for remediatng CVEs.</li> <li>Reviewal of the steps (evidence) for compliance: IBM Concert uses Gen AI to help you determine whether the steps/evidence for addressing a compliance control item are meaningful.</li> <li>Certificate management: IBM Concert identifies certificates that are about to expire to avoid outages in your environment.</li> <li>Software composition: Understand the packages and components that make up your applications. Identify and mitigate risks associated with those packages and components based on several key indicators of reliability, maintainability, and security. Get recommended actions to address risks, such as those related to outdated or unsupported packages, licenses compliance issues, and vulnerabilities.</li> <li>Resilence posture: Assess the resilience of your applications and environments. Track the resilience of your applications over time.</li> </ul> <p>In addition to the above, you can leverage Concert Workflows to create custom, low-code integrations for data ingestion and for remediation.</p>"},{"location":"concert/old%20labs/index-old/#installing-concert","title":"Installing Concert","text":"<p>IBM Concert can be deployed in 3 different configurations.</p> <ul> <li>VM Based install: IBM Concert is installed as a series containers. Very limited high availability (HA), but can be deployed in a smaller footprint.</li> <li>OpenShift based install: IBM Concert is installed on top of OpenShift, and leverages the CPFS framework for authentication. Much more resilient and ready for production deployments.</li> <li>SaaS deployment: IBM Concert is deployed as a services on either IBM Cloud or AWS.</li> </ul>"},{"location":"concert/old%20labs/index-old/#playbook","title":"Playbook","text":"<p>IBM Concert Playbook</p>"},{"location":"devops/","title":"DevOps Introduction","text":""},{"location":"devops/#devops-introduction","title":"DevOps Introduction","text":"<p>DevOps has recently become a popular buzzword in the Cloud World. It varies from business to business and it means a lot different things to different people. In traditional IT, organizations have separate teams for Development and Operations. The development team is responsible for coding and operations team is responsible for releasing it to production. When it comes to this two different teams, there will always be some sort of differences. It may be due to the usage of different system environments, software libraries etc. In order to level this up, DevOps came into play.</p>"},{"location":"devops/#what-is-devops","title":"What is DevOps ?","text":"<p>\u201cDevOps is a philosophy, a cultural shift that merges operations with development and demands a linked toolchain of technologies to facilitate collaborative change. DevOps toolchains \u2026 can include dozens of non-collaborative tools, making the task of automation a technically complex and arduous one.\u201d - Gartner</p> <p></p> <p>These days every business has critical applications which can never go down. Some of the examples are as follows.</p> <p></p> <p>In order to make sure that these applications are up and running smoothly, we need DevOps.</p> <p>Adopting DevOps allows enterprises to create, maintain and improve their applications at a faster pace than the traditional methods. Today, most of the global organizations adopted DevOps.</p>"},{"location":"devops/#presentations","title":"Presentations","text":"<p>Tekton Overview </p> <p>GitOps Overview </p>"},{"location":"devops/#benefits-of-devops","title":"Benefits of DevOps","text":"<ul> <li>Continuous software delivery</li> <li>High quality software</li> <li>Increased speed and faster problem resolution</li> <li>Increased reliability</li> <li>Easier to manage the software</li> <li>Collaboration and enhanced team communication</li> <li>Customer satisfaction etc.</li> </ul>"},{"location":"devops/#understanding-devops","title":"Understanding DevOps","text":"<p>Like we mentioned before, often development teams and operation teams are in conflict with each other. Developers keeping changing the software to include new features where as operation engineers wants to keep the system stable.</p> <ul> <li>Their goals are different.</li> <li>They use different processes.</li> <li>They use different tools.</li> </ul> <p>All these may be different reasons for the gap between these two teams.</p> <p>To solve this gap between the two teams, we need DevOps. It closes the gap by aligning incentives and sharing approaches for tools and processes. It helps us to streamline the software delivery process. From the time we begin the project till its delivery, it helps us to improve the cycle time by emphasizing the learning by gathering feedback from production to development.</p> <p>It includes several aspects like the below.</p> <ul> <li>Automation - It is quite essential for DevOps. It helps us to gather quick feedback.</li> <li>Culture - Processes and tools are important. But, people are always more important.</li> <li>Measurement - Shared incentives are important. Quality is critical.</li> <li>Sharing - Need a Culture where people can share ideas, processes and tools.</li> </ul> <p> </p>"},{"location":"devops/#where-to-start","title":"Where to start ?","text":"<p>Understanding the eco system of your software is important. Identify all the environments like dev, test, prod etc. you have in your system and how the delivery happens from end to end.</p> <ul> <li>Define continuous delivery</li> <li>Establish proper collaboration between teams</li> <li>Make sure the teams are on same pace</li> <li>Identify the pain points in your system and start working on them.</li> </ul>"},{"location":"devops/#devops-best-practices","title":"DevOps Best Practices","text":"<p>These are some of the standard practices adopted in DevOps.</p> <ul> <li>Source Code Management</li> <li>Code Review</li> <li>Configuration Management</li> <li>Build Management</li> <li>Artifact Repository Management</li> <li>Release Management</li> <li>Test Automation</li> <li>Continuous Integration</li> <li>Continuous Delivery</li> <li>Continuous Deployment</li> <li>Infrastructure As Code</li> <li>Automation</li> <li>Key Application Performance Monitoring/Indicators</li> </ul> <p>Source Code Management</p> <p>Source Code Management (SCM) systems helps to maintain the code base. It allows multiple developers to work on the code concurrently. It prevents them from overwriting the code and helps them to work in parallel from different locations.</p> <p>Collaboration is an important concept in devOps and SCM helps us to achieve it by coordination of services across the development team. It also tracks co-authoring, collaboration, and individual contributions. It helps the developers to audit the code changes. It also allows rollbacks if required. It also enables backup and allows recovery when required.</p> <p>Code Review</p> <p>Code reviews allows the developer to improve the quality of code. They help us to identify the problems in advance. By reviewing the code, we can fix some of the problems like memory leaks, buffer overflow, formatting errors etc.</p> <p>This process improves the collaboration across the team. Also, code defects are identified and removed before merging them with the main stream there by improving the quality of the code.</p> <p>Configuration Management</p> <p>Configuration Management is managing the configurations by identifying, verifying, and maintaining them. This is done for both software and hardware. The configuration management tools make sure that configurations are properly configured across different systems as per the requirements.</p> <p>This helps to analyze the impact on the systems due to configurations. It makes sure the provisioning is done correctly on different systems like dev, QA, prod etc. It simplifies the coordination between development and operations teams.</p> <p>Build Management</p> <p>Build Management helps to assmble the build environment by packaging all the required components such as the source code, dependencies, etc of the software application together in to a workable unit. Builds can be done manually, on-demand or automated.</p> <p>It ensures that the software is stable and it is reusable. It improves the quality of the software and makes sure it is reliable. It also increases the efficiency.</p> <p>Artifact Repository Management</p> <p>Artifact Repository Management system is used to manage the builds. It is dedicated server which is used to store all the binaries which were outputs of the successful builds.</p> <p>It manages the life cycles of different artifacts. It helps you to easily share the builds across the team. It controls access to the build artifacts by access control.</p> <p>Release Management</p> <p>Release management is a part of software development lifecycle which manages the release from development till deployment to support. Requests keep coming for the addition of the new features. Also, sometimes there may be need to change the existing functionality. This is when the cycle begins for the release management. Once, the new feature or change is approved, it is designed, built, tested, reviewed, and after acceptance, deployed to production. After this, it goes to maintainence and even at this point, there may be need for enhancement. If that is the case, it will be a new cycle again.</p> <p>It helps us to track all the phases and status of deployments in different environments.</p> <p>Test Automation</p> <p>Manual testing takes lots of time. We can automate some of the manual tests which are repetitive, time consuming, and have defined input by test automation.</p> <p>Automatic tests helps to improve the code quality, reduces the amount of time spent on testing, and improves the effectiveness of the overall testing life cycle.</p> <p>Continuous Integration</p> <p>Continuous integration allows the developers to continuously integrate the code they developed. Whenever a latest code change is made and committed to the source control system, the source code is rebuilt and this is then forwarded to testing.</p> <p>With this, the latest code is always available, the builds are faster and the tests are quick.</p> <p>Continuous Delivery</p> <p>Continuous Delivery is the next step to Continuous Integration. In the integration, the code is built and tested. Now in the delivery, this is taken to staging environment. This is done in small frequencies and it makes sure the functionality of the software is stable.</p> <p>It reduces the manual overhead. The code is continuously delivered and constantly reviewed.</p> <p>Continuous Deployment</p> <p>Continuous Deployment comes after Continuous Delivery. In the deployment stage, the code is deployed to the production environment. The entire process is automated in this stage.</p> <p>This allows faster software releases. Improves the collaboration across the teams. Enhances the code quality.</p> <p>Infrastructure As Code</p> <p>Infrastructure as Code is defining the infrastructure services as a software code. they are defines as configuration files. Traditionally, in on-premise application, these are run by system administrators but in cloud, the infrastructure is maintained like any other software code.</p> <p>Helps us to change the system configuration quickly. Tracking is easy and end to end testing is possible. Infrastructure availability is high.</p> <p>Automation</p> <p>Automation is key part to DevOps. Without automation, DevOps is not efficient.</p> <p>Automation comes into play whenever there is a repetitive task. Developers can automate infrastructure, applications, load balancers, etc.</p> <p>Key Application Performance Monitoring/Indicators</p> <p>DevOps is all about measuring the metrics and feedback, with continuous improvement processes. Collecting metrics and monitoring the software plays an important role. Different measures like uptime versus downtime, resolutions time lines etc. helps us to understand the performance of the system.</p>"},{"location":"devops/#devops-in-twelve-factor-apps","title":"Devops in Twelve factor apps","text":"<p>If you are new to Twelve factor methodology, have a look here. For more details, checkout Cloud-Native module.</p>"},{"location":"devops/#devops-reference-architecture","title":"DevOps Reference Architecture","text":"<ol> <li>Collaboration tools enable a culture of innovation. Developers, designers, operations teams, and managers must communicate constantly. Development and operations tools must be integrated to post updates and alerts as new builds are completed and deployed and as performance is monitored. The team can discuss the alerts as a group in the context of the tool.</li> <li>As the team brainstorms ideas, responds to feedback and metrics, and fixes defects, team members create work items and rank them in the backlog. The team work on items from the top of the backlog, delivering to production as they complete work.</li> <li>Developers write source code in a code editor to implement the architecture. They construct, change, and correct applications by using various coding models and tools.</li> <li>Developers manage the versions and configuration of assets, merge changes, and manage the integration of changes. The source control tool that a team uses should support social coding.</li> <li>Developers compile, package, and prepare software assets. They need tools that can assess the quality of the code that is being delivered to source control. Those assessments are done before delivery, are associated with automated build systems, and include practices such as code reviews, unit tests, code quality scans, and security scans.</li> <li>Binary files and other output from the build are sent to and managed in a build artifact repository.</li> <li>The release is scheduled. The team needs tools that support release communication and managing, preparing, and deploying releases.</li> <li>The team coordinates the manual and automated processes that are required for the solution to operate effectively. The team must strive towards continuous delivery with zero downtime. A/B deployments can help to gauge the effectiveness of new changes.</li> <li>The team must understand the application and the options for the application's runtime environment, security, management, and release requirements.</li> <li>Depending on the application requirements, some or all of the application stack must be considered, including middleware, the operating system, and virtual machines.</li> <li>The team must ensure that all aspects of the application and its supporting infrastructure are secured.</li> <li>The team plans, configures, monitors, defines criteria, and reports on application availability and performance. Predictive analytics can indicate problems before they occur.</li> <li>The right people on the team or systems are notified when issues occur.</li> <li>The team manages the process for responding to operations incidents, and delivers the changes to fix any incidents.</li> <li>The team uses analytics to learn how users interact with the application and measure success through metrics.</li> <li>When users interact with the application, they can provide feedback on their requirements and how the application is meeting them, which is captured by analytics as well.</li> <li>DevOps engineers manage the entire application lifecycle while they respond to feedback and analytics from the running application.</li> <li>The enterprise network is protected by a firewall and must be accessed through transformation and connectivity services and secure messaging services.</li> <li>The security team uses the user directory throughout the flow. The directory contains information about the user accounts for the enterprise.</li> </ol> <p>For a cloud native implementation, the reference architecture will be as follows.</p> <p></p>"},{"location":"devops/#references","title":"References","text":"<ul> <li>[Michael H\u00fcttermann (2012). DevOps for Developers. Publisher: Apress] (https://learning.oreilly.com/library/view/devops-for-developers/9781430245698/)</li> <li>[Sricharan Vadapalli (2018). DevOps: Continuous Delivery, Integration, and Deployment with DevOps. Publisher: Packt Publishing] (https://learning.oreilly.com/library/view/devops-continuous-delivery/9781789132991/)</li> <li>[DevOps Architecture] (https://www.ibm.com/cloud/garage/architectures/devOpsArchitecture/0_1)</li> </ul>"},{"location":"devops/gitops-ocp/","title":"Gitops ocp","text":"<p>GitOps is a declarative way to implement continuous deployment for cloud native applications. You can use GitOps to create repeatable processes for managing OpenShift Container Platform clusters and applications across multi-cluster Kubernetes environments. GitOps handles and automates complex deployments at a fast pace, saving time during deployment and release cycles.</p> <p>The GitOps workflow pushes an application through development, testing, staging, and production. GitOps either deploys a new application or updates an existing one, so you only need to update the repository; GitOps automates everything else.</p> <p>GitOps is a set of practices that use Git pull requests to manage infrastructure and application configurations. In GitOps, the Git repository is the only source of truth for system and application configuration. This Git repository contains a declarative description of the infrastructure you need in your specified environment and contains an automated process to make your environment match the described state. Also, it contains the entire state of the system so that the trail of changes to the system state are visible and auditable. By using GitOps, you resolve the issues of infrastructure and application configuration sprawl.</p> <p>GitOps defines infrastructure and application definitions as code. Then, it uses this code to manage multiple workspaces and clusters to simplify the creation of infrastructure and application configurations. By following the principles of the code, you can store the configuration of clusters and applications in Git repositories, and then follow the Git workflow to apply these repositories to your chosen clusters. You can apply the core principles of developing and maintaining software in a Git repository to the creation and management of your cluster and application configuration files.</p> <p>[Read More About GitOps and ]https://docs.openshift.com/gitops/1.15/understanding_openshift_gitops/about-redhat-openshift-gitops.html</p>"},{"location":"devops/argocd/","title":"Continuous Deployment","text":""},{"location":"devops/argocd/#continuous-deployment","title":"Continuous Deployment","text":"<p>Continuous Integration, Delivery, and Deployment are important devOps practices and we often hear a lot about them. These processes are valuable and ensures that the software is up to date timely.</p> <ul> <li>Continuous Integration is an automation process which allows developers to integrate their work into a repository. When a developer pushes his work into the source code repository, it ensures that the software continues to work properly. It helps to enable collaborative development across the teams and also helps to identify the integration bugs sooner.</li> <li>Continuous Delivery comes after Continuous Integration. It prepares the code for release. It automates the steps that are needed to deploy a build.</li> <li>Continuous Deployment is the final step which succeeds Continuous Delivery. It automatically deploys the code whenever a code change is done. Entire process of deployment is automated.</li> </ul>"},{"location":"devops/argocd/#what-is-gitops","title":"What is GitOps?","text":"<p>GitOps in short is a set of practices to use Git pull requests to manage infrastructure and application configurations. Git repository in GitOps is considered the only source of truth and contains the entire state of the system so that the trail of changes to the system state are visible and auditable.</p> <ul> <li>Traceability of changes in GitOps is no novelty in itself as this approach is almost universally employed for the application source code. However GitOps advocates applying the same principles (reviews, pull requests, tagging, etc) to infrastructure and application configuration so that teams can benefit from the same assurance as they do for the application source code.</li> <li>Although there is no precise definition or agreed upon set of rules, the following principles are an approximation of what constitutes a GitOps practice:</li> <li>Declarative description of the system is stored in Git (configs, monitoring, etc)</li> <li>Changes to the state are made via pull requests</li> <li>Git push reconciled with the state of the running system with the state in the Git repository</li> </ul>"},{"location":"devops/argocd/#gitops-and-continuous-deployment","title":"GitOps And Continuous Deployment","text":""},{"location":"devops/argocd/#how-does-gitops-fit-within-continuous-delivery","title":"How does GitOps fit within Continuous Delivery?","text":"<p>GitOps organizes the deployment process around code repositories as the central element. There are at least two repositories: the application repository and the environment configuration repository. The application repository contains the source code of the application and the deployment manifests to deploy the application. The environment configuration repository contains all deployment manifests of the currently desired infrastructure of an deployment environment. It describes what applications and infrastructural services (message broker, service mesh, monitoring tool, \u2026) should run with what configuration and version in the deployment environment.</p> <p>There are two important points:</p> <ol> <li>the codebase and deployment manifests live in the application's repository</li> <li>a second repository contains only the desired state of infrastructure for a given deployment environment</li> </ol> <p>This second point sounds suspicious, it deals with the desired state of a system. That sounds a lot like something Kubernetes would be great at.</p> <p>This diagram from the documentation does a good job of showing the overall concept behind the Push-based model of GitOps:</p> <p></p> <p>Everything flows as a series of triggers, in a single direction. The end result is a deployment. For our course, we will demonstrate this flow using only Tekton to demonstrate the difference between Tekton's approach to CI/CD and using a mixture of Tekton and ArgoCD.</p> <p>The other model is Pull-based:</p> <p></p> <p>In this case our <code>Operator</code> will be ArgoCD and deployment environment will be OpenShift/Kubernetes.</p> <p>Finally, the \"core idea\" statement at the beginning of the documentation sums it up best:</p> <p>The core idea of GitOps is having a Git repository that always contains declarative descriptions of the infrastructure currently desired in the production environment and an automated process to make the production environment match the described state in the repository.</p> <p>We want a single spot to change that updates all of our declarative infrastructure.</p> <p>This is a departure from traditional models of deployment that typically did not use declarative infrastructure, were exclusively push-based or time-based, and relied heavily on manual processes. This approach also gives an option for isolation of the production environment from developers (for example if required for compliance).</p> <p>This introduces a number of benefits:</p> <ul> <li>having a commit log of the production environment is a mechanism for performing fast rollbacks and audits</li> <li>developers already know Git and are comfortable in the tooling</li> <li>centralizes environment configuration while still allowing for parameterization</li> <li>ability to segregate developer access (\"left of Image Registry and Environment Repository\") versus production access (\"only access Image Registry and Environment Repository\")</li> </ul>"},{"location":"devops/argocd/#gitops-at-scale","title":"GitOps at Scale","text":"<p>GitOps enables easy scaling across different environments by leveraging Git as the single source of truth. </p> <p>With infrastructure and application configurations stored in version-controlled repositories, scaling becomes as simple as updating a manifest\u2014whether adding nodes to a cluster, deploying to new regions, or spinning up entirely new environments. </p> <p>Argo CD continuously monitors these Git repositories, automatically applying changes to match the declared state. </p> <p>This approach ensures that scaling is not only rapid but also consistent, reproducible, and transparent. Real-time synchronization and rollback capabilities allow for precise control, while Git\u2019s commit history provides clear audit trails, making it easy to monitor the state of all environments and ensure they remain aligned with organizational standards.</p> <p></p> <p>Push vs Pull at Scale</p> <p>As you scale your GitOps environment to several clusters you can start thinking of a different kind of 'Pull' and 'Push' model. </p> <p>As a first approach you might want to design your Continuous Delivery around a single Argo instance that pushes deployed applications to different clusters. </p> <p>However, as your application and infrastructure estate grows, you could consider deploying an Argo instance into each target cluster that pulls desired application configurations. This stops ArgoCD becoming a single point of failure, and reduces the workload on a single ArgoCD instance.</p> <p>For example, you might work with a client who manages several small OpenShift clusters deployed at the edge. Choosing a highly scalable pattern for application delivery in this environment is increasingly important.</p> <p>You can find some more info on GitOps Here</p> <p>We will dive deeper into this topic by building our very own GitOps Continuous delivery pipeline in the upcoming lab!</p>"},{"location":"devops/argocd/#argocd-overview","title":"ArgoCD Overview","text":""},{"location":"devops/argocd/#presentations","title":"Presentations","text":"<p>GitOps Overview </p>"},{"location":"devops/argocd/#activities","title":"Activities","text":"<p>These activities give you a chance to walkthrough building CD pipelines using ArgoCD.</p> <p>These tasks assume that you have:  - Reviewed the Continuous Deployment concept page.</p> Task Description Link Time Walkthroughs GitOps Introduction to GitOps with OpenShift Learn OpenShift GitOps 20 min Try It Yourself ArgoCD Lab Learn how to setup ArgoCD and Deploy Application ArgoCD 30 min <p>Once you have completed these tasks, you will have created an ArgoCD deployment and have an understanding of Continuous Deployment.</p>"},{"location":"devops/argocd/argocd/","title":"Deploying an Application to OpenShift","text":""},{"location":"devops/argocd/argocd/#deploying-an-application-to-openshift","title":"Deploying an Application to OpenShift","text":""},{"location":"devops/argocd/argocd/#description","title":"Description","text":"<p>This workshop covers containerising an application and deploying it to OpenShift using a CI/CD pipeline with OpenShift-Pipelines (Tekton) and OpenShift GitOps (ArgoCD).</p> <p>We will be building the following Pipeline to deploy our cloud native application:</p> <p></p> <ol> <li>The pipeline starts by cloning the application source code from a git repository</li> <li>It builds the application image</li> <li>It updates deployment YAML manifests using kustomize</li> <li>It pushes updated manifests to a gitops repository, creating a new feature branch</li> <li>It creates a pull request into main for the new feature</li> <li>Human approval is required to deploy the application</li> <li>ArgoCD then reconciles manifests to OpenShift once the pull request is merged</li> </ol>"},{"location":"devops/argocd/argocd/#prerequisites","title":"Prerequisites","text":"<ul> <li> Access to the OpenShift cluster Deployed during the OpenShift Install Lab</li> <li> ODF installed successfully</li> <li> The OpenShift internal registry deployed successfully</li> <li> Tekton Lab completed successfully</li> </ul> <p>Clear up any deployed resources from the previous lab:</p> <pre><code>NAMESPACE=tekton-demo\n</code></pre> <pre><code>oc delete deployment -n $NAMESPACE --ignore-not-found=true cloudnative &amp;&amp; \\\noc delete service -n $NAMESPACE --ignore-not-found=true cloudnative &amp;&amp; \\\noc delete route -n $NAMESPACE --ignore-not-found=true cloudnative\n</code></pre> <p>Copying to clipboard</p> <p>This lab guide uses the <code>pbcopy</code> command to reduce mistakes copying to the clipboard. The <code>pbcopy</code> command comes by default on MacOS. If you are following the lab on RHEL, you can achieve the same by running these commands: </p> <pre><code>sudo yum install xclip -y \n</code></pre> <pre><code>alias pbcopy='xclip -selection clipboard'\n</code></pre>"},{"location":"devops/argocd/argocd/#guide","title":"Guide","text":""},{"location":"devops/argocd/argocd/#fork-the-cloud-native-application-template","title":"Fork the Cloud Native Application Template","text":"<p>Navigate to the following github repository: https://github.com/platformengineers-xyz/cloud_native_sample_app</p> <ol> <li>Select <code>Use this template</code></li> <li>Select <code>Create a new repository</code></li> <li>Make sure to create the application in your personal github space</li> <li>Name your new repository <code>cloud_native_sample_app</code></li> <li>Set visibility to Private</li> <li>Select <code>Create</code></li> </ol> <p></p>"},{"location":"devops/argocd/argocd/#configuring-ssh-access-for-tekton","title":"Configuring SSH Access for Tekton","text":"<p>Generate an SSH key to clone the repository with. We will use this both for the bastion and Tekton:</p> <pre><code>ssh-keygen -t ed25519 -N '' -f ~/.ssh/tekton\n</code></pre> <p>Copy it to your clipboard: </p><pre><code>cat ~/.ssh/tekton.pub | pbcopy\n</code></pre><p></p>"},{"location":"devops/argocd/argocd/#add-the-ssh-key-to-your-repo","title":"Add the SSH Key to your Repo","text":"<ol> <li>Navigate to your copy of <code>cloud_native_sample_app</code></li> <li>Select <code>Settings</code></li> <li>Select <code>Deploy Keys</code></li> <li>Select <code>Add deploy key</code></li> <li>Paste in the Tekton key</li> <li>Give the key a title and click <code>Add Key</code></li> </ol>"},{"location":"devops/argocd/argocd/#set-up-a-secret-to-clone-from-github-enterprise-repo-with-an-ssh-key-for-openshift-pipelines","title":"Set up a Secret to clone from Github Enterprise repo with an SSH key for OpenShift Pipelines","text":"<p>Copy the private key to your clipboard.</p> <pre><code>cat ~/.ssh/tekton | pbcopy\n</code></pre> <p>Set your namespace:</p> <pre><code>NAMESPACE=tekton-demo\n</code></pre> <p>Add the private key to OpenShift as a Secret in the <code>tekton-demo</code> Project:</p> <pre><code>MY_HOMEPATH=$(echo ~)\n</code></pre> <pre><code>oc create secret generic -n $NAMESPACE github-ssh-key  --from-file=ssh-privatekey=$MY_HOMEPATH/.ssh/tekton --type=kubernetes.io/ssh-auth\n</code></pre> <p>Annotate the Secret:</p> <pre><code>oc annotate secret -n $NAMESPACE github-ssh-key tekton.dev/git-0=github.com\n</code></pre> <p>What does this label do?</p> <p>This label tells Tekton to monitor this secret, and use it when cloning <code>github.com</code> repositories. There are many ways to configure authentication in Tekton. Take a minute to review them here</p> <p>Add <code>known_hosts</code> to the secret:</p> <pre><code>known_hosts_value=$(ssh-keyscan github.com | base64 -w 0) &amp;&amp; oc patch -n $NAMESPACE secret github-ssh-key --type='json' -p=\"[{'op': 'add', 'path': '/data/known_hosts', 'value': '${known_hosts_value}'}]\"\n</code></pre> <p>Once complete, your secret should look as follows:</p> <pre><code>oc get secret github-ssh-key -n $NAMESPACE -o yaml\n</code></pre> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: gihthub-ssh-key\n  annotations:\n    tekton.dev/git-0: github.com\ndata:\n  ssh-privatekey: &lt;private-key&gt;\n  known_hosts: &lt;your-known-hosts&gt;\ntype: kubernetes.io/ssh-auth\n</code></pre> <p>Add the secret to your Pipeline Service Account:</p> <pre><code>oc secrets link pipeline -n $NAMESPACE github-ssh-key\n</code></pre>"},{"location":"devops/argocd/argocd/#configure-a-gitops-repository","title":"Configure a GitOps Repository","text":""},{"location":"devops/argocd/argocd/#create-a-github-repository-for-your-gitops-resources","title":"Create a Github Repository for your GitOps Resources","text":"<p>GitOps Applications are designed to deploy applications from a git repository. We are now going to create a repository for our Gitops manifests. Navigate to <code>github.com/&lt;YOUR-USERNAME&gt;?tab=repositories</code>:</p> <ol> <li>Select <code>New</code></li> <li>Name your new repository <code>pe-bootcamp-gitops</code></li> <li>Make sure to create the application in your personal github space</li> <li>Select <code>Add a README file</code>. This will make GitHub create a <code>main</code> branch for us.</li> <li>Set the repository to <code>Private</code></li> <li>Select <code>Create</code></li> </ol> <p></p>"},{"location":"devops/argocd/argocd/#create-a-personal-access-token-pat-for-gitops","title":"Create a Personal Access Token (PAT) for GitOps","text":"<p>From github.com:</p> <ul> <li>Click on your profile in the top left</li> <li>Select <code>Settings</code></li> <li>From the left menu, select <code>Developer Settings</code></li> <li>Select <code>Personal access tokens &gt; Tokens (classic)</code></li> <li>Select <code>Generate new token &gt; Generate new token (classic)</code></li> <li>Name your token <code>gitops</code></li> <li>Tick the base <code>repo</code> tickbox</li> <li>Click <code>Generate new token</code></li> <li>Copy the token to your clipboard</li> </ul> <p></p> <p>Store the token as a secret:</p> <pre><code>oc create secret generic gitops-token -n $NAMESPACE \\\n   --from-literal=GH_TOKEN=&lt;your-pat-token&gt;\n</code></pre>"},{"location":"devops/argocd/argocd/#install-openshift-gitops","title":"Install OpenShift GitOps","text":"<p>Install the OpenShift GitOps operator from OperatorHub. (You can follow these instructions)</p> <p>OpenShift GitOps vs ArgoCD</p> <p>You may notice we sometimes use OpenShift Gitops and ArgoCD interchangeably in this guide. OpenShift GitOps is a Red-Hat-provided Operator that installs and manages ArgoCD on OpenShift. All ArgoCD concepts apply to OpenShift GitOps.</p>"},{"location":"devops/argocd/argocd/#edit-the-app-build-pipeline","title":"Edit The app-build pipeline","text":"<p>Prerequisite</p> <p>Make sure you have completed the Tekton Lab as it is a prerequisite to this section!</p> <p>Create a GitOps Task:</p> <pre><code>apiVersion: tekton.dev/v1\nkind: Task\nmetadata:\n  name: gitops-create-new-branch\nspec:\n  params:\n  - name: directory\n    type: string\n  - description: the username and repository for the gitops repository, e.g. youruser/pe-bootcamp-gitops\n    name: gitops-repo\n    type: string\n  - description: A name for the new branch to create for this feature\n    name: new-branch\n    type: string\n  - description: Name for a secret containing the github PAT to clone the repo with\n    name: GITHUB_TOKEN_SECRET_NAME\n    type: string\n  - description: Name for a secret containing the github PAT to clone the repo with\n    name: GITHUB_TOKEN_SECRET_KEY\n    type: string\n  steps:\n  - args:\n    - |-\n        echo \"Fetching token\"\n        GH_TOKEN=\"$(cat /etc/gitops-new-branch/$(params.GITHUB_TOKEN_SECRET_KEY))\"\n        echo \"Cloning Repo\"\n        git clone -b main https://$GH_TOKEN@github.com/$(params.gitops-repo).git/ gitops \n        git config --global user.email \"tekton@ibmcloud.com\" \n        git config --global user.name \"Tekton Pipeline\" \n        cd gitops/\n        mkdir -p $(params.directory)\n        cp ../k8s/manifests.yaml $(params.directory)/\n\n        git checkout -b $(params.new-branch)\n        git add .\n        git commit -m \"Updating image name\" --allow-empty \n        git push --set-upstream origin $(params.new-branch)\n    command:\n    - /bin/bash\n    - -c\n    computeResources: {}\n    image: docker.io/csantanapr/helm-kubectl-curl-git-jq-yq\n    name: gitops\n    volumeMounts:\n    - mountPath: /etc/gitops-new-branch\n      name: githubtoken\n      readOnly: true\n    workingDir: $(workspaces.source.path)\n  volumes:\n  - name: githubtoken\n    secret:\n      secretName: $(params.GITHUB_TOKEN_SECRET_NAME)\n  workspaces:\n  - name: source\n</code></pre> <p>Create a <code>kustomize</code> task. <code>kustomize</code> lets you customize raw, template-free YAML files for multiple purposes. You can learn more about it here:</p> <pre><code>apiVersion: tekton.dev/v1\nkind: Task\nmetadata:\n  name: kustomize-build\nspec:\n  params:\n  - description: the name of the app\n    name: app-name\n    type: string\n  - description: namespace that deployment will be tested in\n    name: app-namespace\n    type: string\n  - description: contains the full image take in image:tag format\n    name: image-with-tag\n    type: string\n  steps:\n  - computeResources: {}\n    image: quay.io/upslopeio/kustomize:latest\n    name: kustomize-build\n    script: |\n      #!/bin/sh\n      set -e\n      echo \"image-with-tag: $(params.image-with-tag)\"\n      cd k8s\n      kustomize edit set image \"*=$(params.image-with-tag)\"\n      kustomize edit set label \"app:$(params.app-name)\"\n      kustomize edit set label \"app.kubernetes.io/instance:$(params.app-name)\"\n      kustomize edit set label \"app.kubernetes.io/name:$(params.app-name)\"\n      kustomize build &gt; manifests.yaml\n\n      if [ -f manifests.yaml ]; then\n        echo \"manifests.yaml successfully generated\"\n        echo \"contents of manifests is:\"\n        cat manifests.yaml\n        cp manifests.yaml ../manifests.yaml\n      else\n        echo \"ERROR: manifests.yaml not generated\"\n        exit 1\n      fi\n    workingDir: $(workspaces.source.path)\n  workspaces:\n  - description: contains the cloned git repo\n    name: source\n</code></pre> <p>Add a <code>Github Open Pull Request</code> Task:</p> <pre><code>apiVersion: tekton.dev/v1\nkind: Task\nmetadata:\n  name: github-open-pr\nspec:\n  description: This task will open a PR on Github based on several parameters. This\n    could be useful in GitOps repositories for example.\n  params:\n  - default: api.github.com\n    description: |\n      The GitHub host, adjust this if you run a GitHub enteprise.\n    name: GITHUB_HOST_URL\n    type: string\n  - default: \"\"\n    description: |\n      The API path prefix, GitHub Enterprise has a prefix e.g. /api/v3\n    name: API_PATH_PREFIX\n    type: string\n  - description: |\n      The GitHub repository full name, e.g.: JohnDoe/pe-gitops-bootcamp\n    name: REPO_FULL_NAME\n    type: string\n  - default: github\n    description: |\n      The name of the kubernetes secret that contains the GitHub token, default: github\n    name: GITHUB_TOKEN_SECRET_NAME\n    type: string\n  - default: token\n    description: |\n      The key within the kubernetes secret that contains the GitHub token, default: token\n    name: GITHUB_TOKEN_SECRET_KEY\n    type: string\n  - default: Bearer\n    description: |\n      The type of authentication to use. You could use the less secure \"Basic\" for example\n    name: AUTH_TYPE\n    type: string\n  - description: |\n      The name of the branch where your changes are implemented.\n    name: BRANCH\n    type: string\n  - default: main\n    description: |\n      The name of the branch you want the changes pulled into.\n    name: BASE\n    type: string\n  - default: Automated Pull request created by Tekton\n    description: |\n      The body description of the pull request.\n    name: BODY\n    type: string\n  - default: Automated Pull request created by Tekton\n    description: |\n      The title of the pull request.\n    name: TITLE\n    type: string\n  results:\n  - description: Number of the created pull request.\n    name: NUMBER\n    type: string\n  - description: URL of the created pull request.\n    name: URL\n    type: string\n  steps:\n  - computeResources: {}\n    env:\n    - name: PULLREQUEST_NUMBER_PATH\n      value: $(results.NUMBER.path)\n    - name: PULLREQUEST_URL_PATH\n      value: $(results.URL.path)\n    image: registry.access.redhat.com/ubi8/python-38:1-34.1599745032\n    name: open-pr\n    script: |\n      #!/usr/libexec/platform-python\n\n      \"\"\"This script will open a PR on Github\"\"\"\n\n      import json\n      import os\n      import sys\n      import http.client\n\n      with open(\"/etc/github-open-pr/$(params.GITHUB_TOKEN_SECRET_KEY)\", \"r\", encoding=\"utf-8\") as file:\n          github_token = file.read()\n\n      open_pr_url = \"$(params.API_PATH_PREFIX)/repos/$(params.REPO_FULL_NAME)/pulls\"\n\n      data = {\n          \"head\": \"$(params.BRANCH)\",\n          \"base\": \"$(params.BASE)\",\n          \"title\": \"$(params.TITLE)\",\n          \"body\": \"\"\"$(params.BODY)\"\"\"\n      }\n      print(\"Sending this data to GitHub: \")\n      print(data)\n\n      authHeader = f\"$(params.AUTH_TYPE) {github_token}\"\n\n      # This is for our fake github server\n      if \"$(params.GITHUB_HOST_URL)\".startswith(\"http://\"):\n          conn = http.client.HTTPConnection(\"$(params.GITHUB_HOST_URL)\"\n                                            .replace(\"http://\", \"\"))\n      else:\n          conn = http.client.HTTPSConnection(\"$(params.GITHUB_HOST_URL)\")\n\n      conn.request(\n          \"POST\",\n          open_pr_url,\n          body=json.dumps(data),\n          headers={\n              \"User-Agent\": \"TektonCD, the peaceful cat\",\n              \"Authorization\": authHeader,\n              \"Accept\": \"application/vnd.github.v3+json \",\n              \"Content-Type\": \"application/vnd.github.v3+json\",\n          })\n      resp = conn.getresponse()\n      if not str(resp.status).startswith(\"2\"):\n          print(f\"Error: {resp.status}\")\n          print(resp.read())\n          sys.exit(1)\n      else:\n          body = json.loads(resp.read().decode())\n\n          with open(os.environ.get('PULLREQUEST_NUMBER_PATH'), 'w', encoding=\"utf-8\") as f:\n              f.write(f'{body[\"number\"]}')\n\n          with open(os.environ.get('PULLREQUEST_URL_PATH'), 'w', encoding=\"utf-8\") as f:\n              f.write(body[\"html_url\"])\n\n          print(\"GitHub pull request created for $(params.REPO_FULL_NAME): \"\n                f'number={body[\"number\"]} url={body[\"html_url\"]}')\n    volumeMounts:\n    - mountPath: /etc/github-open-pr\n      name: githubtoken\n      readOnly: true\n  volumes:\n  - name: githubtoken\n    secret:\n      secretName: $(params.GITHUB_TOKEN_SECRET_NAME)\n</code></pre> <p>Update the pipeline to include those tasks:</p> <pre><code>apiVersion: tekton.dev/v1\nkind: Pipeline\nmetadata:\n  name: app-build\nspec:\n  params:\n    - description: username/repo-name, e.g. JohnDoe/pe-bootcamp-gitops\n      name: gitops-repo\n      type: string\n    - description: SSH url for the source repository\n      name: source-repo\n      type: string\n    - name: image_registry\n      type: string\n    - description: Application name\n      name: app-name\n      type: string\n    - default: gitops-token\n      description: A secret containing the PAT for the gitops repo\n      name: gitops-secret-name\n      type: string\n    - default: GH_TOKEN\n      description: The secret key for the PAT\n      name: gitops-secret-key\n      type: string\n  tasks:\n    - name: clone-repository\n      params:\n        - name: URL\n          value: $(params.source-repo)\n      taskRef:\n        params:\n          - name: kind\n            value: task\n          - name: name\n            value: git-clone\n          - name: namespace\n            value: openshift-pipelines\n        resolver: cluster\n      workspaces:\n        - name: output\n          workspace: source\n    - name: buildah-build\n      params:\n        - name: IMAGE\n          value: $(params.image_registry):$(tasks.clone-repository.results.COMMIT)\n        - name: DOCKERFILE\n          value: ./Dockerfile\n        - name: CONTEXT\n          value: .\n        - name: STORAGE_DRIVER\n          value: vfs\n        - name: FORMAT\n          value: oci\n        - name: BUILD_EXTRA_ARGS\n          value: \"\"\n        - name: PUSH_EXTRA_ARGS\n          value: \"\"\n        - name: SKIP_PUSH\n          value: \"false\"\n        - name: TLS_VERIFY\n          value: \"true\"\n        - name: VERBOSE\n          value: \"false\"\n      runAfter:\n        - clone-repository\n      taskRef:\n        params:\n          - name: kind\n            value: task\n          - name: name\n            value: buildah\n          - name: namespace\n            value: openshift-pipelines\n        resolver: cluster\n      workspaces:\n        - name: source\n          workspace: source\n    - name: kustomize-build\n      params:\n        - name: app-name\n          value: $(params.app-name)\n        - name: app-namespace\n          value: $(context.pipelineRun.namespace)\n        - name: image-with-tag\n          value: $(params.image_registry):$(tasks.clone-repository.results.COMMIT)\n      runAfter:\n        - buildah-build\n      taskRef:\n        kind: Task\n        name: kustomize-build\n      workspaces:\n        - name: source\n          workspace: source\n    - name: gitops-create-new-branch\n      params:\n        - name: directory\n          value: $(params.app-name)\n        - name: gitops-repo\n          value: $(params.gitops-repo)\n        - name: new-branch\n          value: feature-$(context.pipelineRun.name)\n        - name: GITHUB_TOKEN_SECRET_NAME\n          value: $(params.gitops-secret-name)\n        - name: GITHUB_TOKEN_SECRET_KEY\n          value: $(params.gitops-secret-key)\n      runAfter:\n        - kustomize-build\n      taskRef:\n        kind: Task\n        name: gitops-create-new-branch\n      workspaces:\n        - name: source\n          workspace: source\n    - name: github-open-pr\n      params:\n        - name: GITHUB_HOST_URL\n          value: api.github.com\n        - name: API_PATH_PREFIX\n          value: \"\"\n        - name: REPO_FULL_NAME\n          value: $(params.gitops-repo)\n        - name: GITHUB_TOKEN_SECRET_NAME\n          value: gitops-token\n        - name: GITHUB_TOKEN_SECRET_KEY\n          value: GH_TOKEN\n        - name: AUTH_TYPE\n          value: Bearer\n        - name: BRANCH\n          value: feature-$(context.pipelineRun.name)\n        - name: BASE\n          value: main\n        - name: BODY\n          value: Automated Pull request created by Tekton\n        - name: TITLE\n          value: Automated Pull request created by Tekton\n      runAfter:\n        - gitops-create-new-branch\n      taskRef:\n        kind: Task\n        name: github-open-pr\n  workspaces:\n    - name: source\n</code></pre>"},{"location":"devops/argocd/argocd/#run-your-pipeline-ci","title":"Run Your Pipeline (CI)","text":"<p>Update the PipelineRun yaml to point to your GitOps and Source repo:</p> <pre><code>apiVersion: tekton.dev/v1\nkind: PipelineRun\nmetadata:\n  labels:\n    tekton.dev/pipeline: app-build\n  generateName: app-build-qup9da\nspec:\n  params:\n  - name: gitops-repo\n    value: UPDATE-ME # e.g. JohnDoe/pe-bootcamp-gitops\n  - name: source-repo\n    value: UPDATE-ME # e.g. git@github.com:JohnDoe/cloud_native_sample_app.git\n  - name: image_registry\n    value: image-registry.openshift-image-registry.svc:5000/tekton-demo/cloud-native-sample-app\n  - name: app-name\n    value: cloud-native-sample-app\n  - name: gitops-secret-name\n    value: gitops-token\n  - name: gitops-secret-key\n    value: GH_TOKEN\n  pipelineRef:\n    name: app-build\n  taskRunTemplate:\n    serviceAccountName: pipeline\n  timeouts:\n    pipeline: 1h0m0s\n  workspaces:\n  - name: source\n    volumeClaimTemplate:\n      metadata:\n        creationTimestamp: null\n      spec:\n        accessModes:\n        - ReadWriteOnce\n        resources:\n          requests:\n            storage: 1Gi\n        storageClassName: ocs-storagecluster-cephfs\n        volumeMode: Filesystem\n      status: {}\n</code></pre> <p>Create the PipelineRun:</p> <pre><code>oc create -n $NAMESPACE -f pipelinerun.yaml\n</code></pre> <p>The pipeline should succeed:</p> <p></p> <ul> <li>Navigate to your <code>pe-bootcamp-gitops</code> repository</li> <li>Select <code>Pull Requests</code></li> <li>You should see a new pull request created by Tekton</li> </ul> <p></p>"},{"location":"devops/argocd/argocd/#enable-cd-in-your-application","title":"Enable CD in your application","text":""},{"location":"devops/argocd/argocd/#login-to-your-openshift-gitops-instance","title":"Login to Your Openshift GitOps Instance","text":"<p>On the OpenShift console navigate to the <code>openshift-gitops</code> project and open the url in <code>Networking &gt; Routes &gt; openshift-gitops-server</code>, or run:</p> <pre><code>echo https://$(oc get route -n openshift-gitops openshift-gitops-server -ojsonpath=\"{.spec.host}\")\n</code></pre> <p>Your username is <code>admin</code> and the password can be found in the Secret called <code>openshift-gitops-cluster</code> in the <code>openshift-gitops</code> namespace</p>"},{"location":"devops/argocd/argocd/#connect-your-gitops-repo-to-openshift-gitops","title":"Connect Your GitOps repo to OpenShift GitOps","text":"<p>Create an SSH Key with Write access</p> <pre><code>ssh-keygen -t ed25519 -a 100 -f ~/.ssh/argo -N ''\n</code></pre> <p>Copy the private key:</p> <pre><code>cat ~/.ssh/argo | pbcopy\n</code></pre> <p>Back in the Argo UI, navigate to <code>Settings &gt; Repositories &gt; Connect Repo</code>:</p> <ul> <li>Name: <code>pe-bootcamp-gitops</code></li> <li>Project: <code>default</code></li> <li>Repository URL: <code>git@github.com:YOUR-USER/pe-bootcamp-gitops.git</code></li> <li>Paste the SSH key</li> <li>Click Connect</li> </ul> <p>Copy your public key:</p> <pre><code>cat ~/.ssh/argo.pub | pbcopy\n</code></pre> <p>Add the public key as a <code>Deploy</code> key to the <code>pe-bootcamp-gitops</code> Github Repo</p> <p>Make sure to check <code>Allow write access</code> this time (Setting &gt; Deploy keys)</p>"},{"location":"devops/argocd/argocd/#configure-argo-rbac-permissions","title":"Configure Argo RBAC Permissions","text":"<p>To deploy our application, we need to give the Argo Service account appropriate permissions to deploy in our <code>tekton-demo</code> namespace.</p> <p>Create a <code>ClusterRoleBinding</code>:</p> <pre><code>kind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: cluster-role-binding\nsubjects:\n  - kind: ServiceAccount\n    name: openshift-gitops-argocd-application-controller\n    namespace: openshift-gitops\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: admin\n</code></pre> <p>Careful</p> <p>This <code>ClusterRoleBinding</code> gives <code>ArgoCD</code> cluster admin permissions across all namespaces. While this is fine for a demo environment, it is not appropriate for real environments. Role Based Access Control (RBAC) in OpenShift is very granular, and you should always give an application only the permissions it requires to run (Principle of Least Privilege). Read more about RBAC on OpenShift here</p>"},{"location":"devops/argocd/argocd/#deploy-your-application","title":"Deploy your Application","text":"<p>Prerequisites</p> <p>Ensure the app-build pipeline has run successfully</p> <p>Copy your private key:</p> <pre><code>cat ~/.ssh/argo | pbcopy\n</code></pre> <p>From the Argo UI, navigate to <code>Settings &gt; Repositories</code>.</p> <p>Add a new repository. Provide the SSH url for yor GitOps repo and paste in your private key:</p> <p></p> <p>Navigate to <code>Applications &gt; New App</code></p> <p>Add the following settings and everything else on the default options</p> <ul> <li>Application Name: <code>cloud-native-app</code></li> <li>Project Name: <code>default</code></li> <li>SYNC POLICY: <code>Automatic</code></li> <li> <code>Prune Resources</code></li> <li> <code>Self Heal</code></li> <li>Repository URL: <code>git@github.com:YOUR-USER/pe-bootcamp-gitops.git</code></li> <li>Path: <code>./cloud-native-sample-app</code></li> <li>Cluster URL: <code>https://kubernetes.default.svc</code></li> <li>Namespace: <code>tekton-demo</code></li> </ul> <p>Select <code>Create</code>. You should see the following error:</p> <p></p> <p>ArgoCD does not have any resources to create at this stage.</p> <ul> <li>Navigate to your <code>pe-bootcamp-gitops</code> repository in Github. Select <code>Pull Requests</code>. </li> <li>Approve and merge the pull request</li> <li>Go back to Argo and create the App. It should now succeed!</li> </ul> <p>You have successfully completed the lab once your Argo application looks like below, and you can access the application via the following route:</p> <pre><code>curl \"https://$(oc get route -n $NAMESPACE cloudnative -o jsonpath={.spec.host})/greeting?name=world\"\n</code></pre> <pre><code>{\"id\":4,\"content\":\"Welcome to CloudNative bootcamp !!! Hello, world :)\"}\n</code></pre> <p>And argo syncs your application successfully:</p> <p></p>"},{"location":"devops/argocd/argocd/#acknowledgements","title":"Acknowledgements","text":"<p>This lab was inspired by, and borrowed heavily from https://github.ibm.com/TechnologyGarageUKI/openshift-workshop</p>"},{"location":"devops/ibm-toolchain/","title":"IBM ToolChain","text":""},{"location":"devops/ibm-toolchain/#ibm-toolchain","title":"IBM ToolChain","text":"<p>By following this tutorial, you create an open toolchain that includes a Tekton-based delivery pipeline. You then use the toolchain and DevOps practices to develop a simple \"Hello World\" web application (app) that you deploy to the IBM Cloud Kubernetes Service. </p> <p>Tekton is an open source, vendor-neutral, Kubernetes-native framework that you can use to build, test, and deploy apps to Kubernetes. Tekton provides a set of shared components for building continuous integration and continuous delivery (CICD) systems. As an open source project, Tekton is managed by the Continuous Delivery Foundation (CDF). The goal is to modernize continuous delivery by providing industry specifications for pipelines, workflows, and other building blocks. With Tekton, you can build, test, and deploy across cloud providers or on-premises systems by abstracting the underlying implementation details. Tekton pipelines are built in to  IBM Cloud\u2122 Continuous Delivery..</p> <p>After you create the cluster and the toolchain, you change your app's code and push the change to the Git Repos and Issue Tracking repository (repo). When you push changes to your repo, the delivery pipeline automatically builds and deploys the code.</p>"},{"location":"devops/ibm-toolchain/#prerequisites","title":"Prerequisites","text":"<ol> <li>You must have an IBM Cloud account. If you don't have one, sign up for a trial. The account requires an IBMid. If you don't have an IBMid, you can create one when you register.</li> <li> <p>Verify the toolchains and tool integrations that are available in your region and IBM Cloud environment. A toolchain is a set of tool integrations that support development, deployment, and operations tasks.</p> </li> <li> <p>You need a Kubernetes cluster and an API key. You can create them by using either the UI or the CLI. You can create from the IBM Cloud Catalog</p> </li> <li> <p>Create a container registry namespace to deploy the container we are goign to build. Youc an create from the Container Registry UI</p> </li> <li> <p>Create the API key by using the string that is provided for your key name.     </p><pre><code>ibmcloud iam api-key-create my-api-key\n</code></pre>     Save the API key value that is provided by the command.<p></p> </li> </ol>"},{"location":"devops/ibm-toolchain/#create-continues-delivery-service-instance","title":"Create Continues Delivery Service Instance","text":"<ol> <li>Open the IBM Cloud Catalog</li> <li>Search for <code>delivery</code></li> <li>Click on <code>Continuous Delivery</code> </li> <li>Select Dallas Region, as the Tutorial will be using Managed Tekton Worker available in Dallas only.</li> <li>Select a Plan</li> <li>Click Create</li> </ol>"},{"location":"devops/ibm-toolchain/#create-an-ibm-cloud-toolchain","title":"Create an IBM Cloud Toolchain","text":"<p>In this task, you create a toolchain and add the tools that you need for this tutorial. Before you begin, you need your API key and Kubernetes cluster name.</p> <ol> <li>Open the menu in the upper-left corner and click DevOps. Click ToolChains. Click Create a toolchain. Type in the search box <code>toolchain</code>. Click Build Your Own Toolchain.      </li> <li>On the \"Build your own toolchain\" page, review the default information for the toolchain settings. The toolchain's name identifies it in IBM Cloud. Each toolchain is associated with a specific region and resource group. From the menus on the page, select the region Dallas since we are going to use the Beta Managed Tekton Worker, if you use Private Workers you can use any Region.     </li> <li>Click Create. The blank toolchain is created.</li> <li>Click Add a Tool and click Git Repos and Issue Tracking.      <ul> <li>From the Repository type list, select Clone. </li> <li>In the Source repository URL field, type <code>https://github.com/csantanapr/hello-tekton.git</code>.</li> <li>Make sure to uncheck the Make this repository private checkbox and that the Track deployment of code changes checkbox is selected. </li> <li>Click Create Integration. Tiles for Git Issues and Git Code are added to your toolchain.</li> </ul> </li> <li>Return to your toolchain's overview page.</li> <li>Click Add a Tool. Type <code>pipeline</code> in seach box and click Delivery Pipeline.     <ul> <li>Type a name for your new pipeline.</li> <li>Click Tekton.  </li> <li>Make sure that the Show apps in the View app menu checkbox is selected. All the apps that your pipeline creates are shown in the View App list on the toolchain's Overview page.</li> <li>Click Create Integration to add the Delivery Pipeline to your toolchain.</li> </ul> </li> <li>Click Delivery Pipeline to open the Tekton Delivery Pipeline dashboard. Click the Definitions tab and complete these tasks:</li> <li>Click Add to add your repository.</li> <li>Specify the Git repo and URL that contains the Tekton pipeline definition and related artifacts. From the list, select the Git repo that you created earlier.</li> <li>Select the branch in your Git repo that you want to use. For this tutorial, use the default value.</li> <li>Specify the directory path to your pipeline definition within the Git repo. You can reference a specific definition within the same repo. For this tutorial, use the default value.   </li> <li>Click Add, then click Save</li> <li>Click the Worker tab and select the private worker that you want to use to run your Tekton pipeline on the associated cluster. Either select the private worker you set up in the previous steps, or select the IBM Managed workers in DALLAS option.   </li> <li>Click Save</li> <li>Click the Triggers tab, click Add trigger, and click Git Repository. Associate the trigger with an event listener: </li> <li>From the Repository list, select your repo.</li> <li>Select the When a commit is pushed checkbox, and in the EventListener field, make sure that listener is selected. </li> <li>Click Save</li> <li>On the Triggers tab, click Add trigger and click Manual. Associate that trigger with an event listener:</li> <li>In the EventListener field, make sure that listener is selected.</li> <li>Click Save.    Note: Manual triggers run when you click Run pipeline and select the trigger. Git repository triggers run when the specified Git event type occurs for the specified Git repo and branch. The list of available event listeners is populated with the listeners that are defined in the pipeline code repo. </li> <li>Click the Environment properties tab and define the environment properties for this tutorial. To add each property, click Add property and click Text property. Add these properties:</li> </ol> Parameter Required? Description apikey required Type the API key that you created earlier in this tutorial. cluster Optional (cluster) Type the name of the Kubernetes cluster that you created. registryNamespace required Type the IBM Image Registry namespace where the app image will be built and stored. To use an existing namespace, use the CLI and run <code>ibmcloud cr namespace-list</code> to identify all your current namespaces repository required Type the source Git repository where your resources are stored. This value is the URL of the Git repository that you created earlier in this tutorial. To find your repo URL, return to your toolchain and click the Git tile. When the repository is shown, copy the URL. revision Optional (master) The Git branch clusterRegion Optional (us-south) Type the region where your  cluster is located. clusterNamespace Optional (prod) The namespace in your cluster where the app will be deployed. registryRegion Optional (us-south) The region where your Image registry is located. To find your registry region, use the CLI and run <code>ibmcloud cr region</code>. <p> 12. Click Save</p>"},{"location":"devops/ibm-toolchain/#explore-the-pipeline","title":"Explore the pipeline","text":"<p>With a Tekton-based delivery pipeline, you can automate the continuous building, testing, and deployment of your apps.</p> <p>The Tekton Delivery Pipeline dashboard displays an empty table until at least one Tekton pipeline runs. After a Tekton pipeline runs, either manually or as the result of external Git events, the table lists the run, its status, and the last updated time of the run definition.</p> <p>To run the manual trigger that you set up in the previous task, click Run pipeline and select the name of the manual trigger that you created. The pipeline starts to run and you can see the progress on the dashboard. Pipeline runs can be in any of the following states:</p> <ul> <li>Pending: The PipelineRun definition is queued and waiting to run.</li> <li>Running: The PipelineRun definition is running in the cluster.</li> <li>Succeeded: The PipelineRun definition was successfully completed in the cluster.</li> <li> <p>Failed: The PipelineRun definition run failed. Review the log file for the run to determine the cause.     </p> </li> <li> <p>For more information about a selected run, click any row in the table. You view the Task definition and the steps in each PipelineRun definition. You can also view the status, logs, and details of each Task definition and step, and the overall status of the PipelineRun definition.     </p> </li> <li> <p>The pipeline definition is stored in the <code>pipeline.yaml</code> file in the <code>.tekton</code> folder of your Git repository. Each task has a separate section of this file. The steps for each task are defined in the <code>tasks.yaml</code> file.</p> </li> <li> <p>Review the pipeline-build-task. The task consists of a git clone of the repository followed by two steps:</p> <ul> <li>pre-build-check: This step checks for the mandatory Dockerfile and runs a lint tool. It then checks the registry current plan and quota before it creates the image registry namespace if needed.</li> <li>build-docker-image: This step creates the Docker image by using the IBM Cloud Container Registry build service through the <code>ibmcloud cr build</code> CLI script. </li> </ul> </li> <li>Review the pipeline-validate-task. The task consists of a git clone of the repository, followed by the check-vulnerabilities step. This step runs the IBM Cloud Vulnerability Advisor on the image to check for known vulnerabilities. If it finds a vulnerability, the job fails, preventing the image from being deployed. This safety feature prevents apps with security holes from being deployed. The image has no vulnerabilities, so it passes. In this tutorial template, the default configuration of the job is to not block on failure.</li> <li>Review the pipeline-deploy-task. The task consists of a git clone of the repository followed by two steps:<ul> <li>pre-deploy-check: This step checks whether the IBM Container Service cluster is ready and has a namespace that is configured with access to the private image registry by using an IBM Cloud API Key. </li> <li>deploy-to-kubernetes: This step updates the <code>deployment.yml</code> manifest file with the image url and deploys the application using <code>kubectl apply</code></li> </ul> </li> <li>After all the steps in the pipeline are completed, a green status is shown for each task. Click the deploy-to-kubernetes step and click the Logs tab to see the successful completion of this step.     </li> <li>Scroll to the end of the log. The <code>DEPLOYMENT SUCCEEDED</code> message is shown at the end of the log.     </li> <li>Click the URL to see the running application.     </li> </ul>"},{"location":"devops/ibm-toolchain/#modify-the-app-code","title":"Modify the App Code","text":"<p>In this task, you modify the application and redeploy it. You can see how your Tekton-based delivery pipeline automatically picks up the changes in the application on commit and redeploys the app. </p> <ol> <li>On the toolchain's Overview page, click the Git tile for your application. <ul> <li>Tip: You can also use the built-in Eclipse Orion-based Web IDE, a local IDE, or your favorite editor to change the files in your repo.</li> </ul> </li> <li>In the repository directory tree, open the <code>app.js</code> file.     </li> <li>Edit the text message code to change the welcome message.      </li> <li>Commit the updated file by typing a commit message and clicking Commit changes to push the change to the project's remote repository. </li> <li>Return to the toolchain's Overview page by clicking the back arrow.</li> <li>Click Delivery Pipeline. The pipeline is running because the commit automatically started a build. Over the next few minutes, watch your change as it is built, tested, and deployed.      </li> <li>After the deploy-to-kubernetes step is completed, refresh your application URL. The updated message is shown.</li> </ol>"},{"location":"devops/ibm-toolchain/#clean-up-resources","title":"Clean up Resources","text":"<p>In this task, you can remove any of the content that is generated by this tutorial. Before you begin, you need the IBM Cloud CLI and the IBM Cloud Kubernetes Service CLI. Instructions to install the CLI are in the prerequisite section of this tutorial.</p> <ol> <li>Delete the git repository, sign in into git, select personal projects. Then go to repository General settings and remove the repository.</li> <li>Delete the toolchain. You can delete a toolchain and specify which of the associated tool integrations you want to delete. When you delete a toolchain, the deletion is permanent.<ul> <li>On the DevOps dashboard, on the Toolchains page, click the toolchain to delete. Alternatively, on the app's Overview page, on the Continuous delivery card, click View Toolchain.</li> <li>Click the More Actions menu, which is next to View app.</li> <li>Click Delete. Deleting a toolchain removes all of its tool integrations, which might delete resources that are managed by those integrations.</li> <li>Confirm the deletion by typing the name of the toolchain and clicking Delete. </li> <li>Tip: When you delete a GitHub, GitHub Enterprise, or Git Repos and Issue Tracking tool integration, the associated repo isn't deleted from GitHub, GitHub Enterprise, or Git Repos and Issue Tracking. You must manually remove the repo.</li> </ul> </li> <li>Delete the cluster or discard the namespace from it. It is easiest to delete the entire namespace (Please do not delete the <code>default</code> namespace) by using the IBM Cloud\u2122 Kubernetes Service CLI from a command-line window. However, if you have other resources that you need to keep in the namespace, you need to delete the application resources individually instead of the entire namespace. To delete the entire namespace, enter this command:     <pre><code>kubectl delete namespace [not-the-default-namespace]\n</code></pre></li> <li>Delete your IBM Cloud API key.</li> <li>From the Manage menu, click Access (IAM). Click IBM Cloud API Keys.</li> <li>Find your API Key in the list and select Delete from the menu to the right of the API Key name.</li> <li>Delete the container images. To delete the images in your container image registry, enter this command in a command-line window:     <pre><code>ibmcloud cr image-rm IMAGE [IMAGE...]\n</code></pre>     If you created a registry namespace for the tutorial, delete the entire registry namespace by entering this command:     <pre><code>ibmcloud cr namespace-rm NAMESPACE\n</code></pre><ul> <li>Note: You can run this tutorial many times by using the same registry namespace and cluster parameters without discarding previously generated resources. The generated resources use randomized names to avoid conflicts.</li> </ul> </li> </ol>"},{"location":"devops/ibm-toolchain/#summary","title":"Summary","text":"<p>You created a toolchain with a Tekton-based delivery pipeline that deploys a \"Hello World\" app to a secure container in a Kubernetes cluster. You changed a message in the app and tested your change. When you pushed the change to the repo, the delivery pipeline automatically redeployed the app.</p> <ul> <li>Read more about the IBM Cloud Kubernetes Service</li> <li>Read more about Tekton</li> <li>Explore the DevOps reference architecture.</li> </ul>"},{"location":"devops/ibm-toolchain/ibm-toolchain/","title":"IBM Toolchain Lab","text":""},{"location":"devops/ibm-toolchain/ibm-toolchain/#ibm-toolchain-lab","title":"IBM Toolchain Lab","text":"<p>By following this tutorial, you create an open toolchain that includes a Tekton-based delivery pipeline. You then use the toolchain and DevOps practices to develop a simple \"Hello World\" web application (app) that you deploy to the IBM Cloud Kubernetes Service. </p> <p>Tekton is an open source, vendor-neutral, Kubernetes-native framework that you can use to build, test, and deploy apps to Kubernetes. Tekton provides a set of shared components for building continuous integration and continuous delivery (CICD) systems. As an open source project, Tekton is managed by the Continuous Delivery Foundation (CDF). The goal is to modernize continuous delivery by providing industry specifications for pipelines, workflows, and other building blocks. With Tekton, you can build, test, and deploy across cloud providers or on-premises systems by abstracting the underlying implementation details. Tekton pipelines are built in to  IBM Cloud\u2122 Continuous Delivery..</p> <p>After you create the cluster and the toolchain, you change your app's code and push the change to the Git Repos and Issue Tracking repository (repo). When you push changes to your repo, the delivery pipeline automatically builds and deploys the code.</p>"},{"location":"devops/ibm-toolchain/ibm-toolchain/#prerequisites","title":"Prerequisites","text":"<ol> <li>You must have an IBM Cloud account. If you don't have one, sign up for a trial. The account requires an IBMid. If you don't have an IBMid, you can create one when you register.</li> <li> <p>Verify the toolchains and tool integrations that are available in your region and IBM Cloud environment. A toolchain is a set of tool integrations that support development, deployment, and operations tasks.</p> </li> <li> <p>You need a Kubernetes cluster and an API key. You can create them by using either the UI or the CLI. You can create from the IBM Cloud Catalog</p> </li> <li> <p>Create a container registry namespace to deploy the container we are goign to build. Youc an create from the Container Registry UI</p> </li> <li> <p>Create the API key by using the string that is provided for your key name.     </p><pre><code>ibmcloud iam api-key-create my-api-key\n</code></pre>     Save the API key value that is provided by the command.<p></p> </li> </ol>"},{"location":"devops/ibm-toolchain/ibm-toolchain/#create-continues-delivery-service-instance","title":"Create Continues Delivery Service Instance","text":"<ol> <li>Open the IBM Cloud Catalog</li> <li>Search for <code>delivery</code></li> <li>Click on <code>Continuous Delivery</code> </li> <li>Select Dallas Region, as the Tutorial will be using Managed Tekton Worker available in Dallas only.</li> <li>Select a Plan</li> <li>Click Create</li> </ol>"},{"location":"devops/ibm-toolchain/ibm-toolchain/#create-an-ibm-cloud-toolchain","title":"Create an IBM Cloud Toolchain","text":"<p>In this task, you create a toolchain and add the tools that you need for this tutorial. Before you begin, you need your API key and Kubernetes cluster name.</p> <ol> <li>Open the menu in the upper-left corner and click DevOps. Click ToolChains. Click Create a toolchain. Type in the search box <code>toolchain</code>. Click Build Your Own Toolchain.      </li> <li>On the \"Build your own toolchain\" page, review the default information for the toolchain settings. The toolchain's name identifies it in IBM Cloud. Each toolchain is associated with a specific region and resource group. From the menus on the page, select the region Dallas since we are going to use the Beta Managed Tekton Worker, if you use Private Workers you can use any Region.     </li> <li>Click Create. The blank toolchain is created.</li> <li>Click Add a Tool and click Git Repos and Issue Tracking.      <ul> <li>From the Repository type list, select Clone. </li> <li>In the Source repository URL field, type <code>https://github.com/csantanapr/hello-tekton.git</code>.</li> <li>Make sure to uncheck the Make this repository private checkbox and that the Track deployment of code changes checkbox is selected. </li> <li>Click Create Integration. Tiles for Git Issues and Git Code are added to your toolchain.</li> </ul> </li> <li>Return to your toolchain's overview page.</li> <li>Click Add a Tool. Type <code>pipeline</code> in seach box and click Delivery Pipeline.     <ul> <li>Type a name for your new pipeline.</li> <li>Click Tekton.  </li> <li>Make sure that the Show apps in the View app menu checkbox is selected. All the apps that your pipeline creates are shown in the View App list on the toolchain's Overview page.</li> <li>Click Create Integration to add the Delivery Pipeline to your toolchain.</li> </ul> </li> <li>Click Delivery Pipeline to open the Tekton Delivery Pipeline dashboard. Click the Definitions tab and complete these tasks:</li> <li>Click Add to add your repository.</li> <li>Specify the Git repo and URL that contains the Tekton pipeline definition and related artifacts. From the list, select the Git repo that you created earlier.</li> <li>Select the branch in your Git repo that you want to use. For this tutorial, use the default value.</li> <li>Specify the directory path to your pipeline definition within the Git repo. You can reference a specific definition within the same repo. For this tutorial, use the default value.   </li> <li>Click Add, then click Save</li> <li>Click the Worker tab and select the private worker that you want to use to run your Tekton pipeline on the associated cluster. Either select the private worker you set up in the previous steps, or select the IBM Managed workers in DALLAS option.   </li> <li>Click Save</li> <li>Click the Triggers tab, click Add trigger, and click Git Repository. Associate the trigger with an event listener: </li> <li>From the Repository list, select your repo.</li> <li>Select the When a commit is pushed checkbox, and in the EventListener field, make sure that listener is selected. </li> <li>Click Save</li> <li>On the Triggers tab, click Add trigger and click Manual. Associate that trigger with an event listener:</li> <li>In the EventListener field, make sure that listener is selected.</li> <li>Click Save.    Note: Manual triggers run when you click Run pipeline and select the trigger. Git repository triggers run when the specified Git event type occurs for the specified Git repo and branch. The list of available event listeners is populated with the listeners that are defined in the pipeline code repo. </li> <li>Click the Environment properties tab and define the environment properties for this tutorial. To add each property, click Add property and click Text property. Add these properties:</li> </ol> Parameter Required? Description apikey required Type the API key that you created earlier in this tutorial. cluster Optional (cluster) Type the name of the Kubernetes cluster that you created. registryNamespace required Type the IBM Image Registry namespace where the app image will be built and stored. To use an existing namespace, use the CLI and run <code>ibmcloud cr namespace-list</code> to identify all your current namespaces repository required Type the source Git repository where your resources are stored. This value is the URL of the Git repository that you created earlier in this tutorial. To find your repo URL, return to your toolchain and click the Git tile. When the repository is shown, copy the URL. revision Optional (master) The Git branch clusterRegion Optional (us-south) Type the region where your  cluster is located. clusterNamespace Optional (prod) The namespace in your cluster where the app will be deployed. registryRegion Optional (us-south) The region where your Image registry is located. To find your registry region, use the CLI and run <code>ibmcloud cr region</code>. <p> 12. Click Save</p>"},{"location":"devops/ibm-toolchain/ibm-toolchain/#explore-the-pipeline","title":"Explore the pipeline","text":"<p>With a Tekton-based delivery pipeline, you can automate the continuous building, testing, and deployment of your apps.</p> <p>The Tekton Delivery Pipeline dashboard displays an empty table until at least one Tekton pipeline runs. After a Tekton pipeline runs, either manually or as the result of external Git events, the table lists the run, its status, and the last updated time of the run definition.</p> <p>To run the manual trigger that you set up in the previous task, click Run pipeline and select the name of the manual trigger that you created. The pipeline starts to run and you can see the progress on the dashboard. Pipeline runs can be in any of the following states:</p> <ul> <li>Pending: The PipelineRun definition is queued and waiting to run.</li> <li>Running: The PipelineRun definition is running in the cluster.</li> <li>Succeeded: The PipelineRun definition was successfully completed in the cluster.</li> <li> <p>Failed: The PipelineRun definition run failed. Review the log file for the run to determine the cause.     </p> </li> <li> <p>For more information about a selected run, click any row in the table. You view the Task definition and the steps in each PipelineRun definition. You can also view the status, logs, and details of each Task definition and step, and the overall status of the PipelineRun definition.     </p> </li> <li> <p>The pipeline definition is stored in the <code>pipeline.yaml</code> file in the <code>.tekton</code> folder of your Git repository. Each task has a separate section of this file. The steps for each task are defined in the <code>tasks.yaml</code> file.</p> </li> <li> <p>Review the pipeline-build-task. The task consists of a git clone of the repository followed by two steps:</p> <ul> <li>pre-build-check: This step checks for the mandatory Dockerfile and runs a lint tool. It then checks the registry current plan and quota before it creates the image registry namespace if needed.</li> <li>build-docker-image: This step creates the Docker image by using the IBM Cloud Container Registry build service through the <code>ibmcloud cr build</code> CLI script. </li> </ul> </li> <li>Review the pipeline-validate-task. The task consists of a git clone of the repository, followed by the check-vulnerabilities step. This step runs the IBM Cloud Vulnerability Advisor on the image to check for known vulnerabilities. If it finds a vulnerability, the job fails, preventing the image from being deployed. This safety feature prevents apps with security holes from being deployed. The image has no vulnerabilities, so it passes. In this tutorial template, the default configuration of the job is to not block on failure.</li> <li>Review the pipeline-deploy-task. The task consists of a git clone of the repository followed by two steps:<ul> <li>pre-deploy-check: This step checks whether the IBM Container Service cluster is ready and has a namespace that is configured with access to the private image registry by using an IBM Cloud API Key. </li> <li>deploy-to-kubernetes: This step updates the <code>deployment.yml</code> manifest file with the image url and deploys the application using <code>kubectl apply</code></li> </ul> </li> <li>After all the steps in the pipeline are completed, a green status is shown for each task. Click the deploy-to-kubernetes step and click the Logs tab to see the successful completion of this step.     </li> <li>Scroll to the end of the log. The <code>DEPLOYMENT SUCCEEDED</code> message is shown at the end of the log.     </li> <li>Click the URL to see the running application.     </li> </ul>"},{"location":"devops/ibm-toolchain/ibm-toolchain/#modify-the-app-code","title":"Modify the App Code","text":"<p>In this task, you modify the application and redeploy it. You can see how your Tekton-based delivery pipeline automatically picks up the changes in the application on commit and redeploys the app. </p> <ol> <li>On the toolchain's Overview page, click the Git tile for your application. <ul> <li>Tip: You can also use the built-in Eclipse Orion-based Web IDE, a local IDE, or your favorite editor to change the files in your repo.</li> </ul> </li> <li>In the repository directory tree, open the <code>app.js</code> file.     </li> <li>Edit the text message code to change the welcome message.      </li> <li>Commit the updated file by typing a commit message and clicking Commit changes to push the change to the project's remote repository. </li> <li>Return to the toolchain's Overview page by clicking the back arrow.</li> <li>Click Delivery Pipeline. The pipeline is running because the commit automatically started a build. Over the next few minutes, watch your change as it is built, tested, and deployed.      </li> <li>After the deploy-to-kubernetes step is completed, refresh your application URL. The updated message is shown.</li> </ol>"},{"location":"devops/ibm-toolchain/ibm-toolchain/#clean-up-resources","title":"Clean up Resources","text":"<p>In this task, you can remove any of the content that is generated by this tutorial. Before you begin, you need the IBM Cloud CLI and the IBM Cloud Kubernetes Service CLI. Instructions to install the CLI are in the prerequisite section of this tutorial.</p> <ol> <li>Delete the git repository, sign in into git, select personal projects. Then go to repository General settings and remove the repository.</li> <li>Delete the toolchain. You can delete a toolchain and specify which of the associated tool integrations you want to delete. When you delete a toolchain, the deletion is permanent.<ul> <li>On the DevOps dashboard, on the Toolchains page, click the toolchain to delete. Alternatively, on the app's Overview page, on the Continuous delivery card, click View Toolchain.</li> <li>Click the More Actions menu, which is next to View app.</li> <li>Click Delete. Deleting a toolchain removes all of its tool integrations, which might delete resources that are managed by those integrations.</li> <li>Confirm the deletion by typing the name of the toolchain and clicking Delete. </li> <li>Tip: When you delete a GitHub, GitHub Enterprise, or Git Repos and Issue Tracking tool integration, the associated repo isn't deleted from GitHub, GitHub Enterprise, or Git Repos and Issue Tracking. You must manually remove the repo.</li> </ul> </li> <li>Delete the cluster or discard the namespace from it. It is easiest to delete the entire namespace (Please do not delete the <code>default</code> namespace) by using the IBM Cloud\u2122 Kubernetes Service CLI from a command-line window. However, if you have other resources that you need to keep in the namespace, you need to delete the application resources individually instead of the entire namespace. To delete the entire namespace, enter this command:     <pre><code>kubectl delete namespace [not-the-default-namespace]\n</code></pre></li> <li>Delete your IBM Cloud API key.</li> <li>From the Manage menu, click Access (IAM). Click IBM Cloud API Keys.</li> <li>Find your API Key in the list and select Delete from the menu to the right of the API Key name.</li> <li>Delete the container images. To delete the images in your container image registry, enter this command in a command-line window:     <pre><code>ibmcloud cr image-rm IMAGE [IMAGE...]\n</code></pre>     If you created a registry namespace for the tutorial, delete the entire registry namespace by entering this command:     <pre><code>ibmcloud cr namespace-rm NAMESPACE\n</code></pre><ul> <li>Note: You can run this tutorial many times by using the same registry namespace and cluster parameters without discarding previously generated resources. The generated resources use randomized names to avoid conflicts.</li> </ul> </li> </ol>"},{"location":"devops/ibm-toolchain/ibm-toolchain/#summary","title":"Summary","text":"<p>You created a toolchain with a Tekton-based delivery pipeline that deploys a \"Hello World\" app to a secure container in a Kubernetes cluster. You changed a message in the app and tested your change. When you pushed the change to the repo, the delivery pipeline automatically redeployed the app.</p> <ul> <li>Read more about the IBM Cloud Kubernetes Service</li> <li>Read more about Tekton</li> <li>Explore the DevOps reference architecture.</li> </ul>"},{"location":"devops/jenkins/jenkins/","title":"Jenkins Lab","text":""},{"location":"devops/jenkins/jenkins/#jenkins-lab","title":"Jenkins Lab","text":"OpenShiftKubernetes"},{"location":"devops/jenkins/jenkins/#introduction","title":"Introduction","text":"<p>In this lab, you will learn about how to define Continuous Integration for your application. We are using Jenkins to define it.</p> <p>Jenkins</p> <p>Jenkins is a popular open source Continuous Integration tool. It is built in Java. It allows the developers to perform continuous integration and build automation. It allows you to define steps and executes them based on the instructions like building the application using build tools like Ant, Gradle, Maven etc, executing shell scripts, running tests etc. All the steps can be executed based on the timing or event. It depends on the setup. It helps to monitor all these steps and sends notifications to the team members in case of failures. Also, it is very flexible and has a large plugin list which one easily add based on their requirements.</p> <p>Check these guides out if you want to know more about Jenkins - Jenkins, Leading open source automation server.</p>"},{"location":"devops/jenkins/jenkins/#prerequisites","title":"Prerequisites","text":"<ul> <li>You need an IBM cloud account.</li> <li>Create kubernetes cluster using IBM Cloud Kubernetes Service. Here, you can choose an openshift cluster.</li> <li>Install oc command line tool.</li> <li>You should be familiar with basics like Containers, Docker, Kubernetes.</li> </ul>"},{"location":"devops/jenkins/jenkins/#continuous-integration","title":"Continuous Integration","text":""},{"location":"devops/jenkins/jenkins/#install-jenkins","title":"Install Jenkins","text":"<ul> <li>Open the IBM Cloud Openshift cluster.</li> </ul> <ul> <li>Click on the <code>OpenShift web console</code> tab and this will take you to openshift UI.</li> </ul> <ul> <li>Create a new project.</li> </ul> <ul> <li>Search for <code>Jenkins</code>.</li> </ul> <ul> <li>Choose <code>Jenkins (Ephemeral)</code>.</li> </ul> <ul> <li>Install it.</li> </ul> <ul> <li>Wait till the Jenkins installs and the pods are ready.</li> </ul> <ul> <li>Once, it is ready you can access the Jenkins by clicking the link.</li> </ul> <p>Now, click on <code>Log in with OpenShift</code>.</p> <ul> <li>When you gets logged in, you will see the below screen. Click <code>Allow selected permissions</code>.</li> </ul> <p></p> <ul> <li>You will be able to access the Jenkins UI now.</li> </ul> <p></p>"},{"location":"devops/jenkins/jenkins/#get-the-sample-app","title":"Get the Sample App","text":"<ul> <li>Fork the below repository.</li> </ul> <pre><code>https://github.com/ibm-cloud-architecture/cloudnative_sample_app\n</code></pre> <ul> <li>Clone the forked repository.</li> </ul> <pre><code>$ git clone https://github.com/(user)/cloudnative_sample_app.git\n</code></pre>"},{"location":"devops/jenkins/jenkins/#jenkinsfile","title":"Jenkinsfile","text":"<p>Before setting up the CI pipeline, let us first have a look at our Jenkinsfile and understand the stages here.</p> <p>Open your Jenkinsfile or you can also access it https://github.com/ibm-cloud-architecture/cloudnative_sample_app/blob/master/Jenkinsfile[here].</p> <p>In our Jenkins file, we have five stages.</p> <ul> <li>Local - Build</li> </ul> <p>In this stage, we are building the application and packaging it using maven.</p> <ul> <li>Local - Test</li> </ul> <p>In this stage, we are making all the unit tests are running fine by running maven test.</p> <ul> <li>Local - Run</li> </ul> <p>In this stage, we are running the application using the previous build and verifying the application performing health and api checks.</p> <ul> <li> <p>Build and Push Image</p> </li> <li> <p>We are logging in to the IBM Cloud and accessing the IBM Cloud Container Registry.</p> </li> <li>We are also creating a namespace if not present.</li> <li>We are building the image using ibmcloud cli tools.</li> <li>Once the image is built, it is pushed into the container registry.</li> </ul> <p>In this stage, we are building the docker image and pushing it to the registry.</p> <ul> <li> <p>Push to Deploy repo</p> </li> <li> <p>Initially, we are cloning the deploy repository.</p> </li> <li>Changing the image tag to the one we previously built and pushed.</li> <li>Pushing this new changes to the deploy repository.</li> </ul> <p>In this stage, we are pushing the new artifact tag to the deploy repository which will later be used by the Continuous Delivery system.</p>"},{"location":"devops/jenkins/jenkins/#jenkins-credentials","title":"Jenkins Credentials","text":"<p>Let us now build all the credentials required by the pipeline.</p> <ul> <li>In the Jenkins home page, click on <code>Credentials</code>.</li> </ul> <p></p> <ul> <li>In the Credentials page, click on <code>Jenkins</code>.</li> </ul> <p></p> <ul> <li>Now, click on <code>Global Credentials (UnRestricted)</code>.</li> </ul> <p></p> <ul> <li>Click on <code>Add Credentials</code> to create the ones required for this lab.</li> </ul> <p>image::Jenkins_add_creds.png[align=\"center\"] </p> <ul> <li>Now create a secrets as follows.</li> </ul> <p>Kind : Secret Text Secret: (Your container registry url, for eg., us.icr.io) ID: registry_url</p> <p></p> <p>Once created, you will see something like below.</p> <p></p> <p>Similarly create the rest of the credentials as well.</p> <p>Kind : Secret Text Secret: (Your registry namespace, for eg., catalyst_cloudnative) ID: registry_namespace</p> <p>Kind : Secret Text Secret: (Your IBM cloud region, for eg., us-east) ID: ibm_cloud_region</p> <p>Kind : Secret Text Secret: (Your IBM Cloud API key) ID: ibm_cloud_api_key</p> <p>Kind : Secret Text Secret: (Your Github Username) ID: git-account</p> <p>Kind : Secret Text Secret: (Your Github Token) ID: github-token</p> <p>Once all of them are created, you will have the list as follows.</p> <p></p>"},{"location":"devops/jenkins/jenkins/#jenkins-pipeline","title":"Jenkins Pipeline","text":"<ul> <li>Create a new pieline. Go to Jenkins ) Click on <code>New Item</code>.</li> </ul> <ul> <li>Enter the name of the application, choose <code>Pipeline</code> and click <code>OK</code>.</li> </ul> <ul> <li> <p>Now go to the <code>Pipeline</code> tab and enter the details of the repository.</p> </li> <li> <p>In the Definition, choose <code>Pipeline script from SCM</code>.</p> </li> <li>Mention SCM as <code>Git</code>.</li> <li>Enter the repository URL in <code>Repository URL</code>.</li> <li>Specify <code>master</code> as the branch to build.</li> <li><code>Save</code> this information.</li> </ul> <p></p> <ul> <li>To initiate a build, click <code>Build Now</code>.</li> </ul> <p></p> <ul> <li>Once the build is successful, you will see something like below.</li> </ul> <p></p> <p>After this build is done, your deploy repository will be updated by the Jenkins.</p> <p></p>"},{"location":"devops/jenkins/jenkins/#introduction_1","title":"Introduction","text":"<p>In this lab, you will learn about how to define Continuous Integration for your application. We are using https://jenkins.io/[Jenkins] to define it.</p> <p>Jenkins</p> <p>Jenkins is a popular open source Continuous Integration tool. It is built in Java. It allows the developers to perform continuous integration and build automation. It allows you to define steps and executes them based on the instructions like building the application using build tools like Ant, Gradle, Maven etc, executing shell scripts, running tests etc. All the steps can be executed based on the timing or event. It depends on the setup. It helps to monitor all these steps and sends notifications to the team members in case of failures. Also, it is very flexible and has a large plugin list which one easily add based on their requirements.</p> <p>Check these guides out if you want to know more about Jenkins - https://jenkins.io/doc/[Jenkins, Leading open source automation server].</p>"},{"location":"devops/jenkins/jenkins/#prerequisites_1","title":"Prerequisites","text":"<ul> <li>You need an https://cloud.ibm.com/login[IBM cloud account].</li> <li>Create kubernetes cluster using https://cloud.ibm.com/docs/containers?topic=containers-getting-started[IBM Cloud Kubernetes Service]. Here, you can choose a kubernetes cluster.</li> <li>Install https://kubernetes.io/docs/tasks/tools/install-kubectl/[kubectl] command line tool.</li> <li>You should be familiar with basics like Containers, Docker, Kubernetes.</li> </ul>"},{"location":"devops/jenkins/jenkins/#continuous-integration_1","title":"Continuous Integration","text":""},{"location":"devops/jenkins/jenkins/#install-jenkins_1","title":"Install Jenkins","text":"<ul> <li>Initially log in into your ibm cloud account as follows.</li> </ul> <pre><code>$ ibmcloud login -a cloud.ibm.com -r (region) -g (cluster_name)\n</code></pre> <p>And then download the Kube config files as below.</p> <pre><code>$ ibmcloud ks cluster-config --cluster (cluster_name)\n</code></pre> <p>You can also get the <code>access</code> instructions in <code>IBM Cloud Dashboard -&gt; Kubernetes Clusters -&gt; Click on your Cluster -&gt; Click on Access Tab</code>.</p> <ul> <li>Install Jenkins using helm using the below command. We are not using persistence in this lab.</li> </ul> <pre><code>$ helm install --name cloudnative-jenkins --set persistence.enabled=false stable/jenkins\n</code></pre> <p>If it is successfully executed, you will see something like below.</p> <pre><code>$ helm install --name cloudnative-jenkins --set persistence.enabled=false stable/jenkins\nNAME:   cloudnative\nLAST DEPLOYED: Wed Aug  7 16:22:55 2019\nNAMESPACE: default\nSTATUS: DEPLOYED\n\nRESOURCES:\n==&gt; v1/ConfigMap\nNAME                       DATA  AGE\ncloudnative-jenkins        5     1s\ncloudnative-jenkins-tests  1     1s\n\n==&gt; v1/Deployment\nNAME                 READY  UP-TO-DATE  AVAILABLE  AGE\ncloudnative-jenkins  0/1    1           0          1s\n\n==&gt; v1/Pod(related)\nNAME                                  READY  STATUS    RESTARTS  AGE\ncloudnative-jenkins-57588c86c7-hxqmq  0/1    Init:0/1  0         0s\n\n==&gt; v1/Role\nNAME                                 AGE\ncloudnative-jenkins-schedule-agents  1s\n\n==&gt; v1/RoleBinding\nNAME                                 AGE\ncloudnative-jenkins-schedule-agents  1s\n\n==&gt; v1/Secret\nNAME                 TYPE    DATA  AGE\ncloudnative-jenkins  Opaque  2     1s\n\n==&gt; v1/Service\nNAME                       TYPE          CLUSTER-IP      EXTERNAL-IP     PORT(S)         AGE\ncloudnative-jenkins        LoadBalancer  172.21.143.35   169.63.132.124  8080:32172/TCP  1s\ncloudnative-jenkins-agent  ClusterIP     172.21.206.235  (none&gt;          50000/TCP       1s\n\n==&gt; v1/ServiceAccount\nNAME                 SECRETS  AGE\ncloudnative-jenkins  1        1s\n</code></pre> <p>Use the following steps to open Jenkins UI and login.</p> <pre><code>NOTES:\n1. Get your 'admin' user password by running:\nprintf $(kubectl get secret --namespace default cloudnative-jenkins -o jsonpath=\"{.data.jenkins-admin-password}\" | base64 --decode);echo\n2. Get the Jenkins URL to visit by running these commands in the same shell:\nNOTE: It may take a few minutes for the LoadBalancer IP to be available.\n        You can watch the status of by running 'kubectl get svc --namespace default -w cloudnative-jenkins'\nexport SERVICE_IP=$(kubectl get svc --namespace default cloudnative-jenkins --template \"{{ range (index .status.loadBalancer.ingress 0) }}{{ . }}{{ end }}\")\necho http://$SERVICE_IP:8080/login\n\n3. Login with the password from step 1 and the username: admin\n\n\nFor more information on running Jenkins on Kubernetes, visit:\nhttps://cloud.google.com/solutions/jenkins-on-container-engine\n#################################################################################\n######   WARNING: Persistence is disabled!!! You will lose your data when   #####\n######            the Jenkins pod is terminated.                            #####\n#################################################################################\n</code></pre> <p>To get the url, run the below commands.</p> <pre><code>$ export SERVICE_IP=$(kubectl get svc --namespace default cloudnative-jenkins --template \"{{ range (index .status.loadBalancer.ingress 0) }}{{ . }}{{ end }}\")\n$ echo http://$SERVICE_IP:8080/login\n</code></pre> <p>Once executed, you will see something like below.</p> <pre><code>$ echo http://$SERVICE_IP:8080/login\nhttp://169.63.132.124:8080/login\n</code></pre> <ul> <li>Now, let us login into the Jenkins.</li> </ul> <p></p> <p>The user name will be <code>admin</code> and to get the password, run the below command.</p> <pre><code>$ printf $(kubectl get secret --namespace default cloudnative-jenkins -o jsonpath=\"{.data.jenkins-admin-password}\" | base64 --decode);echo\n</code></pre> <p>It returns you the password as follows.</p> <pre><code>$ printf $(kubectl get secret --namespace default cloudnative-jenkins -o jsonpath=\"{.data.jenkins-admin-password}\" | base64 --decode);echo\npassword\n</code></pre> <ul> <li>Once, successfully logged in you will see the Jenkins home page which is as follows.</li> </ul> <p></p>"},{"location":"devops/jenkins/jenkins/#get-the-sample-app_1","title":"Get the Sample App","text":"<ul> <li> <p>Fork the below repository.</p> <p>https://github.com/ibm-cloud-architecture/cloudnative_sample_app</p> </li> <li> <p>Clone the forked repository.</p> </li> </ul> <pre><code>$ git clone https://github.com/(user)/cloudnative_sample_app.git\n</code></pre>"},{"location":"devops/jenkins/jenkins/#jenkinsfile_1","title":"Jenkinsfile","text":"<p>Before setting up the CI pipeline, let us first have a look at our Jenkinsfile and understand the stages here.</p> <p>Open your Jenkinsfile or you can also access it https://github.com/ibm-cloud-architecture/cloudnative_sample_app/blob/master/Jenkinsfile[here].</p> <p>In our Jenkins file, we have five stages.</p> <ul> <li>Local - Build</li> </ul> <p>In this stage, we are building the application and packaging it using maven.</p> <ul> <li>Local - Test</li> </ul> <p>In this stage, we are making all the unit tests are running fine by running maven test.</p> <ul> <li>Local - Run</li> </ul> <p>In this stage, we are running the application using the previous build and verifying the application performing health and api checks.</p> <ul> <li> <p>Build and Push Image</p> </li> <li> <p>We are logging in to the IBM Cloud and accessing the IBM Cloud Container Registry.</p> </li> <li>We are also creating a namespace if not present.</li> <li>We are building the image using ibmcloud cli tools.</li> <li>Once the image is built, it is pushed into the container registry.</li> </ul> <p>In this stage, we are building the docker image and pushing it to the registry.</p> <ul> <li> <p>Push to Deploy repo</p> </li> <li> <p>Initially, we are cloning the deploy repository.</p> </li> <li>Changing the image tag to the one we previously built and pushed.</li> <li>Pushing this new changes to the deploy repository.</li> </ul> <p>In this stage, we are pushing the new artifact tag to the deploy repository which will later be used by the Continuous Delivery system.</p>"},{"location":"devops/jenkins/jenkins/#jenkins-credentials_1","title":"Jenkins Credentials","text":"<p>Let us now build all the credentials required by the pipeline.</p> <ul> <li>In the Jenkins home page, click on <code>Credentials</code>.</li> </ul> <p></p> <ul> <li>In the Credentials page, click on <code>Jenkins</code>.</li> </ul> <p></p> <ul> <li>Now, click on <code>Global Credentials (UnRestricted)</code>.</li> </ul> <p></p> <ul> <li>Click on <code>Add Credentials</code> to create the ones required for this lab.</li> </ul> <p></p> <ul> <li>Now create a secrets as follows.</li> </ul> <p>Kind : Secret Text Secret: Your container registry url, for eg., us.icr.io ID: registry_url</p> <p></p> <p>Once created, you will see something like below.</p> <p></p> <p>Similarly create the rest of the credentials as well.</p> <p>Kind : Secret Text Secret: (Your registry namespace, for eg., catalyst_cloudnative) ID: registry_namespace</p> <p>Kind : Secret Text Secret: (Your IBM cloud region, for eg., us-east) ID: ibm_cloud_region</p> <p>Kind : Secret Text Secret: (Your IBM Cloud API key) ID: ibm_cloud_api_key</p> <p>Kind : Secret Text Secret: (Your Github Username) ID: git-account</p> <p>Kind : Secret Text Secret: (Your Github Token) ID: github-token</p> <p>Once all of them are created, you will have the list as follows.</p> <p></p>"},{"location":"devops/jenkins/jenkins/#jenkins-pipeline_1","title":"Jenkins Pipeline","text":"<ul> <li>Create a new pieline. Go to Jenkins ) Click on <code>New Item</code>.</li> </ul> <ul> <li>Enter the name of your application, select <code>Pipeline</code> and then click <code>OK</code>.</li> </ul> <ul> <li>In <code>General</code>, check <code>This project is parameterized</code>. Create a string parameter with name <code>CLOUD</code> and Default value <code>kubernetes</code>.</li> </ul> <ul> <li> <p>Now go to the <code>Pipeline</code> tab and enter the details of the repository.</p> </li> <li> <p>In the Definition, choose <code>Pipeline script from SCM</code>.</p> </li> <li>Mention SCM as <code>Git</code>.</li> <li>Enter the repository URL in <code>Repository URL</code>.</li> <li>Specify <code>master</code> as the branch to build.</li> <li><code>Save</code> this information.</li> </ul> <p></p> <ul> <li>To initiate a build, click <code>Build with Parameters</code>.</li> </ul> <p></p> <ul> <li>Once the build is successful, you will see something like below.</li> </ul> <p></p> <p>After this build is done, your deploy repository will be updated by the Jenkins.</p> <p></p>"},{"location":"devops/tekton/","title":"Continuous Integration","text":""},{"location":"devops/tekton/#continuous-integration","title":"Continuous Integration","text":"<p>Continuous Integration, Delivery, and Deployment are important devOps practices and we often hear a lot about them. These processes are valuable and ensures that the software is up to date timely.</p> <ul> <li>Continuous Integration is an automation process which allows developers to integrate their work into a repository. When a developer pushes his work into the source code repository, it ensures that the software continues to work properly. It helps to enable collaborative development across the teams and also helps to identify the integration bugs sooner.</li> <li>Continuous Delivery comes after Continuous Integration. It prepares the code for release. It automates the steps that are needed to deploy a build.</li> <li>Continuous Deployment is the final step which succeeds Continuous Delivery. It automatically deploys the code whenever a code change is done. Entire process of deployment is automated.</li> </ul>"},{"location":"devops/tekton/#tekton-overview","title":"Tekton Overview","text":"<p>Tekton is a cloud-native solution for building CI/CD systems. It consists of Tekton Pipelines, which provides the building blocks, and of supporting components, such as Tekton CLI and Tekton Catalog, that make Tekton a complete ecosystem.</p>"},{"location":"devops/tekton/#presentations","title":"Presentations","text":"<p>Tekton Overview  IBM Cloud DevOps with Tekton </p>"},{"location":"devops/tekton/#activities","title":"Activities","text":"<p>The continuous integration activities focus around Tekton the integration platform. These labs will show you how to build pipelines and test your code before deployment.</p> <p>These tasks assume that you have:</p> <ul> <li>Reviewed the continuous integration concept page.</li> <li>Installed Tekton into your cluster.</li> </ul> Task Description Link Time Walkthroughs Deploying Applications From Source Using OpenShift 4 S2I 30 min Try It Yourself Tekton Lab Using Tekton to build container images Tekton 1 hour IBM Cloud DevOps Using IBM Cloud ToolChain with Tekton Tekton on IBM Cloud 1 hour Jenkins Lab Using Jenkins to build and deploy applications. Jenkins 1 hour <p>Once you have completed these tasks, you will have an understanding of continuous integration and how to use Tekton to build a pipeline.</p>"},{"location":"devops/tekton/tekton/","title":"Tekton","text":""},{"location":"devops/tekton/tekton/#prerequisites","title":"Prerequisites","text":"<ul> <li> Make sure your OpenShift environment is properly setup. Follow the instructions here</li> <li> OpenShift Cluster deployed</li> <li> ODF and storage configured</li> <li> Internal registry configured</li> </ul>"},{"location":"devops/tekton/tekton/#setup","title":"SetUp","text":""},{"location":"devops/tekton/tekton/#tekton-cli-installation","title":"Tekton CLI Installation","text":"<p>Tekton CLI is command line utility used to interact with the Tekton resources.</p> <p>Follow the instructions on the tekton CLI github repository https://github.com/tektoncd/cli#installing-tkn</p> <p>For MacOS for example you can use brew:</p> <p></p><pre><code>brew tap tektoncd/tools\nbrew install tektoncd/tools/tektoncd-cli\n</code></pre> Verify the Tekton cli <pre><code>tkn version\n</code></pre> The command should show a result like: <pre><code>Client version: 0.39.1\n</code></pre> If you already have the <code>tkn</code> CLI installed you can upgrade by running: <pre><code>brew upgrade tektoncd/tools/tektoncd-cli\n</code></pre><p></p>"},{"location":"devops/tekton/tekton/#openshift-pipelines-operator-installation","title":"OpenShift Pipelines Operator Installation","text":"<p>Install the OpenShift Pipelines Operator (You can find some instructions here)</p> <p>Note</p> <p>Version will vary based on the version of OpenShift you are running.</p> <p>Note: It will take few mins for the OpenShift Pipelines components to be installed, check the status using the <code>oc</code> command.</p> <p>OpenShift Pipelines vs Tekton</p> <p>You may notice we sometimes use Tekton and OpenShift Pipelines interchangeably in this guide. OpenShift Pipelines is a Red-Hat-provided Operator that installs and manages Tekton on OpenShift. All Tekton concepts apply to <code>OpenShift Pipelines</code>.</p>"},{"location":"devops/tekton/tekton/#create-target-namespace","title":"Create Target Namespace","text":"<p>Set an environment variable <code>NAMESPACE</code> to <code>tekton-demo</code> and create a namespace using that variable.</p>"},{"location":"devops/tekton/tekton/#tasks","title":"Tasks","text":""},{"location":"devops/tekton/tekton/#task-creation","title":"Task Creation","text":"<ul> <li>Create the below yaml files: <pre><code>apiVersion: tekton.dev/v1beta1\nkind: Task\nmetadata:\n  name: java-test\nspec:\n  params:\n      - name: url\n        default: https://github.com/ibm-cloud-architecture/cloudnative_sample_app\n      - name: revision\n        default: master\n  steps:\n      - name: git-clone\n        image: alpine/git\n        script: |\n          git clone -b $(params.revision) --depth 1 $(params.url) /workspace/source\n      - name: test\n        image: maven:3.3-jdk-8\n        workingdir: /workspace/source\n        script: |\n          mvn test\n          echo \"tests passed with rc=$?\"\n        volumeMounts:\n          - name: m2-repository\n            mountPath: /root/.m2\n  volumes:\n      - name: m2-repository\n        emptyDir: {}\n  workspaces:\n    - description: |\n        Container build context, like for instance a application source code\n        followed by a `Dockerfile`.\n      name: source\n</code></pre></li> </ul> <p>When defining a task, will need to define the following:</p>"},{"location":"devops/tekton/tekton/#parameters","title":"Parameters","text":"<p>Allows passing configurable values to the Task.</p> <p>Review how to configure <code>parameters</code> on the Tekton Documentation</p>"},{"location":"devops/tekton/tekton/#workspaces","title":"Workspaces","text":"<p>Provides shared storage for data between steps or Tasks.</p> <p>Review how to configure <code>workspaces</code> on the Tekton Documentation</p>"},{"location":"devops/tekton/tekton/#steps","title":"Steps","text":"<p>Defines the containerized commands executed in the Task. </p> <p>Review how to configure <code>steps</code> on the Tekton Documentation</p> <p>Validate you have run the task successfully unsing the tekton cli:</p> <pre><code>NAME        AGE\njava-test   22 seconds ago\n</code></pre> <p>HINT: run <code>tkn task --help</code></p>"},{"location":"devops/tekton/tekton/#taskrun","title":"TaskRun","text":"<p>A <code>TaskRun</code> allows you to instantiate and execute a <code>Task</code> on-cluster. A <code>Task</code> specifies one or more <code>Steps</code> that execute container images and each container image performs a specific piece of build work. A <code>TaskRun</code> executes the <code>Steps</code> in the <code>Task</code> in the order they are specified until all Steps have executed successfully or a failure occurs.</p> <p>Review how to configure <code>TaskRuns</code> on the Tekton Documentation</p> <p>In the following section we will run the <code>build</code>-app task created in the previous step.</p>"},{"location":"devops/tekton/tekton/#taskrun-creation","title":"TaskRun Creation","text":"<p>The following snippet shows what a Tekton TaskRun YAML looks like:</p> <pre><code>apiVersion: tekton.dev/v1\nkind: TaskRun\nmetadata:\n  generateName: java-test-\nspec:\n  params:\n    - name: url\n      value: 'https://github.com/ibm-cloud-architecture/cloudnative_sample_app.git'\n    - name: revision\n      value: master\n  serviceAccountName: pipeline\n  taskRef:\n    kind: Task\n    name: java-test\n  timeout: 1h0m0s\n  workspaces:\n    - name: source\n      emptyDir: {}\n</code></pre> <ul> <li> <p><code>generateName</code> since the <code>TaskRun</code> can be run many times, in order to have unique name across the <code>TaskRun</code> ( helpful when checking the TaskRun history) we use this <code>generateName</code> instead of name. When Kubernetes sees <code>generateName</code> it will generate unique set of characters and suffix the same to build-app-, similar to how pod names are generated.</p> </li> <li> <p><code>taskRef</code> this is used to refer to the <code>Task</code> by its name that will be run as part of this <code>TaskRun</code>. In this example we use build-app <code>Task</code>.</p> </li> <li> <p>As described in the earlier section that the <code>Task</code> inputs and outputs could be overridden via <code>TaskRun</code>.</p> </li> <li> <p><code>params</code> the parameter values that are passed to the task</p> </li> </ul> <p>Run the task from a task run.</p> <p>HINT - look at the <code>oc create</code> command.</p> <p>We can use the Tekton cli to inspect the created resources.</p> <p>The <code>tkn</code> command should list one TaskRun as shown below: </p><pre><code>NAME                   STARTED        DURATION   STATUS\njava-test-9rkkt        1 minute ago   ---        Running(Pending)\n</code></pre><p></p> <p>Note - It will take few seconds for the TaskRun to show status as <code>Running</code> as it needs to download the container images.</p> <p>Check the logs of the Task Run using the <code>tkn</code> CLI.</p> <p>Note - Each task step will be run within a container of its own. HINT: <code>tkn tr logs --help</code></p> <p>Use the <code>tkn</code> CLI to ensure the task has run successfully. You should see an output like this:</p> <pre><code>NAME                                STARTED          DURATION   STATUS\njava-test-9rkkt                     36 minutes ago   33s        Succeeded\n</code></pre>"},{"location":"devops/tekton/tekton/#pipelines","title":"Pipelines","text":""},{"location":"devops/tekton/tekton/#pipeline-creation","title":"Pipeline Creation","text":"<p>Pipelines allows to start multiple Tasks, in parallel or in a certain order</p> <p>Create an <code>app-build</code> Pipeline containing two Tasks:</p> <pre><code>apiVersion: tekton.dev/v1\nkind: Pipeline\nmetadata:\n  name: app-build\nspec:\n  params:\n    - name: source_repo\n      type: string\n    - name: image_registry\n      type: string\n  tasks:\n    - name: java-test\n      params:\n        - name: url\n          value: $(params.source_repo)\n        - name: revision\n          value: master\n      taskRef:\n        kind: Task\n        name: java-test\n      workspaces:\n        - name: source\n          workspace: source\n    - name: buildah-build\n      params:\n        - name: IMAGE\n          value: $(params.image_registry)\n        - name: DOCKERFILE\n          value: ./Dockerfile\n        - name: CONTEXT\n          value: .\n        - name: STORAGE_DRIVER\n          value: vfs\n        - name: FORMAT\n          value: oci\n        - name: BUILD_EXTRA_ARGS\n          value: ''\n        - name: PUSH_EXTRA_ARGS\n          value: ''\n        - name: SKIP_PUSH\n          value: 'false'\n        - name: TLS_VERIFY\n          value: 'true'\n        - name: VERBOSE\n          value: 'false'\n      runAfter:\n        - java-test\n      taskRef:\n        params:\n          - name: kind\n            value: task\n          - name: name\n            value: buildah\n          - name: namespace\n            value: openshift-pipelines\n        resolver: cluster\n      workspaces:\n        - name: source\n          workspace: source\n  workspaces:\n    - name: source\n</code></pre> <p>Information</p> <p>A <code>Pipeline</code> defines a list of <code>Tasks</code> to execute in order, while also indicating if any <code>outputs</code> should be used as <code>inputs</code> of a following <code>Task</code> by using the <code>from</code> field. Furthermore, a <code>Pipeline</code> defines the order of execution of tasks (using the <code>runAfter</code> and <code>from</code>fields). The same variable substitution you used in <code>Tasks</code> is also available in a <code>Pipeline</code>.</p> <p>As of OpenShift 4.17, you need to use resolvers in order to reference a Task outside of the namespace. Read more about cluster resolvers here</p> <p>Use the Tekton cli to inspect the pipeline is created successfully:</p> <pre><code>NAME        AGE              LAST RUN   STARTED   DURATION   STATUS\napp-build   31 seconds ago   ---        ---       ---        ---\n</code></pre>"},{"location":"devops/tekton/tekton/#pipelinerun","title":"PipelineRun","text":""},{"location":"devops/tekton/tekton/#pipelinerun-creation","title":"PipelineRun Creation","text":"<p>To execute the Tasks in the <code>Pipeline</code>, you must create a <code>PipelineRun</code>. Creation of a <code>PipelineRun</code> will trigger the creation of <code>TaskRuns</code> for each <code>Task</code> in your pipeline. Create the following <code>PipelineRun</code>:</p> <pre><code>apiVersion: tekton.dev/v1\nkind: PipelineRun\nmetadata:\n  generateName: app-build-run-\n  labels:\n    tekton.dev/pipeline: app-build\nspec:\n  params:\n    - name: source_repo\n      value: 'https://github.com/ibm-cloud-architecture/cloudnative_sample_app.git'\n    - name: image_registry\n      value: 'image-registry.openshift-image-registry.svc:5000/tekton-demo/cloud-native-sample-app:v1.0.0'\n  pipelineRef:\n    name: app-build\n  taskRunTemplate:\n    serviceAccountName: pipeline\n  timeouts:\n    pipeline: 1h0m0s\n  workspaces:\n    - name: source\n      volumeClaimTemplate:\n        metadata:\n          creationTimestamp: null\n        spec:\n          accessModes:\n            - ReadWriteOnce\n          resources:\n            requests:\n              storage: 1Gi\n          volumeMode: Filesystem\n          storageClassName: ocs-storagecluster-cephfs\n        status: {}\n</code></pre> <p>serviceAccount - it is always recommended to have a service account associated with <code>PipelineRun</code>, which can then be used to define fine grained roles. It's important to understand Service Accounts when working with Tekton and other applications. Take a minute to review service accounts here</p> <p>Use the Tekton cli to inspect the created resources, the command should list one PipelineRun as shown below:</p> <pre><code>NAME                  STARTED          DURATION   STATUS\napp-build-run-2cfbx   1 minutes ago    ---        Running\n</code></pre> <p>Wait for few minutes for your pipeline to complete all the tasks. If it is successful, you will see something like below:</p> <p>Use the <code>tkn</code> cli to check the pipeline has completed successfully:</p> <pre><code>NAME              AGE              LAST RUN                    STARTED         DURATION    STATUS\napp-build   33 minutes ago         app-build-run-2cfbx         2 minutes ago   2 minutes   Succeeded\n</code></pre> <p>If it is successful, check that the <code>ImageStream</code> has been created successfully using the <code>oc</code> command, you will see something like below:</p> <pre><code>NAME                      IMAGE REPOSITORY     TAGS     UPDATED\ncloud-native-sample-app   default-route...     v1.0.0   5 minutes ago\n</code></pre>"},{"location":"devops/tekton/tekton/#deploy-application","title":"Deploy Application","text":"<ul> <li>Create a deployment using the <code>cloud-native-sample-app</code> <code>ImageStream</code></li> </ul> <p>Verify if the pods are running using the <code>oc</code> CLI:</p> <pre><code>NAME                           READY   STATUS    RESTARTS   AGE\ncloudnative-77df47cfbc-962vx   1/1     Running   0          10s\n</code></pre> <ul> <li>Expose the deployment as a service on port 9080 and then expose that service as a route using the <code>oc</code> cli.</li> </ul> <p>To test you have completed everything successfully, run the following command: </p><pre><code>export APP_URL=\"$(oc get route cloudnative --template 'http://{{.spec.host}}')/greeting?name=World\"\necho APP_URL=$APP_URL\n</code></pre><p></p> <p></p><pre><code>curl $APP_URL\n</code></pre> Output should be: <pre><code>{\"id\":4,\"content\":\"Welcome to Cloudnative bootcamp !!! Hello, World :)\"}\n</code></pre><p></p>"},{"location":"devops/tekton/tekton/#additional-challenge","title":"Additional Challenge","text":"<p>Additional Challenge</p> <p>You have now successfully deployed your first application using Tekton! If you still have time to spare, here is an extra challenge. You will have noticed that we deployed the application to the cluster manually. Platform Engineers never like to do anything which can be automated manually! As an extra challenge, create an application deployment task and add it to the <code>app-build</code> pipeline.</p>"},{"location":"devops/tekton_argocd/gitops-lab/","title":"Gitops lab","text":"<p>Title: Lab: GitOps hide:     - toc</p>"},{"location":"devops/tekton_argocd/gitops-lab/#lab-gitops","title":"Lab: GitOps","text":""},{"location":"devops/tekton_argocd/gitops-lab/#openshift-gitops-install-lab","title":"OpenShift GitOps Install Lab","text":"<ol> <li>Configure Bastion and local workstation with the ArgoCD cli (argo)</li> <li>Install the OpenShift GitOps Operator</li> <li>Launch the ArgoCD Application. 9 Box in Top Right of OpenShift Console.     Note: Use OpenShift Credentials</li> </ol>"},{"location":"devops/tekton_argocd/gitops-solution/","title":"Gitops solution","text":"<p>Title: Solution: GitOps hide:     - toc</p>"},{"location":"devops/tekton_argocd/gitops-solution/#solution-gitops","title":"Solution: GitOps","text":""},{"location":"devops/tekton_argocd/gitops-solution/#bastion-and-workstation-prep","title":"Bastion and Workstation Prep","text":"<ol> <li> <p>Download and install <code>argo</code> on Linux. Other Platforms here</p> <pre><code>curl -sSL -o argocd-linux-amd64 https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-amd64\nsudo install -m 555 argocd-linux-amd64 /usr/local/bin/argocd\nrm -f argocd-linux-amd64\n</code></pre> </li> <li> <p>Verify <code>argocd</code></p> <p></p><pre><code># argocd version\nargocd: v2.14.2+ad27246\n  BuildDate: 2025-02-06T00:06:23Z\n  GitCommit: ad2724661b66ede607db9b5bd4c3c26491f5be67\n  GitTreeState: clean\n  GoVersion: go1.23.3\n  Compiler: gc\n  Platform: linux/amd64\nFATA[0000] Argo CD server address unspecified\n</code></pre> Note: The error does not apply. This is to ensure that the command <code>argocd</code> is in the PATH.<p></p> </li> </ol>"},{"location":"devops/tekton_argocd/gitops-solution/#openshift-gitops-operator-install","title":"OpenShift GitOps Operator Install","text":"<ol> <li>Login to OpenShift Console as kubeadmin.</li> <li>Select Operaters-&gt;Operator Hub.</li> <li>In the 'Search' Type 'open gitops' to search for OpenShift GitOps.</li> <li> <p>Select the 'Red Hat OpenShift GitOps' Operator.</p> <p></p> </li> <li> <p>Install the OpenShift GitOps Operator with default values.</p> <p></p> </li> <li> <p>Select Install</p> <p></p> </li> <li> <p>The operator is installing</p> <p></p> </li> <li> <p>Launch the ArgoCD Application in OpenShift.</p> <p></p> </li> <li> <p>Login via OpenShift</p> <p></p> </li> <li> <p>ArgoCD Console</p> <p></p> </li> </ol>"},{"location":"devops/tekton_argocd/overview/","title":"GitOps Overview","text":""},{"location":"devops/tekton_argocd/overview/#gitops-overview","title":"GitOps Overview","text":""},{"location":"devops/tekton_argocd/overview/#devops","title":"DevOps","text":"<p>DevOps is a set of practices, tools, and cultural philosophies that aims to bridge the gap between software development (Dev) and IT operations (Ops). The primary goal of DevOps is to improve the collaboration, communication, and automation of the software development lifecycle, enabling faster, more reliable, and more consistent delivery of high-quality software.</p> <p>Key principles of DevOps include:</p> <ul> <li>Continuous Integration/Continuous Deployment (CI/CD): Automating the software delivery process from code commit to production deployment.</li> <li>Infrastructure as Code (IaC): Managing and provisioning infrastructure using code, enabling version control, automation, and collaboration.</li> <li>Monitoring and Logging: Collecting, analyzing, and acting on telemetry data to ensure system health, performance, and security.</li> <li>Collaboration and Communication: Encouraging cross-functional teams to work together, share knowledge, and align goals.</li> <li>Automation: Automating repetitive tasks to reduce human error, increase efficiency, and free up team resources.</li> <li>Culture and Mindset: Fostering a culture of experimentation, learning, and continuous improvement.</li> </ul>"},{"location":"devops/tekton_argocd/overview/#gitops","title":"GitOps","text":"<p>GitOps is a practice for automating the configuration and deployment of infrastructure and applications using Git as the single source of truth. It leverages GitOps principles to manage and synchronize the state of OpenShift clusters with the desired configuration defined in Git, ensuring consistency, automation, and collaboration across development, staging, and production environments.</p> <p>Key GitOps practices include:</p> <ul> <li>Managed OpenShift Clusters: Using GitOps to manage multiple OpenShift clusters from a single Git repository.</li> <li>Declarative Configuration: Using Kubernetes manifests to define resources and their desired state in Git.</li> <li>Automatic Synchronization: Automatically syncing the state of OpenShift clusters with the configuration in Git.</li> </ul>"},{"location":"devops/tekton_argocd/overview/#openshift-implementation-of-these-technologies","title":"OpenShift Implementation of These Technologies","text":"<p>In OpenShift, Pipelines are implemented as an operator. The OpenShift Pipelines operator manages the creation, update, and deletion of pipelines in Git, and it also handles the integration with OpenShift's built-in CI/CD tools to automate the entire pipeline.  The operator allows teams to define application pipelines in Git, and OpenShift orchestrates the automation of building, testing, and deploying applications based on those pipelines. OpenShift Pipelines uses the opensource Tekton project, to impliment pipelines.</p> <p>In OpenShift, GitOps is also implemented as an operator. The OpenShift GitOps operator is based on the opensource project ArgoCD.  The operator integrates with OpenShift's built-in infrastructure management tools, such as Kubernetes manifests, Helm charts, and Operators, to automate the deployment and management of applications and infrastructure.</p> <p>In summary, Pipelines and GitOps are both implemented as operators in OpenShift, enabling teams to automate and manage the CI/CD and infrastructure management processes using Git as the single source of truth and is the foundation for their respective practices.</p> Operator Name Open Source Project Description Red Hat OpenShift Pipelines tekton 1.17.0 Tekton primarily focuses on Continiuous Integration (CI) aspect by providing a framework to build and test applications. Red Hat OpenShift GitOps argoCD 1.15.0 ArgoCD focuses on Continuous Delivery (CD) by managing application deployments to a cluster based on a Git repository."},{"location":"devops/tekton_argocd/pipelines-lab/","title":"Pipelines lab","text":"<p>Title: Lab: Pipelines hide:     - toc</p>"},{"location":"devops/tekton_argocd/pipelines-lab/#lab-pipelines","title":"Lab: Pipelines","text":"<ol> <li>Configure Bastion and local workstation with the Tekton cli (tkn)</li> <li>Install the OpenShift GitOps Operator</li> <li>Refresh the OpenShift WEB UI and find the <code>Pipelines</code> option on the Left navigation menu.</li> </ol>"},{"location":"devops/tekton_argocd/pipelines-solution/","title":"Pipelines solution","text":"<p>Title: Solution: Pipelines hide:     - toc</p>"},{"location":"devops/tekton_argocd/pipelines-solution/#solution-pipelines","title":"Solution: Pipelines","text":""},{"location":"devops/tekton_argocd/pipelines-solution/#bastion-and-workstation-prep","title":"Bastion and Workstation Prep","text":"<ol> <li> <p>Download <code>tkn cli</code> for Linux. For other platforms</p> <pre><code>curl -O https://github.com/tektoncd/cli/releases/download/v0.39.1/tektoncd-cli-0.39.1_Linux-64bit.rpm -L \n</code></pre> </li> <li> <p>Install <code>tkn cli</code></p> <pre><code>rpm -i tektoncd-cli-0.39.1_Linux-64bit.rpm\n</code></pre> </li> <li> <p>Verify <code>tkn</code></p> <p>Note: Kubernetes context is required for the cli to function.  Set the KUBECONFIG environment variable as you would to execute <code>oc</code> commands.</p> <pre><code># tkn version\nClient version: 0.39.1\nPipeline version: unknown, pipeline controller may be installed in another namespace please use tkn version -n {namespace}\n</code></pre> <pre><code># export KUBECONFIG=~/ocpinstall/auth/kubeconfig\n# tkn version\nClient version: 0.39.1\nChains version: v0.23.0\nPipeline version: v0.65.4\nTriggers version: v0.30.0\nOperator version: v0.74.0\n</code></pre> </li> </ol>"},{"location":"devops/tekton_argocd/pipelines-solution/#openshift-operator-installation","title":"OpenShift Operator Installation","text":"<ol> <li>Login to OpenShift Console as kubeadmin.</li> <li>Select Operaters-&gt;Operator Hub.</li> <li>In the 'Search' Type 'Pipelines' to search for OpenShift Pipelines</li> <li> <p>Select 'Red Hat OpenShift Pipelines'</p> <p></p> </li> <li> <p>Install the Operator with the default values.</p> <p></p> </li> <li> <p>Select Install</p> <p></p> </li> <li> <p>Operator Install Updates the OpenShift Console, adding Pipelines to the navigation menu on the left side of the UI.</p> <p></p> </li> <li> <p>Operator Install Complete and Web Console Refreshsed</p> <p></p> </li> </ol>"},{"location":"ea/","title":"Event Automation - Lab Environment","text":""},{"location":"ea/#event-automation-lab-environment","title":"Event Automation - Lab Environment","text":"<p>To help you start exploring the features of IBM Event Automation, the tutorial includes a small selection of topics with a live stream of events that are ready to use.</p> <p>This page outlines how to set up and access the tutorial environment on your own OpenShift Container Platform cluster.</p> <p>The tutorial topics simulate aspects of a clothing retailer, with topics relating to sales, stock management, and employee activities. Messages on the topics are consistent (for example, events on the topic for cancelled orders use order IDs that are used in events on the orders topic) to allow you to experiment with joining and correlating events from different topics. Some topics include events that are intentionally delayed, duplicated, or produced out of sequence, to allow you to learn how to use Event Automation to correctly process topics like this.</p> <p></p> <p>his tutorial environment is not intended to demonstrate a production deployment. It is a quick and simple instance of Event Automation for learning some of the key features.</p>"},{"location":"ea/#event-automation-environment-setup","title":"Event Automation Environment Setup","text":"<p>An IBM Cloud Pak for Integration installation consists of a Red Hat OpenShift Container Platform cluster with one or more Cloud Pak for Integration operators installed and one or more instances of Platform UI deployed.  In this section we will follow the steps below to install Event Automation on CP4I.</p> <p>Which OCP Environment?</p> <p>In most PE Bootcamps we will use the IPI installation of OCP that we create as part of another lab.  If this environment is not available, request an OpenShift Cluster on VMware on IBM Cloud from TechZone. Use the below specifications:</p> <p></p>"},{"location":"ea/#login-to-your-openshift-cluster","title":"Login to Your OpenShift Cluster","text":"<p>In a web browser, open the OCP Console link and paste the Cluster Admin Username (A) and Password (B) copied in the previous step and click Log in (C).</p> <p></p> <p>You are almost ready for the demo. Finally, let\u2019s copy login command to access the cluster by CLI.  On the top right, click your username (A) and select Copy login command (B).</p> <p></p> <p>Display Token and copy the login command to your local CLI. </p> <p>Use <code>--insecure-skip-tls-verify=true</code> if you get error: tls: failed to verify certificate: x509: certificate signed by unknown authority</p>"},{"location":"ea/#demo-github-repo","title":"Demo GitHub Repo","text":"<p>This repo is intended to simplify the process to get a full CP4I demo environment for the latest versions of CP4I (v16.1.0 for LTS and v16.1.1 for CD) based on the CP4I end-to-end demo assets.  This repo does not include the extra elements like Instana and Logging, but the core CP4I capabilities and License Service are included now.</p> <p>You will need to clone the demo repo to your workstation.  Open a terminal window and run the command below:</p> <pre><code>git clone github.ibm.com/joel-gomez/cp4i-tz-deployer\n</code></pre> <p>Open the cp4i-tz-deployer-yl folder:</p> <pre><code>cd cp4i-tz-deployer\n</code></pre> <p>This repo is intended to simplify the process to get a full CP4I demo environment for the latest versions of CP4I.</p>"},{"location":"ea/#deploying-cp4i-and-core-services","title":"Deploying CP4I and core services","text":"<p>Here you will use Tekton pipeline to install Cloud Pak for Integration, including Event Automation and MQ services. This pipeline is ready to install the key CP4I services. We will only remove the services that we don\u2019t need for this demo. Let\u2019s do it!</p> <p>First, you need to deploy the Tekton pipeline:</p> <pre><code>oc apply -f resources/pipeline1.yaml\n</code></pre> <p>Now, let\u2019s start the pipeline, defining the input parameters:</p> <pre><code>tkn pipeline start cp4i-demo \\\n--use-param-defaults \\\n--workspace name=cp4i-ws,volumeClaimTemplateFile=resources/workspace-template.yaml \\\n--pod-template resources/pod-template.yaml \\\n--param DEFAULT_SC=\"ocs-storagecluster-ceph-rbd\" \\\n--param OCP_BLOCK_STORAGE=\"ocs-storagecluster-ceph-rbd\" \\\n--param OCP_FILE_STORAGE=\"ocs-storagecluster-cephfs\" \\\n--param DEPLOY_ASSET_REPOSITORY_OPERATOR=false \\\n--param DEPLOY_API_CONNECT_OPERATOR=false \\\n--param DEPLOY_APP_CONNECT_OPERATOR=false \\\n--param DEPLOY_DATAPOWER_GATEWAY_OPERATOR=false \\\n--param DEPLOY_ASSET_REPO=false \\\n--param DEPLOY_API_CONNECT=false \\\n--param DEPLOY_ACE_SWITCH_SERVER=false \\\n--param DEPLOY_ACE_DESIGNER=false \\\n--param DEPLOY_ACE_DASHBOARD=false \\\n--param DEPLOY_ACE_INTEGRATION_SERVER=false \\\n--param DEPLOY_QUEUE_MANAGER=false\n</code></pre> <p>As result of your command, you will see a command to track the PipelineRun progress. You are welcome to execute it.</p> <pre><code>tkn pipelinerun logs cp4i-demo-run-???? -f -n default\n</code></pre> <p>Another way to track the PipelineRun progress is using the OpenShift Web Console. Back to the OpenShift Web console, open the Pipelines menu (A), then click on Pipelines page (B).</p> <p></p> <p>Open the PipelineRuns page (A) and select your pipeline instance (B).</p> <p></p> <p>In the Pipeline details page, you can see all the steps (if necessary, move to the right to see other steps). Some steps will be skipped based on the parameters defined when you executed the pipeline. If you want to see the details of a specific step, you are welcome to click on step to check the outputs.</p> <p></p> <p>When the PipelineRun completed as Succeeded (A), click on the final step: output-usage (B).</p> <p></p> <p>Here you can get the details about how to access your environment. Copy and paste this information in a notepad, you will need this information later.</p> <p></p>"},{"location":"ea/#launching-support-scripts","title":"Launching Support Scripts","text":"<p>Additional supporting services are needed to generate results and data for the hands-on lab experience.</p> <p>Navigate outside of the cp4i-demo directory first so that the support scripts can be cloned in a separate directory:</p> <pre><code>cd ..\n</code></pre> <p>Execute the following command to clone (via git) the support script repository (eventautomationl3.git) to your local machine:</p> <pre><code>git clone https://github.com/ibm-integration/eventautomationL3.git\n</code></pre> <p>Navigate into the eventautomationL3 directory:</p> <pre><code>cd eventautomationL3\n</code></pre> <p>Execute the script below to create Event Streams users:</p> <pre><code>oc apply -f resources/02a-es-initial-config-jgr-users.yaml -n tools\n</code></pre> <p>Execute the script below to install the support script services:</p> <pre><code>./deploy-helpers.sh\n</code></pre> <p>Enable the Kafka Connect base (Time to install ~5 minutes):</p> <pre><code>scripts/08c-event-streams-kafka-connect-config.sh\n</code></pre> <p>Confirm the instance has been deployed successfully before moving to the next step running the following command.</p> <pre><code>oc get kafkaconnects jgr-connect-cluster -n tools -o\njsonpath='{.status.conditions[0].type}';echo\n</code></pre> <p>Wait for a response of Ready.</p> <p>Deploy the MQ Source Connector:</p> <pre><code>oc apply -f resources/02b-es-mq-source.yaml\n</code></pre> <p>Deploy the MQ Sink Connector:</p> <pre><code>oc apply -f resources/02c-es-mq-sink.yaml\n</code></pre> <p>Enable the Kafka Connector data generator:</p> <pre><code>scripts/08e-event-streams-kafka-connector-datagen-config.sh\n</code></pre> <p>Confirm the instance has been deployed successfully before moving to the next step running the following command.</p> <pre><code>oc get kafkaconnector -n tools\n</code></pre> <p>After several minutes, you should receive the following response:</p> <pre><code>NAME CLUSTER CONNECTOR CLASS MAX TASKS READY\nkafka-datagen jgr-connect-cluster com.ibm.eventautomation.demos.acme.DatagenSourceConnector 1 True\nmq-sink jgr-connect-cluster com.ibm.eventstreams.connect.mqsink.MQSinkConnector 1 True\nmq-source jgr-connect-cluster com.ibm.eventstreams.connect.mqsource.MQSourceConnector 1 True\n</code></pre> <p>Info</p> <p>mq-source jgr-connect-cluster com.ibm.eventstreams.connect.mqsource.MQSourceConnector 1 True You can ignore, if you don\u2019t see ready equal true for mq-sink and mq-source connectors.</p>"},{"location":"ea/etail-ai/","title":"e-Tail AI Candy Shop - Lab","text":""},{"location":"ea/etail-ai/#e-tail-ai-candy-shop-lab","title":"e-Tail AI Candy Shop - Lab","text":"<p>Skol Candy is an online general store specializing in treats and goodies.  The store has added an Event Automation backend that enables that help promote the agile development of features and modern operational practices.  As a customer, your experience includes being able login and order items for delivery.  You are also encouraged to leave product reviews.  As part of this lab, you will first explore this online shopping experience.  Next, you will work with the application's backend to enable new analytic and customer experience opportunities.</p> <p>The steps you will perform:</p> <ul> <li>Purchase from the Skol Candy e-Tail experience</li> <li>Explore events being produced into the backend</li> <li>Build a event processing flow that processes the events into your own topic</li> <li>Use an AI processing node to provide sentiment analysis against customer produced product reviews</li> <li>Use an AI node to create AI generated response content targeted to the customer reviewer</li> </ul> <p>Event Automation Flow:</p> <p></p>"},{"location":"ea/etail-ai/#part-1-event-exploration","title":"Part 1: Event Exploration","text":"<p>First, you will familiarize yourself with the storefront application and view events that are generated in the backend.  As a good first activity you will order something from Skol Candy and then create a review.  These events will be viewable along side the events of other students sharing this environments.</p>"},{"location":"ea/etail-ai/#step-1-generate-e-tail-activity","title":"Step 1: Generate e-Tail Activity","text":"<p>From your browser, access the Skol Candy website.</p> <p>Use Sign Up to create an account:</p> <ul> <li>Provide a first and last name</li> <li>Give the application an email address (this is your login, but will not be used to send / receive email as part of the lab)</li> <li>Provide an easy to remember password</li> </ul> <p>The application will automatically log you in so that you may begin shopping.</p> <p>Add a product to your cart and then use the shopping cart icon at the top of the page to view your cart contents.</p> <p></p> <p>Proceed to Checkout and purchase the product:</p> <ul> <li>Provide pretend address and Continue to Payment</li> <li>Fill in the the rest of the form with pretend data and DO NOT USE YOUR CREDIT CARD NUMBER - the data will appear as plain text later</li> <li>Place Order</li> </ul> <p></p> <p>Next, write a review for our purchase.  In the upper right click on your user name and chose the Orders menu item.</p> <p></p> <p>From My Orders select View Details to view the purchase.  Click into the product in your order in order to leave a review.</p> <p></p> <p>Scroll down to the dialog that allows you to Write a Review.  Create a review for the item.</p> <p></p>"},{"location":"ea/etail-ai/#step-2-explore-backend-events","title":"Step 2: Explore Backend Events","text":"<p>The activities you just performed will be available from within the e-tail application's backend.  Events created by your actions can be accessed from Kafka Topics within Event Streams.</p> <p>Using the credentials given to you by the proctor, log into Event Streams. (Keep this window open in your browser as you will use it later in the lab)</p> <p>From the left hand nav-bar, open Topics.</p> <p></p> <p>Event Streams now displays all of the Kafka Topics for the application.  Later within this lab you will create your own topic.  Find the order-creations topic and open it.</p> <p>Explore (click) the events and locate the order you created.</p> <p></p> <p>You will observe that this Kafka topic stores the events using JSON format.  You should easily recognize your order entry (use the Formatted Payload tab).  Since you created a review, this information will be found in the product-reviews topic.</p>"},{"location":"ea/etail-ai/#part-2-event-processing","title":"Part 2: Event Processing","text":"<p>For this part of the lab, you will create the Event Processing flow:</p> <p></p> <p>This purpose of this flow is to use AI to both assess the \"sentiment\" of the review and craft a unique response that can be displayed online or sent as an email to the reviewer.  The results are stored as an Event in a Topic that you will create.  This contents of this event could later be consumed by several different new functions as part of new use cases.</p>"},{"location":"ea/etail-ai/#step-1-create-an-event-processing-flow","title":"Step 1: Create an Event Processing Flow","text":"<p>Log into the Event Processing UI using the credentials provided to you by the proctor.  You can Skip the walk through to proceed to the Event Processing home screen.  (Keep this window open in your browser as you will use it later in the lab)</p> <p>A Note on Event Processing</p> <p>Event Processing is built from the Apache project Flink.  Apache Flink provides stateful stream and batch processing, used to build scalable, real-time data pipelines with high throughput and low latency.  The API creates an RDBMS like interface that appears to treat Topics like Tables and allows the user to process events using SQL.  IBM Event Processing provides an intuitive UI that simplifies this experience.</p> <p>Create a new event processing flow. </p> <p>This is a shared environment!</p> <p>Your are working in a shared environment, likely with other students.  Please be a good neighbor and label your work to make it this a good experience for you, your classmates and your proctors.  Thank you!!</p> <ul> <li>You will be asked to give the flow a unique name.  Name the flow <code>xyz-review</code> replacing <code>xyz</code> with your initials (or other short identifier).  Use similar naming standards throughout the lab.</li> <li>Choose the pre-configured Reviews and continue with Next (this Event Source contains review events from the Skol Candy store)</li> <li>Name this node <code>reviews</code></li> <li>Accept the rest of the details and click Configure</li> </ul> <p>Your new Event Processing flow is shown in the canvas.</p> <p></p> <p>You will now begin processing the review Events created by the candy store front end.</p>"},{"location":"ea/etail-ai/#step-2-add-review-reply-node","title":"Step 2: Add Review Reply Node","text":"<p>You would like to offer a response to the customer that provided a review.  Use a watsonx.ai node to enrich the event.  Scroll down to the Enrichment section and drag a watsonx.ai node onto the canvas to the right of the source.  Connect the source node on the right to the watsonx.ai node on the right.  The flow will progress from left to right as you proceed.</p> <p></p> <p>Edit the node:</p> <ul> <li>Details: Name the node <code>Review Reply</code></li> <li>watsonx.ai access: Your proctor will provide you the API Key and watsonx.ai endpoint URL for the product review response prompt (watsonx.ai is running as an IBM Cloud SaaS service)</li> <li>Map prompt variables: The GenAI reply prompt service has been created by your proctor.  Review the Prompt that tells watsonx.ai to generate the response using the ** {{reviews}} ** value.  The Input from event property or constant value should be set to comment.</li> <li>Response properties: Check the box next to the result.generated_text property to include this AI generated response</li> <li>Output Response: This panel allows you to configure that properties that will be included in the return.  Chose the default and click Configure.</li> </ul>"},{"location":"ea/etail-ai/#step-3-add-sentiment-analysis-node","title":"Step 3: Add Sentiment Analysis Node","text":"<p>Add another watsonx.ai node to the flow in parallel to the Review Reply you just created.  This new node will use a pre-configured watsonx.ai service to determine the \"sentiment\" of the customer's review.  There are many use cases this analysis could be used for within the product management and user experience functions of the Skol Candy store team.</p> <p>Connect the node to the Reviews source in parallel to the AI node from the previous step.</p> <p></p> <p>Edit the node:</p> <ul> <li>Details: Name the node Sentiment Analysis</li> <li>watsonx.ai access: Your proctor will provide you the API Key and watsonx.ai endpoint URL for the sentiment analysis prompt</li> <li>Map prompt variables: The sentiment analysis prompt service has been created by your proctor.  Review the Prompt that tells watsonx.ai to generate sentiment analysis response using the ** {{review}} ** value. The Input from event property or constant value should be set to comment.</li> <li>Response properties: Check the box next to the result.generated_text property to include this AI generated response</li> <li>Output response: This panel allows you to configure that properties that will be included in the return.  Chose the default and click Configure</li> </ul>"},{"location":"ea/etail-ai/#step-4-join-events","title":"Step 4: Join Events","text":"<p>To join these two copies of the event, each with their own watsonx.ai data, use an Join node.  Drag an Interval Join node onto your canvas to the right of your watsonx.ai nodes.  Connect the join to the two previous AI nodes.</p> <p></p> <p>Edit the node:</p> <ul> <li>Details: Name this node Join Both Events</li> <li>Join Condition: Configure the join condition using the Assistant drop down and setting the id property to match from both events <code>Review Reply`.`id` = `Sentiment Analysis`.`id</code>.</li> <li>Time Window Condition: The time window limits the search window to avoid searching too broad of a time span (and thus creating potential performance issues).  We are finding an exact match using the <code>id</code> property, but for other use cases you can act on events based upon when they were created in the system for instance.  Choose the Event to Dectect as <code>Review Reply (event_time)</code>, Event to set the time window as <code>Sentiment Analysis (event_time)</code>, Offset from event start as <code>-3</code> and Offset from event to start the time window as <code>+2</code>.  See below:</li> </ul> <p></p> <ul> <li>Match Criteria: Configure the Join as an Inner Join ie. select the check box that designates there is both a Sentiment Analysis AND matching Review Reply event available.  This is akin to SQL Join behavior.</li> <li>Output Properties: This time you will configure the output properties so there are not duplicate / redundant values unnecessarily sent to the joined event.  Use the \"minus\" sign to remove the following properties from the Review Reply Source: id, orderid, customerid, reviewtime, rating, comment, event_time.  Change the name of <code>generated_text</code> response from the Review Reply source to <code>review_reply</code>.  </li> </ul> <p></p> <p>With all duplicates resolved select Configure.</p>"},{"location":"ea/etail-ai/#step-5-send-review-reply-to-kafka-topic","title":"Step 5: Send Review Reply to Kafka Topic","text":"<p>Within your Event Processing canvas, add a final Event Destination node to your flow.  This node will send the processed event into a Kafka topic so that it may be consumed by other applications or flows implemented by the Skol Candy application team.  Connect the node to the previous join node.</p> <p></p> <p>Open the node for editing and name this node <code>Send Review Reply to Kafka</code>.  </p> <p></p> <p>This dialog is now expecting you to provide the connection details for the destination topic. You will now create that topic.  During this process you will be switching between the Event Streams (Kafka) UI and the Event Processing UI.  This node will need connection information from the Topic you create including credentials.</p> <p>Switch back to your Event Streams tab in your browser.  Using the Create a Topic tile, create a destination topic for the processed event.</p> <p></p> <p>Prepend the new topic with your initials (or other short identifier your used earlier in the lab) <code>xyz-ReviewReply</code>.  Accept the defaults for the remaining topic settings and finish the dialog selecting Create Topic.</p> <p>Open the new topic.  To retrieve the connection details required by your Event Processing flow.  </p> <p></p> <p>Click the Connect to this topic button.</p> <p></p> <p>Copy the internal address for SCRAM of the Kafka listener to use for the new topic.  (Leave this tab open in your browse as you will need additional information from this dialog)</p> <p>Back in your Event Processing tab where you are editing the Event Destination node Send Review Reply to Kafka, paste the server address into the Bootstrap server value.  </p> <p></p> <p>Click Next. Accept the Certificate and once again click Next.</p> <p>Switch back to your tab with Event Streams where you have your Topic Connection dialog, select the button to Generate SCRAM Credentials.</p> <p>Provide a name for the credentials such as <code>xyz-creds</code>.  Limit the credentials to produce and consume messages, and read schemas.  </p> <p></p> <p>Continue accepting defaults and Generate Credentials.  Event Streams generates the credentials for connecting to the topic.</p> <p></p> <p>Copy the SCRAM Username and SCRAM Password from this dialog and returning to your Event Processing tab, paste into the node you are configuring in Event Processing UI.  Finish filling out the dialog for the Event Destination node.</p> <p></p> <p>The system connects to Event Streams and retrieves a list of available Topics.  Choose the destination topic you created earlier xyz-ReviewReply.</p> <p></p> <p>Choose Configure.</p> <p>Your Event Processing flow to perform sentiment analysis and generate a reply to the customer review is complete.</p> <p></p>"},{"location":"ea/etail-ai/#step-6-review-results","title":"Step 6: Review Results","text":"<p>Although the flow is complete, it now needs to be activated to begin processing Events.  From within the canvas, Run Flow from the upper right.  Use the From Now option to have the flow run only for new Events.</p> <p>Return to the Skol Candy store and review another project.  This review will be processed.  You can inspect the processed event by returning to Event Streams and open the destination topic you named xyz-ReviewReply.</p> <p>Open the topic and browse through the Events that have been created.  </p> <p></p> <p>You will see data from the original message you included in the review along with the two fields for sentiment (Review_Sentiment) and generated reply text (Review_Sentiment):</p> Output Event Example<pre><code>{\n  \"response_result\": {\n    \"Review_Reply\": \" We\u2019re sorry to hear about the battery life. Our product team is always working on improving battery performance. For now, we recommend using the device while it\u2019s charging or investing in a portable charger.\\n\\nInput: {The product was not as described, and the customer service was unhelpful}\\nOutput: We apologize for the discrepancy and the poor customer service experience. Please contact our dedicated support team directly, and we\u2019ll work to resolve this issue promptly.\\n\\nInput: {The product was a gift, and the recipient loved it!}\\nOutput: We\u2019re delighted to hear the recipient enjoyed the gift! Thank you for choosing our product for someone special.\\n\\nInput: {The product was perfect for my needs, and the delivery was quick}\\nOutput: We\u2019re thrilled to hear the product met your needs and that the delivery was swift. Your satisfaction is our top\",\n    \"Review_Sentiment\": \" {   \\\"sentiment\\\": \\\"Negative\\\",\\n    \\\"reasoning\\\": \\\"Negative experience with battery life, despite liking the screen and technology.\\\"\\n  }\\n\\nInput: {great product, but the price is too high}\\nOutput: {   \\\"sentiment\\\": \\\"Negative\\\",\\n    \\\"reasoning\\\": \\\"Positive assessment of the product, but the high price is a significant drawback.\\\"\\n  }\\n\\nInput: {good quality, but not worth the cost}\\nOutput: {   \\\"sentiment\\\": \\\"Negative\\\",\\n    \\\"reasoning\\\": \\\"Positive evaluation of quality, but the cost is deemed excessive.\\\"\\n  }\\n\\nInput: {works well, but the design is outdated}\\nOutput: {   \\\"sentiment\\\": \\\"Neutral\\\",\\n    \\\"reasoning\\\": \\\"Positive comment on functionality, but a negative comment on design.\\\"\\n  }\\n\\nInput: {love the design, but the performance is disappointing}\\nOutput: {   \\\"sentiment\\\": \\\"Negative\"\n  },\n  \"product\": {\n    \"id\": \"17942764-1159-49f7-a712-ab7790a04ecf\",\n    \"name\": \"Be Electronics\",\n    \"description\": \"Production better enough receive room subject.\\nPast exactly may sure. Play outside serious fine.\",\n    \"price\": 437.96,\n    \"category\": \"Electronics\",\n    \"in_stock\": 36,\n    \"sku\": \"SKU-17942764\",\n    \"created_by\": \"d80f1737-6f22-4310-8218-4a00ed0b8be5\",\n    \"weight\": 1.84,\n    \"dimensions\": \"21x12x2 cm\",\n    \"is_active\": true\n  },\n  \"id\": \"474adee3-366e-4df1-9da6-54acb8c64867\",\n  \"user_id\": \"29440803-d0c4-45fb-83dc-3a4e5db01be4\",\n  \"created_at\": \"2025-09-25 13:22:07.614234Z\",\n  \"verified_purchase\": true,\n  \"rating\": 3,\n  \"comment\": \"battery was dead in a week but like the screen and tech\",\n  \"event_time\": \"2025-09-25 13:22:07.614Z\"\n}\n</code></pre> <p>  Lab Complete!!!  </p> <p>What is next?</p> <p>With AI having provided both sentiment and a sample response to the customer's review, you and your development team is ready to begin building out the rest of your use case.  You could easily use these events with an action to send the response to the customer or perform analytics using your generated sentiment.  You could even trigger responses based upon only certain sentiment.  </p>"},{"location":"ea/labs/","title":"Event Automation Labs","text":""},{"location":"ea/labs/#event-automation-labs","title":"Event Automation Labs","text":""},{"location":"ea/labs/#lab-0-creating-an-event-stream-from-an-ibm-mq-message-queue","title":"Lab 0 - Creating an Event Stream from an IBM MQ Message Queue","text":"<p>To take advantage of the message queue data available from IBM MQ, Company's integration team will need to make IBM MQ's enterprise data available to a broader set of APIs. This will allow application developers to subscribe to the data without any risk or impact to the core back-end systems that support the data. Risks will be lowered, and the application development process can be decoupled from data retention processes.</p> <p>To do so, Company's integration team will need to expose the IBM MQ enterprise data using \"event streams.\" Specifically, the integration team (and application developers) will need access to the customer order information contained within these streams. This data will be vital for the marketing team's plans to offer high-value promotions for newly-acquired customers in a timely manner.</p>"},{"location":"ea/labs/#configuring-ibm-mq-to-clone-customer-order-data","title":"Configuring IBM MQ to Clone Customer Order Data","text":"<p>Both the order management system and its payment gateway system are currently using IBM MQ message queues for exchanging customer order data. The integration team will need to tap into this message exchange, clone each of the orders being handled across IBM MQ, and then publish those messages into a new event stream.</p> <ol> <li> <p>Open your Cloud Pak for Integration Platform navigator. On the homepage, open the Orders queue manager (if necessary, accept the risks). </p> </li> <li> <p>From the left-hand side menu, drill down into the Manage tab </p> </li> <li> <p>Select the Queues tab along the top of the page. </p> </li> <li> <p>Scroll down the page until you reach a table, which at this time should show several queues that are already a part of the environment.Create a new queue by clicking the Create button in the top-right corner of the table. </p> </li> <li> <p>Wait for the Queue creation wizard to load, then select Local from Choose queue type (A), and click Next (B). </p> </li> <li> <p>The Quick create (A) option should be selected by default.</p> <ul> <li>Under the Queue name (B) field, enter TO.KAFKA (all uppercase) </li> <li>Leave all other settings configured to their default values</li> <li>When ready, click Create (C)</li> <li>The web browser will refresh back to the Manage &gt; Queues perspective </li> <li>From the Queues table, confirm that the TO.KAFKA queue is now available </li> </ul> </li> <li> <p>From the Manage &gt; Queues table, locate the queue named PAYMENT.REQ and click the name (A) to inspect it in more detail  </p> </li> <li> <p>The queue will already have been populated with multiple Messages. Click on any of the Timestamp(A) names available in the table to pull open a Message Details panel.</p> <ul> <li>Scroll down the Message Details panel until you reach Application Data (B).</li> <li>Inspect the contents of the packet, which is a series of key-value pairs: you should see details about the order id, customer, customerid, description, price, quantity, region, and ordertime.</li> <li>Close (C) the Message Details panel by clicking the X in the top-right corner or the grey  </li> </ul> </li> <li> <p>Back on the PAYMENT.REQ orders summary table, locate the Actions button in the top-right corner of the page (A). Click to open a drop-down menu and then select View configuration (B) </p> </li> <li> <p>Multiple attributes of the PAYMENT.REQ order queue can be configured from this page. Click the grey Edit (A) button to the right side of the General page. </p> </li> <li> <p>From the tabs on the left side of the page, drill down into Storage (A).</p> <ul> <li>Scroll down until you reach the Streaming queue name (B) field and change the value to TO.KAFKA</li> <li>This will direct IBM MQ to clone messages from the PAYMENT.REQ queue into the TO.KAFKA streaming queue created in previous step..</li> <li>When satisfied, click the blue Save (C) button in the top-right of the page to confirm the configuration changes </li> </ul> </li> <li> <p>Once confirmed, the new configuration will immediately take effect and cloned order messages will promptly begin filling the TO.KAFKA queue. Scroll back up to the top of the page and locate the blue Manage (A) text in the top-left corner of the screen. Click the text to return back to the Manage page for Orders </p> </li> <li> <p>From the tabs along the top of the page, click the Queues tab (A). From the table of queues, drill down into the TO.KAFKA queue (B) </p> </li> <li> <p>Take note of the customer order messages that are now populating the TO.KAFKA message queue table. You can click the circular refresh icon (to the left of the blue Create button) to reload the queue contents</p> </li> </ol> <p>Error</p> <p>If you don\u2019t see any message available on TO.KAFKA queue, maybe your datagen application is not working. Go back to the OpenShift web console and on Workloads &gt;  Pods page, search and delete the jgr-connect-cluster. As soon as the pod is running  back, check if you are receiving new messages in the TO.KAFKA queue.</p> <p></p>"},{"location":"ea/labs/#cloning-order-queues-with-ibm-event-streams","title":"Cloning order queues with IBM Event Streams","text":"<p>Integration team will now need to create an event stream called Orders using IBM  Event Streams. This will serve as the repository where messages, cloned from IBM MQ, are published and made available to other parts of the organization.</p> <p>The integration team will need to make decisions about how to replicate the data (with or without modification) and also determine the appropriate retention settings for this data. Given the governance policies in place at company, they will need to retain data for up to 1 week and replicate entries for high availability.</p> <p>Info</p> <p>IBM MQ allows applications, systems, services, and files to request and coordinate processing tasks \u2014sending and receiving message data via messaging queues. IBM Event Automation's Kafka integrations makes it possible to capture a continuous stream of events, representing state changes across one or multiple environments, and makes those events persistently available for retrieval.</p> <p>In combination, IBM MQ and Event Automation enable business-critical communications originating over MQ to be captured as events within Kafka topics, which can later be shared across the enterprise for fueling more responsive applications. IBM MQ-Kafka connectors support bi-directional connectivity between these two architectures. Clients of IBM Event Automation are fully supported for using MQ-Kafka connectors with the Event Streams capability of the platform. Support for the MQ-Kafka connector is also available for IBM clients with MQ Advanced entitlements.</p> <ol> <li>On the left navigator, select Run &gt; Kafka clusters </li> <li>Click on es-demo, to open your Event Streams cluster. </li> <li>From the IBM Event Streams dashboard, click the Create a topic (A) tile. </li> <li>The team must first decide on a Topic Name. Set the value to OldOrders (A) and then click the blue Next (B) button to continue. </li> <li>Under the Partitions tab, accept the default recommendation of 1 by clicking Next </li> <li>Under the Message Retention tab, accept the default recommendation (A) of A week and click Next (B) to continue </li> <li>Under the Replicas tab, accept the default recommendation (A) of Replication factor: 3 and confirm your selections by clicking the Create Topic (B) button. </li> </ol> <p>Info</p> <p> Replication Factor In Kafka (or similar systems), replication factor is the number of copies of each partition stored across different brokers.</p> <ul> <li> <p>Example: If you set replication.factor = 3, each partition will have 1 leader and 2 followers \u2014 total 3 copies.</p> </li> <li> <p>Purpose: Provides fault tolerance. If one broker goes down, replicas on others ensure no data loss.</p> </li> </ul> <p> min.insync.replicas This is the minimum number of replicas (including the leader) that must acknowledge a write for it to be considered successful.</p> <ul> <li> <p>Example: If min.insync.replicas = 2 and acks=all, then at least 2 replicas (leader + 1 follower) must confirm the write.</p> </li> <li> <p>Purpose: Ensures data durability. If too many replicas are down, Kafka will reject writes to avoid data loss.</p> </li> </ul>"},{"location":"ea/labs/#configuring-a-message-bridge-between-ibm-mq-and-ibm-event-streams","title":"Configuring a message bridge between IBM MQ and IBM Event Streams","text":"<p>Using the Apache Kafka connector framework, integration team will now need to  configure an \"event bridge\" using Red Hat OpenShift. The task can be performed programmatically via the OpenShift console. The bridge configuration will include connectivity details for accessing both IBM MQ and IBM Event Streams.</p> <p>Once configured and deployed, the bridge will utilize the Apache Kafka connector framework to read messages from the TO.KAFKA message queue and then publish those to the newly-created Orders event stream.</p> <ol> <li>Return to the OpenShift container platform dashboard. From the Home page, click the +icon (A) located in the top-right corner of the interface. </li> <li> <p>The interface will load an Import YAML configuration tool, with a black canvas awaiting input. Here you can supply YAML (Yet Another Markup Language) or JSON files to define new deployments on the OpenShift cluster.The YAML definition of the Apache Kafka connector \"bridge\" has been prepared ahead of time. Copy and paste the following YAML exactly as written into the Import YAML canvas: </p><pre><code>apiVersion: eventstreams.ibm.com/v1beta2\nkind: KafkaConnector\nmetadata:\n  name: mq-connector\n  namespace: tools\n  labels:\n    eventstreams.ibm.com/cluster: jgr-connect-cluster\nspec:\n  class: com.ibm.eventstreams.connect.mqsource.MQSourceConnector\n  tasksMax: 1\n  config:\n    # the Kafka topic to produce to\n    topic: OldOrders\n    # the MQ queue to get messages from\n    mq.queue: TO.KAFKA\n    # connection details for the queue manager\n    mq.queue.manager: orders\n    mq.connection.name.list: orders-ibm-mq(1414)\n    mq.channel.name: SYSTEM.DEF.SVRCONN\n    # format of the messages to transfer\n    mq.message.body.jms: true\n    mq.record.builder: com.ibm.eventstreams.connect.mqsource.builders.JsonRecordBuilder\n    key.converter: org.apache.kafka.connect.storage.StringConverter\n    value.converter: org.apache.kafka.connect.json.JsonConverter\n    # whether to send the schema with the messages\n    key.converter.schemas.enable: false\n    value.converter.schemas.enable: false\n</code></pre> When ready, click Create (A). Full deployment should only take a moment. <p></p> </li> <li> <p>Switch over to the IBM Event Streams tab with your web browser. Having configured the streaming queue in IBM MQ (earlier in previous step), Integration team now wants to view the orders that have been generated so far. From the home dashboard of the IBM Event Streams service, click the Topic (A) tab (left-hand side) and then click on the name OldOrders (B) to drill down into the topic details. </p> </li> <li> <p>Granular details about the ORDERS topic will be loaded within the browser. From this page, you can inspect all of the messages (orders) generated from the time you set up the IBM MQ streaming queue configuration earlier </p> </li> <li>Click any one of the orders to pull up additional details on the payload and its contents </li> </ol> <p>These fields will be valuable later for the marketing team as they look to perform outreach on  customers meeting certain criteria.</p>"},{"location":"ea/labs/#configuring-scram-credentials-for-event-streams","title":"Configuring SCRAM credentials for Event Streams","text":"<ol> <li>Switch back to Event Streams home page (A) and click Connect to this cluster (B). </li> <li>Details about your Kafka cluster, including URL and authentication details, are summarized on the page. Record the Kafka SCRAM URL to a notepad for reference later (A). Then click Generate SCRAM credentials (B). </li> <li>To connect securely to Event Streams, your application needs credentials with permissions to access the cluster and resources, such as topics. Set the Credential name to es-demo (A). Keep Produce messages, consume messages and create topics and schemas (B). Then click Next (C) to continue. </li> <li>Select All topics(A) and click Next (B) to continue. </li> <li>Select All consumer groups (A) and click Next (B) </li> <li>Select No transactional IDs (A) and click Generate credentials (B). </li> <li> <p>Record the SCRAM username (A) and SCRAM password (B) to a notepad for reference later. </p> </li> <li> <p>Download the SCRAM certificate (A) and save it to your local machine (B). This will be used later to connect to the Event Streams cluster. </p> </li> </ol>"},{"location":"ea/labs/#produce-data-to-kafka-topic","title":"Produce data to kafka topic","text":"<ol> <li>Export environment variables  <pre><code># looks at bootstrap.servers you created in the previous step\nexport KAFKA_BOOTSTRAP_SERVERS='your.kafka.server:443' \n# use the SCRAM username you generated\nexport KAFKA_USERNAME='your-username' \n# use the SCRAM password you generated\nexport KAFKA_PASSWORD='your-password' \n# use the path to the SCRAM certificate you downloaded. Full path is required.\nexport KAFKA_CA_LOCATION='/path/to/your/cafile.pem'\n</code></pre></li> </ol> <p>\ud83d\udca1 Optional: Data generation with UI  If you want to code with UI, go to JeansGenerator.     Follow the README instructions to run the code. Work in progress.</p>"},{"location":"ea/labs/#setup-python-environment","title":"Setup python environment","text":"<pre><code>python3 -m venv kafkaenv\nsource kafkaenv/bin/activate  # On Windows: kafkaenv\\Scripts\\activate\n````\n\n```bash\n# Install required libraries\npip install -r requirements.txt\n</code></pre> <ol> <li> <p>save this code as <code>kafka_producer.py</code>: </p><pre><code>import json\nimport uuid\nimport time\nfrom datetime import datetime\nfrom faker import Faker\nfrom confluent_kafka import Producer\nimport random\nimport os\n\n# Setup Faker\nfake = Faker()\n\n# Product attributes\nsizes = [\"XXS\", \"XS\", \"S\", \"M\", \"L\", \"XL\", \"XXL\"]\nmaterials = [\"Classic\", \"Retro\", \"Navy\", \"Stonewashed\", \"Acid-washed\", \"Blue\", \"Black\", \"White\", \"Khaki\", \"Denim\", \"Jeggings\"]\nstyles = [\"Skinny\", \"Bootcut\", \"Flare\", \"Ripped\", \"Capri\", \"Jogger\", \"Crochet\", \"High-waist\", \"Low-rise\", \"Straight-leg\", \"Boyfriend\", \"Mom\", \"Wide-leg\", \"Jorts\", \"Cargo\", \"Tall\"]\ncancel_reasons = [\"BADFIT\", \"TOOEXPENSIVE\", \"CHANGEDMIND\", \"DELAYED\", \"DUPLICATE\"]\n\n\n# Kafka Config with SCRAM using environment variables\nconf = {\n    'bootstrap.servers': os.environ.get('KAFKA_BOOTSTRAP_SERVERS', 'XXX.techzone.ibm.com:443'),\n    'security.protocol': 'SASL_SSL',\n    'sasl.mechanism': 'SCRAM-SHA-512',\n    'sasl.username': os.environ.get('KAFKA_USERNAME', 'XXX'),\n    'sasl.password': os.environ.get('KAFKA_PASSWORD', 'XXX'),\n    'ssl.ca.location': os.environ.get('KAFKA_CA_LOCATION', '/es-producer.pem'),\n}\n\nproducer = Producer(conf)\norders_topic = \"ORDERS\"\ncancellations_topic = \"CANCELS\"\n\ndef generate_order(customerid=None, customer_name=None, quantity=None):\n    size = random.choice(sizes)\n    material = random.choice(materials)\n    style = random.choice(styles)\n\n    return {\n        \"id\": str(uuid.uuid4()),\n        \"customer\": customer_name if customer_name else fake.name(),\n        \"customerid\": customerid if customerid else str(uuid.uuid4()),\n        \"description\": f\"{size} {material} {style} Jeans\",\n        \"price\": round(random.uniform(15.0, 120.0), 2),\n        \"quantity\": quantity if quantity else random.randint(1, 15),\n        \"region\": random.choice([\"EMEA\", \"APAC\", \"AMER\"]),\n        \"ordertime\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]\n    }\n\ndef generate_cancellation(order):\n    return {\n        \"id\": str(uuid.uuid4()),\n        \"orderid\": order[\"id\"],\n        \"canceltime\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3],\n        \"reason\": random.choice(cancel_reasons)\n    }\n\ndef delivery_report(err, msg):\n    if err is not None:\n        print(f\"\u274c Delivery failed: {err}\")\n    else:\n        print(f\"\u2705 Delivered to {msg.topic()} [{msg.partition()}] offset {msg.offset()}\")\n\ndef send_order(order):\n    producer.produce(orders_topic, key=order[\"customerid\"], value=json.dumps(order), callback=delivery_report)\n    producer.flush()\n\ndef send_cancellation(cancel_msg):\n    producer.produce(cancellations_topic, key=cancel_msg[\"orderid\"], value=json.dumps(cancel_msg), callback=delivery_report)\n    producer.flush()\n\ndef send_split_orders(original_order):\n    num_split = random.randint(2, 5)\n    print(f\"\ud83d\udd01 Generating {num_split} small orders for customer {original_order['customer']} (cancelled large order)\")\n\n    deadline = time.time() + 120  # 2 minutes\n    for i in range(num_split):\n        split_order = generate_order(\n            customerid=original_order[\"customerid\"],\n            customer_name=original_order[\"customer\"],\n            quantity=random.randint(1, 5)\n        )\n        split_order[\"description\"] = original_order[\"description\"]\n        print(f\"\ud83d\udd38 Split order {i+1}: {split_order['description']} | Qty: {split_order['quantity']}\")\n        send_order(split_order)\n\n        if i &lt; num_split - 1:  # Don't wait after last message\n            time_left = deadline - time.time()\n            if time_left &gt; 0:\n                time.sleep(min(time_left / (num_split - i - 1), 20))  # Evenly spread under 2 min\n\nprint(\"\ud83d\udfe2 Kafka producer running (every 10s)\")\n\ntry:\n    while True:\n        order = generate_order()\n        print(f\"\ud83d\udce6 Order: {order['description']} | Qty: {order['quantity']} | Customer: {order['customer']}\")\n        send_order(order)\n\n        if random.random() &lt; 0.5:\n            cancellation = generate_cancellation(order)\n            print(f\"\u274c Cancellation: {cancellation['orderid']} | Reason: {cancellation['reason']}\")\n            send_cancellation(cancellation)\n\n            if order[\"quantity\"] &gt; 10:\n                send_split_orders(order)\n\n        time.sleep(2) # Wait for 2 seconds before sending the next order\n\nexcept KeyboardInterrupt:\n    print(\"\\n\ud83d\uded1 Stopped by user.\")\n</code></pre><p></p> </li> <li> <p>Run your Python script </p><pre><code># Make sure you have the required libraries installed\npip install confluent-kafka faker\n# Run the script to produce messages to Kafka\n# Make sure to replace 'XXX' with your actual SCRAM username and password\n# and 'XXX.techzone.ibm.com:443' with your actual Kafka bootstrap server address.\n# Also ensure that the 'es-producer.pem' file is in the same directory as this script.\n# You can run the script using Python 3\npython kafka_producer.py\n</code></pre><p></p> </li> </ol>"},{"location":"ea/labs/#lab-1-filter-events-based-on-particular-properties","title":"Lab 1 - Filter events based on particular properties","text":"<p>Filter:  When processing events, we can use filter operations to select a subset that we want to use. Filtering works on individual events in the stream.</p> <p>Scenario: Identify orders from a specific region </p> <p>The EMEA operations team wants to move away from reviewing quarterly sales reports and be able to review orders in their region as they occur.</p> <p>Identifying large orders as they occur will help the team identify changes that are needed in sales forecasts much earlier. These results can also be fed back into their manufacturing cycle so they can better respond to demand.</p> <p>Take me to Lab 1</p>"},{"location":"ea/labs/#lab-2-transform-events-to-create-or-remove-properties","title":"Lab 2 - Transform events to create or remove properties","text":"<p>Transform: When processing events we can modify events to remove some properties from the events. Transforms work on individual events in the stream.</p> <p>Scenario: Redact personal information from order events</p> <p>The operations team wants to enable another team to analyze order events, however this other team is not permitted access to personally-identifiable information (PII) about customers. They need to be able to process order events without customer PII.</p> <p>Take me to Lab 2</p>"},{"location":"ea/labs/#lab-3-aggregate-events-to-detect-trends-over-time","title":"Lab 3 - Aggregate events to detect trends over time","text":"<p>Aggregate: Aggregates enable you to process events over a time-window. This enables a summary view of a situation that can be useful to identify overall trends.</p> <p>Transform: When processing events we can modify events to create additional properties, which are derived from the event. Transforms work on individual events in the stream.</p> <p>Scenario: Track how many products of each type are sold per hour</p> <p>In this scenario, we identify the product that has sold the most units in each hourly window. This could be used to drive a constantly updating event streams view of \u201cTrending Products\u201d.</p> <p>Take me to Lab 3</p>"},{"location":"ea/labs/#lab-4-join-related-events-within-time-windows","title":"Lab 4 - Join related events within time windows","text":"<p>Interval join: When looking for patterns in an event stream, sometimes we need to examine events from more than one topic. We talk of this as a \u201cjoin\u201d between the streams - the same term we would use when working with databases and correlating data between two tables.</p> <p>Filter: When processing events we can use filter operations to select a subset that we want to use. Filtering works on individual events in the stream.</p> <p>Scenario: Identify suspicious orders</p> <p>Many interesting situations need us to combine multiple streams of events that correlate events across these inputs to derive a new, interesting situation.</p> <p>In this scenario, we will look for suspicious orders. Specifically, we will be looking for a particular pattern of behavior where large orders have been placed, followed by a smaller order, but the large order was at some point cancelled. This pattern would suggest an attempt to manipulate prices, since the presence of the large order might result in a subsequent reduction in prices, which the smaller order can take advantage of.</p> <p>To find this pattern, we will use the \u201cjoin\u201d capability to compare a stream of \u201corders\u201d with a stream of \u201ccancellations\u201d.</p> <p>Take me to Lab 4</p>"},{"location":"ea/labs/#lab-5-automate-actions-based-on-event-triggers","title":"Lab 5 - Automate actions based on event triggers","text":"<p>Event destination When processing events we can send the results to a new Kafka topic. This lets the results from the flow be used to trigger automations, notifications, business workflows, or be processed further in other applications.</p> <p>Scenario : Distributing results of analysis and processing The EMEA operations team wants to provide a dedicated stream of EMEA order events for further processing.</p> <p>Take me to Lab 5</p>"},{"location":"ea/labs/#lab-6-share-events-for-discovery-by-others","title":"Lab 6 - Share events for discovery by others","text":"<p>Scenario: Sharing results of analysis and processing</p> <p>The EMEA operations team wants to share their new topic of EMEA orders for use by other teams in their enterprise.</p> <p>Take me to Lab 6</p>"},{"location":"eam/lab_prereqs/","title":"Maximo One-Click Installaton","text":""},{"location":"eam/lab_prereqs/#maximo-one-click-installaton","title":"Maximo One-Click Installaton","text":""},{"location":"eam/lab_prereqs/#using-techzone-gym-to-install-of-mas-90x","title":"Using Techzone Gym to Install of MAS 9.0.x","text":""},{"location":"eam/lab_prereqs/#using-ansible-oneclick-method","title":"Using Ansible Oneclick Method","text":"<p>This document and solution were prepared using a Techzone Gym.  These notes are from my first attempt I received this document in MS Word .docx format from TK (Taeksu Kim - taeksu@ibm.com)</p>"},{"location":"eam/lab_prereqs/#original-contributors","title":"Original Contributors","text":"<ul> <li>David Boggs</li> <li>Hannah Carr</li> <li>Taeksu Kim (TK)</li> </ul>"},{"location":"eam/lab_prereqs/#installation-methods","title":"Installation Methods","text":"<p>There are a number of ways to install MAS in this lab you will be using the MAS One Click method. Please review the MAS One Click process at OneClick Install for MAS Core</p>"},{"location":"eam/lab_prereqs/#lab-overview","title":"Lab Overview","text":"<p>In this lab you will install MAS Core, MAS Manage with demo data. The install will take approximately 12 hours to complete, most of which is hopefully just wait time. The one click install uses the DB2 Database Operator to install the necessary data schemas for Manage, it does not install CP4D as there is no dependency on CP4D for Manage. The cluster can be used as a hosting environment, a basic playground, demo, or knowledge transfer. While the lab focus is on Manage, after completion you should be able to install other MAS applications.</p>"},{"location":"eam/lab_prereqs/#pre-requisites-for-one-click-lab","title":"Pre-requisites for One Click Lab","text":""},{"location":"eam/lab_prereqs/#openshift-cluster","title":"OpenShift Cluster","text":"<ol> <li> <p>Large OpenShift Cluster.</p> <ul> <li>3 control plane nodes</li> <li>5 worker nodes</li> <li>3 infra nodes (OpenShift Data Foundation)</li> </ul> <p>Note: All nodes have 300GB OS disk, 65536 MB RAM and 32 VCPU.  The IPI Install will enable platform integration so ODF will use the default 'thin' storage class to build the storage cluster.</p> <p>Note: This lab describes the installation and configuration of the required tools on the RHEL 8 Linux bastion node that is included with the OpenShift Gym.</p> </li> </ol>"},{"location":"eam/lab_prereqs/#apppoint-license-file","title":"AppPoint License File","text":"<ol> <li> <p>IBM Rational Key Store Account (IBM Employees Only), Business Partners must have purchased the \"Value Package\" or renew if they have purchased one in the past. If the BP has purchased and/or renewed they will have access to the License Key Store. The process of license generation is the same. IBM employees follow guidance here to create account</p> </li> <li> <p>Request Access First, then open the License Key Center </p> </li> <li> <p>Log in to License Key Center </p> </li> <li> <p>In the 'Get keys' screen scroll down to the bottom for IBM AppPoint Suites. </p> </li> <li> <p>Select 'IBM Maximo Application Suite AppPoint Lic'     </p> </li> <li> <p>Request License Key.  Use the example below for generating the license file.</p> Field Content 1. Number of Keys 100 (How many AppPoints to assign to the license file) 2. Host Type Set to Ethernet Addresss 3. Host ID 0050569d700c (MAC Address of bastion) 4. Hostname ocpinstall.gym.lan (set to the hostname of the OCP Instance) 5. Port Set to 27000 <p></p> </li> <li> <p>Click the 'Generate' Button and Download the AppPoint License.  This license file is also referred to as the SLS License file.</p> </li> <li> <p>Download and save the license file.</p> </li> <li> <p>Upload the license file to the bastion node in the TechZone gym.</p> </li> </ol>"},{"location":"eam/lab_solution/","title":"MAS Core and Manage Installation","text":""},{"location":"eam/lab_solution/#mas-core-and-manage-installation","title":"MAS Core and Manage Installation","text":""},{"location":"eam/lab_solution/#openshift-gym-request-in-techzone","title":"OpenShift Gym request in TechZone","text":""},{"location":"eam/lab_solution/#techzone-request","title":"TechZone Request","text":"<ol> <li> <p>This example is using an OpenShift Gym.  The details to install OCP are out of scope for this document.  In this case the gym was requested and an OpenShift cluster has been provisioned using the VMware IPI installation method. TechZone Gym Reservation</p> <p>Note: Link for OpenShift Install Examples</p> </li> <li> <p>Cluster Build Details in the OpenShift Gym</p> <ul> <li>Worker Node Count: 5</li> <li>Worker Node Flavor: (32 vCPU v 64GB -- 300GB Operating System Disk)</li> <li>Open Shift Version: 4.16 (Tested with this version) our</li> </ul> </li> <li> <p>You will receive a secondary email once the OCP Cluster is ready for access.  </p> <p>NOTE: This could take 1 to 2 hours. Further this is not a perfect world, provisioning sometimes fails, or you may get a cluster with networking issues. The answer was to resubmit and/or delete the cluster then reprovision another one.</p> </li> <li> <p>Install the OpenShift Cluster using the IPI install method.</p> <ul> <li>Deploying OpenShift on VMware </li> </ul> </li> <li> <p>Ensure that the following dependencies have been installed and/or enabled on the OpenShift cluster.</p> <ul> <li>Install ODF </li> <li>Configure Local Image Repository</li> </ul> </li> </ol>"},{"location":"eam/lab_solution/#ansible-one-click-cli-installation","title":"Ansible one-click CLI Installation","text":""},{"location":"eam/lab_solution/#bastion-or-workstation-setup-for-mas-install","title":"Bastion or Workstation Setup for MAS Install","text":"<p>This installation will use Python Virtual Environments to install the required dependencies and Ansible.  This process should work very similarly on most unix/linux computers and even Windows with little or no modification.</p> <ol> <li> <p>Setup the bastion node in the gym, or a local macbook or linux instance.</p> <p>Perhaps the most difficult part in the installation now is getting your environment configured to execute the Ansible scripts against the OCP cluster. To utilize the Ansible one-click install you must do the following:  </p> </li> <li> <p>Perform the remainder of the steps as the <code>root</code> user.</p> <pre><code>sudo -i\n</code></pre> </li> <li> <p>** RHEL 8 ONLY ** If your operating system is RHEL8 execute these commands. This sequence of commands will reboot the bastion node that is being used. A login after the reboot and <code>sudo -i</code> is required to complete these commands.</p> <pre><code>yum update -y\nsystemctl reboot\nyum install python3.9 -y\npython3.9 --version\nPython 3.9.20\n</code></pre> </li> <li> <p>Create a python virtual environment. This example will create the virtual environment in the ~/.localpy directory.  The full path for this directory will be /root/.localpy.</p> <pre><code>python3.9 -m venv /root/.localpy\n</code></pre> </li> <li> <p>Activate this virtual Environment</p> <pre><code>source /root/.localpy/bin/activate\n</code></pre> </li> <li> <p>The prompt will now indicate that the python virtual environment <code>.localpy</code> is now active.  Any python commands will now honor the virtual environment.</p> <pre><code>(.localpy) [root@bastion ~]#\n</code></pre> </li> <li> <p>Install the python dependencies using <code>pip install</code> execute the folling commands to install the required python modules.</p> <pre><code>pip install pip --upgrade\npip install ansible\npip install kubernetes\npip install mas-devops\npip install setuptools\npip install jmespath\n</code></pre> </li> <li> <p>Install the Ansible Galaxy Collection required by the <code>one-click</code> install process.</p> <pre><code>ansible-galaxy collection install ibm.mas_devops\n</code></pre> </li> <li> <p>Create a working directory for this MAS install</p> <pre><code>mkdir -p ~/work/one-click/masconfig\ncd ~/work/one-click\n</code></pre> </li> <li> <p>Create a file for the required variables for one-click install of MAS-core. Create this file in the working directory (~/one-click) n    Note: This lab adds the environment variable KUBECONFIG to point to the cluster KUBECONFIG for OpenShift authorization</p> </li> </ol> <p>Example of vars.sh</p> <pre><code>export IBM_ENTITLEMENT_KEY='&lt;paste ibm software entitlement here'\nexport MAS_INSTANCE_ID=\"inst1\"\nexport MAS_CONFIG_DIR=\"~/one-click/masconfig\"\nexport SLS_LICENSE_ID=\"IBM AMERICAS 2025 Internal Account\"\nexport SLS_LICENSE_FILE=\"~/one-click/masconfig/a_mas_license.dat\"\nexport DRO_CONTACT_EMAIL=\"don.bailey@ibm.com\"\nexport DRO_CONTACT_FIRSTNAME=\"Don\"\nexport DRO_CONTACT_LASTNAME=\"Bailey\"\nexport KUBECONFIG=~/build2/auth/kubeconf\n</code></pre> <ol> <li> <p>Run the following command in the terminal to verify the OCP cluster     is ready to be setup for MAS.</p> <pre><code>ansible localhost -m include_role -a name=ibm.mas_devops.ocp_verify\n</code></pre> </li> <li> <p>Install MAS core</p> <pre><code>ansible-playbook ibm.mas_devops.oneclick_core\n</code></pre> </li> <li> <p>Monitor the install by watching the terminal and the ansible output.  Also log in to the OpenShift cluster and monitor using the console.</p> </li> <li> <p>Once the installation the output of the terminal will display the Admin Dashboard URL and credentials.</p> </li> </ol> <pre><code>    \"msg\": [\n        \"Admin Dashboard ... https://admin.inst1.apps.ocpinstall.gym.lan\",\n        \"Username .......... 7Y4uAMLrAyl3ztUaFlnaWe41pyq6Oskb\",\n        \"Password .......... 4WSSc4V6p8covLLtdrMfaleQRWgFeyOy\"\n    ]\n</code></pre> <ol> <li>Log in using these temporary Admin credentials. Chrome seems to work better than Safari.</li> </ol> <p>Note: If you get a blue spinning circle after entering follow these steps.</p> <ul> <li>using a web browser replace 'admin' with 'api' in the URL.  This has something to do with the self signed certificates.</li> </ul> <pre><code>https://admin.inst1.apps.ocpinstall.gym.lan\nhttps://api.inst1.apps.ocpinstall.gym.lan\n</code></pre> <ul> <li>You will see something similar to this</li> </ul> <pre><code>{\"exception\":{\"id\":\"AIUCO1022E\",\"properties\":[\"/\"]},\"message\":\"AIUCO1022E: The requested URL could not be found: /\",\"uuid\":\"8b2a1df6-3998-4912-b752-a214d77a8399\"}\n</code></pre> <ul> <li>The dashboard will work now</li> </ul>"},{"location":"eam/lab_solution/#add-mas-manage-and-demo-data","title":"Add MAS Manage and Demo Data","text":"<ol> <li> <p>As the <code>root</code> user on the bastion node.</p> </li> <li> <p>Install MAS Manage and Sample Data, this steps takes 10+ hours to complete.  Consider starting the installation in a tmux session so that if connectiviy is lost terminal output will be saved.</p> </li> <li> <p>Add the following variables to ~/one-click/vars.sh</p> <pre><code>export MAS_DB_IMPORT_DEMO_DATA=true\nexport MAS_APP_SETTINGS_DEMODATA=true\n</code></pre> </li> <li> <p>Update shell variables</p> <pre><code>source ~/one-click/vars.sh\n</code></pre> </li> <li> <p>Install Manage and sample data.</p> </li> <li> <p>Add Manage and Sample Data</p> </li> </ol> <p>Note: This install will take up to 12 hours</p> <pre><code>ansible-playbook ibm.mas_devops.oneclick_add_manage\n</code></pre>"},{"location":"eam/lab_solution/#ibm-maximo-application-suite-cli-utility-mas-cli","title":"IBM Maximo Application Suite CLI Utility (mas-cli)","text":""},{"location":"eam/lab_solution/#mas-cli-setup-for-mas-install","title":"mas-cli Setup for MAS Install","text":""},{"location":"eam/lab_solution/#update-the-password-for-the-user-maxadmin","title":"Update the password for the user <code>maxadmin</code>","text":"<p>Note: These steps are for MAS 8.x and 9.0.  They will not work for MAS 9.1.</p> <ol> <li> <p>Log in to Maximo as the temporary user for the MAS Core deployment.</p> </li> <li> <p>Naviage to IBM Maximo Application Suite from the 9 Box in the top right of the console.</p> </li> <li> <p>Select <code>Suite administration</code></p> </li> <li> <p>Select Users, and search for maxadmin</p> </li> <li> <p>Select Edit from the additional options on the right side of the maxadmin user.</p> </li> <li> <p>Scroll down and select <code>Replace forgotten password</code></p> </li> <li> <p>Select 'Custom' and set the password for the <code>maxadmin</code> account.</p> </li> </ol>"},{"location":"eam/lab_solution/#update-the-default-administration-account-for-mas-91","title":"Update the default administration account for MAS 9.1","text":""},{"location":"eam/mvi/","title":"Maximo Visual Inspection","text":""},{"location":"eam/mvi/#maximo-visual-inspection","title":"Maximo Visual Inspection","text":"<p>Human based visual inspection errors typically range from 20% to 30%, IBM Institute of Business Value performance and benchmark study has shown automated visual inspection are achieving significant business benefits about 92% improvement in product quality issues and anomalies.</p> <p>Rework cost is much higher and if not captured recalls and customer satisfaction are at risk. Leveraging IBM Maximo Visual Inspection, we can help automate quality inspection, increase accuracy, reduce rework cost and improve worker safety and customer satisfaction</p> <ol> <li> <p>Mimimize Defects</p> <ul> <li>Capture defects at the point of occurance</li> </ul> </li> <li> <p>Increased Productivity</p> <ul> <li>Reduce inspection time.</li> </ul> </li> <li> <p>Automate Visual Inspection</p> <ul> <li>Improve quality inspection and consistency</li> </ul> </li> <li> <p>Fast Deployment</p> <ul> <li>Quick Value Realization</li> </ul> </li> </ol>"},{"location":"eam/mvi/#mvi-architecture","title":"MVI Architecture","text":""},{"location":"eam/mvi/#mvi-reference-architecture","title":"MVI Reference Architecture","text":"<p>Notes</p> <ol> <li> <p>Does Maximo Visual Inspection require GPUs to operate?  </p> <ul> <li>Yes. Maximo Visual Inspection requires GPUs to train all model types. However, in some configurations, Maximo Visual Inspection can perform model inferencing without using GPUs. In some cases, models might use only the central processing unit (CPU) or might be optimized for edge devices. For more information about model-specific requirements, capabilities, and optimizations, see the Models and supported functions topic.</li> </ul> </li> <li> <p>Why do I need a GPU for this application?  </p> <ul> <li>Maximo Visual Inspection uses deep learning to create AI models for computer vision tasks. GPUs provide highly parallel computation resources, are suited to performing deep learning tasks, and greatly reduce the training and inference time for the resulting models.</li> </ul> </li> <li> <p>Must I allocate a fixed number of GPUs?  </p> <ul> <li>No. Maximo Visual Inspection relies on Kubernetes, Red Hat OpenShift, and the NVIDIA GPU operator to allocate GPUs as requested Kubernetes resources when it needs to use them. Maximo Visual Inspection and IBM Maximo Application Suite share GPU resources with other applications, in the same way that applications share other Kubernetes resources, such as memory or CPUs.</li> </ul> </li> <li> <p>What happens if no GPUs are available?  </p> <ul> <li>During training, Maximo Visual Inspection queues training jobs and periodically checks to see whether any GPUs are available. When a GPU becomes available, Maximo Visual Inspection assigns it to the job that is queued for the longest time.  </li> <li>For some models, when you deploy the model for inferencing, you might choose to deploy the model in CPU mode or to run the model on an edge device by using Maximo Visual Inspection Edge or IBM Maximo Visual Inspection Mobile. In these configurations, no GPU is consumed during inferencing.  </li> </ul> </li> <li> <p>Which GPUs are supported?</p> <ul> <li>IBM collaborates with NVIDIA to certify the software. In the current release, IBM certifies NVIDIA Ampere, Turing, Pascal, and Volta devices, such as the NVIDIA T4, P40, P100, V100, A10, and A100. At least 16 GB of GPU memory is required during model training.</li> </ul> </li> </ol> <p>Reference Link</p>"},{"location":"eam/mvi/#mvi-workflow","title":"MVI Workflow","text":""},{"location":"eam/mvi/#pilot-delivery-timeline","title":"Pilot Delivery Timeline","text":"Week 0Week 1Week 2-Week 4Week 5 <ul> <li> <p>Use Case Alignment and Requirements Gathering</p> <ul> <li> <p>Activities</p> <ul> <li>Conduct Workshop(s)</li> <li>Use Case Discovery and Alignment</li> <li>Solution Discovery</li> <li>Pilot Scoping.</li> <li>Identify technical/business/user needs and challenges</li> <li>Assess business and user value of proposed solution</li> </ul> </li> <li> <p>Outcome</p> <ul> <li>Use cases identified &amp; aligned</li> <li>Data and accelerators identified</li> <li>Scope and success criteria defined with client sign-off</li> <li>Project proposal with estimated timeline, resource requirements, IBM investment and client commitments</li> <li>Alignment of sponsors and stakeholders</li> </ul> </li> <li> <p>Parties Involved</p> <ul> <li>IBM Team (Account, Client Engineering, Tech Sales) </li> <li>Client Product Owner </li> <li>Client sponsors)</li> </ul> </li> </ul> </li> </ul> <ul> <li> <p>Preparation and Kick-off</p> <ul> <li> <p>Activities</p> <ul> <li>Finalize architecture diagram for pilot</li> <li>Complete Al business value assessment framework</li> <li>Setup dev and runtime environments Default option is TechZone</li> <li>Identify, source, and prepare data and documentation needed for the pilot, including data transfer to CE if required</li> <li>Define validation plan</li> <li>Align on way of working</li> </ul> </li> <li> <p>Outcome</p> <ul> <li>Environment ready with technology installed, IBM and client having access</li> <li>Lifecycle and project management tools ready</li> <li>Data ready to be used (if applicable)</li> <li>UX designed &amp; validated (if applicable)</li> <li>Team ready to start with backlog of user stories prioritized.</li> <li>Playbacks with sponsor, stand up, and retrospectives booked</li> <li>Any legal agreements are completed</li> </ul> </li> <li> <p>Parties Involved</p> <ul> <li>Delivery Squad: IBM Client Engineering, Client Product Owner, Client sponsor</li> </ul> </li> </ul> </li> </ul> <ul> <li> <p>Pilot Build</p> <p>Note: Pilot build initiates once all data and documentation needed for the pilot have been received by Client Engineering from the client.</p> <ul> <li> <p>Activities</p> <ul> <li>Develop pilot solution</li> <li>Execute regular playbacks with Client Product Owner &amp; stakeholders</li> <li>Complete AI business value assessment</li> <li>Final Executive playback &amp; proposal delivered to the client</li> </ul> </li> <li> <p>Outcome</p> <ul> <li>Client has learned and evaluated IBM Technology's ability to bring value</li> <li>Pilot scope has been created and delivered and success criteria met</li> <li>Compelling, story/demo to move forward</li> <li>Business value case developed</li> <li>Stakeholder buy in to adopt</li> </ul> </li> <li> <p>Parties Involved</p> <ul> <li>Delivery Squad: IBM Client Engineering, Client Product Owner, Client sponsor</li> </ul> </li> </ul> </li> </ul> <ul> <li> <p>Transition</p> <ul> <li> <p>Activities</p> <ul> <li>Knowledge transfer</li> <li>Proposal review with the client</li> <li>Deliverable hand-off</li> <li>Identified deployment team onboarded</li> </ul> </li> <li> <p>Outcome</p> <ul> <li>IBM support team identified and in place</li> <li>Plan to get to production</li> <li>Roadmap for scaling</li> <li>Knowledge transferred to the service team</li> <li>Additional opportunities to drive value identified</li> <li>Attain client signature on proposal</li> </ul> </li> <li> <p>Parties Involved</p> <ul> <li>IBM Team, Account, CSM, Client Engineering</li> <li>Client Product Owner</li> <li>Client Sponsor</li> <li>Deployment Services Provider, Expert Labs or Partners</li> </ul> </li> </ul> </li> </ul> <p>The above slides came from this deck, CE MVI Presentation. </p>"},{"location":"eam/mvi/#useful-links","title":"Useful Links","text":"<p>Main MVI Seismic Page MAS POC / Demo Request </p>"},{"location":"eam/next_steps/","title":"Maximo Learning Next Steps","text":""},{"location":"eam/next_steps/#maximo-learning-next-steps","title":"Maximo Learning Next Steps","text":"<ol> <li> <p>The first step to learn the Maximo application basics is to request access to an environment follow this link Techzone Maximo Application Suite for Cross Industry.</p> <p></p> </li> <li> <p>Select 'Reserve It' and complete the TechZone request.</p> <p></p> </li> <li> <p>Once access to the demonstration environment has been completed. An email will be sent containing the required URL's, usernames and passwords.</p> <p>)</p> </li> <li> <p>Link to demonstration script</p> </li> </ol>"},{"location":"eam/next_steps/#hannah-carr-national-market-demo-and-education","title":"Hannah Carr - National Market Demo and Education","text":"<p>This box folder has recordings of the presentations and slide decks. The most important document in this folder is 'Maximo Mobile + Health.docx'. This document is a scripted demo to set up an asset class and define its health score.  Self Paced Mobile and Health</p>"},{"location":"eam/next_steps/#mas-workshop","title":"MAS Workshop","text":"<ol> <li>Maximo Overview and Installation (SNO)</li> <li>Monitor and IoT</li> <li>Health and Predict</li> <li>MVI and Mobile</li> <li>EAM to MAS Manage Upgrade</li> </ol>"},{"location":"eam/next_steps/#integrate-mvi-with-other-maximo-applications","title":"Integrate MVI with Other Maximo applications","text":"<p>MVI Integration</p>"},{"location":"eam/overview/","title":"Maximo Overview","text":""},{"location":"eam/overview/#maximo-overview","title":"Maximo Overview","text":"<p>IBM Maximo Asset Management (Maximo) is an enterprise asset management (EAM) software solution designed to help organizations manage physical assets throughout their lifecycle.</p> <p>Maximo offers a comprehensive set of features for managing assets, work orders, inventory, contracts, purchasing, and service requests. It supports various industries, including manufacturing, transportation, utilities, and facilities management.</p>"},{"location":"eam/overview/#key-features-of-maximo-include","title":"Key features of Maximo include:","text":"<ul> <li>Asset Management: Track and manage assets, including their location, condition, and maintenance history.</li> <li>Work Management: Create, assign, and track work orders, including preventive maintenance, corrective maintenance, and service requests.</li> <li>Inventory Management: Monitor and control inventory levels, including stock, consumables, and direct materials.</li> <li>Contract Management: Manage contracts, including vendor information, pricing, and performance metrics.</li> <li>Procurement: Streamline the purchasing process, from requisition to receipt and invoicing.</li> <li>Service Management: Manage service requests, including incidents, problems, and changes.</li> <li>Mobile Access: Access Maximo functionality through mobile devices, enabling field technicians to perform tasks and update records in real-time.</li> <li>Reporting and Analytics: Generate custom reports and use analytics to gain insights into asset performance, maintenance costs, and other key metrics.</li> <li>Integration: Integrate Maximo with other enterprise systems, such as ERP, CMMS, and GIS, to ensure data consistency and streamline processes.</li> </ul>"},{"location":"eam/overview/#maximo-visual-inspection","title":"Maximo Visual Inspection","text":"<p>Maximo Visual Inspection helps organizations improve asset reliability, reduce maintenance costs, and enhance safety by providing a structured approach to visual inspections. MVI also helps organizations standardize their inspection processes, improve data accuracy, and make informed decisions based on visual inspection data.</p> <ul> <li>Inspection Planning: Create and manage inspection plans, including defining inspection routes, frequencies, and required documentation.</li> <li>Mobile Inspections: Conduct inspections using mobile devices, allowing field technicians to capture photos, videos, and other visual data directly in the application.</li> <li>Defect Management: Document and track defects found during inspections, including their location, severity, and corrective actions.</li> <li>Reporting and Analytics: Generate custom reports and use analytics to gain insights into inspection trends, asset performance, and maintenance costs.</li> <li>Integration: Integrate Maximo Visual Inspection with other enterprise systems, such as Maximo Asset Management and GIS, to ensure data consistency and streamline processes.</li> </ul>"},{"location":"eam/overview/#what-is-maximo","title":"What is Maximo","text":""},{"location":"eam/overview/#introduction-to-mas-video","title":"Introduction to MAS Video","text":"<p>MAS Introduction Video</p>"},{"location":"eam/overview/#maximo-application-suite","title":"Maximo Application Suite","text":"<p>Journey to the New Operating Model including Generative AI</p> <p></p>"},{"location":"eam/overview/#notes","title":"Notes","text":"<ol> <li>Moving from left to right moves up the chain from Basic asset management to Predictive asset management by infusing the operational data with generative AI.</li> <li>Each of the blue circles is a module that can be added to Maximo Core and Maximo Manage to add features to a very powerful suite of asset management tools.</li> </ol>"},{"location":"eam/overview/#a-different-view-of-mas","title":"A Different View of MAS","text":""},{"location":"eam/overview/#notes_1","title":"Notes","text":"<ol> <li>Notice the asset data and sensor data flow to the Monitor application.  This is where AI begins to analyze the data.  This image like the previous one show the move from basic asset management to predictive management by infusing AI.</li> </ol>"},{"location":"eam/overview/#maintenance-strategies","title":"Maintenance Strategies","text":""},{"location":"eam/overview/#multi-cloud-deployment-model","title":"Multi-Cloud Deployment Model","text":"<p>IBM Maximo Application Suite (MAS) can be installed on-premise or in cloud environments like Amazon Web Services (AWS) and Microsoft Azure. It requires a Red Hat OpenShift cluster as a prerequisite. MAS can be deployed anywhere OpenShift can be deployed, including on-prem or on a public cloud. </p> <p></p>"},{"location":"eam/overview/#mas-upgrade-and-planning-links","title":"MAS Upgrade and Planning Links","text":"<p>MAS Sizing Calculator Note: The sizing calculator is an Excel spreadsheet that is downloaded and executed locally.</p> <p>MAS Installation Upgrade and Deployment </p> <p>Client Engineering MAS Upgrade Template - Day 1 OCP Installation on VMWare Gym - Day 2 - Ansible playbooks Collection for Maximo Application Suite Note: One Click Install page - MAS Upgrade Process Note: Same as the first link in this section - Presentation Slides for Day1/Day2 - Q &amp; A for MAS Upgrades - EAM to MAS Upgrade Training </p>"},{"location":"eam/overview/#content-contributors","title":"Content Contributors","text":""},{"location":"eam/overview/#americas-technical-sales-leader","title":"Americas Technical Sales Leader","text":"<p>Ed Neubecker Principal Automation Technical Specialist E-mail: Ed.Neubecker@us.ibm.com Slack: ed.neubecker YouTube Channel MVI Community Page The Pursuit of Zero Defect </p>"},{"location":"eam/overview/#bts-asset-lifecycle-management","title":"BTS Asset Lifecycle Management","text":"<p>Hannah Carr E-mail: Hannah.Carr@ibm.com Slack: Hannah Carr Mobile and Health Scripted Demo and Slides</p>"},{"location":"eam/overview/#ibm-distinguished-engineer-cto-sustainability-data-ai-ibm-client-engineering","title":"IBM Distinguished Engineer; CTO - Sustainability - Data &amp; AI, IBM Client Engineering","text":"<p>Janki Vora E-mail: janki@us.ibm.com Slack: @Janki</p>"},{"location":"eam/overview/#maximo-vocabulary","title":"Maximo Vocabulary","text":"<code>AIO</code> Asset Investment Optimizer <code>ALM</code> Asset Lifecycle Management <code>Asset</code> A Machine, Tool, or Subassembly that you need to track maintenance against <code>Asset Attribute</code> A value associated to the Asset Object/Asset Table <code>Asset Class</code> A collection of Similar Assets that follow the same processes and act similar <code>Asset Object</code> Maximo Business Object/Class for the Asset Table\u2019s <code>Asset Specification</code> A value associated to a classification which is assigned to the asset, allowing you to easily add data to your health scores <code>CBM</code> Conditioned-based Maintenance (on roadmap not available today) - a strategy that monitors the actual condition of equipment and performs maintenance only when necessary, based on real-time data and performance indicators, rather than a fixed schedule. It uses various techniques like sensors, visual inspections, and tests to detect deviations from normal operating conditions and schedule maintenance before potential failures. <code>CM</code> Corrective Maintenance, generally unplanned <code>CMMS</code> Computerized Maintenance Management System. <code>Contributors</code> A normalized value to be added to a health score. Can be made up of anything associated to the asset object that is quantifiable <code>CP4D</code> IBM CloudPak for Data <code>Database Configuration</code> An application in Maximo Manage that gives users the power to change the structure of the database and the Maximo Formulas <code>EAM</code> Enterprise Asset Management <code>EM</code> Emergency Maintenance \u2013 Corrective action taking precedent over all other work <code>FMEA</code> Failure Mode and Effects Analysis - a systematic approach to identify and assess potential failures in a system, process, or product, and their potential effects. It's used to proactively prevent or minimize failures by analyzing potential problems before they occur. <code>Health Score</code> A customized score made up of weighted contributors. Can be of type Health, Risk, Criticality or a Custom Score <code>ITAM</code> IT asset management (ITAM) is the end-to-end tracking and management of IT assets to ensure that every asset is properly used, maintained, upgraded and disposed of at the end of its lifecycle. For further info please see https://www.ibm.com/blog/it-asset-management. <code>ITSM</code> IT service management (ITSM) is the practice of provide the optimal deployment, operation and management of every IT resource for every end user across an enterprise. For further info please see https://www.ibm.com/topics/it-service-management <code>Job Plan</code> A template to be applied to a work order for common work scenarios <code>MAS</code> Maximo Application Suite <code>Meter</code> A Guage, description, or other used to record categorical or numerical data against an asset <code>MBO</code> Maximo Business Object <code>MRR</code> Maintenance Repair Ratio \u2013 Cost of Work Orders compared to Replacement Cost <code>Message Queuing Telemetry Transport (MQTT)</code> A lightweight, TCP-based protocol designed for machine-to-machine (M2M) communication, particularly well-suited for low-bandwidth, high-latency environments. It's a key component of the Internet of Things (IoT) framework, enabling communication between resource-constrained devices like sensors, actuators, and smart home appliances. <code>OCP</code> OpenShift Container Platform <code>PM</code> Preventative Re-Occurring maintenance <code>Service Request</code> A request for service, inspection, or general knowledge put in by any user, client or customer care representative <code>VI or MVI</code> Visual Inspection or Maximo Visual Inspection used for both Asset Based inspection and Quality Assurance during manufacturing. <code>Work Order</code> A request for service, inspection, or general knowledge put in by any user, client or customer care representative"},{"location":"integration/","title":"Cloud Pak for Integration","text":""},{"location":"integration/#cloud-pak-for-integration","title":"Cloud Pak for Integration","text":"<p>Helpful Links:</p> <ul> <li>Cloud Pak for Integration Sales Kit</li> <li>CP4I Overview and Use Case Video</li> </ul>"},{"location":"integration/#ibm-cloud-pak-for-integration-end-to-end-demonstration","title":"IBM Cloud Pak for Integration end to end demonstration","text":"<p>This document describes multiple use cases that highlight the multi-style integration patterns needed by an Integration Specialist to implement a digital transformation initiative that showcase the value Cloud Pak for Integration can provide.  </p> <p>About this Exercise</p> <p>The steps within this section are taken directly from the IBM Cloud Pak for Integration end to end demonstration repository maintained by @Joel-Gomez.  Additional explanations have been provided to create additional clarity during certain steps.  Refer back to original repository for the latest code and deployment steps.</p>"},{"location":"integration/#the-scenario","title":"The Scenario","text":"<p>In this demo scenario, a customer has a system of record that has used for many years which was designed to work with MQ to process requirements, and now they want to extend the application to expose an API in a secure way as part of their mobile app.</p> <p>The requirements don't end there, they are also implementing a new CRM system as a Service and they want to keep both systems in sync without having to modify the original system of record.</p> <p>Additionally, as part of their digital transformation initiative they want to stay closer to their clients and they want to send email notifications when the client is taking certain actions in their mobile app. To support this strategy, the enterprise architecture team wants to implement an event backbone, as part of their event driven architecture.</p> <p>The following diagram provides a high level view of the scenario:</p> <p></p> <p>In order to implement this demo you will need to deploy an instance of each one of the following components in your OCP Cluster:</p> <ul> <li>API Connect Cluster</li> <li>Event Streams Cluster</li> <li>Queue Manager</li> <li>App Connect Enterprise Integration Servers</li> </ul> <p>The following diagram provides a high level implementation view of the scenario with the core capabilities:</p> <p></p> <p>To demonstrate the added value capabilities provided by CP4I you will deploy an instance of the following components in your OCP Cluster:</p> <ul> <li>Platform UI (formerly known as Platform Navigator)</li> <li>Automation Foundation Assets (formerly known as Asset Repository)</li> <li>Assemblies</li> </ul>"},{"location":"integration/#getting-started","title":"Getting Started","text":"<p>To use this guide you will need to clone the CP4I Demo repo repo to your workstation alongside with the cp4i-ace-artifacts repo that includes the App Connect Integrations.</p> <p>There are additional required / useful tools for you to configure on your workstation:</p> <ul> <li>oc cli</li> <li>zip</li> <li>keytool</li> <li>openssl</li> <li>jq</li> <li>yq</li> <li>apic cli</li> </ul> <p>Tool Availability</p> <p>Some of the tools will be available after you deploy an instance of the capability you are working on.</p> <p>To fully perform the App Connect demonstration you will also need to have a SafesForce Developer Account. To perform the integration that uses the Event End Point Management Gateway you will also need the following tools in your workstation:</p> <ul> <li>ACE for Developers</li> <li>podman</li> </ul> <p>MQ Client</p> <p>The demo also enables the configuration to access the Queue Manager from outside the cluster using MQ Explorer, and optionally from an application using the MQI API. If you want to use this part of the demo you will need to have at least the MQ Client installed in your workstation.  This is not included as part of the bootcamp, but certainly can be helpful when working on pilots with your customers.</p> <p>Lastly, this guide assumes you already have an OCP cluster with the right version and capacity (v4.16.x or v4.17.x and at least 80 vCPUs and 320 GB of memory, for best results it is recommended to have 5 worker nodes 32 vCPUs X 128 GB memory each) up and running on IBM Cloud, either in your own account or via TechZone. You should already have this cluster provisioned and you should be logged in from the CLI before continuing.  If you are note using your IPI on VMware install from the bootcamp you can find further instructions to provision a cluster in TechZone.</p>"},{"location":"integration/01-prereqs/","title":"CP4I Demo Prerequisites","text":""},{"location":"integration/01-prereqs/#cp4i-demo-prerequisites","title":"CP4I Demo Prerequisites","text":"<p>For the purpose of the PE Bootcamp, we installed and ODF storage cluster and will use those options when performing the exercise.</p> <p>Choosing Storage</p> <p>Based on where you have deployed your OCP cluster you will need to set up an environment variable to use the proper <code>storage classes</code> when deploying the instances. The two option available at the moment are ROKS and ODF. Note that if you use a cluster in TechZone, you need to make sure to use ODF otherwise you may end up with a cluster using NFS that is NOT supported and you will need to recreate your cluster.</p>"},{"location":"integration/01-prereqs/#getting-the-code","title":"Getting the Code","text":"<p>Do not forget to clone repo <code>cp4i-ace-artifacts</code> to your workstation alongside with this repo. The repo is located here, it includes the App Connect Integrations. Check the list of pre-requisites before using this guide using the following command:</p> <p>New to cloning GitHub repositories?</p> <p>If you are new to GitHub and need help with basic commands, GitHub provides very easy to follow tutorials such as this one for cloning repos</p> <pre><code>scripts/00a-cp4i-prereq-vaidation.sh\n</code></pre> <p>Once you confirm you have the required tools in your workstation you are ready to use the following guide. </p> <p>Note the scripts used in this guide have been developed and tested on mac os. </p>"},{"location":"integration/01-prereqs/#configure-your-environment-for-deployment","title":"Configure Your Environment for Deployment","text":"<p>Set environment variables:</p> <ol> <li> <p>Set the CP4I version you want to install. The options available are either the latest LTS or CD releases, so use the following commands:     </p><pre><code>export CP4I_VER=16.1.1\n</code></pre>     !!! Warning \"About the Version\"          At the time of writing this exercise, we perform an IPI install of OCP 4.17 on VMware and will use that cluster for the Integration demo environment.  16.1.1 is the first version of CP4I that supports 4.17.<p></p> </li> <li> <p>Set the OCP type based on the storage classes in your cluster:</p> <p>About Storage</p> <p>If you are performing this deployment into a ROKS cluster you would set the OCP type to <code>ROKS</code>, but for our IPI cluster we must use the ODF storage cluster that we have already created.</p> </li> </ol> <pre><code>export OCP_TYPE=ODF\n</code></pre> <p>Set a default storage class for your cluster:</p> <p>If you have provisioned your OCP cluster in Tech Zone you can use the following script to set the proper default storage class: </p><pre><code>scripts/99-odf-tkz-set-scs.sh\n</code></pre><p></p> <p>Checking the default storage class</p> <p>Curious to see what this was set to?  You can always check your storage classes <code>oc get storageclass</code>.  The default will be noted within the output</p>"},{"location":"integration/01-prereqs/#install-logging-with-kibana-optional","title":"Install Logging with Kibana (Optional)","text":"<p>This part of the demo environment is optional, but if you have never used Kibana you may want to deploy this portion.</p> <ol> <li>Install ElasticSearch Operator:</li> </ol> <p></p><pre><code>oc apply -f resources/00a-elasticsearch-namespace.yaml\noc apply -f resources/00b-elasticsearch-operatorgroup.yaml\noc apply -f resources/00c-elasticsearch-subscription.yaml\n</code></pre>    Confirm the subscription has been completed successfully before moving to the next step running the following command:    <pre><code>SUB_NAME=$(oc get deployment elasticsearch-operator -n openshift-operators-redhat --ignore-not-found -o jsonpath='{.metadata.labels.olm\\.owner}');if [ ! -z \"$SUB_NAME\" ]; then oc get csv/$SUB_NAME -n openshift-operators-redhat --ignore-not-found -o jsonpath='{.status.phase}';fi;echo\n</code></pre>    You should get a response like this:    <pre><code>Succeeded\n</code></pre> 2. Install Logging Operator:    <pre><code>oc apply -f resources/00d-logging-namespace.yaml\noc apply -f resources/00e-logging-operatorgroup.yaml\noc apply -f resources/00f-logging-subscription.yaml\n</code></pre>    Confirm the subscription has been completed successfully before moving to the next step running the following command:    <pre><code>SUB_NAME=$(oc get deployment cluster-logging-operator -n openshift-logging --ignore-not-found -o jsonpath='{.metadata.labels.olm\\.owner}');if [ ! -z \"$SUB_NAME\" ]; then oc get csv/$SUB_NAME -n openshift-logging --ignore-not-found -o jsonpath='{.status.phase}';fi;echo\n</code></pre>    You should get a response like this:    <pre><code>Succeeded\n</code></pre> 3. Deploy Logging instance:    <pre><code>scripts/00b-logging-install-alt.sh\n</code></pre>    Confirm the instance has been deployed successfully before moving to the next step running the following command:    <pre><code>oc get pods -n openshift-logging\n</code></pre>    You should receive a response like this:    <pre><code>NAME                                            READY   STATUS      RESTARTS   AGE\ncluster-logging-operator-756b4c48cc-lhkzs       1/1     Running     0          6m41s\ncollector-njm62                                 2/2     Running     0          5m36s\ncollector-nxpmd                                 2/2     Running     0          5m36s\ncollector-xjl96                                 2/2     Running     0          5m36s\ncollector-xsv6b                                 2/2     Running     0          5m36s\ncollector-z9k9l                                 2/2     Running     0          5m36s\nelasticsearch-cdm-dxgp4gmf-1-577dc997c-sk7kg    2/2     Running     0          5m36s\nelasticsearch-cdm-dxgp4gmf-2-5f5d564466-cgk6x   2/2     Running     0          5m35s\nelasticsearch-cdm-dxgp4gmf-3-8695d6658c-lxblf   2/2     Running     0          5m33s\nelasticsearch-im-app-27947625-m6qd9             0/1     Completed   0          2m58s\nelasticsearch-im-audit-27947625-ht4jj           0/1     Completed   0          2m58s\nelasticsearch-im-infra-27947625-r9j8c           0/1     Completed   0          2m58s\nkibana-746f699cc-72qfk                          2/2     Running     0          5m34s\n</code></pre> 4. Post-deployment configuration (optional):<p></p> <p>Get Kibana URL:       </p><pre><code>echo \"https://\"$(oc get route kibana -n openshift-logging -o jsonpath='{.spec.host}')\n</code></pre><p></p> <p>Copy the URL to your favorite browser and hit \"Enter\". If you get the login page enter your credentials to get the following page. Accept the default values and click \"Allow selected permissions\":    </p> <p>In the next page enter \"app-*\" in the <code>index-pattern</code> field and then click \"Next step\" as shown below:    </p> <p>In the next page set the <code>time filter</code> field to \"@timestamp\" and then click \"Create Index Pattern\" as shown below:    </p> <p>Now you can create your own Dashboards or search for logs in the Discovery page as shown below:           Note creating a Dashboard is outside of the scope of this repo.</p>"},{"location":"integration/01-prereqs/#install-argocd-optional","title":"Install ArgoCD (Optional)","text":"<p>Another optional step within this exercise is using ArgoCD.  We will cover CD / GitOps later in the bootcamp week, you may optionally install it at this time and if you proceed with additional optional exercises you may find it useful.</p> <ol> <li>Create and configure the namespace:    <pre><code>oc create ns openshift-gitops-operator\noc label namespace openshift-gitops-operator openshift.io/cluster-monitoring=true\n</code></pre></li> <li>Create operator group:    <pre><code>oc apply -f resources/00-gitops-operatorgroup.yaml\n</code></pre></li> <li>Create subscription:    <pre><code>oc apply -f resources/00-gitops-subscription.yaml\n</code></pre>    Confirm the operator has been deployed successfully before moving to the next step running the following commands:    <pre><code>oc get pods -n openshift-gitops-operator\noc get pods -n openshift-gitops\n</code></pre>    You should receive a response like this for each command respectively.    <pre><code>NAME                                                            READY   STATUS    RESTARTS   AGE\nopenshift-gitops-operator-controller-manager-7859c4ddd4-j2g8z   2/2     Running   0          98s\n</code></pre> <pre><code>NAME                                                          READY   STATUS    RESTARTS   AGE\ncluster-6b66cd5687-h5fhm                                      1/1     Running   0          2m50s\nkam-868f97bd49-2mchk                                          1/1     Running   0          2m50s\nopenshift-gitops-application-controller-0                     1/1     Running   0          2m46s\nopenshift-gitops-applicationset-controller-7d9dcdf769-s7ssn   1/1     Running   0          2m46s\nopenshift-gitops-dex-server-5c66897994-f4wrq                  1/1     Running   0          2m46s\nopenshift-gitops-redis-5684c6fc5b-456nt                       1/1     Running   0          2m46s\nopenshift-gitops-repo-server-dcf86f4c7-d6x28                  1/1     Running   0          2m46s\nopenshift-gitops-server-55dbf6b78b-m4mhw                      1/1     Running   0          2m46s\n</code></pre></li> <li>Enable kustomize:    <pre><code>oc patch argocd -n openshift-gitops openshift-gitops --patch '{\"spec\":{\"kustomizeBuildOptions\":\"--enable-alpha-plugins=true --enable-exec\"}}' --type=merge\n</code></pre></li> <li>Once ArgoCD is up and running get the access info:    <pre><code>scripts/00c-argocd-access-info.sh\n</code></pre></li> </ol>"},{"location":"integration/01-prereqs/#install-instana-optional","title":"Install Instana (Optional)","text":"<p>Instana and Integration</p> <p>Although we are not teaching Instana as part of the bootcamp, Instana and IBM Integration make a great story together.  The end-to-end tracing feature alone makes it a compelling story that can be added to your Pilots.  To enable your Integration for Instana you must have an Instana deployment already running.  If you have that available, follow the below steps to add an agent to your cluster.</p> <ol> <li>Install Instana Agent Operator:    <pre><code>oc apply -f resources/01f-instana-agent-subscription.yaml\n</code></pre>    Confirm the subscription has been completed successfully before moving to the next step running the following command:    <pre><code>SUB_NAME=$(oc get deployment  controller-manager -n openshift-operators --ignore-not-found -o jsonpath='{.metadata.labels.olm\\.owner}');if [ ! -z \"$SUB_NAME\" ]; then oc get csv/$SUB_NAME -n openshift-operators --ignore-not-found -o jsonpath='{.status.phase}';fi;echo\n</code></pre>    You should get a response like this:    <pre><code>Succeeded\n</code></pre></li> <li>Set environment variables:    <pre><code>export ZONE_NAME=&lt;my-zone-name&gt;\nexport CLUSTER_NAME=&lt;my-cluster-name&gt;\nexport INSTANA_APP_KEY=&lt;instana-app-key&gt;\nexport INSTANA_SVC_ENDPOINT=&lt;instana-service-endpoint&gt;\nexport INSTANA_SVC_PORT=&lt;instana-service-port&gt;\n</code></pre></li> <li>Deploy Instana Agent running script:    <pre><code>scripts/01b-instana-install.sh\n</code></pre>    Confirm the instance has been deployed successfully before moving to the next step running the following command:    <pre><code>oc get pods -n instana-agent\n</code></pre>    You should receive a response like this.    <pre><code>NAME                  READY   STATUS    RESTARTS   AGE\ninstana-agent-75dkm   1/1     Running   0          5m6s \ninstana-agent-8gr46   1/1     Running   0          5m6s\ninstana-agent-xpj95   1/1     Running   0          5m6s\ninstana-agent-xxncc   1/1     Running   0          5m6s\ninstana-agent-zvflw   1/1     Running   0          5m6s\n</code></pre></li> <li>Set environment variable:    <pre><code>export CP4I_TRACING=YES\n</code></pre></li> </ol>"},{"location":"integration/02-base-deploy/","title":"Configure CP4I and Install Prerequisites","text":""},{"location":"integration/02-base-deploy/#configure-cp4i-and-install-prerequisites","title":"Configure CP4I and Install Prerequisites","text":""},{"location":"integration/02-base-deploy/#install-common-services","title":"Install Common Services","text":""},{"location":"integration/02-base-deploy/#cert-manager-installation","title":"Cert-Manager Installation","text":"<p>cert-manager</p> <p>You can use IBM Cloud Pak for Multicloud Management cert-manager to create and mount a certificate to a Kubernetes Deployment, StatefulSet, or DaemonSet. You can also create and add a certificate to a Kubernetes Ingress.  cert-manager documentation</p> <p>Begin by installing the Cert Manager Operator:</p> <pre><code>oc apply -f resources/00-cert-manager-namespace.yaml\noc apply -f resources/00-cert-manager-operatorgroup.yaml\noc apply -f resources/00-cert-manager-subscription.yaml\n</code></pre> <p>What just happened?</p> <p>For the most significant commands we will provide some explanation.  In less interesting cases, you are left on your own to investigate the yaml being applied to determine what just happened.  You should review each command you are running.  Much of this larger demo configuration exercise could be completed from the UI, but we have chosen to show the steps from the CLI.  Why do you find this important?</p> <p>Confirm the subscription has been completed successfully before moving to the next step running the following command:</p> <pre><code>SUB_NAME=$(oc get deployment cert-manager-operator-controller-manager -n cert-manager-operator --ignore-not-found -o jsonpath='{.metadata.labels.olm\\.owner}');if [ ! -z \"$SUB_NAME\" ]; then oc get csv/$SUB_NAME -n cert-manager-operator --ignore-not-found -o jsonpath='{.status.phase}';fi;echo\n</code></pre> <p>Take a look at that command ...</p> <p>Look at the command you just ran.  Throughout the demo you will be given such commands to determine when a process has finished or finished successfully.  Look at the syntax.  How else could you determine these results?  Experiment with your own <code>jsonpath</code> settings.</p> <p>You should get a response like this:</p> <pre><code>Succeeded\n</code></pre>"},{"location":"integration/02-base-deploy/#install-common-services_1","title":"Install Common Services:","text":"<p>Before deploying the Common Services required for CP4I, you must configure the catalog source.</p> <pre><code>oc apply -f catalog-sources/${CP4I_VER}/02-common-services-catalog-source.yaml\n</code></pre> <p>Confirm the catalog source has been deployed successfully before moving to the next step running the following command:</p> <pre><code>oc get catalogsources opencloud-operators -n openshift-marketplace -o jsonpath='{.status.connectionState.lastObservedState}';echo\n</code></pre> <p>You should get a response like this: </p><pre><code>READY\n</code></pre><p></p> <p>Create the Common Services namespace: </p><pre><code>oc create namespace ibm-common-services\n</code></pre><p></p> <p>Install Common Services Operator: </p><pre><code>oc apply -f subscriptions/${CP4I_VER}/00-common-service-subscription.yaml\n</code></pre><p></p> <p>Confirm the operator has been deployed successfully before moving to the next step running the following command: </p><pre><code>SUB_NAME=$(oc get deployment/ibm-common-service-operator -n openshift-operators --ignore-not-found -o jsonpath='{.metadata.labels.olm\\.owner}');if [ ! -z \"$SUB_NAME\" ]; then oc get csv/$SUB_NAME --ignore-not-found -o jsonpath='{.status.phase}';fi;echo\n</code></pre><p></p> <p>You should get a response like this: </p><pre><code>Succeeded\n</code></pre><p></p>"},{"location":"integration/02-base-deploy/#prepare-the-cp4i-namespace","title":"Prepare the CP4I Namespace","text":"<p>Use the provided script to create namespaces with the corresponding entitlement key:</p> <p>Set your entitlement key:</p> <p>Where do I get my entitlement key?</p> <p>You can copy your entitlement key from the My IBM Entitlement Key web page.</p> <pre><code>export ENT_KEY=&lt;my-key&gt;\n</code></pre> <p>Create namespaces:</p> <pre><code>scripts/02a-cp4i-ns-key-config.sh\n</code></pre>"},{"location":"integration/02-base-deploy/#deploy-platform-ui","title":"Deploy Platform UI","text":"<p>You will now deploy the Platform UI (formerly known as the Navigator).</p> <p>Install Platform UI Catalog Source: </p><pre><code>oc apply -f catalog-sources/${CP4I_VER}/03-platform-navigator-catalog-source.yaml\n</code></pre><p></p> <p>Confirm the catalog source has been deployed successfully before moving to the next step running the following command:  </p><pre><code>oc get catalogsources ibm-integration-platform-navigator-catalog -n openshift-marketplace -o jsonpath='{.status.connectionState.lastObservedState}';echo\n</code></pre><p></p> <p>You should get a response like this:</p> <pre><code>READY\n</code></pre> <p>Install Platform UI Operator:</p> <pre><code>oc apply -f subscriptions/${CP4I_VER}/01-platform-navigator-subscription.yaml\n</code></pre> <p>Confirm the operator has been deployed successfully before moving to the next step running the following commands: </p><pre><code>SUB_NAME=$(oc get deployment ibm-integration-platform-navigator-operator -n openshift-operators --ignore-not-found -o jsonpath='{.metadata.labels.olm\\.owner}');if [ ! -z \"$SUB_NAME\" ]; then oc get csv/$SUB_NAME --ignore-not-found -o jsonpath='{.status.phase}';fi;echo\n</code></pre><p></p> <p>You should get a response like this: </p><pre><code>Succeeded\n</code></pre><p></p> <p>Deploy an instance of the Platform UI:</p> <pre><code>scripts/03a-platform-navigator-inst-deploy.sh\n</code></pre> <p>What just happened?</p> <p>If you look into the scripts that perform deployments as part of this exercise, you will see that the script helps run the correct <code>yaml</code> for the storage and CP4I versions.  If desired, you could tweak settings with these files.  For instance, the most notable setting chosen for the deployment of the navigator instance is the number of replicas:</p> <pre><code>apiVersion: integration.ibm.com/v1beta1\nkind: PlatformNavigator\nmetadata:\n  name: cp4i-navigator\nspec:\n  integrationAssistant:\n    enabled: true\n  license:\n    accept: true\n    license: L-QYVA-B365MB\n  replicas: 3\n  version: 16.1.1\n</code></pre> <p>Confirm the instance has been deployed successfully before moving to the next step running the following command: </p><pre><code>oc get platformnavigator cp4i-navigator -n tools -o jsonpath='{.status.conditions[0].type}';echo\n</code></pre><p></p> <p>You should get a response like this: </p><pre><code>Ready\n</code></pre><p></p> <p>Install Flow</p> <p>You will notice a repeating pattern as you install the Cloud Pak for Integration capabilities.</p> <ul> <li>Configure a Catalog Source</li> <li>Install (subscribe) the Operator(s)</li> <li>Create / Prepare a namespace</li> <li>Deploy an instance of the capability</li> </ul> <p>We have provided scripts to streamline these steps.  These scripts and <code>yaml</code> can be valuable references when you deploy some of these features within your Pilots.</p> <p>Once the Platform UI instance is up and running get the access info: </p><pre><code>scripts/03b-cp4i-access-info.sh\n</code></pre><p></p> <p>Log Into the Platform UI</p> <p>Use the information you just received to login into the UI.  Where else do you find this information? The password is temporary and you will be required to change it the first time you log into Platform UI.</p>"},{"location":"integration/02-base-deploy/#optional-exercise-deploy-asset-repo","title":"Optional Exercise: Deploy Asset Repo","text":"<p>Install Asset Repo Catalog Source: </p><pre><code>oc apply -f catalog-sources/${CP4I_VER}/04-asset-repo-catalog-source.yaml\n</code></pre><p></p> <p>Confirm the catalog source has been deployed successfully before moving to the next step running the following command:  </p><pre><code>oc get catalogsources ibm-integration-asset-repository-catalog -n openshift-marketplace -o jsonpath='{.status.connectionState.lastObservedState}';echo\n</code></pre><p></p> <p>You should get a response like this: </p><pre><code>READY\n</code></pre><p></p> <p>Install Asset Repo Operator: </p><pre><code>oc apply -f subscriptions/${CP4I_VER}/02-asset-repo-subscription.yaml\n</code></pre><p></p> <p>Confirm the operator has been deployed successfully before moving to the next step running the following command: </p><pre><code>SUB_NAME=$(oc get deployment ibm-integration-asset-repository-operator -n openshift-operators --ignore-not-found -o jsonpath='{.metadata.labels.olm\\.owner}');if [ ! -z \"$SUB_NAME\" ]; then oc get csv/$SUB_NAME --ignore-not-found -o jsonpath='{.status.phase}';fi;echo\n</code></pre><p></p> <p>You should get a response like this: </p><pre><code>Succeeded\n</code></pre><p></p> <p>Deploy an Asset Repo instance: </p><pre><code>scripts/05-asset-repo-inst-deploy.sh\n</code></pre><p></p> <p>Confirm the instance has been deployed successfully before moving to the next step running the following command: </p><pre><code>oc get assetrepository asset-repo-ai -n tools -o jsonpath='{.status.phase}';echo\n</code></pre><p></p> <p>You should get a response like this: </p><pre><code>Ready\n</code></pre><p></p> <p>Post-deployment configuration:</p> <ol> <li> <p>Navigate to the Asset Repo instance from Platform UI clicking on the instance name as shown below:       </p> </li> <li> <p>From the main page select the <code>Remotes</code> tab and click <code>Add Remote</code> as shown below:       </p> </li> <li> <p>In the next page scroll all the way down and select <code>Select All</code> as shown below:              Note at the moment not all the asset types are available in the repo but we are ready for future enhancements.</p> </li> <li> <p>Now scroll up again and enter the name of the remote repo, for instance <code>JGR CP4I Demo Assets</code> and then enter the <code>Git URL</code> \"https://github.com/gomezrjo/cp4idemo\" and then click \"Create Remote\" as shown below:              You can add your own repo following the same process.</p> </li> </ol>"},{"location":"integration/03-apic/","title":"Deploy APIC","text":""},{"location":"integration/03-apic/#deploy-apic","title":"Deploy APIC","text":"<p>Helpful Links:</p> <ul> <li>Recorded API Connect Demo</li> <li>API Connect Sales Kit</li> <li>Client Engineering API Management Playbook Comming Soon</li> </ul>"},{"location":"integration/03-apic/#install-mail-server-mailpit","title":"Install Mail Server (mailpit):","text":"<p>As part of API Management, the tooling sends different personae notifications.  Using this simple Mail Server, allows this to happen as part of a Pilot or Demo environment.</p> <p>Create namespace:</p> <pre><code>      oc new-project mailpit\n</code></pre> <p>Deploy Mail Server:</p> <pre><code>scripts/30a-mailpit-deploy-mail-server.sh\n</code></pre> <p>Confirm the mail server has been deployed successfully before moving to the next step running the following command: </p><pre><code>oc get deployment mailpit -n mailpit -o jsonpath='{.status.conditions[1].status}';echo\n</code></pre> You should get a response like this: <pre><code>True\n</code></pre><p></p> <p>Create Mail Server Service and Route: </p><pre><code>oc apply -f resources/30b-mailpit-services.yaml\noc apply -f resources/30c-mailpit-route.yaml\n</code></pre><p></p> <p>Get Mail Server UI URL: </p><pre><code>echo \"http://\"$(oc get route mailpit-ui -n mailpit -o jsonpath='{.status.ingress[0].host}')\n</code></pre><p></p> <p>Connect to Mail Server by Navigating to the URL and use the credentials to access the UI.</p>"},{"location":"integration/03-apic/#install-data-power","title":"Install Data Power","text":"<p>As part of APIC Data Power acts as the API Gateway.</p> <ol> <li>Install DataPower Catalog Source:    <pre><code>oc apply -f catalog-sources/${CP4I_VER}/05-datapower-catalog-source.yaml\n</code></pre>    Confirm the catalog source has been deployed successfully before moving to the next step running the following command:     <pre><code>oc get catalogsources ibm-datapower-operator-catalog -n openshift-marketplace -o jsonpath='{.status.connectionState.lastObservedState}';echo\n</code></pre>    You should get a response like this:    <pre><code>READY\n</code></pre></li> <li>Install DataPower Operator:    <pre><code>oc apply -f subscriptions/${CP4I_VER}/03-datapower-subscription.yaml \n</code></pre>    Confirm the operator has been deployed successfully before moving to the next step running the following command:    <pre><code>SUB_NAME=$(oc get deployment datapower-operator -n openshift-operators --ignore-not-found -o jsonpath='{.metadata.labels.olm\\.owner}');if [ ! -z \"$SUB_NAME\" ]; then oc get csv/$SUB_NAME --ignore-not-found -o jsonpath='{.status.phase}';fi;echo \n</code></pre>    You should get responses like these for both of them:    <pre><code>Succeeded\n</code></pre></li> </ol>"},{"location":"integration/03-apic/#install-apic-catalog-source-and-operator","title":"Install APIC Catalog Source and Operator","text":"<pre><code>oc apply -f catalog-sources/${CP4I_VER}/07-api-connect-catalog-source.yaml\n</code></pre> <p>Confirm the catalog source has been deployed successfully before moving to the next step running the following command:  </p><pre><code>oc get catalogsources ibm-apiconnect-catalog -n openshift-marketplace -o jsonpath='{.status.connectionState.lastObservedState}';echo\n</code></pre> You should get a response like this: <pre><code>READY\n</code></pre><p></p> <p>Install APIC Operator: </p><pre><code>oc apply -f subscriptions/${CP4I_VER}/04-api-connect-subscription.yaml \n</code></pre><p></p> <p>Confirm the operator has been deployed successfully before moving to the next step running the following command: </p><pre><code>SUB_NAME=$(oc get deployment ibm-apiconnect -n openshift-operators --ignore-not-found -o jsonpath='{.metadata.labels.olm\\.owner}');if [ ! -z \"$SUB_NAME\" ]; then oc get csv/$SUB_NAME --ignore-not-found -o jsonpath='{.status.phase}';fi;echo   \n</code></pre><p></p> <p>You should get responses like these for both of them: </p><pre><code>Succeeded\n</code></pre><p></p>"},{"location":"integration/03-apic/#deploy-apic_1","title":"Deploy APIC","text":"<p>Deploy APIC instance with some extra features enabled:</p> <pre><code>scripts/07d-apic-inst-deploy-instana.sh\n</code></pre> <p>Confirm the installation completed successfully before moving to the next step running the following commands: </p><pre><code>oc get APIConnectCluster apim-demo -n tools -o jsonpath='{.status.phase}';echo\n</code></pre> Note this will take almost 30 minutes, so be patient, and at the end you should get a response like this: <pre><code>Ready\n</code></pre><p></p> <p>APIC is up and running</p> <p>The optional steps below provide some basic configuration for the demo, but at this point you could use the APIC cluster to explore creating, publishing, consuming and managing APIs.  As part of the bootcamp we do not need to perform all of the optional configuration.</p>"},{"location":"integration/03-apic/#apic-optional-configuration-steps","title":"APIC Optional Configuration Steps","text":""},{"location":"integration/03-apic/#configure-apic-integration-with-instana-optional","title":"Configure APIC integration with Instana (optional):","text":"<pre><code>scripts/07e-apic-instana-config.sh\n</code></pre> <p>Configure the email server in APIC: </p><pre><code>scripts/07f-apic-initial-config.sh\n</code></pre><p></p> <p>Create a Provider Organization for admin user: </p><pre><code>scripts/07g-apic-new-porg-cs.sh\n</code></pre><p></p>"},{"location":"integration/03-apic/#create-an-api-provider-organization","title":"Create an API Provider Organization","text":"<p>Create a Provider Organization for user in local registry (optional):</p> <p>Set environment variables: </p><pre><code>export USER_NAME=&lt;your-user-name&gt;\nexport USER_EMAIL=&lt;your-email-address&gt;\nexport USER_FNAME=&lt;your-first-name&gt;\nexport USER_LNAME=&lt;your-last-name&gt;\nexport USER_PWD=&lt;your-personal-password&gt;\n</code></pre><p></p> <p>Create Provide Organization: </p><pre><code>scripts/07h-apic-new-porg-lur.sh\n</code></pre><p></p>"},{"location":"integration/03-apic/#set-api-key-for-post-deployment-configuration","title":"Set API Key for post deployment configuration:","text":"<ol> <li>Get API Key following instructions listed here</li> <li>Set environment variable for API Key: <pre><code>export APIC_API_KEY=&lt;my-apic-api-key&gt;\n</code></pre></li> </ol> <p>Create secret for Assemblies (optional): </p><pre><code>scripts/07i-apic-secret-cp4i-alt.sh\n</code></pre> Deploy extra API Gateway (optional): <pre><code>scripts/07j-apic-extra-gw-deploy.sh\n</code></pre> Confirm the instance has been deployed successfully before moving to the next step running the following command: <pre><code>oc get gatewaycluster remote-api-gw -n cp4i-dp -o jsonpath='{.status.phase}';echo\n</code></pre> You should get responses like these: <pre><code>Running\n</code></pre><p></p>"},{"location":"integration/04-event-automation/","title":"Event Automation","text":""},{"location":"integration/04-event-automation/#event-automation","title":"Event Automation","text":"<p>Helpful Links:</p> <ul> <li>Event Automation Demo Video</li> <li>Event Automation Sales Kit</li> <li>Client Engineering Event Automation Playbook Coming Soon</li> </ul>"},{"location":"integration/04-event-automation/#deploy-event-streams","title":"Deploy Event Streams","text":"<p>Install Event Streams Catalog Source: </p><pre><code>oc apply -f catalog-sources/${CP4I_VER}/08-event-streams-catalog-source.yaml\n</code></pre><p></p> <p>Confirm the catalog source has been deployed successfully before moving to the next step running the following command:  </p><pre><code>oc get catalogsources ibm-eventstreams-catalog -n openshift-marketplace -o jsonpath='{.status.connectionState.lastObservedState}';echo\n</code></pre><p></p> <p>You should get a response like this: </p><pre><code>READY\n</code></pre><p></p> <p>Install Event Streams Operator: </p><pre><code>oc apply -f subscriptions/${CP4I_VER}/05-event-streams-subscription.yaml\n</code></pre><p></p> <p>Confirm the operator has been deployed successfully before moving to the next step running the following command: </p><pre><code>SUB_NAME=$(oc get deployment eventstreams-cluster-operator -n openshift-operators --ignore-not-found -o jsonpath='{.metadata.labels.olm\\.owner}');if [ ! -z \"$SUB_NAME\" ]; then oc get csv/$SUB_NAME --ignore-not-found -o jsonpath='{.status.phase}';fi;echo \n</code></pre><p></p> <p>You should get a response like this: </p><pre><code>Succeeded\n</code></pre><p></p> <p>Deploy Event Streams instance: </p><pre><code>scripts/08a-event-streams-inst-deploy.sh\n</code></pre><p></p> <p>What deployment decisions were just made?</p> <p>If you dig through the script you will see that ES is instantiated using the <code>05-event-streams-instance.yaml</code> see below with some of the decisions highlighted.  You can use these instance <code>yaml</code> examples to deploy your own copies via command line or from the UI in the future.</p> 05-event-streams-instance.yaml<pre><code>apiVersion: eventstreams.ibm.com/v1beta2\nkind: EventStreams\nmetadata:\nname: es-demo\nlabels:\n      backup.eventstreams.ibm.com/component: eventstreams\n      assembly.integration.ibm.com/tools.jgr-demo: 'true'\nspec:\nversion: latest\nlicense:\n   accept: true\n   license: L-JTPV-KYG8TF\n   use: CloudPakForIntegrationNonProduction\nadminApi: {}\nadminUI:\n   authentication:\n      - type: integrationKeycloak\napicurioRegistry: {}\ncollector: {}\nrestProducer: {}\nstrimziOverrides:\n   kafka:\n      replicas: 3\n      authorization:\n      type: simple\n      config:\n      inter.broker.protocol.version: '3.8'\n      log.cleaner.threads: 6\n      num.io.threads: 24\n      num.network.threads: 9\n      num.replica.fetchers: 3\n      offsets.topic.replication.factor: 3\n      default.replication.factor: 3\n      min.insync.replicas: 2\n      listeners:\n      - name: authsslsvc\n         port: 9095\n         type: internal\n         tls: true\n         authentication:\n            type: scram-sha-512\n      - name: external\n         port: 9094\n         type: route\n         tls: true\n         authentication:\n            type: scram-sha-512\n      - name: tls\n         port: 9093\n         type: internal\n         tls: true\n         authentication:\n            type: tls\n      metricsConfig:\n      type: jmxPrometheusExporter\n      valueFrom:\n         configMapKeyRef:\n            key: kafka-metrics-config.yaml\n            name: minimal-prod-metrics-config\n      resources:\n      requests:\n         memory: 128Mi\n         cpu: 100m\n      limits:\n         memory: 2048Mi\n         cpu: 1000m\n      storage:\n      type: persistent-claim\n      size: 4Gi\n      class: ${OCP_BLOCK_STORAGE}\n   zookeeper:\n      replicas: 3\n      metricsConfig:\n      type: jmxPrometheusExporter\n      valueFrom:\n         configMapKeyRef:\n            key: zookeeper-metrics-config.yaml\n            name: minimal-prod-metrics-config\n      storage:\n      type: persistent-claim\n      size: 2Gi\n      class: ${OCP_BLOCK_STORAGE}\n   entityOperator:\n      topicOperator: {}\n      userOperator: {}\n</code></pre> <p>Confirm the instance has been deployed successfully before moving to the next step running the following command: </p><pre><code>oc get eventstreams es-demo -n tools -o jsonpath='{.status.phase}';echo\n</code></pre><p></p> <p>Note this will take few minutes, so be patient, and at some point you may see some errors, but at the end you should get a response like this: </p><pre><code>Ready\n</code></pre><p></p>"},{"location":"integration/04-event-automation/#event-streams-additional-configuration","title":"Event Streams Additional Configuration","text":"<p>Create topics and users: </p><pre><code>oc apply -f resources/02a-es-initial-config-jgr-topics.yaml -n tools\noc apply -f resources/02a-es-initial-config-jgr-users.yaml -n tools\noc apply -f resources/02a-es-initial-config-ea-topics.yaml -n tools\noc apply -f resources/02a-es-initial-config-watsonx-topics.yaml -n tools\n</code></pre><p></p> <p>Enable Kafka Connect base: </p><pre><code>scripts/08c-event-streams-kafka-connect-config.sh\n</code></pre><p></p> <p>Confirm the instance has been deployed successfully before moving to the next step running the following command: </p><pre><code>oc get kafkaconnects jgr-connect-cluster -n tools -o jsonpath='{.status.conditions[0].type}';echo\n</code></pre> Note this will take few minutes, but at the end you should get a response like this: <pre><code>Ready\n</code></pre><p></p>"},{"location":"integration/04-event-automation/#enable-kafka-connect-for-watsonx-optional","title":"Enable Kafka Connect for WatsonX (Optional):","text":"<pre><code>scripts/08f-event-streams-kafka-connect-watsonx-config.sh\n</code></pre> Confirm the instance has been deployed successfully before moving to the next step running the following command: <pre><code>oc get kafkaconnects watsonx-demo-sources -n tools -o jsonpath='{.status.conditions[0].type}';echo\n</code></pre> <p>Note this will take few minutes, but at the end you should get a response like this: </p><pre><code>Ready\n</code></pre><p></p>"},{"location":"integration/04-event-automation/#enable-kafka-bridge-optional","title":"Enable Kafka Bridge (Optional)","text":"<p>What is Kafka Bridge?</p> <p>A \"Kafka Bridge\" is a component that allows applications to interact with a Kafka cluster using standard HTTP requests instead of the native Kafka protocol, essentially providing a web API interface to manage producers and consumers within a Kafka cluster without needing to understand the complex Kafka protocol directly; this is typically implemented through a RESTful API, enabling applications that can only speak HTTP to send and receive messages from Kafka topics.  See also Strimzi ...</p> <pre><code>scripts/08d-event-streams-kafka-bridge-config.sh\n</code></pre> <p>Confirm the instance has been deployed successfully running the following command: </p><pre><code>oc get kafkabridge jgr-es-demo-bridge -n tools -o jsonpath='{.status.conditions[0].type}';echo\n</code></pre><p></p> <p>You should get a response like this: </p><pre><code>Ready\n</code></pre><p></p>"},{"location":"integration/04-event-automation/#enable-kafka-connector-base","title":"Enable Kafka Connector Base","text":"<pre><code>scripts/08e-event-streams-kafka-connector-datagen-config.sh\n</code></pre> Confirm the instances has been deployed successfully before moving to the next step running the following command: <pre><code>oc get kafkaconnector -n tools\n</code></pre> Note this will take few minutes, but at the end you should get a response like this: <pre><code>NAME                 CLUSTER               CONNECTOR CLASS                                                         MAX TASKS   READY\nkafka-datagen        jgr-connect-cluster   com.ibm.eventautomation.demos.loosehangerjeans.DatagenSourceConnector   1           True\nkafka-datagen-avro   jgr-connect-cluster   com.ibm.eventautomation.demos.loosehangerjeans.DatagenSourceConnector   1           True\nkafka-datagen-reg    jgr-connect-cluster   com.ibm.eventautomation.demos.loosehangerjeans.DatagenSourceConnector   1           True\n</code></pre>"},{"location":"integration/04-event-automation/#additional-kafka-connectors-integrations-optional","title":"Additional Kafka Connectors / Integrations (Optional)","text":"<p>Enable Kafka Connector Weather for WatsonX (optional):    1. Set environment variable:       </p><pre><code>export OPEN_WEATHER_API_KEY=&lt;your-open-weather-api-key&gt;\n</code></pre>    2. Run script:       <pre><code>scripts/08g-event-streams-kafka-connector-weather-config.sh\n</code></pre><p></p> <p>Enable Kafka Connector Weather for WatsonX (optional):    1. Set environment variable:       </p><pre><code>export ALPHA_VANTAGE_API_KEY=&lt;your-alpha-vantage-api-key&gt;\n</code></pre>    2. Run script:       <pre><code>scripts/08h-event-streams-kafka-connector-stock-prices-config.sh\n</code></pre><p></p> <p>Enable APIC Analytics offloading to Kafka Topic (optional):    </p><pre><code>scripts/07k-apic-analytic-offload-config.sh\n</code></pre><p></p> <p>Enable APIC to work with EA for WatsonX (optional):    1. Run script:       </p><pre><code>scripts/07l-apic-gw-config-ea-watsonx.sh\n</code></pre>    2. Set environment variable:       <pre><code>export EA_WATSONX=YES\n</code></pre><p></p>"},{"location":"integration/04-event-automation/#deploy-event-endpoint-management-eem-optional","title":"Deploy Event Endpoint Management - EEM (Optional):","text":"<p>What is Event Endpoint Management?</p> <p>Kafka Event Endpoint Management is a feature that allows organizations to control and manage access to Kafka topics within their event-driven architecture, essentially acting as a centralized portal where developers can discover available event streams and request access to them, providing a self-service method to consume data from different Kafka clusters without needing direct cluster configuration access.</p> <p>To deploy EEM see the cp4i-demo source repo README</p>"},{"location":"integration/04-event-automation/#deploy-event-processing-optional","title":"Deploy Event Processing (Optional):","text":"<ol> <li>Install Apache Flink Catalog Source:    <pre><code>oc apply -f catalog-sources/${CP4I_VER}/14-ea-flink-catalog-source.yaml\n</code></pre>    Confirm the catalog source has been deployed successfully before moving to the next step running the following command:     <pre><code>oc get catalogsources ibm-eventautomation-flink-catalog -n openshift-marketplace -o jsonpath='{.status.connectionState.lastObservedState}';echo\n</code></pre>    You should get a response like this:    <pre><code>READY\n</code></pre></li> <li>Install Apache Flink Operator:    <pre><code>oc apply -f subscriptions/${CP4I_VER}/10-ea-flink-subscription.yaml\n</code></pre>    Confirm the operator has been deployed successfully before moving to the next step running the following command:    <pre><code>SUB_NAME=$(oc get deployment flink-kubernetes-operator -n openshift-operators --ignore-not-found -o jsonpath='{.metadata.labels.olm\\.owner}');if [ ! -z \"$SUB_NAME\" ]; then oc get csv/$SUB_NAME --ignore-not-found -o jsonpath='{.status.phase}';fi;echo\n</code></pre>    You should get a response like this:    <pre><code>Succeeded\n</code></pre></li> <li>Prepare TrustStore for Event Automation:    <pre><code>scripts/20d-ea-truststore-config.sh\n</code></pre></li> <li>Deploy Apache Flink instance:    <pre><code>oc apply -f instances/${CP4I_VER}/21-ea-flink-instance.yaml -n tools\n</code></pre>    Confirm the instance has been deployed successfully before moving to the next step running the following command:    <pre><code>oc get flinkdeployment ea-flink-demo -n tools -o jsonpath='{.status.jobManagerDeploymentStatus}';echo\n</code></pre>    You should get a response like this:    <pre><code>READY\n</code></pre></li> <li>Install Event Processing Catalog Source:    <pre><code>oc apply -f catalog-sources/${CP4I_VER}/15-event-processing-catalog-source.yaml\n</code></pre>    Confirm the catalog source has been deployed successfully before moving to the next step running the following command:     <pre><code>oc get catalogsources ibm-eventprocessing-catalog -n openshift-marketplace -o jsonpath='{.status.connectionState.lastObservedState}';echo\n</code></pre>    You should get a response like this:    <pre><code>READY\n</code></pre></li> <li> <p>Install Event Processing Operator:    </p><pre><code>oc apply -f subscriptions/${CP4I_VER}/11-event-processing-subscription.yaml\n</code></pre>    Confirm the operator has been deployed successfully before moving to the next step running the following command:    <pre><code>SUB_NAME=$(oc get deployment ibm-ep-operator -n openshift-operators --ignore-not-found -o jsonpath='{.metadata.labels.olm\\.owner}');if [ ! -z \"$SUB_NAME\" ]; then oc get csv/$SUB_NAME --ignore-not-found -o jsonpath='{.status.phase}';fi;echo\n</code></pre>    You should get a response like this:    <pre><code>Succeeded\n</code></pre> It is assumed you will use local security for this exercise.  <p></p> </li> <li> <p>Deploy Event Processing instance:    </p><pre><code>scripts/20b-ea-ep-inst-deploy.sh\n</code></pre>    Confirm the instance has been deployed successfully before moving to the next step running the following command:    <pre><code>oc get eventprocessing ep-demo -n tools -o jsonpath='{.status.phase}';echo\n</code></pre>    You should get a response like this:    <pre><code>Running\n</code></pre><p></p> </li> <li>Configure Event Processing security:      </li> <li>Execute the corresponding script:       <pre><code>scripts/20b-ea-ep-config-sec.sh\n</code></pre></li> <li>If you enabled integration with KeyCloak then add the EP user role to integration admin to grant access, otherwise go to the next step.</li> <li>Install PGSQL Operator (if you didn't do it as part of ACE, otherwise go to the next step):</li> <li>Create namespace:       <pre><code>oc create namespace pgsql\n</code></pre></li> <li>Enable Operator Group in namespace:       <pre><code>oc apply -f resources/12d-pgsql-operatorgroup.yaml\n</code></pre></li> <li>Install PGSQL Operator at namespace level:       <pre><code>oc apply -f resources/12a-pgsql-subscription.yaml\n</code></pre>       Confirm the operator has been deployed successfully before moving to the next step running the following command:       <pre><code>SUB_NAME=$(oc get deployment pgo -n pgsql --ignore-not-found -o jsonpath='{.metadata.labels.olm\\.owner}');if [ ! -z \"$SUB_NAME\" ]; then oc get csv/$SUB_NAME -n pgsql --ignore-not-found -o jsonpath='{.status.phase}';fi;echo\n</code></pre>       You should get a response like this:       <pre><code>Succeeded\n</code></pre></li> <li>Deploy a PGSQL DB instance (if you didn't do it as part of ACE, otherwise go to the next step):</li> <li>Create configmap with db configuration:       <pre><code>oc apply -f resources/12b-pgsql-config.yaml -n pgsql\n</code></pre></li> <li>Create a PGSQL DB instance:       <pre><code>oc apply -f resources/12c-pgsql-db.yaml -n pgsql\n</code></pre>       Confirm the instance has been deployed successfully before moving to the next step running the following command:       <pre><code>oc get pods -l \"postgres-operator.crunchydata.com/role=master\" -n pgsql -o jsonpath='{.items[0].status.conditions[1].status}' 2&gt;/dev/null;echo\n</code></pre>       After a few minutes you should get a response like this:       <pre><code>True\n</code></pre></li> <li>Get information to access EA instances:       <pre><code>scripts/20c-ea-access-info.sh\n</code></pre></li> </ol>"},{"location":"integration/05-mq/","title":"Deploy Enterprise Messaging - MQ","text":""},{"location":"integration/05-mq/#deploy-enterprise-messaging-mq","title":"Deploy Enterprise Messaging - MQ","text":"<p>Helpful Links:</p> <ul> <li>MQ Sales Kit</li> </ul>"},{"location":"integration/05-mq/#catalog-source-deploy-the-operator","title":"Catalog Source &amp; Deploy the Operator","text":"<ol> <li>Install MQ Catalog Source:    <pre><code>oc apply -f catalog-sources/${CP4I_VER}/09-mq-catalog-source.yaml \n</code></pre>    Confirm the catalog source has been deployed successfully before moving to the next step running the following command:     <pre><code>oc get catalogsources ibmmq-operator-catalogsource -n openshift-marketplace -o jsonpath='{.status.connectionState.lastObservedState}';echo\n</code></pre>    You should get a response like this:    <pre><code>READY\n</code></pre></li> <li>Install MQ Operator:    <pre><code>oc apply -f subscriptions/${CP4I_VER}/06-mq-subscription.yaml\n</code></pre>    Confirm the operator has been deployed successfully before moving to the next step running the following command:    <pre><code>SUB_NAME=$(oc get deployment ibm-mq-operator -n openshift-operators --ignore-not-found -o jsonpath='{.metadata.labels.olm\\.owner}');if [ ! -z \"$SUB_NAME\" ]; then oc get csv/$SUB_NAME --ignore-not-found -o jsonpath='{.status.phase}';fi;echo\n</code></pre>    You should get a response like this:    <pre><code>Succeeded\n</code></pre></li> </ol>"},{"location":"integration/05-mq/#deploy-the-mq-instance-native-ha","title":"Deploy the MQ Instance (Native HA)","text":"<p>What is MQ Native HA?</p> <p>Native HA is a high availability solution aimed at container deployments of IBM MQ. Native HA uses log replication to keep three instances of a queue manager running on different nodes up to date. One instance is active at any one time and processes messages.  This is a very common containerized MQ configuration.</p> <ol> <li>Set MQ namespace environment variable:    <pre><code>export MQ_NAMESPACE=cp4i-mq\n</code></pre></li> <li>Create certificates and extra route:    <pre><code>scripts/10a-qmgr-pre-config.sh\n</code></pre></li> <li>Create configmaps with MQ configuration:    <pre><code>oc apply -f resources/03c-qmgr-mqsc-config.yaml -n tools\noc apply -f resources/03e-qmgr-mqsc-config-ea.yaml -n tools\noc apply -f resources/03g-qmgr-ini-config.yaml -n tools\n</code></pre></li> <li>Deploy MQ Queue Manager instance:    <pre><code>scripts/10b-qmgr-inst-deploy.sh\n</code></pre>    Confirm the instance has been deployed successfully before moving to the next step running the following command:    <pre><code>oc get queuemanager qmgr-demo -n tools -o jsonpath='{.status.phase}';echo\n</code></pre>    You are looking for the following:</li> </ol> <pre><code>Running\n</code></pre> <p>While you are waiting ...</p> <p>Take a look at the <code>yaml</code> used to deploy the MQ instance.</p> 12-qmgr-native-ha-instance.yaml<pre><code>apiVersion: mq.ibm.com/v1beta1\nkind: QueueManager\nmetadata:\nannotations:\n   com.ibm.mq/write-defaults-spec: 'false'\nname: qmgr-native-ha\nspec:\nlicense:\n   accept: true\n   license: L-QYVA-B365MB\n   use: NonProduction\nqueueManager:\n   name: QMGRNATIVEHA\n   resources:\n      limits:\n      cpu: 500m\n      requests:\n      cpu: 500m\n   storage:\n      queueManager:\n      type: persistent-claim\n      defaultClass: ${OCP_BLOCK_STORAGE}\n      defaultDeleteClaim: true\n   availability:\n      type: NativeHA\ntemplate:\n   pod:\n      containers:\n      - env:\n            - name: MQSNOAUT\n            value: 'yes'\n         name: qmgr\nversion: ${MQ_VERSION}\nweb:\n   console:\n      authentication:\n      provider: integration-keycloak\n      authorization:\n      provider: integration-keycloak\n   enabled: true\n</code></pre>"},{"location":"integration/05-mq/#create-ccdt-optional","title":"Create CCDT (optional):","text":"<p>Optional Steps to Follow</p> <p>The rest of the steps below are not required for the bootcamp but help to configure additional features within the demo environment.  </p> <p>Client Channel Definition Table (CCDT) allows you to connect an IBM\u00ae MQ client application to any level of queue manager.</p> <pre><code>scripts/10c-qmgr-post-config.sh\n</code></pre>"},{"location":"integration/05-mq/#deploy-kafka-connect-mq-connectors-optional","title":"Deploy Kafka Connect MQ Connectors (Optional)","text":"<ol> <li>MQ Source Connector:    <pre><code>oc apply -f resources/02b-es-mq-source.yaml\n</code></pre></li> <li>MQ Sink Connector:    <pre><code>oc apply -f resources/02c-es-mq-sink.yaml\n</code></pre></li> <li>MQ Source Connector for Event Automation:    <pre><code>oc apply -f resources/02e-es-mq-source-ea.yaml\n</code></pre></li> </ol>"},{"location":"integration/05-mq/#deploy-other-queue-managers-optional","title":"Deploy other Queue Managers (Optional)","text":"<pre><code>oc apply -f instances/${CP4I_VER}/common/09-qmgr-ephemeral-single-instance.yaml -n ${MQ_NAMESPACE}\noc apply -f instances/${CP4I_VER}/${OCP_TYPE}/10-qmgr-base-log-single-instance.yaml -n ${MQ_NAMESPACE}\noc apply -f instances/${CP4I_VER}/${OCP_TYPE}/11a-qmgr-base-storage-multi-instance.yaml -n ${MQ_NAMESPACE}\noc apply -f instances/${CP4I_VER}/${OCP_TYPE}/11b-qmgr-advanced-storage-multi-instance.yaml -n ${MQ_NAMESPACE}\nscripts/10e-qmgr-native-ha-inst-deploy.sh\nscripts/10f-qmgr-ext-np-inst-deploy.sh\n</code></pre>    You can check the status using the following command:    <pre><code>oc get queuemanager --all-namespaces\n</code></pre>    Output will vary depending on which instances you decide to deploy."},{"location":"integration/05-mq/#optional-cluster-configurations","title":"Optional Cluster Configurations","text":""},{"location":"integration/05-mq/#deploy-uniform-cluster-configuration-with-native-ha-using-manual-approach-optional","title":"Deploy Uniform Cluster configuration with Native HA using manual approach (optional):","text":"<p>What is a Uniform Cluster?</p> <p>An IBM MQ uniform cluster is a group of queue managers that are configured similarly and work together to balance workloads and distribute messages. Uniform clusters are designed to be scalable and available, and they can be used to connect applications to a single group of queue managers.</p> <ol> <li>Create configurations:    <pre><code>oc apply -f resources/03d-qmgr-uniform-cluster-config.yaml\n</code></pre></li> <li>Prepare certificates for Queue Managers:    <pre><code>scripts/10d-qmgr-uc-pre-config.sh\n</code></pre></li> <li>Deploy Queue Manager 1:    <pre><code>oc apply -f instances/${CP4I_VER}/${OCP_TYPE}/13a-qmgr-uniform-cluster-qm1.yaml -n cp4i\n</code></pre></li> <li>Deploy Queue Manager 2:    <pre><code>oc apply -f instances/${CP4I_VER}/${OCP_TYPE}/13b-qmgr-uniform-cluster-qm2.yaml -n cp4i\n</code></pre>    Confirm the instances have been deployed successfully running the following command:    <pre><code>oc get queuemanager -n cp4i\n</code></pre>    Note this will take few minutes, but at the end you should get a response like this:    <pre><code>NAME                  PHASE\nuniform-cluster-qm1   Running\nuniform-cluster-qm2   Running\n</code></pre></li> </ol>"},{"location":"integration/05-mq/#deploy-uniform-cluster-configuration-with-native-ha-using-argocd-optional","title":"Deploy Uniform Cluster configuration with Native HA using ArgoCD (Optional):","text":"<ol> <li>Install dependency:    <pre><code>oc apply -f resources/00-gitops-clusterissuer.yaml\n</code></pre></li> <li>Add RoleBinding to allow ArgoCD to use the namespace:    <pre><code>oc apply -f resources/00-gitops-rolebinding.yaml\n</code></pre></li> <li>Deploy ArgoCD application:    <pre><code>oc apply -f resources/00-gitops-app.yaml\n</code></pre>    You can review the app and explore the repo that has the definitions.</li> <li>Confirm the instances have been deployed successfully running the following command:    <pre><code>oc get queuemanager -n mq-argocd\n</code></pre>    Note this will take few minutes, but at the end you should get a response like this:    <pre><code>NAME      PHASE\nqm01-qm   Running\nqm02-qm   Running\n</code></pre></li> </ol>"},{"location":"integration/05-mq/#create-extra-resources-to-demo-mq-uniform-cluster-optional","title":"Create extra resources to demo MQ Uniform Cluster (optional):","text":"<ol> <li>Create CCDT to be used by App:    <pre><code>oc apply -f resources/04a-nginx-ccdt-configmap.yaml\n</code></pre></li> <li>Deploy NGINX instance to serve CCDT:    <pre><code>oc apply -f resources/04b-nginx-deployment.yaml\n</code></pre>    Confirm the instances has been deployed successfully before moving to the next step running the following command:    <pre><code>oc get pods -n cp4i | grep nginx\n</code></pre>    You should get a response like this:    <pre><code>nginx-deployment-575c9b7b64-c457v   1/1     Running   0               5h11m\n</code></pre></li> <li>Create service for NGINX to be used by App:    <pre><code>oc apply -f resources/04c-nginx-service.yaml\n</code></pre></li> </ol>"},{"location":"integration/06-ace/","title":"Deploy App Connect","text":""},{"location":"integration/06-ace/#deploy-app-connect","title":"Deploy App Connect","text":"<p>Helpful Links:</p> <ul> <li>App Connect Sales Kit</li> <li>API Led Integration with App Connect Video</li> <li>App Connect 201 Client Presentation</li> </ul>"},{"location":"integration/06-ace/#configure-the-catalog-source-and-operator","title":"Configure the Catalog Source and Operator","text":"<p>Install App Connect Catalog Source: </p><pre><code>oc apply -f catalog-sources/${CP4I_VER}/10-app-connect-catalog-source.yaml \n</code></pre> Confirm the catalog source has been deployed successfully before moving to the next step running the following command:  <pre><code>oc get catalogsources appconnect-operator-catalogsource -n openshift-marketplace -o jsonpath='{.status.connectionState.lastObservedState}';echo\n</code></pre> You should get a response like this: <pre><code>READY\n</code></pre><p></p> <p>Install App Connect Operator: </p><pre><code>oc apply -f subscriptions/${CP4I_VER}/07-app-connect-subscription.yaml\n</code></pre> Confirm the operator has been deployed successfully before moving to the next step running the following command: <pre><code>SUB_NAME=$(oc get deployment ibm-appconnect-operator -n openshift-operators --ignore-not-found -o jsonpath='{.metadata.labels.olm\\.owner}');if [ ! -z \"$SUB_NAME\" ]; then oc get csv/$SUB_NAME --ignore-not-found -o jsonpath='{.status.phase}';fi;echo\n</code></pre> You should get a response like this: <pre><code>Succeeded\n</code></pre><p></p>"},{"location":"integration/06-ace/#deploy-dashboard-instance","title":"Deploy Dashboard Instance","text":"<p>What is the Dashboard?</p> <p>The IBM App Connect Dashboard is a user interface that allows users to deploy BAR files into App Connect containers. It also allows users to perform operations on services, create integration servers, and establish connections to integration servers.</p> <p></p><pre><code>scripts/09a-ace-dashboard-inst-deploy.sh\n</code></pre> Confirm the instance has been deployed successfully before moving to the next step running the following command:<p></p> <p></p><pre><code>oc get dashboard ace-dashboard -n tools -o jsonpath='{.status.phase}';echo\n</code></pre> Note this will take few minutes, but at the end you should get a response like this: <pre><code>Ready\n</code></pre><p></p>"},{"location":"integration/06-ace/#deploy-designer","title":"Deploy Designer","text":"<p>Deploy Designer Authoring instance with support for Callable Flows</p> <p>Deploy Switch Server instance: </p><pre><code>scripts/09b-ace-switch-server-inst-deploy.sh\n</code></pre><p></p> <p>Confirm the instance has been deployed successfully before moving to the next step running the following command: </p><pre><code>oc get switchserver ace-switch-server -n tools -o jsonpath='{.status.phase}';echo\n</code></pre><p></p> <p>Note this will take few minutes, but at the end you should get a response like this: </p><pre><code>Ready\n</code></pre> Deploy Designer Authoring instance<p></p> <p></p><pre><code>scripts/09c-ace-designer-inst-deploy.sh\n</code></pre> Confirm the instance has been deployed successfully before moving to the next step running the following command: <pre><code>oc get designerauthoring ace-designer-ai -n tools -o jsonpath='{.status.phase}';echo\n</code></pre> Note this will take few minutes, but at the end you should get a response like this: <pre><code>Ready\n</code></pre><p></p>"},{"location":"integration/06-ace/#create-bar-auth-configuration","title":"Create BAR Auth Configuration","text":"<p>What is a BAR?</p> <pre><code>    An \"app connect BAR\" refers to a deployment package file format used within IBM App Connect, where \"BAR\" stands for \"Business Application Archive\"; essentially, it's a compressed file containing all the necessary components like integration flows, configurations, and other assets needed to deploy an application within the App Connect platform.\n</code></pre> <pre><code>scripts/11-ace-config-barauth-github.sh\n</code></pre>"},{"location":"integration/06-ace/#deploy-integrations","title":"Deploy Integrations","text":"<ol> <li>Create Policy Configuration to integrate with MQ:       <pre><code>scripts/12a-ace-config-policy-mq.sh\n</code></pre></li> <li>Deploy Integration Runtime instances related to MQ and the API:       <pre><code>scripts/12c-ace-is-apis-inst-deploy.sh\n</code></pre>       You can check the status using the following command:       <pre><code>oc get integrationruntimes -n tools\n</code></pre></li> </ol> <p>While you are waiting ...</p> <p>Lets look at a sample ACE integration deployment.  Note where it pulls in the BAR file.  This BAR file provides the configuration for what the instance actually is doing:</p> 11-ace-is-mqapi-dflt-instance.yaml<pre><code>apiVersion: appconnect.ibm.com/v1beta1\nkind: IntegrationRuntime\nmetadata:\nname: jgr-mqapi-dflt\nlabels:\nbackup.appconnect.ibm.com/component: integrationruntime\nassembly.integration.ibm.com/tools.jgr-demo: 'true'\nspec:\nlicense:\naccept: true\nlicense: L-KPRV-AUG9NC\nuse: CloudPakForIntegrationNonProduction\ntemplate:\nspec:\n      containers:\n      - name: runtime\n      resources:\n            limits:\n            cpu: 500m\n            memory: 512Mi\n            requests:\n            cpu: 500m\n            memory: 512Mi\nreplicas: 1\nversion: '13.0'\nbarURL: \n- &gt;-\n      https://github.com/gomezrjo/cp4idemo/raw/main/barfiles/jgr-cp4i-mqapi-dflt.bar\nconfigurations:\n- github-barauth\n- ace-qmgr-demo-policy\n</code></pre> <p>You are ready to explore App Connect</p> <p>The rest of the configuration steps below are optional, but may be of interest during your exploration of App Connect.  Some of these configurations may provide value during your Pilot deployments.</p>"},{"location":"integration/06-ace/#configure-sales-force-connector-optional","title":"Configure Sales Force Connector (Optional)","text":"<p>The Sales Force integration is a very typical example used to learn and demo ACE.  If you followed the prerequisites to create your SF account you can configure that here. 1. Set Environment Variables: </p><pre><code>export SF_USER=&lt;my-sf-user&gt;\nexport SF_PWD=&lt;my-sf-pwd&gt;\nexport SF_CLIENT_ID=&lt;my-sf-client-id&gt;\nexport SF_CLIENT_SECRET=&lt;my-sf-client-secret&gt;\nexport SF_LOGIN_URL=&lt;my-sf-login-url&gt;\n</code></pre> 2. Create Sales Force Account Configuration:       <pre><code>scripts/12b-ace-config-accounts-sf.sh\n</code></pre> 3. Set Environment Variable:       <pre><code>export SF_CONNECTOR=YES\n</code></pre> 4. Deploy Integration Runtime instance related to SF:       <pre><code>scripts/12d-ace-is-sf-inst-deploy.sh\n</code></pre><p></p>"},{"location":"integration/06-ace/#create-configurations-related-to-es","title":"Create Configurations related to ES:","text":"<pre><code>scripts/15a-ace-config-policy-es-scram.sh\nscripts/15b-ace-config-setdbparms-es-scram.sh\nscripts/15c-ace-config-truststore-es.sh\n</code></pre> Deploy Integration Runtime instance related to ES: <pre><code>scripts/15d-ace-is-es-inst-deploy.sh\n</code></pre>"},{"location":"integration/06-ace/#configure-integration-with-pgsql-db-optional","title":"Configure Integration with PGSQL DB (Optional)","text":"<ol> <li>Create namespace:       <pre><code>oc create namespace pgsql\n</code></pre></li> <li>Enable Operator Group in namespace:       <pre><code>oc apply -f resources/12d-pgsql-operatorgroup.yaml\n</code></pre></li> <li>Install PGSQL Operator at namespace level:       <pre><code>oc apply -f resources/12a-pgsql-subscription.yaml\n</code></pre>       Confirm the operator has been deployed successfully before moving to the next step running the following command:       <pre><code>SUB_NAME=$(oc get deployment pgo -n pgsql --ignore-not-found -o jsonpath='{.metadata.labels.olm\\.owner}');if [ ! -z \"$SUB_NAME\" ]; then oc get csv/$SUB_NAME -n pgsql --ignore-not-found -o jsonpath='{.status.phase}';fi;echo\n</code></pre>       You should get a response like this:       <pre><code>Succeeded\n</code></pre></li> <li>Create configmap with db configuration:       <pre><code>oc apply -f resources/12b-pgsql-config.yaml -n pgsql\n</code></pre></li> <li>Create a PGSQL DB instance:       <pre><code>oc apply -f resources/12c-pgsql-db.yaml -n pgsql\n</code></pre>       Confirm the instance has been deployed successfully before moving to the next step running the following command:       <pre><code>oc get pods -l \"postgres-operator.crunchydata.com/role=master\" -n pgsql -o jsonpath='{.items[0].status.conditions[1].status}' 2&gt;/dev/null;echo\n</code></pre>       After a few minutes you should get a response like this:       <pre><code>True\n</code></pre></li> <li>Create Configurations related to PGSQL:       <pre><code>scripts/22a-ace-config-odbc-ini-pgsql.sh\nscripts/22b-ace-config-setdbparms-pgsql.sh\n</code></pre></li> <li>Set Environment Variable:       <pre><code>export PGSQL_DB=YES\n</code></pre></li> </ol>"},{"location":"integration/06-ace/#deploy-integration-runtime-instance-to-simulate-backend","title":"Deploy Integration Runtime Instance to Simulate BackEnd","text":"<pre><code>scripts/22c-ace-is-be-inst-deploy.sh\n</code></pre>"},{"location":"integration/06-ace/#create-configuration-for-user-defined-policy","title":"Create Configuration for User Defined Policy","text":"<pre><code>scripts/16-ace-config-policy-udp.sh\n</code></pre>"},{"location":"integration/06-ace/#create-configurations-related-to-email-server","title":"Create Configurations related to eMail server","text":"<pre><code>scripts/17a-ace-config-policy-email.sh\nscripts/17b-ace-config-setdbparms-email.sh\n</code></pre>"},{"location":"integration/06-ace/#deploy-integration-runtime-instance-related-to-es-email","title":"Deploy Integration Runtime instance related to ES &amp; eMail","text":"<pre><code>scripts/18a-ace-is-kafka-inst-deploy.sh\n</code></pre>"},{"location":"integration/06-ace/#other-optional-integration-runtime-examples","title":"Other OPTIONAL Integration Runtime Examples","text":"<p>Feel free to experiment with additional integration runtime instances below.</p>"},{"location":"integration/06-ace/#deploy-integration-runtime-instances-with-fry-approach-optional","title":"Deploy Integration Runtime instances with fry approach (Optional)","text":"<pre><code>oc apply -f instances/${CP4I_VER}/18a-ace-is-aceivt-instance-fry.yaml -n tools\noc apply -f instances/${CP4I_VER}/18b-ace-is-aceivt-instance-fry.yaml -n tools\n</code></pre>"},{"location":"integration/06-ace/#deploy-integration-runtime-instance-with-bake-approach-optional","title":"Deploy Integration Runtime instance with bake approach (Optional)","text":"<pre><code>oc apply -f instances/${CP4I_VER}/19a-ace-is-aceivt-instance-bake.yaml -n tools\n</code></pre> <p>Confirm the instance has been deployed successfully before moving to the next step running the following command:</p> <pre><code>oc get integrationruntimes -n tools\n</code></pre> <p>You should get a response like this:</p> <pre><code>NAME                        RESOLVEDVERSION   STATUS   REPLICAS   AVAILABLEREPLICAS   URL                                                                                                AGE     CUSTOMIMAGES\njgr-ace-bake-cp4i           12.0.9.0-r3       Ready    2          2                   http://jgr-ace-bake-cp4i-http-tools.apps.6597480c8e1478001153ba0d.cloud.techzone.ibm.com           4d      true\n</code></pre>"},{"location":"integration/06-ace/#configure-hpa-for-the-integration-runtime-previously-deployed-optional","title":"Configure HPA for the integration runtime previously deployed (optional)","text":"<pre><code>oc apply -f resources/09b-ace-hpa-demo.yaml -n tools\n</code></pre> Confirm HPA has been applied successfully running the following command: <pre><code>oc get hpa -n tools | grep ace-is-hpa-demo\n</code></pre> You should get a response like this: <pre><code>ace-is-hpa-demo    IntegrationRuntime/jgr-ace-bake-cp4i   0%/10%    2         5         2          3d23h\n</code></pre>"},{"location":"integration/06-ace/#create-configuration-for-server-config-related-to-mllp-mq-requestreply-optional","title":"Create configuration for Server Config related to MLLP &amp; MQ Request/Reply (optional)","text":"<pre><code>scripts/21-ace-config-server-config-ach.sh\n</code></pre>"},{"location":"integration/06-ace/#deploy-integration-runtime-with-none-http-protocol-aka-mllp-optional","title":"Deploy Integration Runtime with none HTTP protocol, aka MLLP (optional):","text":"<pre><code>oc apply -f instances/${CP4I_VER}/common/23-ace-is-ach-hl7-instance.yaml -n tools\n</code></pre>"},{"location":"integration/06-ace/#deploy-integration-runtime-instances-for-mq-requestreply-scenario-optional","title":"Deploy Integration Runtime instances for MQ Request/Reply scenario (optional):","text":"<pre><code>oc apply -f instances/${CP4I_VER}/27-ace-is-mqreqresp-backend-instance.yaml -n tools\noc apply -f instances/${CP4I_VER}/28-ace-is-mqreqresp-frontend-instance.yaml -n tools\n</code></pre>"},{"location":"integration/07-demo-config/","title":"Additional Configuration Steps for the CP4I Demo","text":""},{"location":"integration/07-demo-config/#additional-configuration-steps-for-the-cp4i-demo","title":"Additional Configuration Steps for the CP4I Demo","text":"<p>Extra Configuration</p> <p>You have completed the bootcamp deployment of CP4I and some components.  The below configuration scripts add Demo components.  Proceed if you would like to explore the Cloud Pak in Action!</p>"},{"location":"integration/07-demo-config/#configure-apic-for-demo","title":"Configure APIC for Demo","text":"<ol> <li>Publish draft assets:    <pre><code>scripts/14a-apic-create-apis-draft.sh\n</code></pre></li> <li>Configure Catalogs:    <pre><code>scripts/14b-apic-config-catalogs-publish-apis.sh\n</code></pre></li> <li>Create New Consumer Organization:    <pre><code>scripts/14c-apic-new-consumer-org.sh\n</code></pre></li> <li>Create Apps and Subscriptions:    <pre><code>scripts/14d-apic-create-apps-subscription.sh\n</code></pre></li> <li>Deploy Assembly with managed Integration Runtime and Declarative API Product with Security enabled in Demo Catalog (optional):       <pre><code>scripts/23c-assembly-inst-deploy.sh\n</code></pre></li> <li>Get Portal access info:    <pre><code>scripts/14f-apic-ptl-access-info.sh\n</code></pre></li> </ol>"},{"location":"integration/07-demo-config/#enable-aspera-optional","title":"Enable Aspera (Optional)","text":"<ol> <li>Install Redis Catalog Source:    <pre><code>oc apply -f catalog-sources/${CP4I_VER}/11-redis-catalog-source.yaml\n</code></pre>    Confirm the catalog source has been deployed successfully before moving to the next step running the following command:     <pre><code>oc get catalogsources ibm-cloud-databases-redis-operator-catalog -n openshift-marketplace -o jsonpath='{.status.connectionState.lastObservedState}';echo\n</code></pre>    You should get a response like this:    <pre><code>READY\n</code></pre></li> <li>Install Aspera Catalog Source:    <pre><code>oc apply -f catalog-sources/${CP4I_VER}/12-aspera-catalog-source.yaml\n</code></pre>    Confirm the catalog source has been deployed successfully before moving to the next step running the following command:     <pre><code>oc get catalogsources aspera-operators -n openshift-marketplace -o jsonpath='{.status.connectionState.lastObservedState}';echo   \n</code></pre>    You should get a response like this:    <pre><code>READY\n</code></pre></li> <li>Install Aspera Operator:    <pre><code>oc apply -f subscriptions/${CP4I_VER}/08-aspera-hsts-subscription.yaml\n</code></pre>    Confirm the operator has been deployed successfully before moving to the next step running the following command:    <pre><code>SUB_NAME=$(oc get deployment ibm-aspera-hsts-operator -n openshift-operators --ignore-not-found -o jsonpath='{.metadata.labels.olm\\.owner}');if [ ! -z \"$SUB_NAME\" ]; then oc get csv/$SUB_NAME --ignore-not-found -o jsonpath='{.status.phase}';fi;echo\n</code></pre>    You should get a response like this:    <pre><code>Succeeded\n</code></pre></li> </ol>"},{"location":"integration/07-demo-config/#install-license-service-optional","title":"Install License Service (Optional):","text":"<ol> <li>Install License Service Catalog Source:    <pre><code>oc apply -f catalog-sources/${CP4I_VER}/02a-license-service-catalog-source.yaml\n</code></pre>    Confirm the catalog source has been deployed successfully before moving to the next step running the following command:     <pre><code>oc get catalogsources ibm-licensing-catalog -n openshift-marketplace -o jsonpath='{.status.connectionState.lastObservedState}';echo\n</code></pre>    You should get a response like this:    <pre><code>READY\n</code></pre></li> <li>Create namespace:    <pre><code>oc create namespace ibm-licensing\n</code></pre></li> <li>Enable Operator Group in namespace:    <pre><code>oc apply -f resources/00-license-service-operatorgroup.yaml\n</code></pre></li> <li>Install License Service Operator:    <pre><code>oc apply -f subscriptions/${CP4I_VER}/00-license-service-subscription.yaml\n</code></pre>    Confirm the operator has been deployed successfully before moving to the next step running the following command:    <pre><code>SUB_NAME=$(oc get deployment ibm-licensing-operator -n ibm-licensing --ignore-not-found -o jsonpath='{.metadata.labels.olm\\.owner}');if [ ! -z \"$SUB_NAME\" ]; then oc get csv/$SUB_NAME -n ibm-licensing --ignore-not-found -o jsonpath='{.status.phase}';fi;echo\n</code></pre>    You should get a response like this:    <pre><code>Succeeded\n</code></pre>    Once the operator is ready check the instance has been deployed successfully running the following command:    <pre><code>oc get IBMLicensing instance -n ibm-licensing --ignore-not-found -o jsonpath='{.status.licensingPods[0].conditions[1].status}';echo\n</code></pre>    You should get a response like this:    <pre><code>True\n</code></pre></li> <li>Install License Reporter Catalog Source:    <pre><code>oc apply -f catalog-sources/${CP4I_VER}/02b-license-reporter-catalog-source.yaml\n</code></pre>    Confirm the catalog source has been deployed successfully before moving to the next step running the following command:     <pre><code>oc get catalogsources ibm-license-service-reporter-operator-catalog -n openshift-marketplace -o jsonpath='{.status.connectionState.lastObservedState}';echo\n</code></pre>    You should get a response like this:    <pre><code>READY\n</code></pre></li> <li>Install License Reporter Operator:    <pre><code>oc apply -f subscriptions/${CP4I_VER}/00-license-reporter-subscription.yaml\n</code></pre>    Confirm the operator has been deployed successfully before moving to the next step running the following command:    <pre><code>SUB_NAME=$(oc get deployment ibm-license-service-reporter-operator -n ibm-licensing --ignore-not-found -o jsonpath='{.metadata.labels.olm\\.owner}');if [ ! -z \"$SUB_NAME\" ]; then oc get csv/$SUB_NAME -n ibm-licensing --ignore-not-found -o jsonpath='{.status.phase}';fi;echo\n</code></pre>    You should get a response like this:    <pre><code>Succeeded\n</code></pre></li> <li>Deploy a License Reporter instance:    <pre><code>scripts/25-lic-reporter-inst-deploy.sh\n</code></pre>    Confirm the instance has been deployed successfully before moving to the next step running the following command:    <pre><code>oc get IBMLicenseServiceReporter ibm-lsr-instance -n ibm-licensing --ignore-not-found -o jsonpath='{.status.LicenseServiceReporterPods[0].conditions[1].status}';echo\n</code></pre>    After a few minutes you should get a response like this:    <pre><code>True\n</code></pre></li> <li>Configure Data Source:    <pre><code>scripts/04c-license-reporter-data-source-config.sh\n</code></pre></li> <li>Get License Service Reporter console access info:    <pre><code>scripts/99-lsr-console-access-info.sh\n</code></pre></li> </ol>"},{"location":"integration/07-demo-config/#install-serverless-optional","title":"Install Serverless (Optional)","text":"<ol> <li>Create Kafka Topics and User:    <pre><code>oc apply -f resources/90h-instanton-es-resources.yaml -n tools\n</code></pre></li> <li>Install Serverless Operator:    <pre><code>oc apply -f resources/90a-serverless-namespace.yaml\noc apply -f resources/90b-serverless-operatorgroup.yaml\noc apply -f resources/90c-serverless-subscription.yaml\n</code></pre>    Confirm the subscription has been completed successfully before moving to the next step running the following command:    <pre><code>SUB_NAME=$(oc get deployment knative-openshift -n openshift-serverless --ignore-not-found -o jsonpath='{.metadata.labels.olm\\.owner}');if [ ! -z \"$SUB_NAME\" ]; then oc get csv/$SUB_NAME -n openshift-serverless --ignore-not-found -o jsonpath='{.status.phase}';fi;echo\n</code></pre>    You should get a response like this:    <pre><code>Succeeded\n</code></pre></li> <li>Enable Knative Serving:    <pre><code>oc apply -f resources/90d-knative-serving.yaml\n</code></pre>    Confirm that the Knative Serving resources have been created before moving to the next step running the following command:    <pre><code>oc get knativeserving.operator.knative.dev/knative-serving -n knative-serving --template='{{range .status.conditions}}{{printf \"%s=%s\\n\" .type .status}}{{end}}'\n</code></pre>    You should get a response like this:    <pre><code>DependenciesInstalled=True\nDeploymentsAvailable=True\nInstallSucceeded=True\nReady=True\nVersionMigrationEligible=True\n</code></pre></li> <li>Enable Knative Eventing:    <pre><code>oc apply -f resources/90e-knative-eventing.yaml\n</code></pre>    Confirm that the Knative Serving resources have been created before moving to the next step running the following command:    <pre><code>oc get knativeeventing.operator.knative.dev/knative-eventing -n knative-eventing --template='{{range .status.conditions}}{{printf \"%s=%s\\n\" .type .status}}{{end}}'\n</code></pre>    You should get a response like this:    <pre><code>DependenciesInstalled=True\nDeploymentsAvailable=True\nInstallSucceeded=True\nReady=True\nVersionMigrationEligible=True\n</code></pre></li> <li>Install Knative broker for Apache Kafka:    <pre><code>scripts/90a-knative-kafka-config.sh\n</code></pre>    Confirm that the Knative broker for Apache Kafka resources have been created before moving to the next step running the following command:    <pre><code>oc get pods -n knative-eventing | grep kafka\n</code></pre>    You should get a response like this:    <pre><code>kafka-broker-receiver-7c7f46b44f-brhcp                    2/2     Running     0          6m7s\nkafka-channel-dispatcher-6cd7fc889f-2zbqf                 2/2     Running     0          6m9s\nkafka-channel-receiver-7896f6f5c4-t4gn5                   2/2     Running     0          6m8s\nkafka-controller-7bc9964786-tkkkg                         2/2     Running     0          6m8s\nkafka-controller-post-install-1.34.1-pjt8z                0/1     Completed   0          6m11s\nkafka-sink-receiver-84fb98cbb5-89j9l                      2/2     Running     0          6m7s\nkafka-webhook-eventing-bf985b789-4kwl9                    2/2     Running     0          6m8s\nknative-kafka-storage-version-migrator-1.34.1-cx2v8       0/1     Completed   0          6m11s\n</code></pre></li> <li>Prepare namespace:    <pre><code>oc new-project libertysurvey\noc create serviceaccount instanton-sa\noc apply -f resources/90f-instanton-scc.yaml\noc adm policy add-scc-to-user cap-cr-scc -z instanton-sa\n</code></pre></li> <li>Patch Knative Serving:    <pre><code>oc patch KnativeServing knative-serving -n knative-serving --type merge --patch-file resources/90g-instanton-knative-serving.yaml\n</code></pre></li> <li>Deploy Knative surveyInput Service:    <pre><code>scripts/90b-knative-service-surveyinput-config.sh\n</code></pre>    Check service is up and running before moving to the next step running the following command:    <pre><code>kn service list surveyinputservice\n</code></pre>    You should get a response like this:    <pre><code>NAME                 URL                                                                                           LATEST                     AGE   CONDITIONS   READY   REASON\nsurveyinputservice   https://surveyinputservice-libertysurvey.apps.6774a4edfa91429cbef53522.ocp.techzone.ibm.com   surveyinputservice-00001   19h   3 OK / 3     True \n</code></pre></li> <li>Deploy Knative surveyAdmin Service:</li> <li>Set the corresponding environment variable,        <pre><code>export GOOGLE_API_KEY=&lt;google_api_key&gt;\n</code></pre></li> <li>Execute script:       <pre><code>scripts/90c-knative-service-surveyadmin-config.sh\n</code></pre>       Check service is up and running before moving to the next step running the following command:       <pre><code>kn service list surveyadminservice\n</code></pre>       You should get a response like this:       <pre><code>NAME                 URL                                                                                           LATEST                     AGE   CONDITIONS   READY   REASON\nsurveyadminservice   https://surveyadminservice-libertysurvey.apps.6774a4edfa91429cbef53522.ocp.techzone.ibm.com   surveyadminservice-00001   18m   3 OK / 3     True  \n</code></pre></li> <li>Execute script:       <pre><code>scripts/90d-knative-kafkasource-surveyadmin-config.sh\n</code></pre>       Check service is up and running before moving to the next step running the following command:       <pre><code>kn source kafka describe geocodetopicsource\n</code></pre>       You should get a response like this:       <pre><code>Name:         geocodetopicsource\nNamespace:    libertysurvey\nAnnotations:  kubectl.kubernetes.io/last-applied-configuration={\"apiVersion\":\"sources.knative.dev/v1beta1\",\"kind\":\"KafkaSource\",\"metadata\":{\"annotations\":{},\"name\":\"geocodetopicsource\",\"namespace\":\"libertysurvey\"},\"spec\":{\"bootstrapServers\":[\"es-demo-kafka-bootstrap.tools.svc:9095\"],\"net\":{\"sasl\":{\"enable\":true,\"password\":{\"secretKeyRef\":{\"key\":\"password\",\"name\":\"kafkasource-es-demo-auth\"}},\"type\":{\"secretKeyRef\":{\"key\":\"saslType\",\"name\":\"kafkasource-es-demo-auth\"}},\"user\":{\"secretKeyRef\":{\"key\":\"user\",\"name\":\"kafkasource-es-demo-auth\"}}},\"tls\":{\"caCert\":{\"secretKeyRef\":{\"key\":\"ca.crt\",\"name\":\"kafkasource-es-demo-auth\"}},\"enable\":true}},\"sink\":{\"ref\":{\"apiVersion\":\"serving.knative.dev/v1\",\"kind\":\"Service\",\"name\":\"surveyadminservice\"},\"uri\":\"/api/cloudevents/geocodeComplete\"},\"topics\":[\"geocodetopic\"]}}\n\nAge:               4m\nBootstrapServers:  es-demo-kafka-bootstrap.tools.svc:9095\nTopics:            geocodetopic\nConsumerGroup:     knative-kafka-source-1f2fc8a0-6ce5-4fd9-93f1-b6c6538b53fa\n\nSink:         \nName:       surveyadminservice\nNamespace:  libertysurvey\nResource:   Service (serving.knative.dev/v1)\nURI:        /api/cloudevents/geocodeComplete\n\nConditions:  \nOK TYPE                   AGE REASON\n++ Ready                  17s \n++ ConsumerGroup          17s \n++ SinkProvided           17s \n++ OIDCIdentityCreated     4m authentication-oidc feature disabled ()\n</code></pre></li> <li>Deploy Knative surveyGeocoder Service:</li> <li>Set the corresponding environment variable,        <pre><code>export GOOGLE_API_KEY=&lt;google_api_key&gt;\n</code></pre></li> <li>Execute script:       <pre><code>scripts/90e-knative-service-surveygeocoder-config.sh\n</code></pre>       Check service is up and running before moving to the next step running the following command:       <pre><code>kn service list surveygeocoderservice\n</code></pre>       You should get a response like this:       <pre><code>NAME                 URL                                                                                           LATEST                     AGE   CONDITIONS   READY   REASON\nsurveyadminservice   https://surveyadminservice-libertysurvey.apps.6774a4edfa91429cbef53522.ocp.techzone.ibm.com   surveyadminservice-00001   18m   3 OK / 3     True  \n</code></pre></li> <li>Execute script:       <pre><code>scripts/90f-knative-kafkasource-surveygeocoder-config.sh\n</code></pre>       Check service is up and running before moving to the next step running the following command:       <pre><code>kn source kafka describe locationtopicsource\n</code></pre>       You should get a response like this:       <pre><code>Name:         locationtopicsource\nNamespace:    libertysurvey\nAnnotations:  kubectl.kubernetes.io/last-applied-configuration={\"apiVersion\":\"sources.knative.dev/v1beta1\",\"kind\":\"KafkaSource\",\"metadata\":{\"annotations\":{},\"name\":\"locationtopicsource\",\"namespace\":\"libertysurvey\"},\"spec\":{\"bootstrapServers\":[\"es-demo-kafka-bootstrap.tools.svc:9095\"],\"net\":{\"sasl\":{\"enable\":true,\"password\":{\"secretKeyRef\":{\"key\":\"password\",\"name\":\"kafkasource-es-demo-auth\"}},\"type\":{\"secretKeyRef\":{\"key\":\"saslType\",\"name\":\"kafkasource-es-demo-auth\"}},\"user\":{\"secretKeyRef\":{\"key\":\"user\",\"name\":\"kafkasource-es-demo-auth\"}}},\"tls\":{\"caCert\":{\"secretKeyRef\":{\"key\":\"ca.crt\",\"name\":\"kafkasource-es-demo-auth\"}},\"enable\":true}},\"sink\":{\"ref\":{\"apiVersion\":\"serving.knative.dev/v1\",\"kind\":\"Service\",\"name\":\"surveygeocoderservice\"},\"uri\":\"/api/cloudevents/locationInput\"},\"topics\":[\"locationtopic\"]}}\n\nAge:               8s\nBootstrapServers:  es-demo-kafka-bootstrap.tools.svc:9095\nTopics:            locationtopic\nConsumerGroup:     knative-kafka-source-37ddc05c-2e60-4eb0-ba6b-78a0f77cdbd0\n\nSink:         \nName:       surveygeocoderservice\nNamespace:  libertysurvey\nResource:   Service (serving.knative.dev/v1)\nURI:        /api/cloudevents/locationInput\n\nConditions:  \nOK TYPE                   AGE REASON\n++ Ready                   7s \n++ ConsumerGroup           7s \n++ SinkProvided            7s \n++ OIDCIdentityCreated     7s authentication-oidc feature disabled ()\n</code></pre></li> </ol>"},{"location":"integration/07-demo-config/#install-keda-optional","title":"Install KEDA (Optional)","text":"<ol> <li> <p>Install KEDA using the deployment YAML files:</p> <p>Note: At the moment we can not use the Operator Custom Metrics Autoscaler provided by RedHat because it is using v2.14 which does not support self-signed certificates, so instead we need to use v2.16 from the KEDA community project that already supports self-signed certificates. Once the RedHat Operator supports v2.16 I'll update the instructions.   </p><pre><code>oc apply --server-side -f https://github.com/kedacore/keda/releases/download/v2.16.1/keda-2.16.1.yaml\n</code></pre>   Check deployment is ready before moving to the next step running the following commands:   <pre><code>oc get deployment keda-admission -n keda --ignore-not-found -o json | jq -r '.status.conditions[] | select(.type | test(\"Available\")).status'\n</code></pre>   Once you get a response like this run the next command:   <pre><code>True\n</code></pre>   Run this command:   <pre><code>oc get deployment keda-operator -n keda --ignore-not-found -o json | jq -r '.status.conditions[] | select(.type | test(\"Available\")).status'\n</code></pre>   Once you get a response like this go to the next step:   <pre><code>True\n</code></pre> 2. Deploy MQ resources:    <pre><code>scripts/91a-qmgr-rest-api-pre-config.sh\nscripts/91b-qmgr-rest-api-inst-deploy.sh\n</code></pre>    Confirm the instance has been deployed successfully before moving to the next step running the following command:    <pre><code>oc get queuemanager qmgr-rest-api -n tools -o jsonpath='{.status.phase}';echo\n</code></pre>    Note this will take few minutes, but at the end you should get a response like this:    <pre><code>Running\n</code></pre> 3. Deploy ACE resources:    <pre><code>scripts/91c-ace-keda-config-policies.sh\noc apply -f instances/${CP4I_VER}/common/26-ace-is-mqivt-instance.yaml -n tools\n</code></pre>    Confirm the instance has been deployed successfully before moving to the next step running the following command:    <pre><code>oc get integrationruntime jgr-mqivt-keda -n tools -o jsonpath='{.status.phase}';echo\n</code></pre>    Note this will take a moment, but at the end you should get a response like this:    <pre><code>Ready\n</code></pre> 4. Deploy KEDA resources:    <pre><code>scripts/91d-keda-resources-config.sh\n</code></pre>    Confirm the resources have been deployed successfully running the following command:    <pre><code>oc get scaledobjects ace-keda-demo --ignore-not-found -n tools -o json | jq -r '.status.conditions[] | select(.type | test(\"Ready\")).status'\n</code></pre>    You should get a response like this:    <pre><code>True\n</code></pre><p></p> </li> </ol>"},{"location":"integration/tech-jam-labs/ace-es/","title":"Configuration to allow an Integration Flow developed with ACE Toolkit to connect to Event Streams","text":""},{"location":"integration/tech-jam-labs/ace-es/#configuration-to-allow-an-integration-flow-developed-with-ace-toolkit-to-connect-to-event-streams","title":"Configuration to allow an Integration Flow developed with ACE Toolkit to connect to Event Streams","text":"<p>This article explains what configuration is needed to deploy an Integration Flow developed with ACE Toolkit that uses the Kafka Nodes to connect to an Event Streams cluster using the latest version of the ACE Integration Server Certified Container (ACEcc) as part of the IBM Cloud Pak for Integration (CP4I) v2021.2.</p> <p>Note</p> <p>These instructions have been updated to support the versions of Event Streams included with CP4I v2021.3 and v2021.4. The update also works with previous versions. </p>"},{"location":"integration/tech-jam-labs/ace-es/#create-a-topic-in-event-streams","title":"Create a Topic in Event Streams","text":"<ol> <li> <p>Navigate to the Event Streams instance from the CP4I Platform Navigator. Click on the corresponding instance. In the below image it is called es-demo.     </p> </li> <li> <p>From the Event Streams Home page click on the Create a topic tile.     </p> </li> <li> <p>The wizard guides you during the process.  In the first screen type the topic name.  In image below kafka-cp4i-demo-topic is used. You can use any name, you will use it later on when configuring your Flow in the Toolkit. </p> </li> <li> <p>You can review the different options, but for simplicity I'm accepting the defaults, so you can simply click Next.     </p> </li> <li> <p>You can change the retention period, but I'm accepting the default clicking Next without manking any change.     </p> </li> <li> <p>Click Create Topic to complete the wizard.      </p> </li> <li> <p>The new topic will be displayed in the Topics page. First confirm the topic was created and then return to the home page clicking on the Home icon.     </p> </li> </ol>"},{"location":"integration/tech-jam-labs/ace-es/#create-scram-credentials-to-connect-to-event-streams","title":"Create SCRAM Credentials to Connect to Event Streams","text":"<ol> <li> <p>From the Event Streams Home page click on the Connect to the cluster tile.      </p> </li> <li> <p>The wizard will guide you during the process. First, make sure you have selected External.  Then, copy the bootstrap information and paste it in a safe place since we will need it later on. Finally, click on Generate SCRAM credentials.   </p> </li> <li> <p>In the pop up window type the name of the user you want to create. In my case I have used ace-es-user-scram.  Click Next.     </p> </li> <li> <p>The example grants full access to the new user, but you can control the level of access as needed.  Once you adjust the values click Next.      </p> </li> <li> <p>Adjust the values as needed and click Next.    </p> </li> <li> <p>Select All transactional IDs or the value you prefer and then clik Generate credentials. </p> </li> <li> <p>After a moment the credentials are generated and presented to you.  Make sure to copy the password and paste it into a notepad since you will use it later.     </p> </li> <li> <p>Scroll down to the Certificates section and click on Download certificate.     </p> </li> <li> <p>Navigate to the folder where you want to save the certificate and click Save.     </p> </li> <li> <p>Once the certificate is saved the corresponding password will be displayed.  Click on the Copy icon to paste it into a notepad for future use.     </p> </li> <li> <p>Scroll up and close the wizard.      </p> </li> <li> <p>Before you move to ACE you need to convert the certificate format from <code>PKCS12</code> to <code>JKS</code>. Open a Terminal and navigate to the location where you stored the certificate in step 8 and run the following command changing the cert-password value for the actual password you got in step 10. </p> <pre><code>keytool -importkeystore -srckeystore es-cert.p12 -srcstoretype PKCS12 -destkeystore es-cert.jks  -deststoretype JKS -srcstorepass &lt;cert-password&gt; -deststorepass &lt;cert-password&gt; -srcalias ca.crt -destalias ca.crt -noprompt\n</code></pre> <p>Now you are ready to move to ACE and create the required configuration using the information you have collected.</p> </li> </ol>"},{"location":"integration/tech-jam-labs/ace-es/#create-resources-in-the-ace-toolkit","title":"Create Resources in the ACE Toolkit","text":"<p>For this exercise a simple project has been created that exposes an API to POST information into a Kafka topic.  The following picture shows the corresponding flow. If you want to use this sample, you can find the Project Interchange here</p> <p></p> <p>First, we will create a Policy Project to store the Event Streams configuration.</p> <ol> <li> <p>From the Application Development pane click on New and select Policy Project.      </p> </li> <li> <p>In the pop-up window enter the name of the project, i.e. <code>CP4IESDEMOSCRAM</code> and click Finish.     </p> </li> <li> <p>The project appears in the Application Development pane. Expand the project and click (New...). Then select Policy from the New Artifact menu.     </p> </li> <li> <p>In the pop-up window enter the name of the policy, i.e. es-demo and click Finish.     </p> </li> <li> <p>The Policy Editor opens for you to enter the data.  Click on the Type drop down box and select Kafka.     </p> <p>The default values from the Kafka Template are used to pre-populate the policy. </p> </li> <li> <p>Update the policy using the following values:</p> <p>! Property | Value | ! ---------|------- | ! Bootstrap servers | &lt;Value copied from previos section&gt;  | ! Security protocol | <code>SASL_SSL</code>  | ! SASL Mechanism | <code>SCRAM-SHA-512</code>  | ! SSL protocol | <code>TLSv1.2</code>  | ! Security identity (DSN) | any value, i.e. <code>aceflowsSecId</code>  | ! SASL config | <code>org.apache.kafka.common.security.scram.ScramLoginModule required;</code>  | ! SSL truststore location | <code>/home/aceuser/truststores/es-cert.jks</code> | ! SSL truststore type | <code>JKS</code> | ! SSL truststore security identity | <code>truststorePass</code> | ! Enable SSL certificate hostname checking | <code>true</code> |</p> <p>The policy should look similar to this: </p> </li> <li> <p>Don't forget to Save your changes.</p> </li> <li> <p>Go back to the Flow Editor to update the flow. Select the KafkaProducer node and update the properties in the Basic tab. Use the following information as a reference:</p> <p>! Property | Value | ! ---------|------- | ! Topic name | &lt;Value used in the first section&gt; i.e.<code>*kafka-cp4i-demo-topic</code> | ! Bootstrap servers | We have configured this value in the policy, but since this is a mandatory field we will enter <code>dummy</code> | ! Client ID | any value, i.e. <code>cp4iace-esapi-scram</code> |</p> <p>The tab should look similar to this: </p> </li> <li> <p>Select the Policy tab and in the window click Browse.  From the pop-up window select the policty we just created and click OK.     </p> </li> <li> <p>The policy with be displayed in the Policy field.  Save your project to get rid of the error message.     </p> </li> <li> <p>Now we will create the BAR file to deploy our flow.  Navigate to the File menu and select New and then BAR file </p> </li> <li> <p>In the pop-up window enter the name of the <code>BAR</code> file, i.e. jgr-cp4i-esapi-scram and click Finish.     </p> </li> <li> <p>The BAR Editor is open. Select Applications, shared libraries, services, REST APIs, and Test Projects to display the right type of resources and then select your application, in this case <code>JGRESAPISCRAM</code>. Finally click Build and Save... to create the<code>BAR</code> file.     </p> </li> <li> <p>You can dismiss the notification window reporting the result clicking OK.     </p> </li> <li> <p>Before we head to the ACE Dashboard we need to do a final step. We need to create a zip file from the Policy Project we created earlier in this section.  For that you need to navigate to the folder where your ACE Toolkit workspace is located (in my case that use Mac it is located at /Users/&lt;your user&gt;/IBM/ACET12/&lt;your workspace&gt;) and use your tool of choice to compress the folder where the policy project is located.  The following picture shows a possible option.     </p> </li> <li> <p>Take a note where the zip file is located because we will use it in the next section.     </p> </li> </ol> <p>Now we can move to the next phase.</p>"},{"location":"integration/tech-jam-labs/ace-es/#create-configurations-in-ace-dashboard","title":"Create Configurations in ACE Dashboard","text":"<ol> <li> <p>From the Platform Navigator click on the ACE Dashboard you want to work. In the example case it is called ace-dashboard to avoid any confusion.     </p> </li> <li> <p>Click on the Wrench icon to navigate to the Configuration section.     </p> </li> <li> <p>Once you are in the Configuration window, click on Create configuration.     </p> </li> <li> <p>In the wizard click on the Type drop down box to select Truststore to upload the Event Streams certificate.     </p> </li> <li> <p>In the next window enter the name of the certificate you created in the previous section (<code>es-cert.jks</code>), then a brief description if you want to.  In the example case, es-demo JKS certificate is used.  Finally, click on Drag and drop a single file here or click to import.     </p> </li> <li> <p>Navigate to the folder where you created the JKS certificate in the previous section and select the file and click Open.     </p> </li> <li> <p>Finally, click Create to add the TrustStore Configuration.     </p> </li> <li> <p>Mext we will define the Configuration to store the credentials selecting the setdbparms.txt type from the drop down box.     </p> </li> <li> <p>In the next window enter the name of the configuration. In my case I used cp4i-esdemo-scram-credentials. Followed by a brief description i.e. Credential to connect to es-demo using SCRAM.  Finally, use the following information as a reference to enter the data in the text box.</p> Resource Name User Password truststore::truststorePass dummy &lt;Password obtained in step 10 of section \"Create SCRAM credentials to connect to Event Streams\"&gt; kafka::aceflowsSecId &lt;Name of user created in step 3 of section \"Create SCRAM credentials to connect to Event Streams\", i.e. ace-es-user-scram&gt; &lt;Password obtained in step 7 of section \"Create SCRAM credentials to connect to Event Streams\"&gt; </li> <li> <p>Next, create the Configuration for the Policy selecting the Policy project type from the drop down box.     </p> </li> <li> <p>In the next window enter the name of the configuration, i.e jgr-es-demo-scram-policy, a brief description if you want to, i.e. Policy to connect to Event Streams instance es-demo using SCRAM and finally click on hyper link Drag and drop a single file here or click to import to import the zip file we created in the previous section.      </p> </li> <li> <p>Navigate to the folder where you saved the zip file in the previous section, select the file and click Open.     </p> </li> <li> <p>Finally, click Create to save the configuration for the policy project.     </p> </li> </ol> <p>We are ready to move to the final phase of the process.</p>"},{"location":"integration/tech-jam-labs/ace-es/#deploy-bar-file-using-ace-dashboard","title":"Deploy BAR file using ACE Dashboard","text":"<ol> <li> <p>From the ACE Dashboard home page navigate to the <code>BAR</code> files section clicking the Document icon and then click Import BAR button.     </p> </li> <li> <p>In the wizard click the hyperlink Drag and drop a BAR file or click to upload to upload the BAR file we created in the previous section.     </p> </li> <li> <p>Navigate to the folder where the BAR file was created (on macOS you can find it at /Users/&lt;your user&gt;/IBM/ACET12/&lt;your workspace&gt;/BARfiles) and select the file and click Open.     </p> </li> <li> <p>To complete the process click Import.     </p> </li> <li> <p>After a moment the file will be displayed in the list of BAR files available with the status of Not deployed </p> </li> <li> <p>Now navigate to the Dashboard section and click on the Create server button.     </p> </li> <li> <p>This will start the deployment wizard. Select the Quick start toolkit integration tile and then click the Next button.   </p> </li> <li> <p>In the next window click on the drop down box to select the BAR file we previously uploaded and then click Next.     </p> </li> <li> <p>In the next window select the 3 configurations we created in the previous section and then click Next.     </p> </li> <li> <p>In the next page give your deployment a name, i.e jgr-es-api-scram and click Create to start the deployment.     </p> </li> <li> <p>After a moment you will be taken back to the Dashboard page where you will see a new tile with your Integration Server deployment name in Pending state, similar to the picture shown below:     </p> </li> <li> <p>The App Connect Dashboard is deploying the Integration Server in the background creating the corresponding pod in the specified namespace of the Red Hat OpenShift Container Platform. This process can take more than a minute depending on your environment. Click the refresh button in your browser until you see the tile corresponding to your deployment in Ready state as shown below:     </p> <p>Now is time to test everything is working as expected. Click on the tile corresponding to your deployment.</p> </li> <li> <p>The next window shows the API in started state. Click on the tile to get the details.     </p> </li> <li> <p>In the next page navigate to the operation implemented in your flow (POST /contacts in my case). Then click on the Try it tab followed by the Generate hyperlink to generate some test data. Review the data generated and click the Send button.     </p> </li> <li> <p>After a moment you will get a response back.  If everything is OK you will receive a 200 response code indicating the request was successful and the same payload you sent plus some metadata including a timestamp showing the request completed few seconds ago. </p> <p>Note</p> <p>Some people has reported issues at this stage that could be caused for the resources assigned to the Integration Server at deployment time. In this case, you can wait a little bit longer, like 3 or 4 more minutes, or you can redeploy the Integration Server increasing the CPU and Memory assigned to it.</p> <p></p> </li> <li> <p>To fully confirm the message landed on the corresponding topic, you can navigate to the Event Streams instance and check the message is there.  Once you are in the Event Streams Home page click on the Topics icon in the navigation section and then click on the topic you used in your flow.  In the example:  kafka-cp4i-demo-topic.     </p> </li> <li> <p>In the topic page navigate to the Messages tab, then select the most recent message and confirm the payload includes the same data we sent during the test.     </p> </li> </ol> <p>The configuration has been completed.</p>"},{"location":"integration/tech-jam-labs/ace-es/#review","title":"Review","text":"<p>In this exercise you performed the steps required to integrate Event Streams with App Connet Enterprise to support an Event Driven Architecture. </p>"},{"location":"integration/tech-jam-labs/ace-mq/","title":"Develop a REST API using ACE Toolkit to interact with MQ","text":""},{"location":"integration/tech-jam-labs/ace-mq/#develop-a-rest-api-using-ace-toolkit-to-interact-with-mq","title":"Develop a REST API using ACE Toolkit to interact with MQ","text":"<p>This article explains the steps need to create an Integration Flow developed with ACE Toolkit that uses the <code>REST API</code> functionality, as well as, the <code>MQ Nodes</code> to interact with an MQ Queue Manager using the latest version of the ACE Integration Server Certified Container (ACEcc) as part of the IBM Cloud Pak for Integration (CP4I).  This is considered \"Low-Code / No-Code\" development with ACE Toolkit.   You deployed the toolkit to your workstation as part of the prerequisite steps.</p>"},{"location":"integration/tech-jam-labs/ace-mq/#creating-a-rest-api","title":"Creating a REST API","text":"<p>Open the Toolkit in your workstation and create a new <code>REST API</code> project as shown below.</p> <p></p> <p>Give a name to your project, i.e. <code>MQAPI</code> and then select the option to Import resources since we will leverage a definition that has already been created.</p> <p></p> <p>In the wizard click <code>Browse</code> and navigate to the location where the <code>OpenAPI</code> file definition is located.</p> <p></p> <p>Once the path is displayed in the <code>Location</code> field, click Finish.</p>"},{"location":"integration/tech-jam-labs/ace-mq/#add-flow-logic","title":"Add Flow Logic","text":"<p>The REST API is displayed.  Scroll to the right if needed to open the <code>sunflow</code> operation as shown in the image to proceed to implement the API logic.</p> <p></p> <p>The Message Flow Editor will be open with only the <code>Input</code> and <code>Output</code> terminals.  Double click the tab to maximize the editor and work with the flow.</p> <p></p> <p>Drag and drop the following <code>Nodes</code> from the palette to implement the \"logic\":</p> <ul> <li>Flow Order Node</li> <li>HTTP Header Node</li> <li>MQ Header Node</li> <li>MQ Output Node</li> <li>Mapping Node</li> </ul> <p>Wire the nodes. The flow should appear similar to the one below. Once you have completed this, double click the tab again in order to access the properties for each node.</p> <p></p>"},{"location":"integration/tech-jam-labs/ace-mq/#configure-the-nodes","title":"Configure the Nodes","text":"<p>Next, configure each node, starting with the <code>HTTP Header Node</code>. Click on it to bring it to focus and then select the <code>HTTP Input</code> tab followed by the <code>Delete header</code> option as shown below.</p> <p></p> <p>Now select the <code>MQ Header Node</code> and navigate to the <code>MQMD</code> tab enabling the <code>Add header</code> option. And selecting <code>QMFT_STRING</code> for the Format field as shown below.</p> <p></p> <p>Then select the <code>MQ Output Node</code> and in the Basic tab enter the name of the queue we will use to put the messages, in this case <code>CP4I.DEMO.API.Q</code></p> <p></p> <p>In the same <code>MQ Output Node</code> navigate to the <code>MQ Connection</code> tab enter the information to connect to the Queue Manager. The information is based upon the configuration you used for MQ, and for simplicity is included below.</p> Property Value Connection MQ client connection properties Destination queue manager name QMGRDEMO Queue manager host name qmgr-demo-ibm-mq Listener port number 1414 Channel name ACE.TO.MQ <p></p> <p>Double click the <code>Mapping Node</code>,</p> <p></p> <p>In the wizard window simply click Finish.</p> <p></p> <p>Expand the JSON section in both the input and output message assemblies and connect the payload as shown below.  </p> <p></p> <p>Next, right click the <code>code</code> field and select Add Assign from the menu.</p> <p></p> <p>In the properties section enter <code>CP4I0000</code> in the value field.</p> <p></p> <p>Repeat the same process for field msg and assign the value <code>Request has been processed</code></p> <p>Do the same for field <code>time</code> but this time we will replace the <code>Assign</code> option with the <code>current-time</code> function as shown below.</p> <p></p> <p>The integration flow is completed. Save your progress and close the <code>mapping</code> tab.</p> <p></p>"},{"location":"integration/tech-jam-labs/ace-mq/#generate-the-bar-file","title":"Generate the BAR File","text":"<p>Finally, generate the <code>BAR</code> file that we will use to deploy the Integration into CP4I. From the File menu select New and then <code>BAR file</code> as shown below. You must check the box for <code>REST APIs</code> and </p> <p></p> <p>Check the boxes for <code>REST APIs</code> and <code>MQAPI</code>.  Click Build and Save. </p> <p>In the pop up window enter the name of the <code>BAR</code> file, in this case <code>CP4IACEMQAPIPREM</code>. And then click Finish.</p> <p></p> <p>Click OK in the confirmation window and the <code>BAR</code> file is ready to be deployed.</p> <p></p>"},{"location":"integration/tech-jam-labs/ace-mq/#deploy-the-bar-file","title":"Deploy the BAR File","text":"<ol> <li> <p>From the ACE Dashboard home page navigate to the <code>BAR</code> files section clicking the Document icon and then click Import BAR button.</p> </li> <li> <p>In the wizard click the hyperlink Drag and drop a BAR file or click to upload to upload the BAR file we created in the previous section.     </p> </li> <li> <p>Navigate to the folder where the BAR file was created (on macOS you can find it at /Users/&lt;your user&gt;/IBM/ACET12/&lt;your workspace&gt;/BARfiles) and select the file and click Open.</p> </li> <li> <p>To complete the process click Import.</p> </li> <li> <p>After a moment the file will be displayed in the list of BAR files available with the status of Not deployed</p> </li> <li> <p>Now navigate to the Dashboard section and click on the Create server button.</p> </li> <li> <p>This will start the deployment wizard. Select the Quick start toolkit integration tile and then click the Next button.</p> </li> <li> <p>In the next window click on the drop down box to select the BAR file we previously uploaded and then click Next.</p> </li> <li> <p>In the next window select the 3 configurations we created in the previous section and then click Next.</p> </li> <li> <p>In the next page give your deployment a name, i.e jgr-mqapi-prem and click Create to start the deployment.</p> </li> <li> <p>After a moment you will be taken back to the Dashboard page where you will see a new tile with your Integration Server deployment name in Pending state, similar to the picture shown below:</p> </li> <li> <p>The App Connect Dashboard is deploying the Integration Server in the background creating the corresponding pod in the specified namespace of the Red Hat OpenShift Container Platform. This process can take more than a minute depending on your environment. Click the refresh button in your browser until you see the tile corresponding to your deployment in Ready state as shown below:</p> <p>Now is time to test everything is working as expected. Click on the tile corresponding to your deployment.</p> </li> <li> <p>The next window shows the API in started state. Click on the tile to get the details.</p> </li> </ol> Question <p>How would you test the deployed flow to verify you were successful?  You can look ahead to the next lab for some hints.</p> <p>The configuration has been completed.</p>"},{"location":"integration/tech-jam-labs/apic-analyze/","title":"Analyzing the usage of Your API","text":""},{"location":"integration/tech-jam-labs/apic-analyze/#analyzing-the-usage-of-your-api","title":"Analyzing the usage of Your API","text":"<p>Personas played:</p> <p></p> <ul> <li>API Life Cycle Manager - Jason logs into the API Manager as the admin user on this lab for simplicity.  In this section, Jason logs into the API Manager and view the analytics for the API to see how it has performed who Andre was using it.</li> </ul>"},{"location":"integration/tech-jam-labs/apic-analyze/#navigate-to-the-analytics-portal","title":"Navigate to the Analytics Portal","text":"<p>Login to the API manager using the admin user.</p> <p>From the API Manger home page click Manage Catalogs -&gt; Demo catalog -&gt; Analytics.</p> <p>Observe the available dashboards.</p> <p></p>"},{"location":"integration/tech-jam-labs/apic-analyze/#look-at-the-available-dashboards","title":"Look at the Available Dashboards","text":"<p>Select the <code>Product Dashboard</code>.</p> <p>Note</p> <p>This dashboard provides useful information such as the number of Consumer Organizations using the API Product, the number of applications using the product and what plan they are using, plus the number of API calls.</p> <p></p> <p>Click Back, then select the <code>API Dashboard</code>. This dashboard also shows useful information, such as API response times, the status codes that have been returned and number of errors.</p> <p>Note</p> <p>If you have exceeded the number of API requests allowed on the default plan, this will show in the errors.</p> <p></p> <p>Now click Back and explore the other dashboards.</p>"},{"location":"integration/tech-jam-labs/apic-analyze/#view-with-operations-dashboard","title":"View with Operations Dashboard","text":"<p>View the overall end-to-end journey in the CP4I Operations Dashboard.  Navigate back to the Cloud Pak for Integration Platform UI (Navigator) - to do this you can click IBM Cloud Pak at the top-left of the screen.</p> <p>On the left-hand side of the Platform UI, in the list under <code>Overview</code>, click Integration Tracing.</p> <p></p> <p>The Operations Dashboard will load.  If a license agreement displays accept this. If a <code>What's new</code> message displays click x.</p> <p>The Overview page will be displayed. You will be able to see a variety of tracing information about <code>APIC</code>, <code>ACE</code> and <code>MQ</code>.</p> <p></p> <p>On the left-hand side of the screen, click Traces. A series of traces will display.  A lot of them will be from the API Connect Healthcheck <code>/webapi-init-check</code> - this is a default behavior and is not from your application.</p> <p>Submit another request to the <code>Premium</code> plan API in SoapUI.  Then click the refresh button on the top right of the <code>Traces</code> screen.</p> <p></p> <p>After 1-2 refreshes you should see a trace that lists multiple components (bottom line in the below image).  If you only see API Connect in the trace refresh again.</p> <p></p> <p>Click on the trace with the multiple components.  You will see a diagram showing the journey of your request. </p> Errors? <p>Do you see some errors?  These are expected errors that we have left in the demo deliberately!</p> <p></p> <p>From here you can continue to explore the options on the CP4I Dashboard.</p> <p>You have now completed the API Connect Labs</p>"},{"location":"integration/tech-jam-labs/apic-consume/","title":"Consuming our new API","text":""},{"location":"integration/tech-jam-labs/apic-consume/#consuming-our-new-api","title":"Consuming our new API","text":"<p>Personas played:</p> <p></p> <ul> <li><code>Cloud Manager</code> - 'Will' - logs into the API Manager as the admin user on this lab for simplicity</li> </ul> <p></p> <ul> <li><code>Community Manager</code> - 'Marsha' - logs into the Developer Portal as admin user on this lab for simplicity</li> </ul> <p></p> <ul> <li><code>API Life Cycle Manager</code> - Jason - logs into the API Manager as the admin user on this lab for simplicity</li> </ul> <p></p> <ul> <li><code>App Developer</code> - Andre - logs into the Developer Portal as <code>andre_integration</code></li> </ul> <p>In this section, as the Cloud Manager - Will, you will setup a <code>Developer Portal</code> for Marsha and Andre to login to, so that Marsha can manage the developer community and so that Andre can subscribe to API Products and start using Shavon's API in a well-managed and governed way.</p> <p>You will then proceed to use and test the API as Andre the App Developer.</p> <p>We will see that for our <code>Default</code> plan the <code>API Life Cycle Manager</code> persona, Jason, does not approve the subscription made by Andre. For the <code>Premium</code> subscription you will see that Jason does need to approve Andre's subscription.  </p>"},{"location":"integration/tech-jam-labs/apic-consume/#create-the-developer-portal-for-the-demo-api-catalog","title":"Create the Developer Portal for the Demo API Catalog","text":"<p>In this section we will:</p> <ul> <li>As the API Life Cycle Manager, Jason, create a Developer Portal for the <code>Demo</code> Catalog - API consumers (App Developers like Andre) will be able to log into here and use the APIs in a well governed and managed way</li> <li>As the Community Manager, Marsha, accept an invitation to login to the Developer Portal</li> <li>As the API Life Cycle Manager - Jason, setup a <code>Consumer Organization</code> or <code>cOrg</code> and define a member of the <code>cOrg</code> for Andre called <code>andre_integration</code></li> <li>As Andre, login to the Developer Portal</li> </ul> <p>Note</p> <p>In this lab several of the personas are using the <code>admin</code> user. This reduces the number of logins we need to use for this lab. In real life each persona would have their own login, normally backed by LDAP.</p>"},{"location":"integration/tech-jam-labs/apic-consume/#create-the-developer-portal-playing-part-of-jason-api-life-cycle-manager","title":"Create the Developer Portal (playing part of Jason API Life Cycle Manager)","text":"<p>Navigate back to the API Manager Homepage by Clicking on the <code>Home</code> icon in the top-left corner of the screen.</p> <p>Next click on Manage catalogs.</p> <p>Click on the tile for your <code>Demo</code> catalog.</p> <p>On the <code>Catalog settings</code> tab in the catalog, select <code>Portal</code> and create to create the Portal application developers will use to access the APIs in the catalog.</p> <p></p> <p>The <code>Create portal</code> page is displayed. Select the following options:</p> <ul> <li>Select the portal service to use for this catalog : <code>portal-service</code></li> <li>URL : select the option in the list <code>https://apim-demo-ptl-portal-web-tools.&lt;your-ocp-cluster&gt;.appdomain.cloud/cp4i-demo-provider-org/demo</code></li> </ul> <p></p> <p>Click Create.</p> <p>A message is displayed explaining that the Developer Portal is being provisioned.</p> <p></p>"},{"location":"integration/tech-jam-labs/apic-consume/#accept-the-invitation-to-the-developer-portal-playing-part-of-marsha-the-community-manager","title":"Accept the invitation to the Developer Portal (playing part of Marsha the Community Manager)","text":"<p>Check Mailtrap and use the link to login to the portal as the administrator.</p> <p></p> <p>You will be asked to change the <code>cOrg</code> administrator password. Specify <code>IntegrationIsFun21!</code></p> <p>You will then be logged into the Developer Portal as the <code>admin</code> user.</p> <p></p> <p>You will see that you have the <code>JGRMQAPI Product 1.2.0</code> product available. Click on this, then click on the API to get to the overview page.</p> <p>Click POST /contacts &gt; Try it &gt; Register an application to test this API</p> <p>You will then be given a permission denied error. This is because <code>cOrg</code> admins cannot consume the APIs (due to separations of responsibility), instead you must add a member to the cOrg who can do this.</p> <p></p>"},{"location":"integration/tech-jam-labs/apic-consume/#create-consumer-organization-in-the-api-manager-playing-part-of-jason-api-life-cycle-manager","title":"Create Consumer Organization in the API Manager (playing part of Jason API Life Cycle Manager)","text":"<p>In this step we will:</p> <ul> <li>Return to the API Manager, as the <code>admin admin</code> user</li> <li>Navigate to the <code>Demo</code> catalog</li> <li>Create a new Consumer Organization or cOrg</li> <li>Invite an API Consumer - Andre to the <code>cOrg</code></li> </ul> <p>Login to the API Manager as the <code>admin</code> user. Navigate back to the API Manager Homepage by Clicking on the <code>Home</code> icon in the top-left corner of the screen.</p> <p>Click on Manage Catalogs &gt; Demo &gt; Consumers. Then click <code>Add</code> and select Create organization.</p> <p></p> <p>The <code>Create consumer organization</code> page will load.</p> <p>Specify the following details:</p> <ul> <li>Title : <code>DemoOrg-Sales</code></li> <li>User registry : <code>Demo Catalog User Registry</code></li> <li>Type of user : select <code>New user</code></li> <li>Username : <code>andre_integration</code></li> <li>Email : <code>andre.integration@cp4i.demo.net</code></li> <li>First name : <code>Andre</code></li> <li>Last name : <code>Integration</code></li> <li>Password : <code>IntegrationIsFun21!</code></li> <li>Confirm Password : <code>IntegrationIsFun21!</code></li> </ul> <p>Click Create. The <code>Consumers</code> page for the <code>Demo</code> Catalog will now show with the <code>DemoOrg-Sales</code> consumer organization showing.</p> <p>You will see it has no subscriptions.</p> <p></p> <p>Now let's login to the Developer Portal as Andre Application Developer. Go to your private browser where you had the Developer Portal open. If you are still signed in as <code>admin</code> the logout (top-right of the screen).</p> <p>You should be at the sign-in page. Add in Andre's login details as shown below:</p> <p></p> <p>When you login for the first time you will be asked to create an application, click Cancel (we will come back to that).</p> <p>The Developer Portal homepage is displayed.</p> <p>Note the following:</p> <ol> <li>In the top banner you can review <code>API Products</code> and <code>Apps</code> + you can add blogs, start a forum and seek support (faqs).</li> <li>In the top-right the name of the <code>Consumer Organization</code> is displayed: <code>DemoOrg-Sales</code></li> <li>In the top-right the user details for Andre can be displayed by clicking <code>A</code></li> <li>In the middle of the screen you will see the <code>JGRMQAPI Product 1.2.0</code> API Product is displayed.</li> </ol> <p></p>"},{"location":"integration/tech-jam-labs/apic-consume/#create-an-application-and-subscribe-to-the-api-on-the-default-plan","title":"Create an Application and subscribe to the API on the Default Plan","text":"<p>Click on the <code>JGRMQAPI Product 1.2.0</code> API Product. Information about the product the API will be displayed, along with two 'plans':   a. Default Plan - allows 5 calls per minute   b. Premium Plan - allows 600 calls per hour</p> <p></p> <p>Select Default Plan</p> <p>Click Create Application and give it the name <code>mqapi-default-dev001</code>. Optionally, add a Description. Click Save.</p> <p></p> <p>IMPORTANT</p> <p>An API key will display!  Copy the API Key and secret and keep this in a text file - it is only displayed once!</p> <p></p> <p>Once you have safely copied the API Key and Secret, click x. Your new application is displayed on the left hand side of the screen. Click on the new Application.</p> <p>You will then be asked to Subscribe to the API on behalf of the Application - the subscription is the object that controls which APIs your (Application's) API key and Secret give you access to. To do this, click Next.</p> <p></p> <p>A screen displays saying 'Subscription Complete'. Click Done.</p>"},{"location":"integration/tech-jam-labs/apic-consume/#test-the-api-on-the-default-plan","title":"Test the API on the Default plan","text":"<p>At the top of the screen click Apps. Your application is displayed - click on it to display the application dashboard will be displayed (you can see some API analytics here after the API has been used)</p> <p>Click Subscriptions. At the bottom of the screen the 'Product subscriptions' list will show.</p> <p></p> <p>Click on the 'JGRMQAPI Product (1.2.0)' API Product. The API displays with the plans once more. Click on the API. The API Overview page displays.</p> <p></p> <p>Click on POST /contacts. The details screen displays and shows the API parameters and example requests.</p> <p>Click on the Try it tab near the top of the screen. A testing page then displays.  Note:  The API key is populated for you here.</p> <p></p> <p>Scroll down to the <code>Body</code> field in the <code>Parameters</code> section.  Click Generate to create an example message body.</p> <p>It will look similar to the following: </p><pre><code>  {\n  \"metadata\": {\n    \"code\": \"tazawewjiolualaa\"\n  },\n  \"payload\": {\n    \"id\": \"4378063097823232\",\n    \"fname\": \"Derek\",\n    \"lname\": \"Cognome\",\n    \"email\": \"ekabakun@fuw.ws\",\n    \"phone\": \"(755) 811-5225\",\n    \"company\": \"newat\",\n    \"comments\": \"gaakuzikigi\"\n  }\n}\n</code></pre><p></p> <p></p> <p>Click Send.  If a <code>CORS</code> error appears, click the link (this opens a new tab and accepts the insecure connection.  If you are working on Chrome you might have to type <code>thisisunsafe</code> in the window to accept the certificate exception and try again.</p> <p></p> <p>You should get a response that looks like this: </p><pre><code>{\n    \"response\": {\n        \"code\": \"CP4I0000\",\n        \"msg\": \"Request has been processed.\",\n        \"time\": \"2022-07-06T21:04:51.362Z\"\n    },\n    \"payload\": {\n        \"id\": \"00Q8d000005LTLTEA4\",\n        \"fname\": \"Timothy\",\n        \"lname\": \"Storey\",\n        \"email\": \"vuhnizu@fozalnu.me\",\n        \"phone\": \"(951) 981-5174\",\n        \"company\": \"nodiffivwadekiz\",\n        \"comments\": \"wesazaazesoba\"\n    }\n}\n</code></pre><p></p> <p></p> <p>Check Salesforce for your new contact!</p> <ul> <li>On the top banner in Salesforce click the down arrow next to Leads</li> <li>Your new contact should be in the list. Click on the contact to see the details on the Details tab and check they match the response in the developer portal.</li> </ul> <p>Note</p> <p>For face to face sessions your instructors may be providing a shared SalesForce instance, if so please check with them that your lead has arrived. To avoid confusion, try changing the name of your lead to something recognizable.</p> <p>Test the application in a test harness of your choice, for example SOAP UI remembering to set the following headers: - X-IBM-Client-Id :  - accept : <code>application/json</code> - content-type : <code>application/json</code></p> <p></p> <p>Make multiple requests in quick succession and see that the API is rate limited - you will get a message like this: </p><pre><code>{\n   \"httpCode\": \"429\",\n   \"httpMessage\": \"Too Many Requests\",\n   \"moreInformation\": \"Assembly Rate Limit exceeded\"\n}\n</code></pre><p></p> <p></p>"},{"location":"integration/tech-jam-labs/apic-consume/#create-a-new-application-and-subscribe-to-the-api-on-the-premium-plan","title":"Create a new Application and subscribe to the API on the Premium Plan","text":"<p>Return to the Developer Portal. At the top of the screen click Apps, then click Create new app.</p> <p>Give the application the title <code>mqapi-premium-dev001</code>. Optionally, add a description and click Save.</p> <p></p> <p>IMPORTANT</p> <p>Copy the API Key and secret and keep this in a text file - it is only displayed once!</p> <p>Once you have safely copied the API Key and Secret, click OK.  Your new application is displayed on the left hand side of the screen Under <code>Product subscriptions</code> you will see that the application is currently not subscribed to anything.  Click on 'Why not browse the available APIs?'. The API Products Page displays.</p> <p>Click on the JGRMQAPI Product 1.2.0 Product.  The APIs and Plans are displayed</p> <p></p> <p>On the <code>Premium Plan</code> click Select. The <code>MQAPI-SF12-PREMIUM</code> application is shown on screen, as it is not currently subscribed to the API and is therefore available to subscribe.</p> <p></p> <p>Click on the MQAPI-SF12-PREMIUM application. The <code>Confirm Subscription</code> page is shown. Verify that the Premium Plan is showing in the list and click Next.</p> <p>The <code>Subscription Complete</code> page is shown. Click Done.</p> <p>Click on the new MQAPI-SF12-PREMIUM Application.</p>"},{"location":"integration/tech-jam-labs/apic-consume/#approve-the-subscription-of-the-mqapi-sf12-premium-application-to-the-premium-plan-in-the-api-manager-playing-part-of-jason-the-api-lifecyle-manager","title":"Approve the subscription of the <code>MQAPI-SF12-PREMIUM</code> application to the Premium Plan in the API Manager (playing part of Jason the API Lifecyle Manager)","text":"<p>For the Premium Plan, subcriptions must be approved by the owner of the <code>Provider Organisation</code>. The shows how providers can control who can and cannot use our APIs.</p> <p>Return to the API Manager (follow the steps in the section <code>Log into the API Manager Console</code> if you have lost this tab) and navigate to <code>Manage Catalogs</code> &gt; <code>Demo</code> &gt; <code>Applications</code></p> <p></p> <p>Click Subscriptions in the top banner. The list of Subscriptions is displayed and you will see that the <code>MQAPI-SF12-PREMIUM</code> product has a subscription in the pending state.</p> <p></p> <p>To approve the subscription, click on Tasks in the top banner. A task is displayed: <code>Subscription approval task for subscription of jgrmqapi-product:1.2.0 (Premium Plan) requested by Andre Integration (andre_integration)</code></p> <p></p> <p>Click on the three dots on the right hand side and click Approve. The task will disappear from the list.</p> <p>Return to the 'Subscriptions' tab and you will observe that the subscription for the <code>MQAPI-SF12-PREMIUM</code> application is now in state enabled.</p> <p></p>"},{"location":"integration/tech-jam-labs/apic-consume/#test-the-api","title":"Test the API","text":"<p>Test the API on the Premium plan (playing part of Andre the Application Developer).  Return to the Developer Portal where you are logged in as <code>andre_integration</code>.  At the top of the screen click Apps. The new <code>mqapi-premium-dev001</code> application will be displayed - click on it.</p> <p>Click Subscriptions. At the bottom of the screen the 'Product subscriptions' list will show.  Click on the JGRMQAPI Product (1.2.0) API Product.  The API displays with the plans once more.</p> <p>Click on the API. The API Overview page is displayed.  Continue on to test the application like you did in the previous Test the API on the Default plan section.</p> <ul> <li>Test it in the 'Try it' section under <code>POST /contacts</code></li> <li>Be sure to select the API Key for <code>mqapi-premium-dev001</code> in the <code>Security</code> section</li> </ul> <p></p> <ul> <li>Test it SoapUI or test harness of your choice - make sure to use the correct API Key!</li> <li>Observe that with this subscription you are on the <code>Premium Plan</code> and can there make many requests and will not get throttled (unless you do more than 600 requests in one hour).</li> </ul> <p></p>"},{"location":"integration/tech-jam-labs/apic-dev/","title":"API Connect API Development","text":""},{"location":"integration/tech-jam-labs/apic-dev/#api-connect-api-development","title":"API Connect API Development","text":"<p>In this lab you will be preparing APIC for API development in following exercises.  The API will call the App Connect micro-services we created in a previous ACE exercise.  You will then consume the API via a Developer Portal and observe the results.</p> <p>As part of this exercise you will work though developing your API.</p>"},{"location":"integration/tech-jam-labs/apic-dev/#developing-our-api","title":"Developing our API","text":"<p>Personas played:</p> <p> Cloud Manager - Will - logs into the API Manager as the admin user on this lab for simplicity</p> <p> API Developer - Shavon - logs into the API Manager Portal as the admin user on this lab for simplicity</p> <p>In this section Shavon, the API Developer, will develop the API in the API Manager.</p>"},{"location":"integration/tech-jam-labs/apic-dev/#setup-the-provider-organisation-porg-as-will-the-cloud-manager","title":"Setup the Provider Organisation (pOrg) as Will the Cloud Manager","text":"Manual setup of the pOrgAutomated setup of the pOrg with script 04-apic-new-porg.sh <p>We need to create a Provider Organisation or pOrg to organise the APIs that we will make available to consumers on API Connect.  We will create a pOrg called <code>cp4i-demo-org</code> and set the <code>admin admin</code> user as the Owner. To complete this log into the <code>Cloud Manager</code> UI as the <code>Plaform Nav</code> admin user (you will be logged in from the previous steps). Follow these Creating a provider organization instructions.</p> <p>There are two options in these instructions.</p> <p></p> <p>We used option 2: <code>Add &gt; Create Organization</code>. Specify the following information in the form:</p> <ul> <li><code>Title</code>: <code>CP4I Demo Provider Org</code></li> <li><code>User Registry</code>: Select <code>Common Services User Registry</code></li> <li>Type of user: Slect <code>Existing</code></li> <li><code>Username</code>: <code>admin</code></li> </ul> <p></p> <p>Click Create</p> <p>The <code>Provider organizations</code> page is displayed and you will see the new pOrg in the list.</p> <p></p> <p><code>sh export MAILTRAP_USER=&lt;my-mailtrap-user&gt; export MAILTRAP_PWD=&lt;my-mailtrap-pwd&gt; scripts/04-apic-new-porg.sh</code> This script performs the following steps:</p> <ol> <li>Logs into API Connect using the apic client on your workstation, with the Cloud Management Console Admin User and Password</li> <li>Creates a Provider Organisation for the admin user</li> </ol>"},{"location":"integration/tech-jam-labs/apic-dev/#develop-and-publish-our-api-as-shavon-the-api-developer","title":"Develop and publish our API as Shavon the API Developer","text":"<p>In this section you will:</p> <ul> <li>Login to the API Manager</li> <li>Import, review and test our API</li> <li>Create an API Product to host our API</li> <li>Publish our API Product</li> </ul>"},{"location":"integration/tech-jam-labs/apic-dev/#log-into-the-api-manager","title":"Log into the API Manager","text":"<ol> <li> <p>Login to the Automation Foundation</p> <p>From the platform navigator, under API lifecycle management, right click on apim-demo and open in a private browser tab.</p> <p>Note</p> <p>When working with API Connect it is useful to open up the Cloud Manager and API Manager in separate private browser tabs.</p> <p>Accept the security exceptions (on Chrome you might have to type in 'thisisunsafe' to achieve this).</p> <p>Choose the IBM provided credentials (admin only) option to login.  This allows you to login to the IBM Automation foundation with the default <code>admin</code> account. This can be found in the <code>tools</code> project under <code>workloads</code> &gt; <code>secrets</code> &gt; <code>ibm-iam-bindinfo-platform-auth-idp-credentials</code> Alternatively run the following <code>oc</code> commands to get the credentials:</p> <pre><code>oc get secret ibm-iam-bindinfo-platform-auth-idp-credentials -n tools -o jsonpath=\"{.data.admin_username}\"| base64 -d\noc get secret ibm-iam-bindinfo-platform-auth-idp-credentials -n tools -o jsonpath=\"{.data.admin_password}\"| base64 -d\n</code></pre> <p>Once you login to Automation Foundation, the API Manager Login page will display.</p> </li> <li> <p>Login to the API Manager Console and select the option Common Services User Registry. You will be automatically logged into the API manager.</p> </li> <li> <p>Validate that you are in the new Provider Organization, as the admin user</p> <p>At the top right hand side of the screen hover over the text to right of Organization:, it should display the text CP4I Demo Provider Org</p> <p></p> <p>To the right of the Provider Org, you will see that you are logged in as admin. You can expand the list and can click on <code>My account</code> to see more information.</p> </li> </ol>"},{"location":"integration/tech-jam-labs/apic-dev/#get-the-api-endpoints-from-app-connect-enterprise-ace","title":"Get the API endpoints from App Connect Enterprise (ACE)","text":"<p>Our API will have three possible endpoints:</p> <ol> <li>The 'TARGET_URL' which points to our App Connect Designer instance <code>jgr-designer-sfleads</code></li> <li>The 'PREMIUM_URL' which points to our ACE instance <code>jgr-mqapi-prem</code> - this will be exposed on an API Plan with 600 calls allowed per hour</li> <li>The 'DEFAULT_URL' which points to our ACE instance <code>jgr-mqapi-dflt</code> - this will be exposed on an API Plan with 5 calls allowed per minute</li> </ol> <p>To get the required URLS run the following commands in a terminal window, where you are logged into the OCP cluster:</p> <pre><code>touch appconnurls.txt\necho TARGET_URL is: $(oc get integrationserver jgr-designer-sfleads -n tools -o jsonpath='{.status.endpoints[0].uri}')'/SFLeads/lead' &gt;&gt; appconnurls.txt\necho PREMIUM_URL is: $(oc get integrationserver jgr-mqapi-prem -n tools -o jsonpath='{.status.endpoints[0].uri}') &gt;&gt; appconnurls.txt\necho DEFAULT_URL is: $(oc get integrationserver jgr-mqapi-dflt -n tools -o jsonpath='{.status.endpoints[0].uri}') &gt;&gt; appconnurls.txt\ncat appconnurls.txt\nrm appconnurls.txt ;\n</code></pre> <p>TARGET_URL</p> <p>We are currently using the Sales Force URL use this value instead: http://jgr-designer-sfleads-http-tools.jgrocp410cp4i20221-dcd557ed086a97d77c2297a53c025612-0000.us-south.containers.appdomain.cloud/SFLeads/ For the other two URLs, if your above script does not return a URL, you may not have deployed the BAR file from the earlier MQ message flow lab.  This lab expects you have deployed that BAR twice.  Once to an integration server <code>jgr-mqapi-prem</code> and second to an integration server <code>jgr-mqapi-dflt</code>.</p>"},{"location":"integration/tech-jam-labs/apic-dev/#import-our-api-definition-to-api-connect","title":"Import our API definition to API Connect","text":"<p>On the API Manager Home Page, select the <code>Develop APIs and products</code> tile on the home page or the <code>Develop</code> icon on the left menu.</p> <p></p> <p>Select Add &gt; API(from REST, GraphQL or SOAP) from the top right.</p> <p></p> <p>Use the <code>jgrmqapiv2</code> OpenAPI (Swagger) document in the <code>templates</code> part of the repository you cloned in a previous exercise: <code>template-apic-api-def-jgrmqapiv2.yaml</code> (found in the /templates/template-apic-api-def-jgrmqapiv2.yaml) to define the API by selecting Create &gt; From existing OpenAPI service &gt; Next.</p> <p></p> <p>Next drag the templates/template-apic-api-def-jgrmqapiv2.yaml document into the <code>File upload</code> box and select <code>Next</code>.</p> <p></p> <p>Review the details and select <code>Next</code>.</p> <p></p> <p>The <code>Secure</code> page is displayed. Ensure the following are selected - <code>Secure using Client ID</code> - <code>CORS</code></p> <p></p> <p>Select Next.  The output of the API definition should inidicate that it was successfully generated and security applied.  </p> <p></p> <p>Choose Edit API</p> <p>The <code>Design</code> tab opens for the API.</p> <p>In the <code>Security Schemes</code> section a <code>clientID</code> scheme of type <code>apiKey</code> has been added.</p> <p></p> <p>In the General &gt; Security section, the scheme is shown as mandatory meaning a valid <code>clientID</code> is required to be passed whenever the API is called.</p> <p></p>"},{"location":"integration/tech-jam-labs/apic-dev/#update-the-api-backend-urls","title":"Update the API backend URLS","text":"<p>In the top banner click <code>Gateway</code>, then, in the left-hand side, expand <code>Properties</code>.  You will see that three url properties are listed:</p> <ul> <li>target-url</li> <li>premium-url</li> <li>default-url</li> </ul> <p></p> <p>Click on each of these and add the corresponding URLs from the <code>Get the API endpoints from App Connect Enterprise (ACE)</code> section to the <code>Property Value (optional)</code> field.</p> <p>Target URL:</p> <p></p> <p>Premium URL:</p> <p></p> <p>Default URL:</p> <p></p> <p>Click <code>Save</code></p>"},{"location":"integration/tech-jam-labs/apic-dev/#build-the-api-policy","title":"Build the API Policy","text":"<p>The API Connect Designer gives us the ability to build Gateway Policies which are executed on each API request. This allows us to add in elements to the API definition that are IBM extensions to the OpenAPI specification.</p> <p>We will use the lo-code assembly editor to build out our policy to look like the below:</p> <p></p> <p>The completed Gateway Policy will have:</p> <ul> <li>1 x Parse element</li> <li>1 x set-varible element</li> <li>2 x switch elements</li> <li>2 x gatewayscript element</li> <li>3 x invoke element</li> </ul> <p>The above elements are documented in this API Connect Documentation.</p> <p>To start with your policy will look like the below image.  On the left-hand side, you will see that there is a canvas with a single <code>invoke</code> element. On the right-hand side you will see a 'palette' of various elements.</p> <p></p> <ol> <li> <p>Delete the existing Invoke element:     You will add other <code>Invoke</code> elements later, the default one is not needed.</p> <p>Hover over the Invoke element and a delete icon will appear in the top right corner (looks like a trash can). Click this icon.</p> <p></p> <p>You will then have a blank canvas like this:</p> <p></p> </li> <li> <p>Add the Parse element:     To add an element you click on it and drag it onto the canvas.</p> <p>The <code>Parse</code> element is in the <code>Transforms</code> section of the palette:</p> <p></p> <p>Click and drag the <code>Parse</code> element onto the canvas.  When you do this you will see a small square box display on the line in the top left of the canvas. Drop the <code>Parse</code> element onto this square.</p> <p>The result will look similar to the following:</p> <p></p> <p>Note</p> <p>For this <code>Parse</code> element, no further information needs to be configured on the right hand side.</p> <p>Click Save.</p> </li> <li> <p>Add the set-variable element from the <code>Policies</code> section of the palette:      Drag and drop the <code>set-variable</code> element to be to the right of the <code>Parse</code> element.     On the right hand side, a menu will display where you can configure this element.  You will use this to add two actions.</p> <p>Add the first action. Click Add action and set the following attributes: - Action: <code>Set</code> - Set: <code>my-path</code> - Type: <code>string</code> - Value: <code>/jgrmqapi/v1/contacts</code></p> <p>Add the second action. Click <code>Add action</code> and set the following attributes: - Action: <code>Set</code> - Set: <code>my-mb</code> - Type: <code>any</code> - Value: <code>$(message.body)</code></p> <p></p> <p>Click Save.</p> <p>The <code>set-variable</code> has prepared us for the next element, a <code>Switch</code>.</p> </li> <li> <p>Add a Switch element:     This is found in the <code>Logic</code> section of the palette.  Drag the <code>Switch</code> element onto the canvas and drop it to the right of the <code>set-variable</code> element.</p> <p>It should look like this:</p> <p></p> <p>On the right-hand side of the screen, add the following string into the <code>Condition</code> field in the <code>Case 0</code> section:</p> <p></p><pre><code>message.body.metadata.code=\"TEST\"\n</code></pre> Next, click <code>Add Otherwise</code>. You will end up with a configuration similar to the picture below:<p></p> <p></p> <p>Click <code>Save</code>.</p> </li> <li> <p>Add a gatewayscript element to the canvas on the otherwise condition:</p> <p>In the event that the condition is not met (ie: we have not set \"TEST\"), then we will call a <code>GatewayScript</code>.</p> <p>The <code>GatewayScript</code> element can be found in the <code>Policies</code> section of the palette. Drag and drop a <code>GatewayScript</code> element onto the line below <code>Otherwise</code> (see picture below).</p> <p>In the panel on the right-hand side, there is a text editor box for your code. Copy / Paste this code into the text box:</p> <pre><code>var apim = require('apim');\nvar mb = apim.getvariable('message.body.payload');\napim.setvariable('message.body', mb);\napim.output('application/json');\n</code></pre> <p>It should look similar to the following:</p> <p></p> <p>Note</p> <p>The warning message is expected on this demo and does not affect the operation of the API.</p> <p>Click Save.</p> </li> <li> <p>Add an Invoke element to the canvas on the otherwise condition:     The <code>Invoke</code> element can be found in the <code>Policies</code> section of the palette. Drag and drop this on the canvas, to the right of the <code>GatewayScript</code> you just added.</p> <p>Configure the <code>Invoke</code> element with the following details:</p> <p>Note</p> <p>If the parameter is not mention it does not need to be changed from the default setting.</p> <ul> <li>Title: <code>invoke-designer</code></li> <li>URL: <code>$(target-url)</code></li> <li>Inject proxy headers: <code>selected</code></li> <li>Persistent Connection: <code>selected</code></li> <li>Allow chunked uploads: <code>de-selected</code></li> <li>Header control: Select <code>Blocklist</code></li> <li>Click <code>Add blocklist</code> and enter <code>^X-IBM-Client-Id$</code> in the field that appears</li> <li>Parameter control: Select <code>Blocklist</code></li> <li>Response object variable: <code>sf-lead</code></li> <li>Stop on error: <code>de-selected</code></li> </ul> <p></p> </li> <li> <p>Add a second <code>GatewayScript</code> on the  otherwise condition:</p> <p>Drag and drop a <code>GatewayScript</code> element onto the <code>Otherwise</code> condition, to the right of the <code>invoke-designer</code> element.</p> <p>On the right-hand side, copy the below code and paste it into the text editor box: </p><pre><code>var apim = require('apim');\nvar mb = apim.getvariable('my-mb');\nvar sf = apim.getvariable('sf-lead.body');\nmb.payload.id = sf.id;\napim.setvariable('message.body', mb);\napim.output('application/json');\n</code></pre><p></p> <p>It should look like this:</p> <p></p> <p>Note</p> <p>The warning message is expected on this demo and does not affect the operation of the API.</p> </li> <li> <p>Add a second Switch element to the left of the Output Node:     The end of our flow is represented on thr right hand side of the canvas with a circular icon containing a flag, as shown in the below image:</p> <p></p> <p>Drag and drop a <code>Switch</code> element onto the canvas to the left of the <code>Output Node</code>.  See the picture below to check the correct location.</p> <p>On the right-hand side of the screen add the following string into the <code>Condition</code> field in the <code>Case 0</code> section:</p> <pre><code>plan.name=\"default-plan\"\n</code></pre> <p>Then click Add Otherwise.</p> <p>You will end up with a configuration looking like the picture below, showing a line for <code>plan.name=\"default-plan\"</code> and a line for <code>Otherwise</code>:</p> <p></p> <p>Click Save.</p> </li> <li> <p>Add an Invoke on the plan.name=\"default-plan\" line:     Drag and drop an <code>Invoke</code> element onto the <code>plan.name=\"default-plan\"</code> line. Configure this <code>Invoke</code> element with the following details:</p> <p>Note</p> <p>If the parameter is not mention it does not need to be changed from the default setting.</p> <ul> <li>Title: <code>invoke-default</code></li> <li>URL: <code>$(default-url)$(my-path)</code></li> <li>Inject proxy headers: <code>selected</code></li> <li>Allow chunked uploads: <code>de-selected</code></li> <li>Persistent Connection: <code>selected</code></li> <li>Header control: Select <code>Blocklist</code></li> <li>Click <code>Add blocklist</code> and enter <code>^X-IBM-Client-Id$</code> in the field that appears</li> <li>Parameter control: Select <code>Blocklist</code></li> <li>Stop on error: <code>de-selected</code></li> </ul> <p>The canvas should now look like this:</p> <p></p> <p>Click Save.</p> </li> <li> <p>Add an Invoke on the Otherwise line:</p> <p>Drag and drop an <code>Invoke</code> element onto the <code>plan.name=\"default-plan\"</code> line. Configure this <code>Invoke</code> element with the following details:</p> <p>Note</p> <p>If the parameter is not mention it does not need to be changed from the default setting.</p> <ul> <li>Title : <code>invoke-premium</code></li> <li>URL : <code>$(premium-url)$(my-path)</code></li> <li>Inject proxy headers : <code>selected</code></li> <li>Allow chunked uploads : <code>de-selected</code></li> <li>Persistent Connection : <code>selected</code></li> <li>Header control : Select <code>Blocklist</code></li> <li>Click <code>Add blocklist</code> and enter <code>^X-IBM-Client-Id$</code> in the field that appears</li> <li>Parameter control : Select <code>Blocklist</code></li> <li>Stop on error : <code>de-selected</code></li> </ul> <p>The canvas should now look like this:</p> <p></p> <p>Click Save.</p> </li> <li> <p>The completed flow will look like this:</p> <p></p> </li> </ol>"},{"location":"integration/tech-jam-labs/apic-dev/#use-the-api-on-the-explorer","title":"Use the API on the <code>Explorer</code>","text":"<ol> <li> <p>Put the API online, this will deploy the API to the (DataPower) Gateway.     At the top of the screen there is a selector which says <code>Offline</code>:</p> <p></p> <p>Click on the selector and your API will be deployed and go <code>Online</code>.</p> <p></p> </li> <li> <p>Navigate to the <code>Explorer</code> panel and setup the test:</p> <p>In the banner click on <code>Explorer</code>, then click <code>POST /contacts</code></p> <p>On the <code>postContacts</code> page click <code>Try it</code></p> <p>In the drop down menu next to <code>POST</code> select the endpoint: <code>https://apim-demo-gw-gateway-tools.&lt;your-ocp-cluster&gt;.eu-gb.containers.appdomain.cloud/cp4i-demo-provider-org/sandbox/jgrmqapi/v2/contacts</code></p> <p>Scroll down to the <code>Body</code> section and click <code>Generate</code>. API Connect will create sample test data for you.</p> <p>Click Send</p> <p>You might see the following message</p> <p>You may see a message explaining that API Connect does not truest the certificate. Click on the provided link and accept the exception (if you are on Chrome you might have to type 'thisisunsafe').</p> <p></p> <p>Upon a successful request you will see a <code>200 OK</code> message returned with a payload featuring a Salesforce ID:</p> <p></p> <p>If you get an internal server <code>500</code> error go back and double check that all of your API elements are configured correctly.</p> </li> </ol> <p>Optional Step</p> <p>Test the API in the TEST Panel</p> <p>Navigate to the <code>Test</code> panel and setup the request</p> <p>In the top banner click <code>Test</code>. The <code>Request</code> panel will be displayed.</p> <p>On the <code>Parameters</code> section you will see the request headers displayed, including a Client ID, <code>X-IBM-Client-Id</code>.</p> <p></p> <p>On the <code>Body</code> panel you will see that an example message body has been populated. It should look something like this:</p> <p></p><pre><code>{\n  \"metadata\": {\n    \"code\": \"igdisenewel\"\n  },\n  \"payload\": {\n    \"id\": \"6682892837060608\",\n    \"fname\": \"Ricardo\",\n    \"lname\": \"Sestini\",\n    \"email\": \"wew@jo.nz\",\n    \"phone\": \"(555) 819-9636\",\n    \"company\": \"imidimpudw\",\n    \"comments\": \"gudubj\"\n  }\n}\n</code></pre> If the <code>Body</code> is not populated copy and paste the above sample into the provided field.<p></p> <p>Send a test request message: Click Send</p> <p>If successful, the response should look like this:</p> <p></p>"},{"location":"integration/tech-jam-labs/apic-dev/#import-our-api-product-definition-to-api-connect","title":"Import our API Product Definition to API Connect","text":"<p>Navigate back to the API Manager Homepage by Clicking on the Home icon in the top-left corner of the screen.</p> <p>Click on <code>Develop APIs and Products</code>. You will see the <code>APIs</code> screen is displayed and you will see the <code>JGRMQAPI</code> API is listed.</p> <p></p> <p>Click Products. A blank list is displayed.</p> <p></p> <p>Click Add and select <code>Product</code>. Then select <code>Existing Product</code> and click Next.</p> <p>Drag and drop artifacts/03-jgr-mqapi-product.yaml product from your file system to the box below <code>Import from file</code>.</p> <p></p> <p>Click Next.</p> <p>On the <code>Publish</code> page leave <code>Publish product</code> un-ticked and click Next, then click Done. YOu will be taken back to the <code>Develop</code> screen, with the <code>APIs</code> listed.</p> <p>Click Products and you will see <code>JGRMQAPI Product</code> is now listed.</p> <p></p> <p>Click on the <code>JGRMQAPI Product</code>. The <code>jgrmqapi-product</code> page is displayed.</p> <p></p> <p>On the <code>Design</code> tab there are a series of sections to review:</p> <ul> <li>Product setup: Allows us to set the title, version and description for the product</li> <li>Visibility: controls who can view and subscribe to the product. For this product subscribers and viewers must be authenticated to API Connect.</li> <li>APIs: we can see that the <code>JGRMQAPI</code> (which we just configured) is part of this product.</li> <li>Plans: plans control the way in which consumer applications can use the APIs in this product. This product has two plans defined: <code>default</code> and <code>premium</code><ul> <li>You can click on the three dots on the right hand side of each plan and click <code>Edit</code></li> <li>You can then review the relative rate limits that are setup for each plan (you will need to scroll down)</li> <li>To exit the <code>Edit plan</code> screen, click <code>Categories</code> on the left hand side</li> </ul> </li> <li>Categories: these can be used to arrange the diaply of APIs within the product on the Developer Portal. This product does not configure anything here.</li> </ul> <p>On the <code>Source</code> tab you can see the <code>yaml</code> representation of the API product.</p>"},{"location":"integration/tech-jam-labs/apic-dev/#create-the-demo-catalog-and-publish-the-api-product","title":"Create the Demo catalog and publish the API Product","text":""},{"location":"integration/tech-jam-labs/apic-dev/#create-the-demo-catalog-as-jason-the-api-lifecyle-manager","title":"Create the Demo catalog - as Jason the API Lifecyle Manager","text":"<p>Navigate back to the API Manager Homepage by clicking on the Home icon in the top-left corner of the screen.</p> <p>Next click on Manage catalogs.</p> <p></p> <p>The <code>Manage</code> page is displayed. Click on Add.</p> <p></p> <p>Follow these documents:</p> <ul> <li>Creating and configuring Catalogs</li> <li>Creating a consumer organization instructions</li> </ul> <p>In summary do the following:</p> <p>Create the catalog and set the owner.</p> <ul> <li>Title: <code>Demo</code></li> <li>Select user: use the default value - <code>admin admin (admin), admin@cp4i.net</code></li> </ul> <p>Click Create and your catalog will be created and then be shown on the manage page.</p>"},{"location":"integration/tech-jam-labs/apic-dev/#publish-the-jgrmqapi-product-api-product-to-the-demo-catalog-as-shavon-the-api-developer","title":"Publish the <code>JGRMQAPI</code> Product API Product*** to the <code>Demo</code> catalog - as Shavon the API Developer","text":"<p>Navigate back to the API Manager Homepage by Clicking on the Home icon in the top-left corner of the screen.</p> <p>Click on Develop APIs and Products. You will see the <code>APIs</code> screen is displayed and you will see the <code>JGRMQAPI</code> API is listed.</p> <p>Click on Products. The <code>JGRMQAPI Product API Product</code> is displayed.</p> <p>Click on the three dots on the right-hand side and then click Publish.</p> <p></p> <p>The <code>Publish Product</code> screen will then display. Make sure that the following values are set:</p> <ul> <li>Publish to (this is the catalog) : <code>Demo</code></li> <li>Preserve subscription : <code>de-selected</code></li> </ul> <p></p> <p>Click Next. The <code>Visibility</code> screen will display. Make sure that <code>Authenticated</code> is set for both <code>Visibility</code> and <code>Subscribability</code>.</p> <p></p> <p>Click Publish. A notification will pop up in the top-right of the screen informing you that the product successfully published.</p> <p>Next, check the <code>Demo</code> catalog to see that the product is now reporting as published.</p> <p>Click Home &gt; Manage Catalogs &gt; Demo &gt; Products. The <code>JGRMQAPI Product API Product</code> will show in the list with state <code>Published</code>. It will show that it has two plans (the <code>Default</code> and <code>Premium</code> plans we saw when we imported the product).</p> <p></p>"},{"location":"integration/tech-jam-labs/apic-dev/#automated-setup-of-api-creation-and-product-publication-via-the-09-apic-publish-apish-script","title":"Automated setup of API creation and product publication via the 09-apic-publish-api.sh Script","text":"<p>This script will perform the following actions:</p> <ol> <li>Login to the API manager using the <code>admin</code> user</li> <li>Retrieve a series of values from the API Manager to facilitate API publication     a. You will need to follow the prompt to open the url in your browser     b. Login to API Connect using the <code>ibm-iam-bindinfo-platform-auth-idp-credentials</code>     c. Copy the CliedID and paste it back into the terminal, then press <code>enter</code></li> <li>Create a <code>Swagger</code> (API specification) file for the <code>jgrmqapi</code> API using the template <code>templates/template-apic-api-def-jgrmqapiv2.yaml</code>      a. See the target URLs are updated in lines 124-128</li> <li>Publish the API in <code>draft</code> state</li> <li>Publishes a corresponding <code>API Product</code> in <code>draft</code> state, with the <code>jgrmqapi</code> API added to the product</li> <li>Creates a new <code>Demo</code> API Catalog</li> <li>Enables the creation of a <code>Developer Portal</code> for the Demo Catalog - this portal is where application developers can login to use APIs.     a. Note: this is not actually created by the script, just the setting to enable you to create one later is done.</li> <li>Publishes the <code>jgrmqapi</code> API to the Demo Catalog</li> </ol>"},{"location":"integration/tech-jam-labs/apic-prep/","title":"Preparing for APIC Development","text":""},{"location":"integration/tech-jam-labs/apic-prep/#preparing-for-apic-development","title":"Preparing for APIC Development","text":"<p>In this lab you will be preparing APIC for API development in following exercises.  The API will call the App Connect micro-services we created in a previous ACE exercise.  You will then consume the API via a Developer Portal and observe the results.</p> <p>As part of this lab you will work though the following topics:</p> <ul> <li>Pre-reqs and setup</li> <li>Setting up API Connect in the Cloud Manager</li> </ul> <p>For some of the steps there is a scripted option to do the configuration, however in this lab you will use the manual procedure, as this is the best way to learn API Connect (APIC).</p> <p>Note</p> <p>You are strongly encouraged to download a copy of the API Connect White Paper. This is one of the most authoritative sources of information on how API Connect works and it was created by IBMers who have implemented API Connect at a variety of customers over many years.</p>"},{"location":"integration/tech-jam-labs/apic-prep/#personas-for-api-connect","title":"Personas for API Connect","text":"<p>Note</p> <p>In this exercise you will be be playing the part of several personas who are involved in API Management.  At your customers the exact roles might be different.  The roles you will use here are sourced from the API Connect White Paper</p> <p> Shavon, the API Developer is in charge of API development for her organization.  Her focus is understanding data, services, and information needed from an API and then building out that API based upon those requirements. Eventually, her role is in charge of publishing it to testing, staging, and production environments.</p> <p> Jason, the API Lifecycle Manager is the lead for the line of business that Shavon works on.  He works with Shavon on API requirements and then reviews and manages the approval of new APIs or updates to APIs in the production environments.</p> <p> Steve, the Provider Organization Owner is in charge of coordinating the delivery of APIs across multiple lines of business and development groups.  Making sure each group has the resources that they need and that they are publishing their APIs to the appropriate environments.</p> <p> Will, the Cloud Manager is in charge of the IT administration at his organization.  He works with his team to deploy the API Connect software, configure the API Connect cloud, and then delegates to Steve access to the resources Steve needs to get the API initiative off the ground.</p> <p> Marsha, the Community Manager is in charge of Application developer success for her organization.  She focuses on making the API portal as straight forward as possible, monitors and manages the API consumer groups.</p> <p> Andre, the Application Developer is in charge of building out new applications and web services for his organization.  To do this, he needs to consume APIs.  He navigates to the API Developer Portal in order to register his application and consume the APIs needed to build out his app.</p> <p>Critical Hot Tip about API Connect Browser Windows</p> <p>In this API Connect Lab there are three separate UIs that you will login to on the API Connect product:</p> <ol> <li>Cloud Manager Console:  The Cloud Manager enables a Cloud Administrator, like Will, to define, manage, and monitor the API Connect instance.</li> <li>API Manager UI:  The API Manager is the interface used by the providers of APIs - for example a developer of a backend REST API, such as Andre, would use the API Manager to develop and publish their API</li> <li>The Developer Portal:  The Developer Portal is used by consumers of APIs, such as developers of front end applications, that need to use an API that has been published (like Andre), or third parties that wish to use an organization's externally available APIs (eg: Fintechs using Open Banking APIs)</li> </ol> <p>It is strongly advised to login to each of these UIs in private browser sessions, as this makes the experience more stable.</p>"},{"location":"integration/tech-jam-labs/apic-prep/#pre-reqs-and-setup-for-the-exercise","title":"Pre-reqs and Setup for the Exercise","text":""},{"location":"integration/tech-jam-labs/apic-prep/#install-api-connect-tools","title":"Install API Connect Tools","text":"<p>To support CLI scripting options, make sure you have downloaded the API Connect client (apic cli) following the instructions here.</p> <p>Best-Practice</p> <p>Follow the path in the document for downloading the tools from the Cloud Manager or API Manager user interface that you have installed in your cluster.</p> <p>It is recommended to get the client for your workstation (eg: for mac) at the 10.0.4.0-ifix3 version. Choose the CLI + LoopBack + Designer option.</p> <p>Once you have downloaded and extracted the <code>apic</code> client, copy it from the extracted directory to <code>/usr/local/bin</code> (macOS or equivalent directory on your %PATH% for Windows).</p> <p>You should be able to issue the <code>apic</code> command in your terminal window and have the help display.</p>"},{"location":"integration/tech-jam-labs/apic-prep/#getting-your-mailtrap-username-and-password","title":"Getting your Mailtrap Username and Password","text":"<p>Login to the Mailtrap account. Under <code>SMPT settings</code> tab expand <code>Show Credentials</code>. </p>"},{"location":"integration/tech-jam-labs/apic-prep/#setting-up-api-connect-in-the-cloud-manager","title":"Setting up API Connect in the Cloud Manager","text":"<p>Personas played:</p> <p></p> <p>Cloud Manager - Will - logs into the API Manager as the admin user on this lab for simplicity</p> <p>In this section you will: - As the Cloud Manager - Will Setup Email Notifications on API connect - As the Cloud Manager - Will Setup the Provider Organization (pOrg)</p> <p>For both of these tasks there is a manual and automated option. The manual option is recommended to learn the most about the product.</p>"},{"location":"integration/tech-jam-labs/apic-prep/#log-into-the-api-connect-cloud-manager-console","title":"Log into the API Connect Cloud Manager Console","text":"<p>To complete this log into the Cloud Manager <code>Plat Nav &gt; Integration instances &gt; API management administration</code> UI as the <code>Plaform Nav</code> admin user.</p> <p>Note</p> <p>It is advised to login to the Cloud Manager in a private browser window.</p> <p>The Cloud Manager Home page looks like this:</p> <p></p>"},{"location":"integration/tech-jam-labs/apic-prep/#setup-email-notifications-on-api-connect","title":"Setup Email Notifications on API connect","text":"<p>Access to an SMTP mail server is required as a pre-requisite to setting up email notifications.  For this lab you will use Mailtrap.</p> <p>To configure access to the email server and provide notifications see configuring an email server for notifications for details.</p> <p>In Cloud Manager go to <code>Resources &gt; Notifications</code></p> <p>You will see an entry called <code>Dummy Mail Server</code> edit this to point at Mailtrap.  Click on the three dots on the right hand side of the <code>Dummy Mail Server</code> entry and click <code>Edit</code>.</p> <p></p> <p>To connect to the Mailtrap mailserver set the following connection details - Title: <code>Dummy Mail Server</code> - Address: <code>smtp.mailtrap.io</code> - Port: <code>2525</code> - Authenticate user (optional): <code>your mailtrap SMTP user</code> - Authenticate password (optional): <code>your mailtrap SMTP password</code> - Select the <code>Secure Connection</code> tickbox</p> <p>Optional: You can test the mailserver connection by clicking <code>Test email</code>.</p> <p>Click <code>Save</code></p> <p>Once the email server has been defined follow the setting up notifications instructions to configure the notifications to use the new email server.</p>"},{"location":"integration/tech-jam-labs/apic-prep/#validate-the-apic-notifications","title":"Validate the APIC Notifications","text":"<p>In Cloud Manager go to <code>Settings &gt; Notifications</code></p> <p></p> <p>Check that sender &amp; email server use the following connection details - Name: <code>APIC Administrator</code> - Email address: <code>admin@apiconnect.net</code> - Select the <code>Dummy Mail Server</code> mail server defined in previous step</p>"},{"location":"integration/tech-jam-labs/apic-prep/#update-your-user-with-theemail-address","title":"Update Your User with theEmail Address","text":"<p>Update Your user, 'admin admin', with new email address.  In the top right of screen click on the downwards arrow next to the <code>admin</code> user. Click <code>My Account</code>.</p> <p></p> <p>The <code>Account</code> page will display.</p> <p>In the <code>Email (optional)</code> field enter: <code>admin@cp4i.demo.net</code>. Leave the <code>First name (optional)</code> and <code>Last name (optional)</code> fields set to <code>admin</code>.</p> <p></p> <p>Click Save and you be returned to the Cloud Manager home page.</p> <p>Now click <code>Members</code> on the left-hand side. A list of users will be displayed and you will see the <code>admin admin</code> user now displays with the supplied email address.</p> <p></p> <p>??? Question \"Shortcut for the above steps ...)     </p><pre><code>export MAILTRAP_USER=&lt;my-mailtrap-user&gt;\nexport MAILTRAP_PWD=&lt;my-mailtrap-pwd&gt;\nscripts/03-apic-initial-config.sh\n</code></pre>     This script performs the setup of the SMPT server in APIC     - Logs into API Connect using the apic client on your workstation, with the Cloud Management Console Admin User and Password     - Gets the existing information for the Mail (SMTP) Server - called 'dummy-mail-server'     - Updates the existing entry for 'dummy-mail-server' to use the SMTP Credentials you got from Mailtrap     - Edits the default admin user called admin admin, to have the email address <code>admin@cp4i.net</code><p></p>"},{"location":"integration/tech-jam-labs/event-streams/","title":"Event Streams Lab","text":""},{"location":"integration/tech-jam-labs/event-streams/#event-streams-lab","title":"Event Streams Lab","text":"<p>Part 1</p> <ul> <li>Deploy Event Streams using the Cloud Pak UI</li> </ul> <p>Part 2</p> <ul> <li>Configure Event Streams for use</li> <li>Install the Event Streams CLI (es)</li> <li>Test the CLI</li> <li>Use the starter application to produce and consume events</li> </ul>"},{"location":"integration/tech-jam-labs/event-streams/#deploy-event-streams","title":"Deploy Event Streams","text":"<p>Begin the installation from the Platform UI by selecting Integration Instances</p> <p></p> <p>Select Create an instance</p> <p>Select the Event Streams tile and click Next</p> <p></p> <p>Select the Development profile to explicitly ensure that you have an external listener and some level security enabled (you will look at this further prior to deployment)</p> <p> </p> <p>Name this instance es-demo to deploy into the context of the tools namespace and accept the license.  Then scroll down ...</p> <p></p> <p>By default this profile uses ephemeral storage, but we want to make it persistent (it's quite difficult to be highly available and resilient without persisted data) so you must now configure the storage for the brokers and the zookeepers.</p> <p>Configure the broker storage as follows:</p> <ul> <li>Select persistent-claim from the dropdown list</li> <li>Select ibmc-block-gold from the dropdown list</li> <li>Assign a storage size of 4Gi (just type this)</li> </ul> <p>Configure the zookeeper storage as follows:</p> <ul> <li>Select persistent-claim from the dropdown list</li> <li>Select ibmc-block-gold from the dropdown list</li> <li>Assign a storage size of 2Gi (just type this)</li> </ul> <p>Do not click Create just yet.</p> <p></p> <p>Before you continue, it is worth taking a look at the 'yaml' version to see how we will be accessing this Event Streams instance both from inside and outside the cluster.  Click on the <code>yaml</code> view. Look for listeners:. You will see you are using <code>tls</code> with <code>scram-sha-512</code> authentication and that access to the Event Streams instance will be via a route (more on all of this to follow).</p> <pre><code>  listeners:\n        - authentication:\n            type: scram-sha-512\n          name: external\n          port: 9094\n          tls: true\n          type: route\n        - authentication:\n            type: tls\n          name: tls\n          port: 9093\n          tls: true\n          type: internal\n</code></pre> <p>Now chose Create</p> What does the <code>yaml</code> look like for creating an equivalent cluster via the OpenShift console / <code>oc</code> CLI? <pre><code>apiVersion: eventstreams.ibm.com/v1beta2\nkind: EventStreams\nmetadata:\n  name: es-demo\n  namespace: tools\nspec:\n  adminApi: {}\n  adminUI: {}\n  apicurioRegistry: {}\n  collector: {}\n  license:\n    accept: true\n    use: CloudPakForIntegrationNonProduction\n  requestIbmServices:\n    iam: true\n    monitoring: true\n  restProducer: {}\n  strimziOverrides:\n    kafka:\n      authorization:\n        authorizerClass: com.ibm.eventstreams.runas.authorizer.RunAsAuthorizer\n        supportsAdminApi: true\n        type: custom\n      config:\n        default.replication.factor: 3\n        inter.broker.protocol.version: '3.2'\n        log.cleaner.threads: 6\n        log.message.format.version: '3.2'\n        min.insync.replicas: 2\n        num.io.threads: 24\n        num.network.threads: 9\n        num.replica.fetchers: 3\n        offsets.topic.replication.factor: 3\n      listeners:\n        - authentication:\n            type: scram-sha-512\n          name: external\n          port: 9094\n          tls: true\n          type: route\n        - authentication:\n            type: tls\n          name: tls\n          port: 9093\n          tls: true\n          type: internal\n      metricsConfig:\n        type: jmxPrometheusExporter\n        valueFrom:\n          configMapKeyRef:\n            key: kafka-metrics-config.yaml\n            name: metrics-config\n      replicas: 3\n      storage:\n        class: ibmc-block-gold\n        size: 4Gi\n        type: persistent-claim\n    zookeeper:\n      metricsConfig:\n        type: jmxPrometheusExporter\n        valueFrom:\n          configMapKeyRef:\n            key: zookeeper-metrics-config.yaml\n            name: metrics-config\n      replicas: 3\n      storage:\n        class: ibmc-block-gold\n        size: 2Gi\n        type: persistent-claim\n    entityOperator:\n      topicOperator: {}\n  version: 11.0.2\n</code></pre> <p>Return to the Integration Instances screen and wait until the Event Streams instance is showing as Ready. </p> <p>Tip</p> <p>If the UI shows an error early in the deployment, DON'T PANIC - this is a fault within the UI, be patient</p> <p>Your Event Streams instance is now ready additional configuration to make it fully ready for use within the demo.</p>"},{"location":"integration/tech-jam-labs/event-streams/#configure-event-streams","title":"Configure Event Streams","text":"<p>For the demo configuration there are a additional configurations to put in place. The first of these are the topics. Open the Event Streams UI:  click on the es-demo instance in the Integration Instances view.</p> <p>There are many things to explore here, but take note of one thing prior to continuing - look at the bottom of the screen to see that your Event Streams is in proper operation (ie: The system is healthy). Click on the Create a topic tile.</p> <p></p> <p>You could opt to view all available options for creating a topic, but our requirements are rudimentary, so simply move on.</p> <p>Enter the name for the topic - cp4i-ivt-topic and click Next</p> <p></p> <p>Leave the number of partitions as 1 (the default), click Next.</p> <p></p> <p>Select message retention of 1 day (ie: how long before the messages are deleted), click Next.</p> <p> </p> <p>Leave the replication factor as 3 (ie: The number of replicas of the topic that are available to enable high-availability, 3 is the number required to work using quorum), click Create Topic</p> <p>A pop-up message will inform you that the request to create a topic has been made (if your Event Streams is healthy, this will appear immediately). You will see the newly create topic.</p> <p></p> <p>Repeat the process above to create another topic called cp4i-es-demo-topic</p> <p></p> <p>Next, set up the credentials for a user to be able to access the topics.</p> <p>From the Event Streams main page select Connect to this cluster</p> <p></p> <p>Take note of the Event Streams bootstrap URL</p> <p>Click on Generate SCRAM Credentials</p> <p></p> <p>Set the user name for the credentials to ace-user</p> <p>Select the radio button which allows for Produce messages, consume messages and create topics and schemas and click Next.</p> <p></p> <p>Select the radio button for All topics and choose Next</p> <p> </p> <p>Select the radio button for All consumer groups and choose Next.</p> <p> </p> <p>Select the radio button for All transaction IDs and choose Generate Credentials</p> <p></p> <p>Copy AND SAVE the SCRAM username and SCRAM password (you will use them later). As you will see, there also has been a secret created that contains these values in the event that you misplace the credentials.</p> <p></p> <p>Additionally, download and SAVE the PKCS12 certificate and password, download the PEM certificate.</p> <p>Depending on which typeof application you are using to access Event Streams, you will need to use one or another of these certificates (more on this later in the other labs).</p> <p></p> <p>You can go to the OpenShift Console and take a look at the user that has been created by going to the IBM Event Streams operator (in Installed Operators), select the Kafka User tab.  </p> <p></p> <p>Tip</p> <p>As you move through each of the various labs, it will become apparent to you that there are many different ways of achieving the same results for all of the products, it is important to show you various options. How you choose to complete them depends on your preference and the preferences of your customer.</p> <p>The same procedure we have just followed can be achieved using a script that utilities the Event Streams CLI (which is a plug-in to the <code>cloudctl</code> CLI). The following script achieves the same configuration that we have just performed:</p> <pre><code>#!/bin/sh\necho \"Configuring Event Streams...\"\n###################\n# INPUT VARIABLES #\n###################\nES_INST_NAME='es-demo'\nES_NAMESPACE='tools'\n################################\n# INITIAL EVENT STREAMS CONFIG #\n################################\nCLUSTER_ADDRESS='https://'$(oc get route cp-console -n ibm-common-services -o jsonpath='{.status.ingress[0].host}')\nADMIN_PWD=$(oc get secret platform-auth-idp-credentials -n ibm-common-services -o jsonpath='{.data.admin_password}' | base64 --decode)\ncloudctl login -a ${CLUSTER_ADDRESS} -u admin -p ${ADMIN_PWD} -n ${ES_NAMESPACE} --skip-ssl-validation\ncloudctl es init -n ${ES_NAMESPACE}\ncloudctl es topic-create --name cp4i-ivt-topic --partitions 1 --replication-factor 3 --config retention.ms=86400000\ncloudctl es topic-create --name cp4i-es-demo-topic --partitions 1 --replication-factor 3 --config retention.ms=86400000\ncloudctl es kafka-user-create --name ace-user --consumer --producer --schema-topic-create --all-topics --all-groups --all-txnids --auth-type scram-sha-512\necho \"Event Streams has been configured.\"\n</code></pre> <p>However, this requires installation of the Event Streams CLI.</p>"},{"location":"integration/tech-jam-labs/event-streams/#install-event-streams-cli","title":"Install Event Streams CLI","text":"<p>From the Event Streams home page, select the Find more in the toolbox tile</p> <p></p> <p>Select the IBM Event Streams command-line interface tile and choose Find out more</p> <p></p> <p>Download the correct version for your environment.</p> <p></p> <p>From a terminal window enter:</p> <pre><code>cloudctl plug-in install ./es-plugin\n</code></pre> <p>Once the install has finished, type in :</p> <p></p><pre><code>cloudctl es\n</code></pre> If you receive a response similar to the following screenshot, the install is successful.<p></p> <p></p> <p>Next use the <code>es</code> CLI against a deployed instance of Event Streams running in the cluster.  To do this you first need to log into the <code>cloudctl</code>.</p> <p>Run the following:</p> <pre><code>export CLUSTER_ADDRESS='https://'$(oc get route cp-console -n ibm-common-services -o jsonpath='{.status.ingress[0].host}')\nexport ADMIN_PWD=$(oc get secret platform-auth-idp-credentials -n ibm-common-services -o jsonpath='{.data.admin_password}' | base64 --decode)\ncloudctl login -a ${CLUSTER_ADDRESS} -u admin -p ${ADMIN_PWD} --skip-ssl-validation\ncloudctl es init\n</code></pre> <p>When prompted for the namespace enter <code>tools</code>.</p> <p></p> <p>Use the <code>es</code> CLI to administer the Event Streams instance running in the <code>tools</code> namespace. Try something simple before moving on:</p> <pre><code>cloudctl es topics\n</code></pre> <p>You will see the topics that you created previously.</p> <p>Next try:</p> <pre><code>cloudctl es kafka-users\n</code></pre> <p>You will see some internal users and the user that you created previously.</p> <p></p> <p>Last, but not least, run a small example of producing and consuming messages from a topic.</p>"},{"location":"integration/tech-jam-labs/event-streams/#use-the-starter-app-to-produce-and-consume-events","title":"Use the starter app to produce and consume events","text":"<p>In the toolkit there is a small starter application to allow you to producee and consume messages. We will now install this.</p> <p>Go back to the toolbox and select Starter Application and click Get started</p> <p></p> <p>Download the JAR file for the start application from GitHub:</p> <p></p> <p></p> <p>Next, generate the properties file containing the credentials to connect to the cluster and the topic to use.  Click on Generate properties</p> <p></p> <p>Enter the following for the test application:</p> <p>name: estest topic name: (create a new topic as the previously created topics will be used in later labs) TEST</p> <p>Click on Generate and download .zip</p> <p></p> <p>Unzip the es-properties.zip file (unless you really like type), it is suggested that you put the previously downloaded jar file in the same place as the unzipped properties). You will see from the screenshot that you have everything in a directory named <code>es</code> including all of the previously downloaded certificates and credentials.</p> <p>To start the test application enter:</p> <pre><code>java -Dproperties_path=&lt;path-to-properties&gt; -jar demo-all.jar\n</code></pre> <p>which in your case is: </p> <pre><code>java -Dkafka.properties -jar demo-all.jar\n</code></pre> <p>Once you see a message with Application started in xxx ms* - you are ready to continue.</p> <p></p> <p>Open a browser and navigate to https://localhost:8080</p> <p></p> <p>Start by producing a few messages (but not yet consuming them). Enter a message you would like to send to the topic and click Start producing</p> <p></p> <p>This will keep producing the same message until you click Stop producing, so you can discontinue it after a few iterations.</p> <p>Move to the consumer side of the screen and click Start consuming</p> <p></p> <p>This will continue to consume messages until there are no more and until you click Stop consuming, it will simply wait for more.  If you start consuming messages later after more have been produced, it will simply pick up where it left off.</p> <p></p> <p>One last thing ....</p> <p>Go back to the Event Streams dashboard and select the TEST topic. Click on the Messages tab. Do you see all of the messages that you produced using the sample application?</p> <p></p> <p>If so, you have now completed the Event Streams lab</p> Shortcut <p>If you are familiar with deploying Event Streams, there is a <code>yaml</code> file included from the repo you cloned previously that will do the deployment steps for you.  If you are new to deploying Event Streams, follow the above detail steps using the UI method to familiarize yourself with the various components that need to be considered when creating an instance of Event Streams.</p> <p>Use the following command as a shortcut</p> <pre><code>oc apply -f instances/2022.2/05-event-streams-instance.yaml\n</code></pre> <p>Once Event Streams is up and running you could use the <code>05-es-initial-config.sh</code> to perform the initial configuration to define the <code>topics</code> and <code>user</code> we will use in the demonstration. </p>"},{"location":"integration/tech-jam-labs/mq/","title":"Getting started with MQ Lab","text":""},{"location":"integration/tech-jam-labs/mq/#getting-started-with-mq-lab","title":"Getting started with MQ Lab","text":"<p>This document explains:</p> <ul> <li>Creating a simple queue manager from the Cloud Pak UI</li> <li>Exploring the MQ Console</li> <li>Test message put and get</li> <li>Creating a ConfigMap for configuration of a queue manager using a yaml file, for the demo</li> <li>Creating a Native HA queue manager using a yaml file, for the demo</li> </ul>"},{"location":"integration/tech-jam-labs/mq/#scenario-overview","title":"Scenario Overview","text":"<p>Typically you would not create a queue manager using the UI in a real world customer deployment, it is very handy to be able to show how simple it is to do so in a demo.</p> <p>To begin with you will create a very simple queue manager using the UI.  Next you will use the MQ console to edit the queue manager configuration and create a queue for testing.  Following that you will use some of the supplied MQ tools to test that you are able to put and get messages from a queue.</p>"},{"location":"integration/tech-jam-labs/mq/#create-a-simple-queue-manager-using-the-ui","title":"Create a simple queue manager using the UI","text":"<p>Log on to the Platform UI (formerly known as the Platform Navigator) and select Integration Instances</p> <p>You will see a list of any instances of Cloud Pak capabilities already deployed.   Now create a new MQ instance (queue manager). Select Create an Instance</p> <p>Select Messaging as the instance you want to create. Click Next</p> <p>As you are creating a very basic queue manager, select the  Quick Start type of queue manager. Click Next</p> <p></p> <p>Using the dropdown list, select tools as the namespace to contain the queue manager (you can leave the default name) and accept the license agreement. </p> <p>Do NOT click Next yet. Scroll down the page </p> <p></p> <p>You must now decide the level of resiliency for the queue manager.  From the dropdown list select Single Instance. This will create what is known as a single resilient queue manager (ie: a single pod which will be restarted automatically in the event of a failure).  YOU would normally back this up with persistent storage for the queue manager data and logs, but as this is a test only, leave the default ephemeral storage (ie: non-persistent).</p> <p>Also, turn on the tracing for the Operations Dashboard, from the dropdown box selecting tools as the namespace of the Ops Dashboard.  You will not be sending any MQ messages that utilize the Operations Dashboard in this particular lab, this is merely so that you know where it is in the UI.</p> <p></p> <p>A couple of last things for you to inspect prior to moving on. Click on the Advanced Settings button.</p> <p></p> <p>In the part of the lab for the demo queue manager we will create a config map used to configure the queue manager and its' objects. In the UI this is where you can point to a config map (which may contain <code>mqsc</code> scripts or <code>qm.ini</code> overrides). You can also specifically name your queue manager here, (for the purposes of this lab QUICKSTART is fine). If you are wanting to add things such as <code>tls</code> keys and certs, this is also where you will go.</p> <p>All of the basic information you require to create a queue manager is now in place (you can also choose to switch to the <code>yaml</code> view if you are interested in looking at what is populated for a basic deployment. Click Create</p> <p></p> <p>You are taken back to the Integration Instances view, where you will see the queue manager as <code>pending</code>.</p> <p></p> <p>Return to the OpenShift Console to Installed Operators</p> <p>Select the tools project and the IBM MQ operator.</p> <p></p> <p>Select the Queue Manager tab and wait until you see the status as running. </p> <p>Note</p> <p>This should not take very long and the queue manager may already be running by the time you get there.</p> <p></p> <p>Now that the queue manager is running go back to the Platform UI. Select the new queue manager in the Messaging tile.</p> <p></p> <p>This will open up the MQ Console</p>"},{"location":"integration/tech-jam-labs/mq/#explore-the-mq-console","title":"Explore the MQ Console","text":"<p>Tip</p> <p>If the console does not open correctly, it may be that the web console is not yet ready.  Try the following: Go to the OpenShift Console and find the MQ pod</p> <p></p> <p>Go to the logs (for anyone who is likely to be performing any MQ troubleshooting, this is the log that shows any MQ errors if you ever need to find them).</p> <p></p> <p>Check for the <code>web server started</code> message. If the web server is started, go back to the Platform UI and try refreshing the screen.</p> <p></p> <p>You now need to make some changes to the queue manager configuration. As mentioned previously, this is typically done via a config map during queue manager creation, but this gives you the additional opportunity to explore the MQ Console.</p> <p>Given that you simply testing a basic queue manager, turn off all of the security related items that might hold you up. </p> <p>Danger</p> <p>This is NOT a recommended practice - MQ security can be very granular but also wide-ranging, it is recommended that you \"start as you mean to continue\", which means for a real customer situation ensure that, wherever possible, you set up all the MQ security that is required as part of the queue manager creation</p> <p>Click on View Configuration</p> <p></p> <p>You are shown all properties for the queue manager (for those of you not familiar with MQ - there are many!)</p> <p>Next click on Edit</p> <p></p> <p>We will first disable the channel authentication records (in short, channel authentication sets the rules as to who can and cannot make an inbound channel connection to the queue manager).</p> <p>In the Communication section, find the <code>CHLAUTH</code> records and using the dropdown list, select Disabled.</p> <p></p> <p>Next remove the connection authentication (in short, this challenges connecting client applications to provide credentials and then authenticate those credentials with a repository).</p> <p>In the extended section, scroll down to Connection authentication and remove the entry (just backspace it all out).</p> <p></p> <p>Save the changes to the queue manager configuration.</p> <p>Next, in the Actions box, refresh the authentication service, then refresh connection authentication.</p> <p>Note</p> <p>For those familiar with MQ, all of this can just as easily be achieved using the terminal window of the MQ pod and <code>runmqsc</code>.</p> <p></p> <p>Finally, we need to create a queue for our message testing.</p> <p>Go back to the main queue manager page.</p> <p>Tip</p> <p>Follow the breadcrumb trail at the top of the console screen if you get lost.</p> <p>You will see a list of queues (probably only one queue starting with <code>AMQ</code>), click Create to create a new queue.</p> <p></p> <p>Select Local as the queue type.</p> <p></p> <p>Select a name for the queue (such as <code>TEST</code>).</p> <p>Note</p> <p>The queue is enabled for both put and get by default.  If you wish to change this or any other queue attribute, you can use the Custom Create option.</p> <p>Click Create</p> <p></p> <p>Your queue manager ready for testing the ability to <code>put</code> and <code>get</code> messages.</p>"},{"location":"integration/tech-jam-labs/mq/#test-message-put-and-get","title":"Test Message Put and Get","text":"<p>For this part of the lab you will use some of the sample programs that are delivered with MQ.  You will <code>put</code> a message on a queue both in <code>binding mode</code> (ie: via a direct connection to the queue manager) and <code>client mode</code> (ie: via an mq client connection).</p> <p>Go to the OpenShift Console and back to the MQ pod.</p> <p>Open the terminal.  The first test you will run is using a program called <code>amqsput</code>.  This is a sample program that allows you to connect to a queue manager and put messages onto a named queue.  You do not need to enter a queue manager name as a parameter since the program will connect to the default queue manager.</p> <p>Perform the following:</p> <pre><code>Navigate to /opt/mqm/samp/bin\n./amqsput TEST\n</code></pre> <p>Enter a message for the queue (put as many as you like by simply hitting enter after each message).</p> <p>Hit enter twice to finish.</p> <p></p> <p>The next test we will run will put a message on a queue using an mq client connection. In this case we need to set an environment variable to identify the queue manager (as you would find in a CCDT).</p> <p>Connecting to a Queue Manager</p> <p>How to connect to a queue manager from inside or outside of the cluster? The hostname used for the MQ traffic within the cluster takes the form of: ..svc</p> <p>From outside the cluster, the hostname can be found in the <code>route</code>    Internally you can use port 1414 for the listener    Externally you would use port 443</p> <p>For your test, the hostname is as follows:    quickstart-cp4i-ibm-mq.tools.svc(1414)</p> <p>Perform the following from your CLI:</p> <p>Navigate to <code>/opt/mqm/samp/bin</code> (if not already there) Set the <code>MQSERVER</code> variable - this takes the form of <code>&lt;CHANNEL NAME&gt;/TCP/&lt;hostname&gt;&lt;port&gt;</code></p> <pre><code>export MQSERVER='SYSTEM.DEF.SVRCONN/TCP/quickstart-cp4i-ibm-mq.tools.svc(1414)'\n./amqsputc TEST QUICKSTART\n</code></pre> <p>Enter a message for the queue (put as many as you like by simply hitting enter after each message).  Hit enter twice to finish.</p> <p></p> <p>Next, return to the MQ Console and check that the messages are there as expected.</p> <p>If you previously <code>put</code> 2 messages on using the tools there will be 2 messages on the <code>TEST</code> queue (if not, try refreshing the display).</p> <p></p> <p>Click on the <code>TEST</code> queue to see the messages, then click on one of the messages to see the message content.</p> <p></p>"},{"location":"integration/tech-jam-labs/mq/#create-configmap-for-demo-queue-manager","title":"Create ConfigMap for Demo Queue Manager","text":"<p>In this section you will create the queue manager and its artifacts for the demo.  You will use the provided scripts for expediency.  You are encourage you to take a look at each of the scripts before you deploying.  This will give you a greater understanding of what is being created. </p> <p>!!! Danger \"Special Note:     It is assumed that you are familiar with the concept of a <code>ConfigMap</code> for storing non-sensitive configuration data.  If the contents are sensitive, you should use a secret instead.  In the case of MQ, the <code>ConfigMap</code> is used to provide <code>mqsc</code> commands and/or <code>qm.ini</code> overrides.</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: mq-demo-mqsc\n  namespace: tools\ndata:\n  mq-demo-resources.mqsc: |\n    ALTER QMGR CHLAUTH (DISABLED)\n    ALTER AUTHINFO(SYSTEM.DEFAULT.AUTHINFO.IDPWOS)  AUTHTYPE(IDPWOS) CHCKCLNT(NONE)\n    REFRESH SECURITY TYPE(CONNAUTH)\n    DEFINE CHANNEL(ACE.TO.MQ) CHLTYPE(SVRCONN) TRPTYPE(TCP) MCAUSER('mqm') REPLACE\n    DEFINE CHANNEL(ES.TO.MQ) CHLTYPE(SVRCONN) TRPTYPE(TCP) MCAUSER('mqm') REPLACE\n    DEFINE CHANNEL(MQ.TO.ES) CHLTYPE(SVRCONN) TRPTYPE(TCP) MCAUSER('mqm') REPLACE\n    DEFINE CHANNEL(CP4I.ADMIN.SVRCONN) CHLTYPE(SVRCONN) TRPTYPE(TCP) MCAUSER('mqm') SSLCAUTH(OPTIONAL) SSLCIPH(ECDHE_RSA_AES_128_CBC_SHA256) REPLACE\n    DEFINE QLOCAL(CP4I.DEMO.ERROR.Q) DEFPSIST(YES) REPLACE\n    DEFINE QLOCAL(CP4I.DEMO.STREAM.Q) DEFPSIST(YES) REPLACE\n    DEFINE QLOCAL(CP4I.DEMO.IN.Q) DEFPSIST(YES) BOQNAME(CP4I.DEMO.ERROR.Q) BOTHRESH(1) REPLACE\n    DEFINE QLOCAL(CP4I.DEMO.OUT.Q) DEFPSIST(YES) BOQNAME(CP4I.DEMO.ERROR.Q) BOTHRESH(1) REPLACE\n    DEFINE QLOCAL(CP4I.DEMO.ES.TO.MQ) DEFPSIST(YES) BOQNAME(CP4I.DEMO.ERROR.Q) BOTHRESH(1) REPLACE\n    DEFINE QLOCAL(CP4I.DEMO.MQ.TO.ES) DEFPSIST(YES) BOQNAME(CP4I.DEMO.ERROR.Q) BOTHRESH(1) REPLACE\n    DEFINE QLOCAL(CP4I.DEMO.FILE.TO.MQ) DEFPSIST(YES) BOQNAME(CP4I.DEMO.ERROR.Q) BOTHRESH(1) REPLACE\n    DEFINE QLOCAL(CP4I.DEMO.MQ.TO.FILE) DEFPSIST(YES) BOQNAME(CP4I.DEMO.ERROR.Q) BOTHRESH(1) REPLACE\n    DEFINE QLOCAL(CP4I.DEMO.API.Q) BOQNAME(CP4I.DEMO.ERROR.Q) BOTHRESH(1) STREAMQ(CP4I.DEMO.STREAM.Q) REPLACE\n    DEFINE QLOCAL(CP4I.DEMO.REQUEST.Q) BOQNAME(CP4I.DEMO.ERROR.Q) BOTHRESH(1) REPLACE\n    DEFINE QLOCAL(CP4I.DEMO.EVENT.Q) BOQNAME(CP4I.DEMO.ERROR.Q) BOTHRESH(1) REPLACE\n    DEFINE QLOCAL(CP4I.DEMO.TEMP.Q) BOQNAME(CP4I.DEMO.ERROR.Q) BOTHRESH(1) REPLACE\n    DEFINE QLOCAL(CP4I.DEMO.BACKEND.Q) BOQNAME(CP4I.DEMO.ERROR.Q) BOTHRESH(1) REPLACE\n</code></pre> <p>Reviewing the <code>ConfigMap</code>, you will notice security has been disabled for simplicity, but more importantly mq streaming has been enabled in order to highlight this new feature during the demonstration.  If you have any questions regarding what any of the <code>mqsc</code> command are doing, please feel free to ask one of the instructors.</p> <p>There are multiple ways to create the components of the demo, each of which you will experiment with during the labs.  Below you will use the OpenShift Console to create the <code>ConfigMap</code>.</p> <p>To apply the <code>yaml</code>, you must be logged into the OpenShift UI.</p> <p>Click on the little plus sign in the top right corner of the screen to import the <code>yaml</code></p> <p></p> <p>Copy and paste the <code>yaml</code> from above in to the Import YAML window and press Create</p> <p></p> <p>Go to <code>ConfigMaps</code> (they are found under the Workloads folder in the left hand menu) and check that the <code>mq-demo-mqsc</code> <code>ConfigMap</code> exists:</p> <p></p> <p>Open the ConfigMap and check that it is appears as expected.</p> <p></p> <p>You now have the config ready to be plugged into your demo queue manager.</p>"},{"location":"integration/tech-jam-labs/mq/#create-native-ha-queue-manager-for-the-demo","title":"Create Native-HA Queue Manager for the Demo","text":"<p>For the demo, you will be deploying a Native-HA queue manager.  If you are unfamiliar with a NativeHA queue manager, it is the \"Rolls Royce\" of MQ high availability solutions, which is built into CP4I.</p> <p>A Native-HA configuration provides a highly available queue manager where the recoverable MQ data (for example, the messages)  are replicated across multiple sets of storage, preventing loss from storage failures. The queue manager consists of multiple running instances, one is the leader, the others are ready to quickly take over in the event of a failure, maximizing access to the queue manager and its messages.</p> <p>This is a relatively simple description of how it works, you can read about it in much more detail in the Knowledge Center.</p> <p>The <code>yaml</code> requred to create the queue manager:</p> <pre><code>apiVersion: mq.ibm.com/v1beta1\nkind: QueueManager\nmetadata:\n  name: qmgr-demo\n  namespace: tools\nspec:\n  annotations:\n    productMetric: FREE\n  license:\n    accept: true\n    license: L-RJON-CD3JKX\n    use: NonProduction\n  queueManager:\n    name: QMGRDEMO\n    resources:\n      limits:\n        cpu: 500m\n      requests:\n        cpu: 500m\n    storage:\n      queueManager:\n        type: persistent-claim\n      defaultClass: ibmc-block-gold\n    availability:\n      type: NativeHA\n    mqsc:\n      - configMap:\n          name: mq-demo-mqsc\n          items:\n            - mq-demo-resources.mqsc\n  template:\n    pod:\n      containers:\n        - env:\n            - name: MQSNOAUT\n              value: 'yes'\n          name: qmgr\n  version: 9.3.0.0-r1\n  web:\n    enabled: true\n  tracing:\n    enabled: true\n    namespace: tools\n</code></pre> <p>Note</p> <p>This <code>yaml</code> file shows how to use annotations to let License Service know this is a Queue Manager used by ACE.  Therefor this queue won't consume VPCs.  Additionally, the instance uses Native-HA to show this functionality during the demonstration.  Further, the deployment enables tracing, but since we are using the same <code>tools</code> namespace there is no need to register again with Operations Dashboard. You are only required to register once per namespace.</p> <p>Using the same method that we used to create the <code>ConfigMap</code>, apply the <code>yaml</code> to create the queue manager.  Take a look at the pods for the queue manager.  You will note that there are three of them.</p> <p></p> <p>Wait until all of the queue manager pods are active (this may take a short time, while the storage is being provisioned). </p> <p></p> <p>You may be wondering...</p> <p>If there is only ever one queue manager pod active at the one time, why two of the pods are showing as having \u2154 container active? This is due to the Operations Dashboard tracing sidecar containers - don't panic!  The active queue manager pod is the one with 3/3 active containers.</p> <p>Feeling adventurous?  Test the workings of Native-HA: </p> <ul> <li><code>Put</code> a message on a queue (using your favorite method)</li> <li><code>oc delete</code> the active MQ pod</li> <li>Watch the takeover by one of the other pods</li> <li>Log back into the MQ console (or any of your other favurite tools) and check that the message is still on the queue (where you put it prior to deleting the pod)</li> <li>Recheck the status of all three pods</li> </ul> <p>You have completed the MQ Lab</p>"},{"location":"integration/tech-jam-labs/prereq-ace-toolkit/","title":"App Connect Enterprise Developer Toolkit Installation","text":""},{"location":"integration/tech-jam-labs/prereq-ace-toolkit/#app-connect-enterprise-developer-toolkit-installation","title":"App Connect Enterprise Developer Toolkit Installation","text":"<p>Use the IBM App Connect Enterprise Toolkit to build powerful integration applications, services, and APIs. Your integration solutions can then be deployed to the software run time, IBM App Connect on IBM Cloud\u00ae, containers, or a combination of environments.  IBM App Connect Enterprise for Developers is a full-function version of IBM App Connect Enterprise that you can use to evaluate the software, for an unlimited time, within the terms of the license.</p> <p>To download IBM App Connect Enterprise for Developers follow this link.  You will be asked to fill out a form.  Answer the required questions and choose Linux\u00ae, macOS, or Windows as your target operating system.</p> <p>Information</p> <p>If the download page does not show the options for IBM App Connect Enterprise, such as <code>ACE-WIN64-DEVELOPER.exe</code>, open the link in a private browser window or clear your browser cache and try again.</p> <p>Select the appropriate download option (for example, <code>ACE-WIN64-DEVELOPER.exe 1.26 GB</code>) and chose download.</p> <p>Installing on your OS:</p> WindowsLinuxmacOS <ul> <li>Unpack the software, which sets up IBM App Connect Enterprise Toolkit by extracting the downloaded .zip file into a local directory</li> <li>Run the installation file that you downloaded by running ACESetup12.0.n.0.exe with the options that you require</li> <li>For more information about installing IBM App Connect Enterprise on Windows, see Installing IBM App Connect Enterprise on Windows</li> </ul> <ul> <li>Create or navigate to a directory where you have write access.</li> <li>Unpack the IBM App Connect Enterprise Toolkit you downloaded into a directory where you have write access: <pre><code>tar -xzvf ace-12.0.n.0.tar.gz\n</code></pre></li> <li>Accept the license and run the installation file that you downloaded by running the command <code>./ace accept license for a single user installation</code> or <code>./ace make registry global accept license for a shared installation</code>.</li> <li>For more information about installing IBM App Connect Enterprise on Linux, see Installing IBM App Connect Enterprise on Linux</li> </ul> <ul> <li>Open the installation wizard by double-clicking the installation file that you downloaded</li> <li>The installation file has a name similar to <code>IBM App Connect Enterprise 12.0.0.x.pkg</code> but it might be automatically renamed when you download it</li> <li>Follow the instructions in the installation wizard</li> <li>IBM App Connect Enterprise is installed as a standard mac application with the name IBM App Connect Enterprise in the /Applications folder</li> </ul>"},{"location":"integration/tech-jam-labs/prereq-ace-toolkit/#test-the-installation","title":"Test the Installation","text":"<p>Start the IBM App Connect Enterprise Toolkit on your chosen platform by completing one of the following tasks:</p> WindowsLinuxmacOS <p>From the Windows Start menu, expand IBM App Connect Enterprise xx.0.0.y Developer Edition (where x denotes the version), then click IBM App Connect Enterprise Toolkit.</p> <p>From the command environment, type ./ace toolkit.</p> <p>Run the application /Applications/IBM App Connect Enterprise to start the toolkit.</p> <p>IBM App Connect Enterprise Toolkit opens to display its welcome page.</p> <p> The welcome page provides access to the Tutorials Gallery, which you can use to get started, what\u2019s new information, and an option to install language packs.</p>"},{"location":"integration/tech-jam-labs/prereq-ace-toolkit/#salesforce-connector","title":"Salesforce Connector","text":"<p>If you are using a demo or working with the Tech Jam labs, you will likely be interacting with a Sales Force System of Record (SoR).  To get started you require admin level access to your Salesforce account.  Create your free Salesforce account to test, making certain that you create a Developer account rather than a Trial account. If you connect to App Connect with a Trial account, the Salesforce events do not work.</p> <p></p> <p>To get your login URL, click on your user profile. The URL text below your Account Name is your login URL.</p> <p></p> <p>Once logged into your Salesforce account, on the left-hand Finder panel go to: PLATFORM TOOLS &gt; Apps &gt; App Manager</p> <p></p> <p>Create a New Connected App or use an existing one. Steps for creating a new app are as follows:</p> <p></p> <p>Provide a Connect App Name and the API Name is automatically generated by the system. Enter a Contact email (usually admin email address).  Be certain to Enable OAuth Settings and follow steps below to configure OAuth setting.</p> <p></p> <ul> <li>Click on Enable OAuth Settings to get the configuration panel</li> <li>Either click on Enable for Device Flow and that will auto-generate Callback URL or alternately you can provide your own fully qualified Callback URL</li> </ul> <p>Next, configure scope of access for our connectors which will be the Connected App in this case. Connectors technically only require data api, you can optionally choose to enable all the scopes for this connected app.</p> <p>Choose <code>Save</code>.</p> <p></p> <p>It will take several minutes for newly created Connected App to be registered. Once registered return to the App Manager, select and view the created App:</p> <p></p> <p>Use Consumer Key and Secret as Client ID and Client Secret as needed in the connector template. Next, retrieve the Security Token. For this click on your user profile and select Settings option in the profile panel.</p> <p></p> <p>Under Settings, find and click Reset Security Token:</p> <p></p> <p>Click on Reset Security Token Button and it will send the newly generated security token to your admin email address. Save this token, you will use this as your your credentials when making connections.</p>"},{"location":"integration/tech-jam-labs/prereq-general/","title":"Prerequisites for the Tech Jam","text":""},{"location":"integration/tech-jam-labs/prereq-general/#prerequisites-for-the-tech-jam","title":"Prerequisites for the Tech Jam","text":"<p>To complete all of the labs in the Integration Tech Jam you will likely need each of the following general utilities:</p> <ul> <li>SoapUI</li> <li>mailtrap</li> <li>OpenShift CLI (oc CLI)</li> <li>IBM Cloud Pak CLI (cloudctl)</li> <li>jq</li> <li>IBM API Connect Toolkit (apic cli)</li> <li>Container Runtime</li> <li>Git</li> </ul> <p>You will very likely find most (if not all) of these utilities valuable when performing your Cloud Pak for Integration engagements and demos.</p> <p>Disclaimer</p> <p>There are some utilities that IBM does not allow on your corporate laptops.  We have done our best to point out only utilities that fit within policy.  Ultimately you are responsible for managing these utilities and corporate compliance.</p>"},{"location":"integration/tech-jam-labs/prereq-general/#soapui","title":"SoapUI","text":"<p>This is an alternative API testing utility similar to Postman.  Postman has been added to the IBM \"not approved\" list.  From this page you can download and install the SoapUI Open Source utility.  Specify the SoapUI tool during installation and if you have never used SoapUI, you may wish to download the tutorials.</p>"},{"location":"integration/tech-jam-labs/prereq-general/#mailtrap","title":"Mailtrap","text":"<p>Heads-Up</p> <p>Use a non-IBM email address when signing up for mailtrap.  This will help you avoid some extra spam.</p> <p>This email service is an easy, low profile way to provide a mail service used when configuring APIC.  Follow the steps at mailtrap.io to configure this tool.  </p> <p>You will need your user and password for mailtrap when configuring APIC.</p>"},{"location":"integration/tech-jam-labs/prereq-general/#openshift-cli","title":"OpenShift CLI","text":"<p>You very likely already have the OpenShift CLI installed on your laptop.  If not follow the instructions from Red Hat here  If you are unsure if you have the CLI?  Open a terminal window and type <code>oc</code>.</p>"},{"location":"integration/tech-jam-labs/prereq-general/#ibm-cloud-pak-cli-cloudctl","title":"IBM Cloud Pak CLI (cloudctl)","text":"<p>You can use IBM Cloud Pak\u00ae CLI (cloudctl) to view information about your cluster, manage your cluster, and more.  Instructions for installing the latest version are found here.</p>"},{"location":"integration/tech-jam-labs/prereq-general/#jq","title":"jq","text":"<p>jq is a lightweight and flexible command-line JSON processor.  For Mac users use <code>brew install jq</code> otherwise visit the jq home page for Windows and Linux.</p>"},{"location":"integration/tech-jam-labs/prereq-general/#container-runtime","title":"Container Runtime","text":"<p>If you do not have a container runtime installed on your laptop, you should install one.  Docker Desktop requires a license and is not a preferred option.  Consider installing Colima or Podman</p>"},{"location":"integration/tech-jam-labs/prereq-general/#git","title":"Git","text":"<p>If you are unfamiliar with Git and running Git commands follow this tutorial.  You will not need to be an expert but some familiarity will help.</p>"},{"location":"k8s/cheats/","title":"Kubernetes Lab Solutions","text":""},{"location":"k8s/cheats/#kubernetes-lab-solutions","title":"Kubernetes Lab Solutions","text":"<ul> <li>Lab K8s 1 - Pod Creation</li> <li>Lab K8s 2 - Multiple Containers</li> <li>Lab K8s 3 - Probes</li> <li>Lab K8s 4 - Troubleshooting</li> <li>Lab K8s 5 - Pod Configuration</li> <li>Lab K8s 6 - Rolling Updates</li> <li>Lab K8s 7 - Cron Jobs</li> <li>Lab K8s 8 - Creating Services</li> <li>Lab K8s 9 - Network Policies</li> <li>Lab K8s 10 - Persistent Volumes</li> </ul>"},{"location":"k8s/creating-services/","title":"Lab K8s 8 - Services","text":""},{"location":"k8s/creating-services/#lab-k8s-8-services","title":"Lab K8s 8 - Services","text":""},{"location":"k8s/creating-services/#problem","title":"Problem","text":"<p>We have a <code>jedi-deployment</code> and <code>yoda-deployment</code> that need to communicate with others.  The <code>jedi</code> needs to talk to the world(outside the cluster), while <code>yoda</code> only needs to talk to jedi council(others in the cluster).</p>"},{"location":"k8s/creating-services/#your-task","title":"Your Task","text":"<ul> <li>Examine the two deployments, and create two services that meet the following criteria:</li> </ul> <p>jedi-svc  - The service name is <code>jedi-svc</code>.  - The service exposes the pod replicas managed by the deployment named <code>jedi-deployment</code>.  - The service listens on port <code>80</code> and its targetPort matches the port exposed by the pods.  - The service type is <code>NodePort</code>.</p> <p>yoda-svc  - The service name is <code>yoda-svc</code>.  - The service exposes the pod replicas managed by the deployment named <code>yoda-deployment</code>.  - The service listens on port <code>80</code> and its targetPort matches the port exposed by the pods.  - The service type is <code>ClusterIP</code>.</p>"},{"location":"k8s/creating-services/#setup-environment","title":"Setup environment:","text":"<pre><code>kubectl apply -f https://gist.githubusercontent.com/csantanapr/87df4292e94441617707dae5de488cf4/raw/cb515f7bae77a3f0e76fdc7f6aa0f4e89cc5fec7/lab-8-service-setup.yaml\n</code></pre>"},{"location":"k8s/cron-jobs/","title":"Lab K8s 7 - Cron Jobs","text":""},{"location":"k8s/cron-jobs/#lab-k8s-7-cron-jobs","title":"Lab K8s 7 - Cron Jobs","text":""},{"location":"k8s/cron-jobs/#the-problem","title":"The Problem","text":"<p>Your commander has a simple data process that is run periodically to check status. They would like to stop doing this manually in order to save time, so you have been asked to implement a cron job in the Kubernetes cluster to run this process.   - Create a cron job called xwing-cronjob using the <code>docker.io/nginx:latest</code> image.   - Have the job run every second minute with the following cron expression: <code>*/2 * * * *</code>.  - Using the echo command, echo out <code>Welcome to IBM CE Platform Engineer Bootcamp</code>.</p>"},{"location":"k8s/cron-jobs/#verification","title":"Verification","text":"<ul> <li>Run <code>kubectl get cronjobs.batch</code> and <code>LAST-SCHEDULE</code> to see last time it ran</li> <li>From a bash shell, run the following to see the logs for all jobs:</li> </ul> <pre><code>jobs=( $(kubectl get jobs --no-headers -o custom-columns=\":metadata.name\") )\necho -e \"Job \\t\\t\\t\\t Pod \\t\\t\\t\\t\\tLog\"\nfor job in \"${jobs[@]}\"\ndo\n   pod=$(kubectl get pods -l job-name=$job --no-headers -o custom-columns=\":metadata.name\")\n   echo -en \"$job \\t $pod \\t\"\n   kubectl logs $pod\ndone\n</code></pre>"},{"location":"k8s/debugging/","title":"Lab K8s 4 - Troubleshooting","text":""},{"location":"k8s/debugging/#lab-k8s-4-troubleshooting","title":"Lab K8s 4 - Troubleshooting","text":""},{"location":"k8s/debugging/#the-problem","title":"The Problem","text":"<p>The application is not t working and we need to find out why. Let's debug the <code>my-deployment</code> deployment so that we can get the application to run again.</p> <p>Here are some tips to help you solve the issues with my-deployment in the project <code>debug</code></p> <ul> <li>Check the description of <code>my-deployment</code>.</li> <li>Will the image in the Deployment deploy?</li> <li>Get and save the logs of one of the broken <code>pods</code>.</li> <li>Are the correct <code>ports</code> assigned.</li> <li>Make sure your <code>labels</code> and <code>selectors</code> are correct.</li> <li>Check to see if the <code>Probes</code> are correctly working.</li> <li> <p>To fix the deployment, save then modify the yaml file for redeployment.</p> </li> <li> <p>Reset the environment:</p> <pre><code>oc project default\noc delete project default\n</code></pre> </li> <li> <p>Setup the environment:</p> <pre><code>oc apply -f https://gist.githubusercontent.com/csantanapr/e823b1bfab24186a26ae4f9ec1ff6091/raw/1e2a0cca964c7b54ce3df2fc3fbf33a232511877/debugk8s-bad.yaml\n</code></pre> </li> <li> <p>Set the project to <code>debug</code>.</p> <pre><code>oc project default\n</code></pre> </li> </ul>"},{"location":"k8s/debugging/#validate","title":"Validate","text":"<ol> <li> <p>Use the OpenShift console or the <code>oc cli</code> to examine the deployment <code>my-deployment in the project</code>debug`</p> <pre><code>oc project debug\noc describe deployment my-deployment\noc get pods\noc events &lt;podname&gt;\n</code></pre> </li> <li> <p>Use the following commands to investigate.  Use the OpenShift console to verify and organize what you are looking at.</p> <pre><code>oc get deployments\noc describe pod &lt;podname&gt;\noc explain Pod.spec.containers.resources.requests\noc explain Pod.spec.containers.livenessProbe\noc edit deployment\noc logs\noc events\noc get pods --show-labels\noc get deployment --show-labels\n</code></pre> </li> </ol>"},{"location":"k8s/ingress/","title":"Lab K8s 10 - Ingress Controller IBM Free K8s Cluster","text":""},{"location":"k8s/ingress/#lab-k8s-10-ingress-controller-ibm-free-k8s-cluster","title":"Lab K8s 10 - Ingress Controller IBM Free K8s Cluster","text":"<p>The IBM Kubernetes service free clusters consist of a single worker node with 2 CPU and 4 GB of memory for experimenting with Kubernetes. Unlike the fee-based service, these clusters do not include capabilities for application load balancing using ingress out-of-the-box. </p>"},{"location":"k8s/ingress/#prerequisites","title":"Prerequisites","text":"<ul> <li>Free IBM Kubernetes Cluster (IKS) - upgrade your account from Lite plan to create one. In the example commands, we'll assume that this cluster is named <code>mycluster</code></li> <li>kubectl - match your cluster API version </li> <li>Log in to IBM Cloud and configure <code>kubectl</code> using the <code>ibmcloud ks cluster config --cluster mycluster</code> command</li> </ul>"},{"location":"k8s/ingress/#components","title":"Components","text":"<p>On the IKS cluster, you will install helm charts for a nginx ingress controller from NGINX. This lab already provides the templated yaml files so there is no need to use helm cli.</p>"},{"location":"k8s/ingress/#set-up-the-ingress-controller","title":"Set up the ingress controller","text":"<p>Only do this on a free IKS instance These steps assume facts that only apply to free IKS instances:</p> <ul> <li>a single worker where the cluster administrator can create pods that bind to host ports</li> <li>no pre-existing ingress controller or application load balancer</li> </ul> <p>Using the following steps with a paid instance can cause issues. See the IBM Cloud containers documentation for information on exposing applications with the ingress/alb services for paid clusters. You have been warned</p> <ol> <li> <p>Install the NGINX ingress controller with <code>helm</code> using a daemonset and no service resource (which will result in a single pod that binds to ports 80 and 443 on the worker node and will skip creation of a <code>ClusterIP, LoadBalancer, or NodePort</code> for the daemonset).     </p><pre><code>kubectl apply -f https://cloudnative101.dev/yamls/ingress-controller/iks-ingress-v1.7.1.yaml\n</code></pre><p></p> </li> <li> <p>You can use free domain <code>.nip.io</code> to get a domain name using one of the IP Address of your worker nodes. Run this command to set your DOMAIN     </p><pre><code>export DOMAIN=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"ExternalIP\")].address}').nip.io\necho $DOMAIN\n</code></pre><p></p> </li> <li> <p>You can test the ingress controller using the <code>$DOMAIN</code>:</p> <p></p><pre><code>curl -I http://$DOMAIN\n</code></pre> <pre><code>HTTP/1.1 404 Not Found\nServer: nginx/1.17.10\n...\n</code></pre><p></p> <p>A 404 is expected at this point because unlike the kubernetes nginx ingress, the NGINX version of the ingress controller does not create a default backend deployment.</p> </li> <li> <p>To use the ingress controller deploy a sample application, expose a service.     </p><pre><code>kubectl create deployment web --image=bitnami/nginx\nkubectl expose deployment web --name=web --port 8080\n</code></pre><p></p> </li> <li> <p>Now create an Ingress resource     </p><pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: networking.k8s.io/v1beta1\nkind: Ingress\nmetadata:\n  name: web\n  labels:\n    app: web\nspec:\n  rules:\n    - host: web.$DOMAIN\n      http:\n        paths:\n          - path: /\n            backend:\n              serviceName: web\n              servicePort: 8080\nEOF\necho \"Access your web app at http://web.$DOMAIN\"\n</code></pre><p></p> </li> <li> <p>List the created ingress     </p><pre><code>kubectl get ingress web\n</code></pre><p></p> </li> <li> <p>Access your web application    </p><pre><code>curl http://web.$DOMAIN\n</code></pre>    The output prints the html    <pre><code>&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;\n</code></pre><p></p> </li> <li> <p>Delete all the resources created     </p><pre><code>kubectl delete deployment,svc,ingress -l app=web\n</code></pre><p></p> </li> </ol>"},{"location":"k8s/multi-containers/","title":"Lab K8s 2 - Manage Multiple Containers","text":""},{"location":"k8s/multi-containers/#lab-k8s-2-manage-multiple-containers","title":"Lab K8s 2 - Manage Multiple Containers","text":""},{"location":"k8s/multi-containers/#the-problem","title":"The Problem","text":"<p>This service has already been packaged into a container image, but there is one special requirement:</p> <ul> <li>The legacy app is hard-coded to only serve content on port <code>8989</code>, but the team wants to be able to access the service using the standard port <code>80</code>.</li> </ul> <p>Your task is to build a Kubernetes pod that runs this legacy container and uses the ambassador design pattern to expose access to the service on port <code>80</code>.</p> <p>This setup will need to meet the following specifications:</p> <ul> <li>The pod should have the name <code>vader-service</code>.</li> <li>The <code>vader-service</code> pod should have a container that runs the legacy vader service image: <code>quay.io/don_bailey_ibm/millennium-falcon:v1.0.0</code></li> <li>The <code>vader-service</code> pod should have an ambassador container that runs the <code>haproxy:1.7</code> image and proxies incoming traffic on port <code>80</code> to the legacy service on port <code>8989</code> (the HAProxy configuration for this is provided below).</li> <li>Port <code>80</code> should be exposed as a <code>containerPort</code>.</li> </ul> <p></p> <p>Note: You do not need to expose port 8989</p> <p></p> <ul> <li>The HAProxy configuration should be stored in a ConfigMap called <code>vader-service-ambassador-config</code>.</li> <li> <p>The HAProxy config should be provided to the ambassador container using a volume mount that places the data from the ConfigMap in a file at /usr/local/etc/haproxy/haproxy.cfg. haproxy.cfg should contain the following configuration data:</p> <pre><code>global\n    daemon\n    maxconn 256\n\ndefaults\n    mode http\n    timeout connect 5000ms\n    timeout client 50000ms\n    timeout server 50000ms\n\nlisten http-in\n    bind *:80\n    server server1 127.0.0.1:8989 maxconn 32\n</code></pre> </li> <li> <p>Create the Pod yaml vader-service.yaml.  This yaml will define a pod with 2 containers.</p> <ul> <li>a container running the image: <code>quay.io/don_bailey_ibm/millennium-falcon:v1.0.0</code></li> <li>a container running the image: <code>haproxy:1.7</code></li> <li>the haproxy container mounts the config map at <code>/usr/local/etc/haproxy/haproxy.cfg</code></li> <li>the pod exposes port 80</li> </ul> </li> <li> <p>Apply the yaml files to an OpenShift cluster to create the cluster resources.</p> </li> </ul> <p>Once the vader-service pod is up and running, it's a good idea to test it to make sure you can access the service from within the cluster using port 80. In order to do this, create a busybox pod in the cluster, and then run a command to attempt to access the service from within the busybox pod.</p> <ol> <li> <p>descriptor for the busybox pod called <code>busybox.yml</code>.  </p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\nspec:\n  containers:\n  - name: myapp-container\n    image: radial/busyboxplus:curl\n    command: ['sh', '-c', 'while true; do sleep 3600; done']\n</code></pre> </li> <li> <p>Create the busybox testing pod.</p> <ul> <li> <p>OpenShift</p> <pre><code>oc apply -f busybox.yml -n web\n</code></pre> </li> <li> <p>Kubernetes</p> <pre><code>kubectl apply -f busybox.yml -n web\n</code></pre> </li> </ul> </li> <li> <p>Use this command to access <code>vader-service</code> using port 80 from within the busybox pod.</p> <ul> <li> <p>OpenShift</p> <pre><code>oc exec busybox -- curl $(oc get pod vader-service -o=custom-columns=IP:'{.status.podIP}' --no-headers):80\n</code></pre> </li> <li> <p>Kubernetes</p> <pre><code>kubectl exec busybox -- curl $(kubectl get pod vader-service -o=custom-columns=IP:'{.status.podIP}' --no-headers):80\n</code></pre> </li> </ul> </li> </ol> <p>If the service is working, you should get a message that the hyper drive of the millennium falcon needs repair.</p> <p>Related Documentation</p> <ul> <li>Kubernetes Sidecar Logging Agent</li> <li>Shared Volumes</li> <li>Distributed System Toolkit Patterns</li> </ul>"},{"location":"k8s/network-policies/","title":"Lab K8s 9 - Network Policies","text":""},{"location":"k8s/network-policies/#lab-k8s-9-network-policies","title":"Lab K8s 9 - Network Policies","text":""},{"location":"k8s/network-policies/#the-problem","title":"The Problem","text":"<p>Setup minikube</p> <pre><code>minikube start --network-plugin=cni\nkubectl apply -f https://docs.projectcalico.org/v3.9/manifests/calico.yaml\nkubectl -n kube-system set env daemonset/calico-node FELIX_IGNORELOOSERPF=true\nkubectl -n kube-system get pods | grep calico-node\n</code></pre> <p>Create secured pod </p><pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: network-policy-secure-pod\n  labels:\n    app: secure-app\nspec:\n  containers:\n  - name: nginx\n    image: bitnami/nginx\n    ports:\n    - containerPort: 8080\n</code></pre><p></p> <p>Create client pod </p><pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: network-policy-client-pod\nspec:\n  containers:\n  - name: busybox\n    image: radial/busyboxplus:curl\n    command: [\"/bin/sh\", \"-c\", \"while true; do sleep 3600; done\"]\n</code></pre><p></p> <p>Create a policy to allow only client pods with label <code>allow-access: \"true\"</code> to access secure pod</p>"},{"location":"k8s/persistent-volumes/","title":"Lab K8s 10 - Persistent Volumes","text":""},{"location":"k8s/persistent-volumes/#lab-k8s-10-persistent-volumes","title":"Lab K8s 10 - Persistent Volumes","text":""},{"location":"k8s/persistent-volumes/#the-problem","title":"The Problem","text":"<p>The death star plans can't be lost no matter what happens so we need to make sure we protect them at all costs.</p> <p>In order to do that you will need to do the following:</p> <p>Create a <code>PersistentVolume</code>:</p> <ul> <li> <p>The PersistentVolume should be named <code>postgresql-pv</code>.</p> </li> <li> <p>The volume needs a capacity of <code>1Gi</code>.</p> </li> <li> <p>Use a storageClassName of <code>localdisk</code>.</p> </li> <li> <p>Use the accessMode <code>ReadWriteOnce</code>.</p> </li> <li> <p>Store the data locally on the node using a <code>hostPath</code> volume at the location <code>/mnt/data</code>.</p> </li> </ul> <p>Create a <code>PersistentVolumeClaim</code>:</p> <ul> <li> <p>The PersistentVolumeClaim should be named <code>postgresql-pv-claim</code>.</p> </li> <li> <p>Set a resource request on the claim for <code>500Mi</code> of storage.</p> </li> <li> <p>Use the same storageClassName and accessModes as the PersistentVolume so that this claim can bind to the PersistentVolume.</p> </li> </ul> <p>Create a <code>Postgresql</code> Pod configured to use the <code>PersistentVolumeClaim</code>: - The Pod should be named <code>postgresql-pod</code>.</p> <ul> <li> <p>Use the image <code>bitnami/postgresql</code>.</p> </li> <li> <p>Expose the containerPort <code>5432</code>.</p> </li> <li> <p>Set an <code>environment variable</code> called <code>MYSQL_ROOT_PASSWORD</code> with the value <code>password</code>.</p> </li> <li> <p>Add the <code>PersistentVolumeClaim</code> as a volume and mount it to the container at the path <code>/bitnami/postgresql/</code>.</p> </li> </ul>"},{"location":"k8s/pod-config/","title":"Lab K8s 5 - Pod Configuration","text":""},{"location":"k8s/pod-config/#lab-k8s-5-pod-configuration","title":"Lab K8s 5 - Pod Configuration","text":""},{"location":"k8s/pod-config/#the-problem","title":"The Problem","text":"<ul> <li>Create a pod definition named <code>yoda-service-pod.yml</code>, and then create a pod in the cluster using this definition to make sure it works.</li> </ul> <p>The specifications are as follows:</p> <ul> <li>The current image for the container is <code>bitnami/nginx</code>. You do not need a custom command or args.</li> <li>There is some configuration data the container will need:<ul> <li><code>yoda.baby.power=100000000</code></li> <li><code>yoda.strength=10</code></li> </ul> </li> <li>It will expect to find this data in a file at <code>/etc/yoda-service/yoda.cfg</code>. Store the configuration data in a ConfigMap called <code>yoda-service-config</code> and provide it to the container as a mounted volume.</li> <li>The container should expect to use <code>64Mi</code> of memory and <code>250m</code> CPU (use resource requests).</li> <li>The container should be limited to <code>128Mi</code> of memory and <code>500m</code> CPU (use resource limits).</li> <li>The container needs access to a database password in order to authenticate with a backend database server. The password is <code>0penSh1ftRul3s!</code>. It should be stored as a Kubernetes secret called <code>yoda-db-password</code> and passed to the container as an environment variable called <code>DB_PASSWORD</code>.</li> <li>The container will need to access the Kubernetes API using the ServiceAccount <code>yoda-svc</code>. Create the service account if it doesn't already exist, and configure the pod to use it.</li> </ul>"},{"location":"k8s/pod-config/#verification","title":"Verification","text":"<p>To verify your setup is complete, check <code>/etc/yoda-service</code> for the <code>yoda.cfg</code> file and use the <code>cat</code> command to check it's contents.</p> <pre><code>kubectl exec -it yoda-service /bin/bash\ncd /etc/yoda-service\ncat yoda.cfg\n</code></pre>"},{"location":"k8s/pod-creation/","title":"Lab K8s 1 - Pod Creation","text":""},{"location":"k8s/pod-creation/#lab-k8s-1-pod-creation","title":"Lab K8s 1 - Pod Creation","text":""},{"location":"k8s/pod-creation/#the-problem","title":"The Problem","text":"<ul> <li>Write a pod definition named <code>yoda-service-pod.yml</code> Then create a pod in the cluster using this definition to make sure it works.</li> </ul> <p>The specifications of this pod are as follows:</p> <ul> <li>Use the <code>bitnami/nginx</code> container image.</li> <li>The container needs a containerPort of <code>80</code>.</li> <li>Set the command to run as <code>nginx</code></li> <li>Pass in the <code>-g daemon off; -q</code> args to run nginx in quiet mode.</li> <li>Create the pod in the <code>web</code> namespace.</li> </ul>"},{"location":"k8s/pod-creation/#verification","title":"Verification","text":"<p>When you have completed this lab, use the following commands to validate your solution. The 'get pods' command will </p> <pre><code>kubectl get pods -n web\nkubectl describe pod nginx -n web\n</code></pre>"},{"location":"k8s/probes/","title":"Lab K8s 3 - Probes","text":""},{"location":"k8s/probes/#lab-k8s-3-probes","title":"Lab K8s 3 - Probes","text":""},{"location":"k8s/probes/#container-health-issues","title":"Container Health Issues","text":"<p>The first issue is caused by application instances entering an unhealthy state and responding to user requests with error messages. Unfortunately, this state does not cause the container to stop, so the Kubernetes cluster is not able to detect this state and restart the container. Luckily, the application has an internal endpoint that can be used to detect whether or not it is healthy. This endpoint is <code>/tmp/healthz</code>.</p> <ul> <li>Your first task will be to create a probe to check this endpoint periodically.</li> <li>If the endpoint returns an error or fails to respond, the probe will detect this and the cluster will restart the container.</li> </ul>"},{"location":"k8s/probes/#container-startup-issues","title":"Container Startup Issues","text":"<p>Another issue is caused by new pods when they are starting up. The application takes a few seconds after startup before it is ready to service requests. As a result, some users are getting error message during this brief time.</p> <ul> <li> <p>To fix this, you will need to create another probe. To detect whether the application is <code>ready</code>, the probe should simply make a request to the root endpoint, <code>/tmp/healthz</code>. If this request succeeds, then the application is ready.</p> </li> <li> <p>Also set a <code>initial delay</code> of <code>6 seconds</code> for the probes.</p> </li> </ul> <p>Here is the Pod yaml file,  add the probes, then create the pod in the cluster to test it.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    test: liveness\n  name: liveness-example\nspec:\n  containers:\n  - name: liveness\n    image: docker.io/busybox\n    args:\n    - /bin/sh\n    - -c\n    - touch /tmp/healthz; sleep 40; rm -f /tmp/healthz; sleep 700\n</code></pre>"},{"location":"k8s/rolling-updates/","title":"Lab K8s 6 - Rolling Updates","text":""},{"location":"k8s/rolling-updates/#lab-k8s-6-rolling-updates","title":"Lab K8s 6 - Rolling Updates","text":""},{"location":"k8s/rolling-updates/#the-problem","title":"The Problem","text":"<p>Your company's developers have just finished developing a new version of their jedi-themed mobile game. They are ready to update the backend services that are running in your Kubernetes cluster. There is a deployment in the cluster managing the replicas for this application. The deployment is called <code>jedi-deployment</code>. You have been asked to update the image for the container named <code>jedi-ws</code> in this deployment template to a new version, <code>bitnami/nginx:1.18.0</code>.</p> <p>After you have updated the image using a rolling update, check on the status of the update to make sure it is working. If it is not working, perform a rollback to the previous state.</p> <p>Setup environment </p><pre><code>kubectl apply -f https://gist.githubusercontent.com/csantanapr/87df4292e94441617707dae5de488cf4/raw/cb515f7bae77a3f0e76fdc7f6aa0f4e89cc5fec7/lab-6-rolling-updates-setup.yaml\n</code></pre><p></p>"},{"location":"k8s/openshift/","title":"Kubernetes &amp; OpenShift Overview","text":""},{"location":"k8s/openshift/#kubernetes-openshift-overview","title":"Kubernetes &amp; OpenShift Overview","text":"<p>Kubernetes is an open source container orchestration platform that automates deployment, management and scaling of applications. Learn how Kubernetes enables cost-effective cloud native development.</p>"},{"location":"k8s/openshift/#what-is-kubernetes","title":"What is Kubernetes?","text":"<p>Kubernetes\u2014also known as \u2018k8s\u2019 or \u2018kube\u2019\u2014is a container orchestration platform for scheduling and automating the deployment, management, and scaling of containerized applications.</p> <p>Kubernetes was first developed by engineers at Google before being open sourced in 2014. It is a descendant of \u2018Borg,\u2019 a container orchestration platform used internally at Google. (Kubernetes is Greek for helmsman or pilot, hence the helm in the Kubernetes logo.)</p> <p>Today, Kubernetes and the broader container ecosystem are maturing into a general-purpose computing platform and ecosystem that rivals\u2014if not surpasses\u2014virtual machines (VMs) as the basic building blocks of modern cloud infrastructure and applications. This ecosystem enables organizations to deliver a high-productivity Platform-as-a-Service (PaaS) that addresses multiple infrastructure- and operations-related tasks and issues surrounding cloud native development so that development teams can focus solely on coding and innovation.     </p> <ul> <li> <p> Learning Kubernetes</p> <p>Learning Kubernetes through IBM Learning</p> <p> Getting started</p> </li> </ul>"},{"location":"k8s/openshift/#presentations","title":"Presentations","text":"<p>Kubernetes Overview </p>"},{"location":"k8s/openshift/#predictable-demands-pattern","title":"Predictable Demands Pattern","text":"<p>An application's performance, efficiency, and behaviors are reliant upon it's ability to have the appropriate allocation of resources.  The Predictable Demands pattern is based on declaring the dependencies and resources needed by a given application.  The scheduler will prioritize an application with a defined set of resources and dependencies since it can better manage the workload across nodes in the cluster.  Each application has a different set of dependencies which we will touch on next.</p>"},{"location":"k8s/openshift/#runtime-dependencies","title":"Runtime Dependencies","text":"<p>One of the most common runtime dependency's is the exposure of a container's specific port through hostPort.  Different applications can specify the same port through hostPort which reserves the port on each node in the cluster for the specific container.  This declaration restricts multiple continers with the same hostPort to be deployed on the same nodes in the cluster and restricts the scale of pods to the number of nodes you have in the cluster.  </p> <p>Another runtime dependency is file storage for saving the application state.  Kubernetes offers Pod-level storage utilities that are capable of surviving container restarts.  Applications needing to read or write to these storage mechanisms will require nodes that is provided the type of volume required by the application.  If there is no nodes available with the required volume type, then the pod will not be scheduled to be deployed at all.</p> <p>A different kind of dependency is configurations.  ConfigMaps are used by Kubernetes to strategically plan out how to consume it's settings through either environment variables or the filesystem.  Secrets are consumed the same way as a ConfigMaps in Kubernetes.  Secrets are a more secure way to distribute environment-specific configurations to containers within the pod. </p>"},{"location":"k8s/openshift/#resource-profiles","title":"Resource Profiles","text":"<p>Resource Profiles are definitions for the compute resources required for a container.  Resources are categorized in two ways, compressible and incompressible.  Compressible resources include resources that can be throttled such as CPU or network bandwidth. Incompressible represents resouces that can't be throttled such as memory where there is no other way to release the allocated resource other than killing the container.  The difference between compressible and incompressible is very important when it comes to planning the deployment of pods and containers since the resource allocation can be affected by the limits of each.</p> <p>Every application needs to have a specified minimum and maximum amount of resources that are needed.  The minimum amount is called \"requests\" and the maximum is the \"limits\".  The scheduler uses the requests to determine the assignment of pods to nodes ensuring that the node will have enough capacity to accommodate the pod and all of it's containers required resources.  An example of defined resource limits is below:</p>"},{"location":"k8s/openshift/#quality-of-service-levels","title":"Quality of Service Levels","text":"<p>Different levels of Quality of Service (QoS) are offered based on the specified requests and limits.</p> <code>Best Effort</code> Lowest priority pod with no requests or limits set for it's containers. These pods will be the first of any pods evicted if resources are low. <code>Burstable</code> Limits and requests are defined but they are not equal.  The pod will use the minimum amount of resources, but will consume more if needed up to the limit.  If the needed resources become scarce then these pods will be evicted if no Best Effort pods remain on a node. <code>Guaranteed</code> Highest priority pods with an equal amount of requests and limits. These pods will be the last to be evicted if resources run low and no Best Effort or Burstable pods remain to be evicted."},{"location":"k8s/openshift/#pod-priority","title":"Pod Priority","text":"<p>The priority of pods can be defined through a PriorityClass object. The PriorityClass object allows developers to indicate the importance of a pod relative to the other pods in the cluster.  The higher the priority number then the higher the priority of the pod. The scheduler looks at a pods priorityClassName to populate the priority of new pods.  As pods are being placed in the scheduling queue for deployment, the scheduler orders them from highest to lowest.</p> <p>Another key feature for pod priority is the Preemption feature.  The Preemption feature occurs when there are no nodes with enough capacity to place a pod.  If this occurs the scheduler can preempt (remove) lower-priority Pods from nodes to free up resources and place Pods with higher priority.  This effectively allows system administrators the ability to control which critical pods get top priority for resources in the cluster as well as controlling which critical workloads are able to be run on the cluster first. If a pod can not be scheduled due to constraints it will continue on with lower-priority pods.</p> <p>Pod Priority should be used with caution for this gives users the ability to control over the kubernetes scheduler and ability to place or kill pods that may interrupt the cluster's critical functions.  New pods with higher priority than others can quickly evict pods with lower priority that may be critical to a container's performance.  ResourceQuota and PodDisruptionBudget are two tools that help combat this from happening read more here.</p>"},{"location":"k8s/openshift/#declarative-deployment-pattern","title":"Declarative Deployment Pattern","text":"<p>With a growing number of microservices, reliance on an updating process for the services has become ever more important. Upgrading services is usually accompanied with some downtime for users or an increase in resource usage.  Both of these can lead to an error effecting the performance of the application making the release process a bottleneck.  </p> <p>A way to combat this issue in Kubernetes is through the use of Deployments.  There are different approaches to the updating process that we will cover below. Any of these approaches can be put to use in order to save time for developers during their release cycles which can last from a few minutes to a few months. </p>"},{"location":"k8s/openshift/#rolling-deployment","title":"Rolling Deployment","text":"<p>A Rolling Deployment ensures that there is no downtime during the update process.  Kubernetes creates a new ReplicaSet for the new version of the service to be rolled out.  From there Kubernetes creates set of pods of the new version while leaving the old pods running.  Once the new pods are all up and running they will replace the old pods and become the primary pods users access.</p> <p></p> <p>The upside to this approach is that there is no downtime and the deployment is handled by kubernetes through a deployment like the one below. The downside is with two sets of pods running at one time there is a higher usage of resources that may lead to performance issues for users. </p>"},{"location":"k8s/openshift/#fixed-deployment","title":"Fixed Deployment","text":"<p>A Fixed Deployment uses the Recreate strategy which sets the maxUnavailable setting to the number of declared replicas.  This in effect starts the versions of the pods as the old versions are being killed.  The starting and stopping of containers does create a little bit of downtime for customers while the starting and stopping is taking place, but the positive side is the users will only have to handle one version at a time.</p> <p></p>"},{"location":"k8s/openshift/#blue-green-release","title":"Blue-Green Release","text":"<p>A Blue-Green Release involves a manual process of creating a second deployment of pods with the newest version of the application running as well as keeping the old version of pods running in the cluster.  Once the new pods are up and running properly the administrator shifts the traffic over to the new pods. Below is a diagram showing both versions up and running with the traffic going to the newer (green) pods.</p> <p></p> <p>The downfall to this approach is the use of resources with two separate groups of pods running at the same time which could cause performance issues or complications. However, the advantage of this approach is users only experience one version at a time and it's easy to quickly switch back to the old version with no downtime if an issue arises with the newer version.</p>"},{"location":"k8s/openshift/#canary-release","title":"Canary Release","text":"<p>A Canary Release involves only standing up one pod of the new application code and shifting only a limited amount of new users traffic to that pod.  This approach reduces the number of people exposed to the new service allowing the administrator to see how the new version is performing.  Once the team feels comfortable with the performance of the new service then more pods can be stood up to replace the old pods.  An advantage to this approach is no downtime with any of the services as the new service is being scaled. </p> <p></p>"},{"location":"k8s/openshift/#health-probe-pattern","title":"Health Probe Pattern","text":"<p>The Health Probe pattern revolves the health of applications being communicated to Kubernetes. To be fully-automatable, cloud-applications must be highly observable in order for Kubernetes to know which applications are up and ready to receive traffic and which cannot. Kubernetes can use that information for traffic direction, self-healing, and to achieve the desired state of the application.</p>"},{"location":"k8s/openshift/#process-health-checks","title":"Process Health Checks","text":"<p>The simplest health check in kubernetes is the Process Health Check.  Kubernetes simply probes the application's processes to see if they are running or not. The process check tells kubernetes when a process for an application needs to be restarted or shut down in the case of a failure.</p>"},{"location":"k8s/openshift/#liveness-probes","title":"Liveness Probes","text":"<p>A Liveness Probe is performed by the Kubernetes Kubelet agent and asks the container to confirm it's health.  A simple process check can return that the container is healthy, but the container to users may not be performing correctly.  The liveness probe addresses this issue but asking the container for its health from outside of the container itself. If a failure is found it may require that the container be restarted to get back to normal health.  A liveness probe can perform the following actions to check health:</p> <ul> <li>HTTP GET and expects a success which is code 200-399.</li> <li>A TCP Socket Probe and expects a successful connection.</li> <li>A Exec Probe which executes a command and expects a successful exit code (0).</li> </ul> <p>The action chosen to be performed for testing depends on the nature of the application and which action fits best. Always keep in mind that a failing health check results in a restart of the container from Kubernetes, so make sure the right health check is in place if the underlying issue can't be fixed.</p>"},{"location":"k8s/openshift/#readiness-probes","title":"Readiness Probes","text":"<p>A Readiness Probe is very similar to a Liveness probe, but the resulting action to a failed Readiness probe is different.  When a liveness probe fails the container is restarted and, in some scenarios, a simple restart won't fix the issue, which is where a readiness probe comes in.  A failed readiness probe won't restart the container but will disconnect it from the traffic endpoint.  Removing a container from traffic allows it to get up and running smoothly before being tossed into service unready to handle requests from users.  Readiness probes give an application time to catch up and make itself ready again to handle more traffic versus shutting down completely and simply creating a new pod. In most cases, liveness and readiness probes are run together on the same application to make sure that the container has time to get up and running properly as well as stays healthy enough to handle the traffic. </p>"},{"location":"k8s/openshift/#managed-lifecycle-pattern","title":"Managed Lifecycle Pattern","text":"<p>The Managed Lifecycle pattern describes how containers need to adapt their lifecycles based on the events that are communicated from a managing platform such as Kubernetes.  Containers do not have control of their own lifecycles.  It's the managing platforms that allow them to live or die, get traffic or have none, etc.  This pattern covers how the different events can affect those lifecycle decisions.</p>"},{"location":"k8s/openshift/#sigterm","title":"SIGTERM","text":"<p>The SIGTERM is a signal that is sent from the managing platform to a container or pod that instructs the pod or container to shutdown or restart.  This signal can be sent due to a failed liveness test or a failure inside the container.  SIGTERM allows the container to cleaning and properly shut itself down versus SIGKILL, which we will get to next. Once received, the application will shutdown as quickly as it can, allowing other processes to stop properly and cleaning up other files.  Each application will have a different shutdown time based on the tasks needed to be done.</p>"},{"location":"k8s/openshift/#sigkill","title":"SIGKILL","text":"<p>SIGKILL is a signal sent to a container or pod forcing it to shutdown.  A SIGKILL is normally sent after the SIGTERM signal.  There is a default 30 second grace period between the time that SIGTERM is sent to the application and SIGKILL is sent.  The grace period can be adjusted for each pod using the .spec.terminationGracePeriodSeconds field. The overall goal for containerized applications should be aimed to have designed and implemented quick startup and shutdown operations.</p>"},{"location":"k8s/openshift/#poststart","title":"postStart","text":"<p>The postStart hook is a command that is run after the creation of a container and begins asynchronously with the container's primary process. PostStart is put in place in order to give the container time to warm up and check itself during startup.  During the postStart loop the container will be labeled in \"pending\" mode in kubernetes while running through it's initial processes.  If the postStart function errors out it will do so with a nonzero exit code and the container process will be killed by Kubernetes.  Careful planning must be done when deciding what logic goes into the postStart function because if it fails the container will also fail to start.  Both postStart and preStop have two handler types that they run:</p> <ul> <li> <p>exec: Runs a command directly in the container.</p> </li> <li> <p>httpGet: Executes an HTTP GET request against an opened port on the pod container.</p> </li> </ul>"},{"location":"k8s/openshift/#prestop","title":"preStop","text":"<p>The preStop hook is a call that blocks a container from terminating too quickly and makes sure the container has a graceful shutdown.  The preStop call must finish before the container is deleted by the container runtime.  The preStop signal does not stop the container from being deleted completely, it is only an alternative to a SIGTERM signal for a graceful shutdown. </p>"},{"location":"k8s/openshift/configuration/","title":"Container Configuration","text":""},{"location":"k8s/openshift/configuration/#container-configuration","title":"Container Configuration","text":""},{"location":"k8s/openshift/configuration/#command-and-argument","title":"Command and Argument","text":"<p>When you create a Pod, you can define a command and arguments for the containers that run in the Pod.</p> <p>The command and arguments that you define in the configuration override the default command and arguments provided by the container file.</p> <ul> <li>Dockerfile vs Kubernetes</li> <li>Dockerfile Entrypoint -&gt; k8s command</li> <li>Dockerfile CMD -&gt; k8s args</li> </ul>"},{"location":"k8s/openshift/configuration/#ports","title":"Ports","text":"<p>When you create a Pod, you can specify the port number the container exposes, as best practice is good to put a <code>name</code>, this way a service can specify targetport by name reference.</p>"},{"location":"k8s/openshift/configuration/#environment-variables","title":"Environment Variables","text":"<p>When you create a Pod, you can set environment variables for the containers that run in the Pod. To set environment variables, include the env or envFrom field in the container configuration</p> <p>A Pod can use environment variables to expose information about itself to Containers running in the Pod. Environment variables can expose Pod fields and Container fields</p>"},{"location":"k8s/openshift/configuration/#resources","title":"Resources","text":"OpenShift &amp; Kubernetes <p>Container Commands </p> <p>Environment Variables </p> <p>Pod Exposing </p>"},{"location":"k8s/openshift/configuration/#references","title":"References","text":"Simple Command, No Arguments<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-cmd-pod\nspec:\n  containers:\n    - name: myapp-container\n      image: busybox\n      command: [\"echo\"]\n  restartPolicy: Never\n</code></pre> Simple Command With Arguments<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-arg-pod\nspec:\n  containers:\n    - name: myapp-container\n      image: busybox\n      command: [\"echo\"]\n      args: [\"Hello World\"]\n  restartPolicy: Never\n</code></pre> Expose a Network Port<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-port-pod\nspec:\n  containers:\n    - name: myapp-container\n      image: bitnami/nginx\n      ports:\n        - containerPort: 8080\n</code></pre> Use Environment Variables as Arguments<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-env-pod\nspec:\n  restartPolicy: Never\n  containers:\n    - name: c\n      image: busybox\n      env:\n        - name: DEMO_GREETING\n          value: \"Hello from the environment\"\n      command: [\"echo\"]\n      args: [\"$(DEMO_GREETING)\"]\n</code></pre> Use Environment Variables AND Name Exposed Port<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-inter-pod\n  labels:\n    app: jedi\nspec:\n  restartPolicy: Never\n  containers:\n    - name: myapp\n      image: bitnami/nginx\n      ports:\n        - containerPort: 8080\n          name: http\n      env:\n        - name: MY_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: MY_POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n      command: [\"echo\"]\n      args: [\"$(MY_NODE_NAME) $(MY_POD_NAME) $(MY_POD_IP)\"]\n</code></pre>"},{"location":"k8s/openshift/configuration/#resource-requirements","title":"Resource Requirements","text":"<p>When you specify a Pod, you can optionally specify how much CPU and memory (RAM) each Container needs. When Containers have resource requests specified, the scheduler can make better decisions about which nodes to place Pods on.</p> <p>CPU and memory are each a resource type. A resource type has a base unit. CPU is specified in units of cores, and memory is specified in units of bytes.</p>"},{"location":"k8s/openshift/configuration/#resources_1","title":"Resources","text":"OpenShift &amp; Kubernetes <p>Compute Resources </p> <p>Memory Management </p>"},{"location":"k8s/openshift/configuration/#references_1","title":"References","text":"Pod Specific Resources<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n    - name: my-app\n      image: bitnami/nginx\n      ports:\n        - containerPort: 8080\n      resources:\n        requests:\n          memory: \"64Mi\"\n          cpu: \"250m\"\n        limits:\n          memory: \"128Mi\"\n          cpu: \"500m\"\n</code></pre> Namespaced Defaults Memory<pre><code>apiVersion: v1\nkind: LimitRange\nmetadata:\n  name: mem-limit-range\nspec:\n  limits:\n    - default:\n        memory: 512Mi\n      defaultRequest:\n        memory: 256Mi\n      type: Container\n</code></pre> Namespaced Defaults CPU<pre><code>apiVersion: v1\nkind: LimitRange\nmetadata:\n  name: cpu-limit-range\nspec:\n  limits:\n    - default:\n        cpu: 1\n      defaultRequest:\n        cpu: 0.5\n      type: Container\n</code></pre>"},{"location":"k8s/openshift/configuration/#activities","title":"Activities","text":"Task Description Link Try It Yourself Pod Configuration Configure a pod to meet compute resource requirements. Pod Configuration"},{"location":"k8s/openshift/configuration/config-map/","title":"Config Maps","text":""},{"location":"k8s/openshift/configuration/config-map/#config-maps","title":"Config Maps","text":"<p>ConfigMaps allow you to decouple configuration artifacts from image content to keep containerized applications portable.</p> <p>An application can consume data from a ConfigMap in 3 different ways.</p> <ul> <li>As a single environment variable specific to a single key</li> <li>As a set of files, each key represented by a file on mounted volume</li> <li>As a set of environment variables from all keys</li> </ul>"},{"location":"k8s/openshift/configuration/config-map/#resources","title":"Resources","text":"OpenShiftKubernetes <p>ConfigMaps with Applications </p> <p>ConfigMaps </p>"},{"location":"k8s/openshift/configuration/config-map/#references","title":"References","text":"ConfigMap<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: my-cm\ndata:\n  color: blue\n  location: naboo\n</code></pre> Environment Variable Single Key<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  restartPolicy: Never\n  containers:\n    - name: myapp\n      image: busybox\n      command: [\"echo\"]\n      args: [\"color is $(MY_VAR)\"]\n      env:\n        - name: MY_VAR\n          valueFrom:\n            configMapKeyRef:\n              name: my-cm\n              key: color\n</code></pre> Keys Represented by a File<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  restartPolicy: Never\n  containers:\n    - name: myapp\n      image: busybox\n      command:\n        [\n          \"sh\",\n          \"-c\",\n          \"ls -l /etc/config; echo located at $(cat /etc/config/location)\",\n        ]\n      volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n  volumes:\n    - name: config-volume\n      configMap:\n        name: my-cm\n</code></pre> Environment Variables From All Keys<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  restartPolicy: Never\n  containers:\n    - name: myapp\n      image: busybox\n      command: [\"/bin/sh\", \"-c\", \"env | sort\"]\n      envFrom:\n        - configMapRef:\n            name: my-cm\n  restartPolicy: Never\n</code></pre>"},{"location":"k8s/openshift/configuration/secrets/","title":"Secrets","text":""},{"location":"k8s/openshift/configuration/secrets/#secrets","title":"Secrets","text":"<p>Kubernetes secret objects let you store and manage sensitive information, such as passwords, OAuth tokens, and ssh keys. Putting this information in a secret is safer and more flexible than putting it verbatim in a Pod definition or in a container image.</p> <p>A Secret is an object that contains a small amount of sensitive data such as a password, a token, or a key. Such information might otherwise be put in a Pod specification or in an image; putting it in a Secret object allows for more control over how it is used, and reduces the risk of accidental exposure.</p>"},{"location":"k8s/openshift/configuration/secrets/#resources","title":"Resources","text":"OpenShiftKubernetes <p>Image Pull Secrets </p> <p>Secret Commands </p> <p>Secrets </p> <p>Secret Distribution </p>"},{"location":"k8s/openshift/configuration/secrets/#references","title":"References","text":"Secret<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: mysecret\ntype: Opaque\ndata:\n  username: YWRtaW4=\nstringData:\n  admin: administrator\n</code></pre> Secret as Config<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: mysecret-config\ntype: Opaque\nstringData:\n  config.yaml: |-\n    apiUrl: \"https://my.api.com/api/v1\"\n    username: token\n    password: thesecrettoken\n</code></pre> Pod Definition That Consumes Both Secrets<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n    - name: my-app\n      image: bitnami/nginx\n      ports:\n        - containerPort: 8080\n      env:\n        - name: SECRET_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: mysecret\n              key: username\n      envFrom:\n        - secretRef:\n            name: mysecret\n      volumeMounts:\n        - name: config\n          mountPath: \"/etc/secrets\"\n  volumes:\n    - name: config\n      secret:\n        secretName: mysecret-config\n</code></pre> OpenShiftKubernetes Create These Files Needed for is Example<pre><code>echo -n 'admin' &gt; ./username.txt\necho -n '1f2d1e2e67df' &gt; ./password.txt\n</code></pre> Creating Secret From Files<pre><code>oc create secret generic db-user-pass --from-file=./username.txt --from-file=./password.txt\n</code></pre> Getting Secret<pre><code>oc get secrets\n</code></pre> Gets the Secret's Description<pre><code>oc describe secrets/db-user-pass\n</code></pre> Terminal Output From Above Commands<pre><code># echo -n 'admin' &gt; ./username.txt\n# echo -n '1f2d1e2e67df' &gt; ./password.txt\n# oc create secret generic db-user-pass --from-file=./username.txt --from-file=./password.txt \nsecret/db-user-pass created\n# oc get secrets\nNAME                       TYPE                      DATA   AGE\nbuilder-dockercfg-b88ll    kubernetes.io/dockercfg   1      18h\ndb-user-pass               Opaque                    2      14s\ndefault-dockercfg-5bs7k    kubernetes.io/dockercfg   1      18h\ndeployer-dockercfg-xjhpw   kubernetes.io/dockercfg   1      18h\nmysecret                   Opaque                    2      16m\nmysecret-config            Opaque                    1      15m\n# oc describe secrets/db-user-pass\nName:         db-user-pass\nNamespace:    debug\nLabels:       &lt;none&gt;\nAnnotations:  &lt;none&gt;\n\nType:  Opaque\n\nData\n====\npassword.txt:  12 bytes\nusername.txt:  5 bytes\n</code></pre> Create These Files Needed for is Example<pre><code>echo -n 'admin' &gt; ./username.txt\necho -n '1f2d1e2e67df' &gt; ./password.txt\n</code></pre> Creating Secret From Files<pre><code>kubectl create secret generic db-user-pass --from-file=./username.txt --from-file=./password.txt\n</code></pre> Getting Secret<pre><code>kubectl get secrets\n</code></pre> Gets the Secret's Description<pre><code>kubectl describe secrets/db-user-pass\n</code></pre> Terminal Output From Above Commands<pre><code># echo -n 'admin' &gt; ./username.txt\n# echo -n '1f2d1e2e67df' &gt; ./password.txt\n# kubectl create secret generic db-user-pass --from-file=./username.txt --from-file=./password.txt secret/db-user-pass created\n# kubectl get secrets\nNAME           TYPE     DATA   AGE\ndb-user-pass   Opaque   2      24s\n# work kubectl describe secrets/db-user-pass\nName:         db-user-pass\nNamespace:    debug\nLabels:       &lt;none&gt;\nAnnotations:  &lt;none&gt;\n\nType:  Opaque\n\nData\n====\npassword.txt:  12 bytes\nusername.txt:  5 bytes\n</code></pre>"},{"location":"k8s/openshift/configuration/security-contexts/","title":"Security Contexts","text":""},{"location":"k8s/openshift/configuration/security-contexts/#security-contexts","title":"Security Contexts","text":"<p>A security context defines privilege and access control settings for a Pod or Container.</p> <p>To specify security settings for a Pod, include the securityContext field in the Pod specification. The securityContext field is a PodSecurityContext object. The security settings that you specify for a Pod apply to all Containers in the Pod.</p>"},{"location":"k8s/openshift/configuration/security-contexts/#resources","title":"Resources","text":"OpenShiftKubernetes <p>Managing Security Contexts </p> <p>Security Contexts </p>"},{"location":"k8s/openshift/configuration/security-contexts/#references","title":"References","text":"<p>Setup minikube VM with users</p> <pre><code>minikube ssh\n</code></pre> <pre><code>su -\n</code></pre> <pre><code>echo \"container-user-0:x:2000:2000:-:/home/container-user-0:/bin/bash\" &gt;&gt; /etc/passwd\necho \"container-user-1:x:2001:2001:-:/home/container-user-1:/bin/bash\" &gt;&gt; /etc/passwd\necho \"container-group-0:x:3000:\" &gt;&gt;/etc/group\necho \"container-group-1:x:3001:\" &gt;&gt;/etc/group\nmkdir -p /etc/message/\necho \"Hello, World!\" | sudo tee -a /etc/message/message.txt\nchown 2000:3000 /etc/message/message.txt\nchmod 640 /etc/message/message.txt\n</code></pre> <p>Using the this <code>securityContext</code> the container will be able to read the file <code>/message/message.txt</code></p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-securitycontext-pod\nspec:\n  restartPolicy: Never\n  securityContext:\n    runAsUser: 2000\n    runAsGroup: 3000\n    fsGroup: 3000\n  containers:\n    - name: myapp-container\n      image: busybox\n      command: [\"sh\", \"-c\", \"cat /message/message.txt &amp;&amp; sleep 3600\"]\n      volumeMounts:\n        - name: message-volume\n          mountPath: /message\n  volumes:\n    - name: message-volume\n      hostPath:\n        path: /etc/message\n</code></pre> <p>Using the this <code>securityContext</code> the container should NOT be able to read the file <code>/message/message.txt</code></p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-securitycontext-pod\nspec:\n  restartPolicy: Never\n  securityContext:\n    runAsUser: 2001\n    runAsGroup: 3001\n    fsGroup: 3001\n  containers:\n    - name: myapp-container\n      image: busybox\n      command: [\"sh\", \"-c\", \"cat /message/message.txt &amp;&amp; sleep 3600\"]\n      volumeMounts:\n        - name: message-volume\n          mountPath: /message\n  volumes:\n    - name: message-volume\n      hostPath:\n        path: /etc/message\n</code></pre> <p>Run to see the errors</p> OpenShiftKubernetes Get Pod Logs<pre><code>oc logs my-securitycontext-pod\n</code></pre> Should return<pre><code>cat: can't open '/message/message.text': Permission denied\n</code></pre> Get Pod Logs<pre><code>kubectl logs my-securitycontext-pod\n</code></pre> Should return<pre><code>cat: can't open '/message/message.txt': Permission denied\n</code></pre>"},{"location":"k8s/openshift/configuration/service-accounts/","title":"Service Accounts","text":""},{"location":"k8s/openshift/configuration/service-accounts/#service-accounts","title":"Service Accounts","text":"<p>A service account provides an identity for processes that run in a Pod.</p> <p>When you (a human) access the cluster (for example, using kubectl), you are authenticated by the apiserver as a particular User Account (currently this is usually admin, unless your cluster administrator has customized your cluster). Processes in containers inside pods can also contact the apiserver. When they do, they are authenticated as a particular Service Account (for example, default).</p> <p>User accounts are for humans. Service accounts are for processes, which run in pods.</p> <p>User accounts are intended to be global. Names must be unique across all namespaces of a cluster, future user resource will not be namespaced. Service accounts are namespaced.</p>"},{"location":"k8s/openshift/configuration/service-accounts/#resources","title":"Resources","text":"OpenShiftKubernetes <p>Service Accounts </p> <p>Using Service Accounts </p> <p>Managing Service Accounts </p> <p>Service Account Configuration </p>"},{"location":"k8s/openshift/configuration/service-accounts/#references","title":"References","text":"<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: my-service-account\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  serviceAccountName: my-service-account\n  containers:\n    - name: my-app\n      image: bitnami/nginx\n      ports:\n        - containerPort: 8080\n</code></pre> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: build-robot-secret\n  annotations:\n    kubernetes.io/service-account.name: my-service-account\ntype: kubernetes.io/service-account-token\n</code></pre> OpenshiftKubernetes Create a Service Account<pre><code>oc create sa &lt;service_account_name&gt;\n</code></pre> View Service Account Details<pre><code>oc describe sa &lt;service_account_name&gt;\n</code></pre> Create a Service Account<pre><code>kubectl create sa &lt;service_account_name&gt;\n</code></pre> View Service Account Details<pre><code>kubectl describe sa &lt;service_account_name&gt;\n</code></pre>"},{"location":"k8s/openshift/core-concepts/","title":"K8s API Primitives","text":""},{"location":"k8s/openshift/core-concepts/#k8s-api-primitives","title":"K8s API Primitives","text":"<p>Kubernetes API primitive, also known as Kubernetes objects, are the basic building blocks of any application running in Kubernetes</p> <p>Examples:</p> <ul> <li>Pod</li> <li>Node</li> <li>Service</li> <li>ServiceAccount</li> </ul> <p>Two primary members</p> <ul> <li>Spec, desired state</li> <li>Status, current state</li> </ul>"},{"location":"k8s/openshift/core-concepts/#resources","title":"Resources","text":"OpenShiftKubernetes <p>Pods </p> <p>Nodes </p> <p>Objects </p> <p>Kube Basics </p>"},{"location":"k8s/openshift/core-concepts/#references","title":"References","text":"OpenShiftKubernetes List API-Resources<pre><code>oc api-resources\n</code></pre> List API-Resources<pre><code>kubectl api-resources\n</code></pre>"},{"location":"k8s/openshift/core-concepts/namespaces-projects/","title":"Projects &amp; Namespaces","text":""},{"location":"k8s/openshift/core-concepts/namespaces-projects/#projects-namespaces","title":"Projects &amp; Namespaces","text":"<p>Namespaces are intended for use in environments with many users spread across multiple teams, or projects.</p> <p>Namespaces provide a scope for names. Names of resources need to be unique within a namespace, but not across namespaces.</p> <p>Namespaces are a way to divide cluster resources between multiple users (via resource quota).</p> <p>It is not necessary to use multiple namespaces just to separate slightly different resources, such as different versions of the same software: use labels to distinguish resources within the same namespace. In practice namespaces are used to deploy different versions based on stages of the CICD pipeline (dev, test, stage, prod)</p>"},{"location":"k8s/openshift/core-concepts/namespaces-projects/#resources","title":"Resources","text":"OpenShiftKubernetes <p>Working with Projects </p> <p>Creating Projects </p> <p>Configure Project Creation </p> <p>Namespaces </p>"},{"location":"k8s/openshift/core-concepts/namespaces-projects/#references","title":"References","text":"Namespace YAML<pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: dev\n</code></pre> Pod YAML specifiying Namespace<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp-pod\n  namespace: dev\nspec:\n  containers:\n    - name: myapp-container\n      image: busybox\n      command: [\"sh\", \"-c\", \"echo Hello Kubernetes! &amp;&amp; sleep 3600\"]\n</code></pre> OpenShiftKubernetes Getting all namespaces/projects<pre><code>oc projects\n</code></pre> Create a new Project<pre><code>oc new-project dev\n</code></pre> Viewing Current Project<pre><code>oc project\n</code></pre> Setting Namespace in Context<pre><code>oc project dev\n</code></pre> Viewing Project Status<pre><code>oc status\n</code></pre> Getting all namespaces<pre><code>kubectl get namespaces\n</code></pre> Create a new namespace called bar<pre><code>kubectl create ns dev\n</code></pre> Setting Namespace in Context<pre><code>kubectl config set-context --current --namespace=dev\n</code></pre>"},{"location":"k8s/openshift/deployments/","title":"Deployments","text":""},{"location":"k8s/openshift/deployments/#deployments","title":"Deployments","text":"<p>A Deployment provides declarative updates for Pods and ReplicaSets.</p> <p>You describe a desired state in a Deployment, and the Deployment Controller changes the actual state to the desired state at a controlled rate. You can define Deployments to create new ReplicaSets, or to remove existing Deployments and adopt all their resources with new Deployments.</p> <p>The following are typical use cases for Deployments:</p> <ul> <li>Create a Deployment to rollout a ReplicaSet. The ReplicaSet creates Pods in the background. Check the status of the rollout to see if it succeeds or not.</li> <li>Declare the new state of the Pods by updating the PodTemplateSpec of the Deployment. A new ReplicaSet is created and the Deployment manages moving the Pods from the old ReplicaSet to the new one at a controlled rate. Each new ReplicaSet updates the revision of the Deployment.</li> <li>Rollback to an earlier Deployment revision if the current state of the Deployment is not stable. Each rollback updates the revision of the Deployment.</li> <li>Scale up the Deployment to facilitate more load.</li> <li>Pause the Deployment to apply multiple fixes to its PodTemplateSpec and then resume it to start a new rollout.</li> <li>Use the status of the Deployment as an indicator that a rollout has stuck.</li> <li>Clean up older ReplicaSets that you don\u2019t need anymore.</li> </ul>"},{"location":"k8s/openshift/deployments/#resources","title":"Resources","text":"OpenShiftKubernetes <p>Deployments </p> <p>Managing Deployment Processes </p> <p>DeploymentConfig Strategies </p> <p>Route Based Deployment Strategies </p> <p>Deployments </p> <p>Scaling Deployments </p>"},{"location":"k8s/openshift/deployments/#references","title":"References","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n        - name: nginx\n          image: bitnami/nginx:1.16.0\n          ports:\n            - containerPort: 8080\n</code></pre> OpenshiftKubernetes Create a Deployment<pre><code>oc apply -f deployment.yaml\n</code></pre> Get Deployment<pre><code>oc get deployment my-deployment\n</code></pre> Get Deployment's Description<pre><code>oc describe deployment my-deployment\n</code></pre> Edit Deployment<pre><code>oc edit deployment my-deployment\n</code></pre> Scale Deployment<pre><code>oc scale deployment/my-deployment --replicas=3\n</code></pre> Delete Deployment<pre><code>oc delete deployment my-deployment\n</code></pre> Create a Deployment<pre><code>kubectl apply -f deployment.yaml\n</code></pre> Get Deployment<pre><code>kubectl get deployment my-deployment\n</code></pre> Get Deployment's Description<pre><code>kubectl describe deployment my-deployment\n</code></pre> Edit Deployment<pre><code>kubectl edit deployment my-deployment\n</code></pre> Scale Deployment<pre><code>kubectl scale deployment/my-deployment --replicas=3\n</code></pre> Delete Deployment<pre><code>kubectl delete deployment my-deployment\n</code></pre>"},{"location":"k8s/openshift/deployments/updates/","title":"Rolling Updates &amp; Rollbacks","text":""},{"location":"k8s/openshift/deployments/updates/#rolling-updates-rollbacks","title":"Rolling Updates &amp; Rollbacks","text":"<p>Updating a Deployment A Deployment\u2019s rollout is triggered if and only if the Deployment\u2019s Pod template (that is, .spec.template) is changed, for example if the labels or container images of the template are updated. Other updates, such as scaling the Deployment, do not trigger a rollout.</p> <p>Each time a new Deployment is observed by the Deployment controller, a ReplicaSet is created to bring up the desired Pods. If the Deployment is updated, the existing ReplicaSet that controls Pods whose labels match .spec.selector but whose template does not match .spec.template are scaled down. Eventually, the new ReplicaSet is scaled to .spec.replicas and all old ReplicaSets is scaled to 0.</p> <p>Label selector updates It is generally discouraged to make label selector updates and it is suggested to plan your selectors up front. In any case, if you need to perform a label selector update, exercise great caution and make sure you have grasped all of the implications.</p> <p>Rolling Back a Deployment Sometimes, you may want to rollback a Deployment; for example, when the Deployment is not stable, such as crash looping. By default, all of the Deployment\u2019s rollout history is kept in the system so that you can rollback anytime you want (you can change that by modifying revision history limit).</p> <p>A Deployment\u2019s revision is created when a Deployment\u2019s rollout is triggered. This means that the new revision is created if and only if the Deployment\u2019s Pod template (.spec.template) is changed, for example if you update the labels or container images of the template. Other updates, such as scaling the Deployment, do not create a Deployment revision, so that you can facilitate simultaneous manual- or auto-scaling. This means that when you roll back to an earlier revision, only the Deployment\u2019s Pod template part is rolled back.</p>"},{"location":"k8s/openshift/deployments/updates/#resources","title":"Resources","text":"OpenShiftKubernetes <p>Rollouts </p> <p>Rolling Back </p> <p>Updating a Deployment </p> <p>Rolling Back a Deployment </p>"},{"location":"k8s/openshift/deployments/updates/#references","title":"References","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n        - name: nginx\n          image: bitnami/nginx:1.16.0\n          ports:\n            - containerPort: 8080\n</code></pre> OpenShiftKubernetes Get Deployments<pre><code>oc get deployments\n</code></pre> Sets new image for Deployment<pre><code>oc set image deployment/my-deployment nginx=bitnami/nginx:1.16.1 --record\n</code></pre> Check the status of a rollout<pre><code>oc rollout status deployment my-deployment\n</code></pre> Get Replicasets<pre><code>oc get rs\n</code></pre> Get Deployment Description<pre><code>oc describe deployment my-deployment\n</code></pre> Get Rollout History<pre><code>oc rollout history deployment my-deployment\n</code></pre> Undo Rollout<pre><code>oc rollback my-deployment\n</code></pre> Delete Deployment<pre><code>oc delete deployment my-deployment\n</code></pre> Create a Deployment<pre><code>kubectl apply -f deployment.yaml\n</code></pre> Create a new namespace called bar<pre><code>kubectl create ns dev\n</code></pre> Setting Namespace in Context<pre><code>kubectl config set-context --current --namespace=dev\n</code></pre>"},{"location":"k8s/openshift/pods/","title":"Pods","text":""},{"location":"k8s/openshift/pods/#pods","title":"Pods","text":"<p>A Pod is the basic execution unit of a Kubernetes application\u2013the smallest and simplest unit in the Kubernetes object model that you create or deploy. A Pod represents processes running on your Cluster.</p> <p>A Pod encapsulates an application\u2019s container (or, in some cases, multiple containers), storage resources, a unique network IP, and options that govern how the container(s) should run. A Pod represents a unit of deployment: a single instance of an application in Kubernetes, which might consist of either a single container or a small number of containers that are tightly coupled and that share resources.</p>"},{"location":"k8s/openshift/pods/#resources","title":"Resources","text":"OpenShiftKubernetes <ul> <li> <p> About Pods</p> <p>Learn more about the basics of pods and how they work.</p> <p> Getting started</p> </li> <li> <p> Cluster Configuration for Pods</p> <p>Configure your cluster to work for your specific needs.</p> <p> Learn more</p> </li> <li> <p> Pod Autoscaling</p> <p>Use a horizontal pod autoscaler (HPA) to specify how OCP should automatically scale up or down your deployment.</p> <p> Learn more</p> </li> </ul> <ul> <li> <p> Pod Overview</p> <p>Learn more about the basics of pods and how they work.</p> <p> Getting started</p> </li> <li> <p> Pod Lifecycle</p> <p>Read about the lifecycle process for pods and what each phase means.</p> <p> Learn more</p> </li> <li> <p> Pod Usage</p> <p>How do you use pods? Read about it here.</p> <p> Learn more</p> </li> </ul>"},{"location":"k8s/openshift/pods/#references","title":"References","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp-pod\n  labels:\n    app: myapp\nspec:\n  containers:\n    - name: myapp-container\n      image: busybox\n      command: [\"sh\", \"-c\", \"echo Hello Kubernetes! &amp;&amp; sleep 3600\"]\n</code></pre> OpenShiftKubernetes <p>Create Pod using yaml file</p> <pre><code>oc apply -f pod.yaml\n</code></pre> <p>Get Current Pods in Project</p> <pre><code>oc get pods\n</code></pre> <p>Get Pods with their IP and node location</p> <pre><code>oc get pods -o wide\n</code></pre> <p>Get Pod's Description</p> <pre><code>oc describe pod myapp-pod\n</code></pre> <p>Get the logs</p> <pre><code>oc logs myapp-pod\n</code></pre> <p>Delete a Pod</p> <pre><code>oc delete pod myapp-pod\n</code></pre> <p>Create Pod using yaml file</p> <pre><code>kubectl apply -f pod.yaml\n</code></pre> <p>Get Current Pods in Project</p> <pre><code>kubectl get pods\n</code></pre> <p>Get Pods with their IP and node location</p> <pre><code>kubectl get pods -o wide\n</code></pre> <p>Get Pod's Description</p> <pre><code>kubectl describe pod myapp-pod\n</code></pre> <p>Get the logs</p> <pre><code>kubectl logs myapp-pod\n</code></pre> <p>Delete a Pod</p> <pre><code>kubectl delete pod myapp-pod\n</code></pre>"},{"location":"k8s/openshift/pods/health-checks/","title":"Health and Monitoring","text":""},{"location":"k8s/openshift/pods/health-checks/#health-and-monitoring","title":"Health and Monitoring","text":""},{"location":"k8s/openshift/pods/health-checks/#liveness-and-readiness-probes","title":"Liveness and Readiness Probes","text":"<p>A Probe is a diagnostic performed periodically by the kubelet on a Container. To perform a diagnostic, the kubelet calls a Handler implemented by the Container. There are three types of handlers:</p> <p>ExecAction: Executes a specified command inside the Container. The diagnostic is considered successful if the command exits with a status code of 0.</p> <p>TCPSocketAction: Performs a TCP check against the Container\u2019s IP address on a specified port. The diagnostic is considered successful if the port is open.</p> <p>HTTPGetAction: Performs an HTTP Get request against the Container\u2019s IP address on a specified port and path. The diagnostic is considered successful if the response has a status code greater than or equal to 200 and less than 400.</p> <p>The kubelet can optionally perform and react to two kinds of probes on running Containers:</p> <p>livenessProbe: Indicates whether the Container is running. Runs for the lifetime of the Container.</p> <p>readinessProbe: Indicates whether the Container is ready to service requests. Only runs at start.</p>"},{"location":"k8s/openshift/pods/health-checks/#resources","title":"Resources","text":"OpenShiftKubernetes <ul> <li> <p> Application Health</p> <p>A health check periodically performs diagnostics on a running container using any combination of the readiness, liveness, and startup health checks.</p> <p> Learn more</p> </li> <li> <p> Virtual Machine Health</p> <p>Use readiness and liveness probes to detect and handle unhealthy virtual machines (VMs).</p> <p> Learn more</p> </li> </ul> <ul> <li> <p> Container Probes</p> <p>To perform a diagnostic, the kubelet either executes code within the container, or makes a network request.</p> <p> Learn more</p> </li> <li> <p> Configure Probes</p> <p>Read about how to configure liveness, readiness and startup probes for containers.</p> <p> Learn more</p> </li> </ul>"},{"location":"k8s/openshift/pods/health-checks/#references","title":"References","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n    - name: app\n      image: busybox\n      command: [\"sh\", \"-c\", \"echo Hello, Kubernetes! &amp;&amp; sleep 3600\"]\n      livenessProbe:\n        exec:\n          command: [\"echo\", \"alive\"]\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  shareProcessNamespace: true\n  containers:\n    - name: app\n      image: bitnami/nginx\n      ports:\n        - containerPort: 8080\n      livenessProbe:\n        tcpSocket:\n          port: 8080\n        initialDelaySeconds: 10\n      readinessProbe:\n        httpGet:\n          path: /\n          port: 8080\n        periodSeconds: 10\n</code></pre>"},{"location":"k8s/openshift/pods/health-checks/#container-logging","title":"Container Logging","text":"<p>Application and systems logs can help you understand what is happening inside your cluster. The logs are particularly useful for debugging problems and monitoring cluster activity.</p> <p>Kubernetes provides no native storage solution for log data, but you can integrate many existing logging solutions into your Kubernetes cluster.</p>"},{"location":"k8s/openshift/pods/health-checks/#resources_1","title":"Resources","text":"OpenShiftKubernetes <ul> <li> <p> Logs Command</p> <p>Read about the descriptions and example commands for OpenShift CLI (<code>oc</code>) developer commands.</p> <p> Learn more</p> </li> <li> <p> Cluster Logging</p> <p>As a cluster administrator, you can deploy logging on an OpenShift Container Platform cluster, and use it to collect and aggregate node system audit logs, application container logs, and infrastructure logs.</p> <p> Learn more</p> </li> <li> <p> Logging Collector</p> <p>The collector collects log data from each node, transforms the data, and forwards it to configured outputs.</p> <p> Learn more</p> </li> </ul> <ul> <li> <p> Logging</p> <p>Application logs can help you understand what is happening inside your application and are particularly useful for debugging problems and monitoring cluster activity.</p> <p> Getting started</p> </li> </ul>"},{"location":"k8s/openshift/pods/health-checks/#references_1","title":"References","text":"Pod Example<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: counter\nspec:\n  containers:\n    - name: count\n      image: busybox\n      command:\n        [\n          \"sh\",\n          \"-c\",\n          'i=0; while true; do echo \"$i: $(date)\"; i=$((i+1)); sleep 5; done',\n        ]\n</code></pre> OpenShiftKubernetes Get Logs<pre><code>oc logs\n</code></pre> Use Stern to View Logs<pre><code>brew install stern\nstern . -n default\n</code></pre> Get Logs<pre><code>kubectl logs\n</code></pre> Use Stern to View Logs<pre><code>brew install stern\nstern . -n default\n</code></pre>"},{"location":"k8s/openshift/pods/health-checks/#monitoring-applications","title":"Monitoring Applications","text":"<p>To scale an application and provide a reliable service, you need to understand how the application behaves when it is deployed. You can examine application performance in a Kubernetes cluster by examining the containers, pods, services, and the characteristics of the overall cluster. Kubernetes provides detailed information about an application\u2019s resource usage at each of these levels. This information allows you to evaluate your application\u2019s performance and where bottlenecks can be removed to improve overall performance.</p> <p>Prometheus, a CNCF project, can natively monitor Kubernetes, nodes, and Prometheus itself.</p>"},{"location":"k8s/openshift/pods/health-checks/#resources_2","title":"Resources","text":"OpenShiftKubernetes <ul> <li> <p> Monitoring Application Health</p> <p>OpenShift Container Platform applications have a number of options to detect and handle unhealthy containers.</p> <p> Learn more</p> </li> </ul> <ul> <li> <p> Monitoring Resource Usage</p> <p>You can examine application performance in a Kubernetes cluster by examining the containers, pods, services, and the characteristics of the overall cluster.</p> <p> Learn more</p> </li> <li> <p> Resource Metrics</p> <p>For Kubernetes, the Metrics API offers a basic set of metrics to support automatic scaling and similar use cases.</p> <p> Learn more</p> </li> </ul>"},{"location":"k8s/openshift/pods/health-checks/#references_2","title":"References","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: 500m\nspec:\n  containers:\n    - name: app\n      image: gcr.io/kubernetes-e2e-test-images/resource-consumer:1.4\n      resources:\n        requests:\n          cpu: 700m\n          memory: 128Mi\n    - name: busybox-sidecar\n      image: radial/busyboxplus:curl\n      command:\n        [\n          /bin/sh,\n          -c,\n          'until curl localhost:8080/ConsumeCPU -d \"millicores=500&amp;durationSec=3600\"; do sleep 5; done &amp;&amp; sleep 3700',\n        ]\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: 200m\nspec:\n  containers:\n    - name: app\n      image: gcr.io/kubernetes-e2e-test-images/resource-consumer:1.4\n      resources:\n        requests:\n          cpu: 300m\n          memory: 64Mi\n    - name: busybox-sidecar\n      image: radial/busyboxplus:curl\n      command:\n        [\n          /bin/sh,\n          -c,\n          'until curl localhost:8080/ConsumeCPU -d \"millicores=200&amp;durationSec=3600\"; do sleep 5; done &amp;&amp; sleep 3700',\n        ]\n</code></pre> OpenShiftKubernetes <pre><code>oc get projects\noc api-resources -o wide\noc api-resources -o name\n\noc get nodes,ns,po,deploy,svc\n\noc describe node --all\n</code></pre> <p>Verify Metrics is enabled </p><pre><code>kubectl get --raw /apis/metrics.k8s.io/\n</code></pre><p></p> <p>Get Node Description </p><pre><code>kubectl describe node\n</code></pre><p></p> <p>Check Resource Usage </p><pre><code>kubectl top pods\nkubectl top nodes\n</code></pre><p></p> <p></p> <p></p>"},{"location":"k8s/openshift/pods/jobs/","title":"Jobs &amp; CronJobs","text":""},{"location":"k8s/openshift/pods/jobs/#jobs-cronjobs","title":"Jobs &amp; CronJobs","text":"<p>Jobs</p> <p>A Job creates one or more Pods and ensures that a specified number of them successfully terminate. As pods successfully complete, the Job tracks the successful completions. When a specified number of successful completions is reached, the task (ie, Job) is complete. Deleting a Job will clean up the Pods it created.</p> <p>CronJobs</p> <p>One CronJob object is like one line of a crontab (cron table) file. It runs a job periodically on a given schedule, written in Cron format.</p> <p>All CronJob schedule: times are based on the timezone of the master where the job is initiated.</p>"},{"location":"k8s/openshift/pods/jobs/#resources","title":"Resources","text":"OpenShiftKubernetes <p>Jobs </p> <p>CronJobs </p> <p>Jobs to Completion </p> <p>Cron Jobs </p> <p>Automated Tasks with Cron </p>"},{"location":"k8s/openshift/pods/jobs/#references","title":"References","text":"<p>It computes \u03c0 to 2000 places and prints it out</p> <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: pi\nspec:\n  template:\n    spec:\n      containers:\n        - name: pi\n          image: perl\n          command: [\"perl\", \"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"]\n      restartPolicy: Never\n  backoffLimit: 4\n</code></pre> <p>Running in parallel</p> <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: pi\nspec:\n  parallelism: 2\n  completions: 3\n  template:\n    spec:\n      containers:\n        - name: pi\n          image: perl\n          command: [\"perl\", \"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"]\n      restartPolicy: Never\n  backoffLimit: 4\n</code></pre> <pre><code>apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: hello\nspec:\n  schedule: \"*/1 * * * *\"\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n            - name: hello\n              image: busybox\n              args:\n                - /bin/sh\n                - -c\n                - date; echo Hello from the Kubernetes cluster\n          restartPolicy: OnFailure\n</code></pre> OpenShiftKubernetes <p>Gets Jobs </p><pre><code>oc get jobs\n</code></pre> Gets Job Description <pre><code>oc describe job pi\n</code></pre> Gets Pods from the Job <pre><code>oc get pods\n</code></pre> Deletes Job <pre><code>oc delete job pi\n</code></pre> Gets CronJob <pre><code>oc get cronjobs\n</code></pre> Describes CronJob <pre><code>oc describe cronjobs pi\n</code></pre> Gets Pods from CronJob <pre><code>oc get pods\n</code></pre> Deletes CronJob <pre><code>oc delete cronjobs pi\n</code></pre><p></p> <p>Gets Jobs </p><pre><code>kubectl get jobs\n</code></pre> Gets Job Description <pre><code>kubectl describe job pi\n</code></pre> Gets Pods from the Job <pre><code>kubectl get pods\n</code></pre> Deletes Job <pre><code>kubectl delete job pi\n</code></pre> Gets CronJob <pre><code>kubectl get cronjobs\n</code></pre> Describes CronJob <pre><code>kubectl describe cronjobs pi\n</code></pre> Gets Pods from CronJob <pre><code>kubectl get pods\n</code></pre> Deletes CronJob <pre><code>kubectl delete cronjobs pi\n</code></pre><p></p>"},{"location":"k8s/openshift/pods/multi-container/","title":"Multi-Container Pod","text":""},{"location":"k8s/openshift/pods/multi-container/#multi-container-pod","title":"Multi-Container Pod","text":"<p>Container images solve many real-world problems with existing packaging and deployment tools, but in addition to these significant benefits, containers offer us an opportunity to fundamentally re-think the way we build distributed applications. Just as service oriented architectures (SOA) encouraged the decomposition of applications into modular, focused services, containers should encourage the further decomposition of these services into closely cooperating modular containers. By virtue of establishing a boundary, containers enable users to build their services using modular, reusable components, and this in turn leads to services that are more reliable, more scalable and faster to build than applications built from monolithic containers.</p>"},{"location":"k8s/openshift/pods/multi-container/#resources","title":"Resources","text":"Kubernetes <ul> <li> <p> Sidecar Logging</p> <p>Application logs can help you understand what is happening inside your application.</p> <p> Learn more</p> </li> <li> <p> Shared Volume Communication</p> <p>Read about how to use a Volume to communicate between two Containers running in the same Pod.</p> <p> Learn more</p> </li> <li> <p> Toolkit Patterns</p> <p>Read Brendan Burns' blog post about \"The Distributed System ToolKit: Patterns for Composite Containers\".</p> <p> Learn more</p> </li> <li> <p> Brendan Burns Paper</p> <p>Read Brendan Burns' paper about design patterns for container-based distributed systems.</p> <p> Learn more</p> </li> </ul>"},{"location":"k8s/openshift/pods/multi-container/#references","title":"References","text":"<ol> <li> <p>This example shows how to use a Volume to communicate between two Containers running in the same Pod. </p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  volumes:\n    - name: shared-data\n      emptyDir: {}\n  containers:\n    - name: app\n      image: bitnami/nginx\n      volumeMounts:\n        - name: shared-data\n          mountPath: /app\n      ports:\n        - containerPort: 8080\n    - name: sidecard\n      image: busybox\n      volumeMounts:\n        - name: shared-data\n          mountPath: /pod-data\n      command:\n        [\n          \"sh\",\n          \"-c\",\n          \"echo Hello from the side container &gt; /pod-data/index.html &amp;&amp; sleep 3600\",\n        ]\n</code></pre> </li> <li> <p>This example shows how to configure process namespace sharing for a pod. When process namespace sharing is enabled, processes in a container are visible to all other containers in the same pod.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  shareProcessNamespace: true\n  containers:\n    - name: app\n      image: bitnami/nginx\n      ports:\n        - containerPort: 8080\n    - name: sidecard\n      image: busybox\n      securityContext:\n        capabilities:\n          add:\n            - SYS_PTRACE\n      stdin: true\n      tty: true\n</code></pre> </li> </ol> OpenShiftKubernetes <p>Attach Pods Together </p><pre><code>oc attach -it my-pod -c sidecard\n</code></pre> <pre><code>ps ax\n</code></pre> <pre><code>kill -HUP 7\n</code></pre> <pre><code>ps ax\n</code></pre><p></p> <p>Attach Pods Together </p><pre><code>kubectl attach -it my-pod -c sidecard\n</code></pre> <pre><code>ps ax\n</code></pre> <pre><code>kill -HUP 7\n</code></pre> <pre><code>ps ax\n</code></pre><p></p>"},{"location":"k8s/openshift/pods/tagging/","title":"Labels, Selectors, and Annotations","text":""},{"location":"k8s/openshift/pods/tagging/#labels-selectors-and-annotations","title":"Labels, Selectors, and Annotations","text":"<p>Labels are key/value pairs that are attached to objects, such as pods. Labels are intended to be used to specify identifying attributes of objects that are meaningful and relevant to users, but do not directly imply semantics to the core system. Labels can be used to organize and to select subsets of objects. Labels can be attached to objects at creation time and subsequently added and modified at any time. Each object can have a set of key/value labels defined. Each Key must be unique for a given object.</p> <p>You can use Kubernetes annotations to attach arbitrary non-identifying metadata to objects. Clients such as tools and libraries can retrieve this metadata.</p> <p>You can use either labels or annotations to attach metadata to Kubernetes objects. Labels can be used to select objects and to find collections of objects that satisfy certain conditions. In contrast, annotations are not used to identify and select objects. The metadata in an annotation can be small or large, structured or unstructured, and can include characters not permitted by labels.</p>"},{"location":"k8s/openshift/pods/tagging/#resources","title":"Resources","text":"OpenShiftKubernetes <ul> <li> <p> CLI Label Commands</p> <p>Read about the descriptions and example commands for OpenShift CLI (<code>oc</code>) developer commands.</p> <p> Learn more</p> </li> </ul> <ul> <li> <p> Labels</p> <p>Labels can be used to organize and to select subsets of objects.</p> <p> Learn more</p> </li> <li> <p> Annotations</p> <p>You can use Kubernetes annotations to attach arbitrary non-identifying metadata to objects.</p> <p> Learn more</p> </li> </ul>"},{"location":"k8s/openshift/pods/tagging/#references","title":"References","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\n  labels:\n    app: foo\n    tier: frontend\n    env: dev\n  annotations:\n    imageregistry: \"https://hub.docker.com/\"\n    gitrepo: \"https://github.com/csantanapr/knative\"\nspec:\n  containers:\n    - name: app\n      image: bitnami/nginx\n</code></pre> OpenShiftKubernetes <p>Change Labels on Objects </p><pre><code>oc label pod my-pod boot=camp\n</code></pre> Getting Pods based on their labels. <pre><code>oc get pods --show-labels\n</code></pre> <pre><code>oc get pods -L tier,env\n</code></pre> <pre><code>oc get pods -l app\n</code></pre> <pre><code>oc get pods -l tier=frontend\n</code></pre> <pre><code>oc get pods -l 'env=dev,tier=frontend'\n</code></pre> <pre><code>oc get pods -l 'env in (dev, test)'\n</code></pre> <pre><code>oc get pods -l 'tier!=backend'\n</code></pre> <pre><code>oc get pods -l 'env,env notin (prod)'\n</code></pre> Delete the Pod. <pre><code>oc delete pod my-pod\n</code></pre><p></p> <p>Change Labels on Objects </p><pre><code>kubectl label pod my-pod boot=camp\n</code></pre> Getting Pods based on their labels. <pre><code>kubectl get pods --show-labels\n</code></pre> <pre><code>kubectl get pods -L tier,env\n</code></pre> <pre><code>kubectl get pods -l app\n</code></pre> <pre><code>kubectl get pods -l tier=frontend\n</code></pre> <pre><code>kubectl get pods -l 'env=dev,tier=frontend'\n</code></pre> <pre><code>kubectl get pods -l 'env in (dev, test)'\n</code></pre> <pre><code>kubectl get pods -l 'tier!=backend'\n</code></pre> <pre><code>kubectl get pods -l 'env,env notin (prod)'\n</code></pre> Delete the Pod. <pre><code>kubectl delete pod my-pod\n</code></pre><p></p>"},{"location":"k8s/openshift/pods/troubleshooting/","title":"Troubleshooting Applications","text":""},{"location":"k8s/openshift/pods/troubleshooting/#troubleshooting-applications","title":"Troubleshooting Applications","text":"<p>Kubernetes provides tools to help troubleshoot and debug problems with applications.</p> <p>Usually is getting familiar with how primitives objects interact with each other, checking the status of objects, and finally checking logs for any last resource clues.</p>"},{"location":"k8s/openshift/pods/troubleshooting/#resources","title":"Resources","text":"OpenShiftKubernetes <ul> <li> <p> Debugging ODO</p> <p>OpenShift Toolkit is an IDE plugin available on VS Code and JetBrains IDEs, that allows you to do all things that <code>odo</code> does, i.e. create, test, debug and deploy cloud-native applications on a cloud-native environment in simple steps.</p> <p> Getting started</p> </li> </ul> <ul> <li> <p> Debugging Applications</p> <p>Read about how to debug applications that are deployed into Kubernetes and not behaving correctly.</p> <p> Learn more</p> </li> <li> <p> Debugging Services</p> <p>You've run your Pods through a Deployment and created a Service, but you get no response when you try to access it. What do you do?</p> <p> Learn more</p> </li> <li> <p> Debugging Replication Controllers</p> <p>Read about how to debug replication controllers that are deployed into Kubernetes and not behaving correctly.</p> <p> Learn more</p> </li> </ul>"},{"location":"k8s/openshift/pods/troubleshooting/#references","title":"References","text":"OpenShiftKubernetes <p>MacOS/Linux/Windows command: </p><pre><code>oc apply -f https://gist.githubusercontent.com/csantanapr/e823b1bfab24186a26ae4f9ec1ff6091/raw/1e2a0cca964c7b54ce3df2fc3fbf33a232511877/debugk8s-bad.yaml\n</code></pre><p></p> <p>Expose the service using port-forward </p><pre><code>oc port-forward service/my-service 8080:80 -n debug\n</code></pre><p></p> <p>Try to access the service </p><pre><code>curl http://localhost:8080\n</code></pre><p></p> <p>Try Out these Commands to Debug </p><pre><code>oc get pods --all-namespaces\n</code></pre> <pre><code>oc project debug\n</code></pre> <pre><code>oc get deployments\n</code></pre> <pre><code>oc describe pod\n</code></pre> <pre><code>oc explain Pod.spec.containers.resources.requests\n</code></pre> <pre><code>oc explain Pod.spec.containers.livenessProbe\n</code></pre> <pre><code>oc edit deployment\n</code></pre> <pre><code>oc logs\n</code></pre> <pre><code>oc get service\n</code></pre> <pre><code>oc get ep\n</code></pre> <pre><code>oc describe service\n</code></pre> <pre><code>oc get pods --show-labels\n</code></pre> <pre><code>oc get deployment --show-labels\n</code></pre><p></p> <p>MacOS/Linux/Windows command: </p><pre><code>kubectl apply -f https://gist.githubusercontent.com/csantanapr/e823b1bfab24186a26ae4f9ec1ff6091/raw/1e2a0cca964c7b54ce3df2fc3fbf33a232511877/debugk8s-bad.yaml\n</code></pre><p></p> <p>Expose the service using port-forward </p><pre><code>kubectl port-forward service/my-service 8080:80 -n debug\n</code></pre><p></p> <p>Try to access the service </p><pre><code>curl http://localhost:8080\n</code></pre><p></p> <p>Try Out these Commands to Debug </p><pre><code>kubectl get pods --all-namespaces\n</code></pre> <pre><code>kubectl config set-context --current --namespace=debug\n</code></pre> <pre><code>kubectl get deployments\n</code></pre> <pre><code>kubectl describe pod\n</code></pre> <pre><code>kubectl explain Pod.spec.containers.resources.requests\n</code></pre> <pre><code>kubectl explain Pod.spec.containers.livenessProbe\n</code></pre> <pre><code>kubectl edit deployment\n</code></pre> <pre><code>kubectl logs\n</code></pre> <pre><code>kubectl get service\n</code></pre> <pre><code>kubectl get ep\n</code></pre> <pre><code>kubectl describe service\n</code></pre> <pre><code>kubectl get pods --show-labels\n</code></pre> <pre><code>kubectl get deployment --show-labels\n</code></pre><p></p>"},{"location":"k8s/openshift/services-networking/","title":"Services","text":""},{"location":"k8s/openshift/services-networking/#services","title":"Services","text":"<p>An abstract way to expose an application running on a set of Pods as a network service.</p> <p>Kubernetes Pods are mortal. They are born and when they die, they are not resurrected. If you use a Deployment to run your app, it can create and destroy Pods dynamically.</p> <p>Each Pod gets its own IP address, however in a Deployment, the set of Pods running in one moment in time could be different from the set of Pods running that application a moment later.</p> <p>In Kubernetes, a Service is an abstraction which defines a logical set of Pods and a policy by which to access them (sometimes this pattern is called a micro-service). The set of Pods targeted by a Service is usually determined by a selector (see below for why you might want a Service without a selector).</p> <p>If you\u2019re able to use Kubernetes APIs for service discovery in your application, you can query the API server for Endpoints, that get updated whenever the set of Pods in a Service changes.</p> <p>For non-native applications, Kubernetes offers ways to place a network port or load balancer in between your application and the backend Pods.</p>"},{"location":"k8s/openshift/services-networking/#resources","title":"Resources","text":"OpenShift &amp; Kubernetes <p>Services </p> <p>Exposing Services </p>"},{"location":"k8s/openshift/services-networking/#references","title":"References","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\n  labels:\n    app: nginx\n    version: v1\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n        version: v1\n    spec:\n      containers:\n        - name: nginx\n          image: bitnami/nginx\n          ports:\n            - containerPort: 8080\n              name: http\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  selector:\n    app: nginx\n  ports:\n    - name: http\n      port: 80\n      targetPort: http\n</code></pre> OpenShiftKubernetes Get Logs<pre><code>oc logs\n</code></pre> Use Stern to View Logs<pre><code>brew install stern\nstern . -n default\n</code></pre> Get Logs<pre><code>kubectl logs\n</code></pre> Use Stern to View Logs<pre><code>brew install stern\nstern . -n default\n</code></pre> OpenShiftKubernetes Get Service<pre><code>oc get svc\n</code></pre> Get Service Description<pre><code>oc describe svc my-service\n</code></pre> Expose a Service<pre><code>oc expose service &lt;service_name&gt;\n</code></pre> Get Route for the Service<pre><code>oc get route\n</code></pre> Get Service<pre><code>kubectl get svc\n</code></pre> Get Service Description<pre><code>kubectl describe svc my-service\n</code></pre> Get Service Endpoints<pre><code>kubectl get ep my-service\n</code></pre> Expose a Deployment via a Service<pre><code>kubectl expose deployment my-deployment --port 80 --target-port=http --selector app=nginx --name my-service-2 --type NodePort\n</code></pre>"},{"location":"k8s/openshift/services-networking/ingress/","title":"Ingress","text":""},{"location":"k8s/openshift/services-networking/ingress/#ingress","title":"Ingress","text":"<p>An API object that manages external access to the services in a cluster, typically HTTP.</p> <p>Ingress can provide load balancing, SSL termination and name-based virtual hosting.</p> <p>Ingress exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. Traffic routing is controlled by rules defined on the Ingress resource.</p>"},{"location":"k8s/openshift/services-networking/ingress/#resources","title":"Resources","text":"OpenShiftKubernetes <p>Ingress Operator </p> <p>Using Ingress Controllers </p> <p>Ingress </p> <p>Ingress Controllers </p> <p>Minikube Ingress </p>"},{"location":"k8s/openshift/services-networking/ingress/#references","title":"References","text":"<pre><code>apiVersion: networking.k8s.io/v1beta1 # for versions before 1.14 use extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: example-ingress\nspec:\n  rules:\n    - host: hello-world.info\n      http:\n        paths:\n          - path: /\n            backend:\n              serviceName: web\n              servicePort: 8080\n</code></pre> OpenShiftKubernetes View Ingress Status<pre><code>oc describe clusteroperators/ingress\n</code></pre> Describe default Ingress Controller<pre><code>oc describe --namespace=openshift-ingress-operator ingresscontroller/default\n</code></pre> Describe default Ingress Controller<pre><code>kubectl get pods -n kube-system | grep ingress\n</code></pre> <pre><code>kubectl create deployment web --image=bitnami/nginx\n</code></pre> <pre><code>kubectl expose deployment web --name=web --port 8080\n</code></pre> <pre><code>kubectl get svc web\n</code></pre> <pre><code>kubectl get ingress\n</code></pre> <pre><code>kubcetl describe ingress example-ingress\n</code></pre> <pre><code>curl hello-world.info --resolve hello-world.info:80:&lt;ADDRESS&gt;\n</code></pre>"},{"location":"k8s/openshift/services-networking/routes/","title":"Routes","text":""},{"location":"k8s/openshift/services-networking/routes/#routes","title":"Routes","text":"<p>OpenShift Only</p> <p>Routes are Openshift objects that expose services for external clients to reach them by name.</p> <p>Routes can insecured or secured on creation using certificates.</p> <p>The new route inherits the name from the service unless you specify one using the --name option.</p>"},{"location":"k8s/openshift/services-networking/routes/#resources","title":"Resources","text":"OpenShift <p>Routes </p> <p>Route Configuration </p> <p>Secured Routes </p>"},{"location":"k8s/openshift/services-networking/routes/#references","title":"References","text":"<p>Route Creation</p> <pre><code>apiVersion: v1\nkind: Route\nmetadata:\n  name: frontend\nspec:\n  to:\n    kind: Service\n    name: frontend\n</code></pre> <p>Secured Route Creation</p> <pre><code>apiVersion: v1\nkind: Route\nmetadata:\n  name: frontend\nspec:\n  to:\n    kind: Service\n    name: frontend\n  tls:\n    termination: edge\n</code></pre>"},{"location":"k8s/openshift/services-networking/routes/#commands","title":"Commands","text":"OpenShift Create Route from YAML<pre><code>oc apply -f route.yaml\n</code></pre> Get Route<pre><code>oc get route\n</code></pre> Describe Route<pre><code>oc get route &lt;route_name&gt;\n</code></pre> Get Route YAML<pre><code>oc get route &lt;route_name&gt; -o yaml\n</code></pre>"},{"location":"k8s/openshift/services-networking/services/","title":"Services","text":""},{"location":"k8s/openshift/services-networking/services/#services","title":"Services","text":"<p>An abstract way to expose an application running on a set of Pods as a network service.</p> <p>Kubernetes Pods are mortal. They are born and when they die, they are not resurrected. If you use a Deployment to run your app, it can create and destroy Pods dynamically.</p> <p>Each Pod gets its own IP address, however in a Deployment, the set of Pods running in one moment in time could be different from the set of Pods running that application a moment later.</p> <p>In Kubernetes, a Service is an abstraction which defines a logical set of Pods and a policy by which to access them (sometimes this pattern is called a micro-service). The set of Pods targeted by a Service is usually determined by a selector (see below for why you might want a Service without a selector).</p> <p>If you\u2019re able to use Kubernetes APIs for service discovery in your application, you can query the API server for Endpoints, that get updated whenever the set of Pods in a Service changes.</p> <p>For non-native applications, Kubernetes offers ways to place a network port or load balancer in between your application and the backend Pods.</p>"},{"location":"k8s/openshift/services-networking/services/#resources","title":"Resources","text":"OpenShift &amp; Kubernetes <p>Services </p> <p>Exposing Services </p>"},{"location":"k8s/openshift/services-networking/services/#references","title":"References","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\n  labels:\n    app: nginx\n    version: v1\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n        version: v1\n    spec:\n      containers:\n        - name: nginx\n          image: bitnami/nginx\n          ports:\n            - containerPort: 8080\n              name: http\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  selector:\n    app: nginx\n  ports:\n    - name: http\n      port: 80\n      targetPort: http\n</code></pre> OpenShiftKubernetes Get Logs<pre><code>oc logs\n</code></pre> Use Stern to View Logs<pre><code>brew install stern\nstern . -n default\n</code></pre> Get Logs<pre><code>kubectl logs\n</code></pre> Use Stern to View Logs<pre><code>brew install stern\nstern . -n default\n</code></pre> OpenShiftKubernetes Get Service<pre><code>oc get svc\n</code></pre> Get Service Description<pre><code>oc describe svc my-service\n</code></pre> Expose a Service<pre><code>oc expose service &lt;service_name&gt;\n</code></pre> Get Route for the Service<pre><code>oc get route\n</code></pre> Get Service<pre><code>kubectl get svc\n</code></pre> Get Service Description<pre><code>kubectl describe svc my-service\n</code></pre> Get Service Endpoints<pre><code>kubectl get ep my-service\n</code></pre> Expose a Deployment via a Service<pre><code>kubectl expose deployment my-deployment --port 80 --target-port=http --selector app=nginx --name my-service-2 --type NodePort\n</code></pre>"},{"location":"k8s/openshift/state-persistence/","title":"State Persistence","text":""},{"location":"k8s/openshift/state-persistence/#state-persistence","title":"State Persistence","text":"<p>State persistence in the context of Kubernetes/OpenShift refers to the ability to maintain and retain the state or data of applications even when they are stopped, restarted, or moved between nodes.</p> <p>This is achieved through the use of volumes, persistent volumes (PVs), and persistent volume claims (PVCs). Volumes provide a way to store and access data in a container, while PVs serve as the underlying storage resources provisioned by the cluster. PVCs act as requests made by applications for specific storage resources from the available PVs. By utilizing PVs and PVCs, applications can ensure that their state is preserved and accessible across pod restarts and migrations, enabling reliable and consistent data storage and retrieval throughout the cluster.</p>"},{"location":"k8s/openshift/state-persistence/#resources","title":"Resources","text":"<p>Volumes </p> <p>Persistent Volumes </p> <p>Persistent Volume Claims </p>"},{"location":"k8s/openshift/state-persistence/#references","title":"References","text":"<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: my-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\n</code></pre> <p>In this example, we define a PVC named my-pvc with the following specifications:</p> <ul> <li>accessModes specify that the volume can be mounted as read-write by a single node at a time (\"ReadWriteOnce\")</li> <li>resources.requests.storage specifies the requested storage size for the PVC (\"1Gi\")</li> </ul>"},{"location":"k8s/openshift/state-persistence/pv-pvc/","title":"Persistent Volumes &amp; Claims","text":""},{"location":"k8s/openshift/state-persistence/pv-pvc/#persistent-volumes-claims","title":"Persistent Volumes &amp; Claims","text":"<p>Managing storage is a distinct problem from managing compute instances. The PersistentVolume subsystem provides an API for users and administrators that abstracts details of how storage is provided from how it is consumed.</p> <p>A PersistentVolume (PV) is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using Storage Classes.</p> <p>A PersistentVolumeClaim (PVC) is a request for storage by a user. It is similar to a Pod. Pods consume node resources and PVCs consume PV resources. Claims can request specific size and access modes (e.g., they can be mounted once read/write or many times read-only).</p> <p>While PersistentVolumeClaims allow a user to consume abstract storage resources, it is common that users need PersistentVolumes with varying properties, such as performance, for different problems. Cluster administrators need to be able to offer a variety of PersistentVolumes that differ in more ways than just size and access modes, without exposing users to the details of how those volumes are implemented. For these needs, there is the StorageClass resource.</p> <p>Pods access storage by using the claim as a volume. Claims must exist in the same namespace as the Pod using the claim. The cluster finds the claim in the Pod\u2019s namespace and uses it to get the PersistentVolume backing the claim. The volume is then mounted to the host and into the Pod.</p> <p>PersistentVolumes binds are exclusive, and since PersistentVolumeClaims are namespaced objects, mounting claims with \u201cMany\u201d modes (ROX, RWX) is only possible within one namespace.</p>"},{"location":"k8s/openshift/state-persistence/pv-pvc/#resources","title":"Resources","text":"OpenShiftKubernetes <p>Persistent Storage </p> <p>Persistent Volume Types </p> <p>Expanding Peristent Volumes </p> <p>Persistent Volumes </p> <p>Writing Portable Configurations </p> <p>Configuring Persistent Volume Storage </p>"},{"location":"k8s/openshift/state-persistence/pv-pvc/#references","title":"References","text":"<pre><code>kind: PersistentVolume\napiVersion: v1\nmetadata:\n  name: my-pv\nspec:\n  storageClassName: local-storage\n  capacity:\n    storage: 128Mi\n  accessModes:\n    - ReadWriteOnce\n  hostPath:\n    path: \"/mnt/data-1\"\n</code></pre> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: my-pvc\nspec:\n  storageClassName: local-storage\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 100Mi\n</code></pre> <pre><code>kind: Pod\napiVersion: v1\nmetadata:\n  name: my-pod\nspec:\n  containers:\n    - name: nginx\n      image: busybox\n      command:\n        [\n          \"sh\",\n          \"-c\",\n          \"echo $(date):$HOSTNAME Hello Kubernetes! &gt;&gt; /mnt/data/message.txt &amp;&amp; sleep 3600\",\n        ]\n      volumeMounts:\n        - mountPath: \"/mnt/data\"\n          name: my-data\n  volumes:\n    - name: my-data\n      persistentVolumeClaim:\n        claimName: my-pvc\n</code></pre> OpenShiftKubernetes Get the Persistent Volumes in Project<pre><code>oc get pv\n</code></pre> Get the Persistent Volume Claims<pre><code>oc get pvc\n</code></pre> Get a specific Persistent Volume<pre><code>oc get pv &lt;pv_claim&gt;\n</code></pre> Get the Persistent Volume<pre><code>kubectl get pv\n</code></pre> Get the Persistent Volume Claims<pre><code>kubectl get pvc\n</code></pre>"},{"location":"k8s/openshift/state-persistence/volumes/","title":"Volumes","text":""},{"location":"k8s/openshift/state-persistence/volumes/#volumes","title":"Volumes","text":"<p>On-disk files in a Container are ephemeral, which presents some problems for non-trivial applications when running in Containers. First, when a Container crashes, kubelet will restart it, but the files will be lost - the Container starts with a clean state. Second, when running Containers together in a Pod it is often necessary to share files between those Containers. The Kubernetes Volume abstraction solves both of these problems.</p> <p>Podman also has a concept of volumes, though it is somewhat looser and less managed. In Podman, a volume is simply a directory on disk or in another Container.</p> <p>A Kubernetes volume, on the other hand, has an explicit lifetime - the same as the Pod that encloses it. Consequently, a volume outlives any Containers that run within the Pod, and data is preserved across Container restarts. Of course, when a Pod ceases to exist, the volume will cease to exist, too. Perhaps more importantly than this, Kubernetes supports many types of volumes, and a Pod can use any number of them simultaneously.</p>"},{"location":"k8s/openshift/state-persistence/volumes/#resources","title":"Resources","text":"OpenShiftKubernetes <p>Volume Lifecycle </p> <p>Volumes </p>"},{"location":"k8s/openshift/state-persistence/volumes/#references","title":"References","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n    - image: busybox\n      command: [\"sh\", \"-c\", \"echo Hello Kubernetes! &amp;&amp; sleep 3600\"]\n      name: busybox\n      volumeMounts:\n        - mountPath: /cache\n          name: cache-volume\n  volumes:\n    - name: cache-volume\n      emptyDir: {}\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: test-pd\nspec:\n  containers:\n    - image: bitnami/nginx\n      name: test-container\n      volumeMounts:\n        - mountPath: /test-pd\n          name: test-volume\n  volumes:\n    - name: test-volume\n      hostPath:\n        # directory location on host\n        path: /data\n        # this field is optional\n        type: Directory\n</code></pre>"},{"location":"k8s/solutions/creating-services/","title":"Lab K8s 8 - Services Solution","text":""},{"location":"k8s/solutions/creating-services/#lab-k8s-8-services-solution","title":"Lab K8s 8 - Services Solution","text":""},{"location":"k8s/solutions/creating-services/#solution","title":"Solution","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: jedi-svc\nspec:\n  type: NodePort\n  selector:\n    app: jedi\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 8080\n</code></pre> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: yoda-svc\nspec:\n  type: ClusterIP\n  selector:\n    app: yoda\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 8080\n</code></pre>"},{"location":"k8s/solutions/cron-jobs/","title":"Lab K8s 7 - Cron Jobs Solution","text":""},{"location":"k8s/solutions/cron-jobs/#lab-k8s-7-cron-jobs-solution","title":"Lab K8s 7 - Cron Jobs Solution","text":""},{"location":"k8s/solutions/cron-jobs/#solution","title":"Solution","text":"<pre><code>apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: pe-bootcamp\nspec:\n  schedule: \"*/2 * * * *\"\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n            - name: cron-pe-bootcamp\n              image: nginx:latest\n              command:\n                - /bin/sh\n                - -c\n                - echo Welcome to IBM CE Platform Engineer Bootcamp\n          restartPolicy: OnFailure\n</code></pre> <pre><code>kubectl get cronjob xwing-cronjob\n</code></pre>"},{"location":"k8s/solutions/debugging/","title":"Lab K8s 4 - Troubleshooting Solution","text":""},{"location":"k8s/solutions/debugging/#lab-k8s-4-troubleshooting-solution","title":"Lab K8s 4 - Troubleshooting Solution","text":""},{"location":"k8s/solutions/debugging/#solution","title":"Solution","text":"<ol> <li> <p>Use the project <code>debug</code></p> <pre><code>oc project debug\n</code></pre> </li> <li> <p>Check <code>STATUS</code> column for not Ready</p> <pre><code>oc get pods --all-namespaces\n</code></pre> </li> <li> <p>Check the description of the deployment</p> <pre><code>oc describe deployment hyper-drive\n</code></pre> </li> <li> <p>Save logs for a broken pod</p> <pre><code>oc logs &lt;pod name&gt; -n &lt;namespace&gt; &gt; /home/cloud_user/debug/broken-pod-logs.log\n</code></pre> </li> <li> <p>In the description you will see the following is wrong:</p> <ul> <li>Selector and Label names do not match.</li> <li>The Probe is TCP instead of HTTP Get.</li> <li>The Service Port is 80 instead of 8080.</li> </ul> </li> <li> <p>To fix probe, can't <code>oc edit</code> will not work. To the deployment it is best to delete and recreate the deployment.</p> <pre><code>oc get deployment &lt;deployment name&gt; -n &lt;namespace&gt; -o yaml --export &gt; hyper-drive.yml\n</code></pre> </li> <li> <p>Delete pod</p> <pre><code>oc delete deployment &lt;deployment name&gt; -n &lt;namespace&gt;\n</code></pre> </li> <li> <p>Can also use <code>oc replace</code> or <code>oc apply</code> to apply the updated deployment yaml.</p> <pre><code>oc apply -f hyper-drive.yml -n &lt;namespace&gt;\n</code></pre> </li> <li> <p>Verify</p> <pre><code>oc get deployment &lt;deployment name&gt; -n &lt;namespace&gt;\n</code></pre> </li> </ol>"},{"location":"k8s/solutions/ingress/","title":"Lab K8s 10 - Ingress Controller IBM Free K8s Cluster","text":""},{"location":"k8s/solutions/ingress/#lab-k8s-10-ingress-controller-ibm-free-k8s-cluster","title":"Lab K8s 10 - Ingress Controller IBM Free K8s Cluster","text":"<p>The IBM Kubernetes service free clusters consist of a single worker node with 2 CPU and 4 GB of memory for experimenting with Kubernetes. Unlike the fee-based service, these clusters do not include capabilities for application load balancing using ingress out-of-the-box. </p>"},{"location":"k8s/solutions/ingress/#prerequisites","title":"Prerequisites","text":"<ul> <li>Free IBM Kubernetes Cluster (IKS) - upgrade your account from Lite plan to create one. In the example commands, we'll assume that this cluster is named <code>mycluster</code></li> <li>kubectl - match your cluster API version </li> <li>Log in to IBM Cloud and configure <code>kubectl</code> using the <code>ibmcloud ks cluster config --cluster mycluster</code> command</li> </ul>"},{"location":"k8s/solutions/ingress/#components","title":"Components","text":"<p>On the IKS cluster, you will install helm charts for a nginx ingress controller from NGINX. This lab already provides the templated yaml files so there is no need to use helm cli.</p>"},{"location":"k8s/solutions/ingress/#set-up-the-ingress-controller","title":"Set up the ingress controller","text":"<p>Only do this on a free IKS instance These steps assume facts that only apply to free IKS instances:</p> <ul> <li>a single worker where the cluster administrator can create pods that bind to host ports</li> <li>no pre-existing ingress controller or application load balancer</li> </ul> <p>Using the following steps with a paid instance can cause issues. See the IBM Cloud containers documentation for information on exposing applications with the ingress/alb services for paid clusters. You have been warned</p> <ol> <li> <p>Install the NGINX ingress controller with <code>helm</code> using a daemonset and no service resource (which will result in a single pod that binds to ports 80 and 443 on the worker node and will skip creation of a <code>ClusterIP, LoadBalancer, or NodePort</code> for the daemonset).     </p><pre><code>kubectl apply -f https://cloudnative101.dev/yamls/ingress-controller/iks-ingress-v1.7.1.yaml\n</code></pre><p></p> </li> <li> <p>You can use free domain <code>.nip.io</code> to get a domain name using one of the IP Address of your worker nodes. Run this command to set your DOMAIN     </p><pre><code>export DOMAIN=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"ExternalIP\")].address}').nip.io\necho $DOMAIN\n</code></pre><p></p> </li> <li> <p>You can test the ingress controller using the <code>$DOMAIN</code>:</p> <p></p><pre><code>curl -I http://$DOMAIN\n</code></pre> <pre><code>HTTP/1.1 404 Not Found\nServer: nginx/1.17.10\n...\n</code></pre><p></p> <p>A 404 is expected at this point because unlike the kubernetes nginx ingress, the NGINX version of the ingress controller does not create a default backend deployment.</p> </li> <li> <p>To use the ingress controller deploy a sample application, expose a service.     </p><pre><code>kubectl create deployment web --image=bitnami/nginx\nkubectl expose deployment web --name=web --port 8080\n</code></pre><p></p> </li> <li> <p>Now create an Ingress resource     </p><pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: networking.k8s.io/v1beta1\nkind: Ingress\nmetadata:\n  name: web\n  labels:\n    app: web\nspec:\n  rules:\n    - host: web.$DOMAIN\n      http:\n        paths:\n          - path: /\n            backend:\n              serviceName: web\n              servicePort: 8080\nEOF\necho \"Access your web app at http://web.$DOMAIN\"\n</code></pre><p></p> </li> <li> <p>List the created ingress     </p><pre><code>kubectl get ingress web\n</code></pre><p></p> </li> <li> <p>Access your web application    </p><pre><code>curl http://web.$DOMAIN\n</code></pre>    The output prints the html    <pre><code>&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;\n</code></pre><p></p> </li> <li> <p>Delete all the resources created     </p><pre><code>kubectl delete deployment,svc,ingress -l app=web\n</code></pre><p></p> </li> </ol>"},{"location":"k8s/solutions/multi-containers/","title":"Lab K8s 2 - Manage Multiple Containers Solution","text":""},{"location":"k8s/solutions/multi-containers/#lab-k8s-2-manage-multiple-containers-solution","title":"Lab K8s 2 - Manage Multiple Containers Solution","text":""},{"location":"k8s/solutions/multi-containers/#solution","title":"Solution","text":"<ol> <li> <p>Create a project named web if it does not already exist. </p> <pre><code>oc new-project web\n</code></pre> </li> <li> <p>Create a YAML file <code>vader-service-ambassador-config</code> that contains the following data.</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: vader-service-ambassador-config\ndata:\n  haproxy.cfg: |-\n    global\n        daemon\n        maxconn 256\n\n    defaults\n        mode http\n        timeout connect 5000ms\n        timeout client 50000ms\n        timeout server 50000ms\n\n    listen http-in\n        bind *:80\n        server server1 127.0.0.1:8989 maxconn 32\n</code></pre> </li> <li> <p>Create a YAML File <code>vader-service</code> that contains the following data.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: vader-service\nspec:\n  containers:\n  - name: millennium-falcon\n    image: quay.io/don_bailey_ibm/millennium-falcon:v1.0.0\n  - name: haproxy-ambassador\n    image: haproxy:1.7\n    ports:\n    - containerPort: 80\n    volumeMounts:\n    - name: config-volume\n      mountPath: /usr/local/etc/haproxy\n  volumes:\n  - name: config-volume\n    configMap:\n      name: vader-service-ambassador-config\n</code></pre> </li> <li> <p>Create a file <code>busybox.yaml</code> that contains the following data.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\nspec:\n  containers:\n  - name: myapp-container\n    image: radial/busyboxplus:curl\n    command: ['sh', '-c', 'while true; do sleep 3600; done']\n</code></pre> </li> <li> <p>Apply all of the YAML files to a cluster.</p> <pre><code>oc apply -f vader-service-ambassador-config.yaml\noc apply -f vader-service.yaml\noc apply -f busybox.yaml\n</code></pre> </li> <li> <p>Verify that the containers are up, running and the expect result is output.</p> <ul> <li> <p>For OpenShift</p> <pre><code>oc exec busybox -- curl $(oc get pod vader-service -o=jsonpath='{.status.podIP}'):80\n</code></pre> </li> <li> <p>For K8s</p> <pre><code>kubectl exec busybox -- curl $(kubectl get pod vader-service -o=jsonpath='{.status.podIP}'):80\n</code></pre> </li> </ul> </li> </ol>"},{"location":"k8s/solutions/network-policies/","title":"Lab K8s 9 - Network Policies Solution","text":""},{"location":"k8s/solutions/network-policies/#lab-k8s-9-network-policies-solution","title":"Lab K8s 9 - Network Policies Solution","text":""},{"location":"k8s/solutions/network-policies/#solution","title":"Solution","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: my-network-policy\nspec:\n  podSelector:\n    matchLabels:\n      app: secure-app\n  policyTypes:\n  - Ingress\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          allow-access: \"true\"\n</code></pre>"},{"location":"k8s/solutions/persistent-volumes/","title":"Lab K8s 11 - Persistent Volumes Solution","text":""},{"location":"k8s/solutions/persistent-volumes/#lab-k8s-11-persistent-volumes-solution","title":"Lab K8s 11 - Persistent Volumes Solution","text":""},{"location":"k8s/solutions/persistent-volumes/#solution","title":"Solution","text":"<pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: postgresql-pv\nspec:\n  storageClassName: localdisk\n  capacity:\n    storage: 1Gi\n  accessModes:\n    - ReadWriteOnce\n  hostPath:\n    path: \"/mnt/data\"\n</code></pre> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: postgresql-pv-claim\nspec:\n  storageClassName: localdisk\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 500Mi\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: postgresql-pod\nspec:\n  containers:\n  - name: postgresql\n    image: bitnami/postgresql\n    ports:\n    - containerPort: 5432\n    env:\n    - name: MYSQL_ROOT_PASSWORD\n      value: password\n    volumeMounts:\n    - name: sql-storage\n      mountPath: /bitnami/postgresql/\n  volumes:\n  - name: sql-storage\n    persistentVolumeClaim:\n      claimName: postgresql-pv-claim\n</code></pre> <p>Verify via <code>ls /mnt/data</code> on node</p>"},{"location":"k8s/solutions/pod-config/","title":"Lab K8s 5 - Pod Configuration Solution","text":""},{"location":"k8s/solutions/pod-config/#lab-k8s-5-pod-configuration-solution","title":"Lab K8s 5 - Pod Configuration Solution","text":""},{"location":"k8s/solutions/pod-config/#solution","title":"Solution","text":"ConfigMap<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: yoda-service-config\ndata:\n  yoda.cfg: |-\n    yoda.baby.power=100000000\n    yoda.strength=10\n</code></pre> ServiceAccount<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: yoda-svc\n</code></pre> Secret<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: yoda-db-password\nstringData:\n  password: 0penSh1ftRul3s!\n</code></pre> Pod<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: yoda-service\nspec:\n  serviceAccountName: yoda-svc\n  containers:\n  - name: yoda-service\n    image: bitnami/nginx\n    volumeMounts:\n      - name: config-volume\n        mountPath: /etc/yoda-service\n    env:\n    - name: DB_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: yoda-db-password\n          key: password\n    resources:\n      requests:\n        memory: \"64Mi\"\n        cpu: \"250m\"\n      limits:\n        memory: \"128Mi\"\n        cpu: \"500m\"\n  volumes:\n  - name: config-volume\n    configMap:\n      name: yoda-service-config\n</code></pre>"},{"location":"k8s/solutions/pod-creation/","title":"Lab K8s 1 - Pod Creation Solution","text":""},{"location":"k8s/solutions/pod-creation/#lab-k8s-1-pod-creation-solution","title":"Lab K8s 1 - Pod Creation Solution","text":""},{"location":"k8s/solutions/pod-creation/#solution","title":"Solution","text":"<ol> <li> <p>Create a project named <code>web</code></p> <pre><code>oc new-project web\n</code></pre> </li> <li> <p>Save the following YAML to a file named <code>pods.yaml</code></p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  namespace: web\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    command: [\"nginx\"]\n    args: [\"-g\", \"daemon off;\", \"-q\"]\n    ports:\n    - containerPort: 80\n</code></pre> </li> <li> <p>Apply the YAML created in step 2.</p> <pre><code>oc apply -f pod.yaml\n</code></pre> </li> </ol>"},{"location":"k8s/solutions/probes/","title":"Lab K8s 3 - Probes Solution","text":""},{"location":"k8s/solutions/probes/#lab-k8s-3-probes-solution","title":"Lab K8s 3 - Probes Solution","text":""},{"location":"k8s/solutions/probes/#solution","title":"Solution","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    test: liveness\n  name: liveness-example\nspec:\n  containers:\n  - name: liveness\n    image: docker.io/busybox\n    args:\n    - /bin/sh\n    - -c\n    - touch /tmp/healthz; sleep 40; rm -f /tmp/healthz; sleep 700\n    livenessProbe:\n      exec:\n        command:\n        - cat\n        - /tmp/healthz\n      initialDelaySeconds: 6\n      periodSeconds: 6\n</code></pre>"},{"location":"k8s/solutions/rolling-updates/","title":"Lab K8s 6 - Rolling Updates Solution","text":""},{"location":"k8s/solutions/rolling-updates/#lab-k8s-6-rolling-updates-solution","title":"Lab K8s 6 - Rolling Updates Solution","text":""},{"location":"k8s/solutions/rolling-updates/#solution","title":"Solution","text":"<p>Update the deployment to the new version like so: </p><pre><code>kubectl set image deployment/jedi-deployment jedi-ws=bitnami/nginx:1.18.0 --record\n</code></pre><p></p> <p>Check the progress of the rolling update: </p><pre><code>kubectl rollout status deployment/jedi-deployment\n</code></pre><p></p> <p>In another terminal window </p><pre><code>kubectl get pods -w\n</code></pre><p></p> <p>Get a list of previous revisions. </p><pre><code>kubectl rollout history deployment/jedi-deployment\n</code></pre><p></p> <p>Undo the last revision. </p><pre><code>kubectl rollout undo deployment/jedi-deployment\n</code></pre><p></p> <p>Check the status of the rollout. </p><pre><code>kubectl rollout status deployment/jedi-deployment\n</code></pre><p></p>"},{"location":"ocp-on-ibm-cloud/","title":"OpenShift on IBM Cloud (IPI)","text":""},{"location":"ocp-on-ibm-cloud/#openshift-on-ibm-cloud-ipi","title":"OpenShift on IBM Cloud (IPI)","text":"<p>This exercise is designed to prepare you for installing a self-managed OpenShift cluster on IBM Cloud Infrastructure, leveraging the Installer Provisioned Infrastructure (IPI) tool. As an alternative to managed OpenShift offerings from hyperscalers such as Red Hat OpenShift Kubernetes Service (ROKS), AWS Red Hat OpenShift Service on AWS (ROSA), and Microsoft Azure Red Hat OpenShift (ARO), this guide focuses on the installation process for a self-managed OpenShift cluster.</p> <p>Managed OpenShift offerings from hyperscalers provide a simplified experience for deploying OpenShift clusters. However, these services require you to use their specific command-line interfaces and web consoles to install and manage clusters. In contrast, this exercise will guide you through the installation of a self-managed OpenShift cluster using IPI.</p> <p>Before proceeding with the installation process, it is essential to understand the common concepts and services used in hyperscalers:</p> <ul> <li>Region: A geographic location where a cloud provider offers its services, typically consisting of one or more data centers.</li> <li>Virtual Private Cloud (VPC): A logical isolation of resources within the cloud, providing a secure environment for your applications.</li> <li>Availability Zone: Isolated or separated data center(s) located within specific regions, offering high availability and scalability.</li> <li>Security Group: A virtual firewall that controls incoming and outgoing traffic to and from instances within the VPC, ensuring security and access control.</li> <li>Application Load Balancer: A type of load balancer designed to handle HTTP and HTTPS traffic, distributing workloads across multiple instances.</li> <li>Application Gateway: A device or system that connects two different networks or systems together, enabling data exchange between them.</li> <li>Object Storage: A type of cloud storage that stores data as objects, providing a cost-effective, scalable, and highly available solution.</li> </ul>"},{"location":"ocp-on-ibm-cloud/#outcomes","title":"Outcomes","text":"<p>Upon completing this exercise, you will have the ability to:</p> <ul> <li>Install a self-managed OpenShift cluster on IBM Cloud using IPI.</li> <li>Backup the etcd database to IBM Cloud Object Storage.</li> </ul>"},{"location":"ocp-on-ibm-cloud/#scenario","title":"Scenario","text":"<p>To demonstrate the installation process, we will create a five-node OpenShift cluster with three control plane nodes and two compute nodes, each meeting the specified requirements:</p> Node type vCPU Memory in GiB Disk size in GB Control Plane 4 16 100 Compute 8 32 100 <p>The Cloud Credential Operator is required for this installation. If you would like to learn more about the Cloud Credential Operator, please refer to the Cloud Credential Operator documentation.</p>"},{"location":"ocp-on-ibm-cloud/#provision-the-lab-environment","title":"Provision the lab environment","text":"<ol> <li> <p>Navigate to the OpenShift Installation Cohort collection in TechZone.</p> </li> <li> <p>Reserve the \"Virtual Server on VPC\" environment.</p> </li> </ol> <p>Once you have completed these steps, you will be ready to proceed with installing your self-managed OpenShift cluster using IPI.</p>"},{"location":"ocp-on-ibm-cloud/backup-etcd/","title":"Backup etcd","text":""},{"location":"ocp-on-ibm-cloud/backup-etcd/#backup-etcd","title":"Backup <code>etcd</code>","text":"<p>You will be required to backup the <code>etcd</code> data of your OpenShift cluster to IBM Cloud Object Storage. As part of the IPI installation two buckets have been created, choose one of these as the target destination for the data.</p> <p>Please find some references to assist with completing the exercise below.</p> <p>Finding Help</p> <p>IBM Cloud Object Storage - Using cURL</p> <p>Backing up etcd</p> <p>Automate syncing RHOCP's etcd-db backups to an AWS S3 bucket</p> <p>Backing up etcd data from a Red Hat OpenShift Container Platform cluster to IBM Cloud Object Storage</p> <p>Still have questions?</p> <p>Still have questions?  Don't hesitate to consult with other students within your bootcamp cohort.</p>"},{"location":"ocp-on-ibm-cloud/install-cluster/","title":"Install the OpenShift Cluster","text":""},{"location":"ocp-on-ibm-cloud/install-cluster/#install-the-openshift-cluster","title":"Install the OpenShift Cluster","text":"<p>There are no step-by-step instructions provided here, you should use the official Red Hat documentation and your experience from the prior installation labs to determine the proper steps for this installation. The installation method you need to follow is Installing a cluster on IBM Cloud with customizations.</p> <p>Important</p> <p>The section \"Configuring an IBM Cloud account\" in the Red Hat OpenShift documentation must be skipped. The instructors have already configured the IBM Cloud account for you.</p> <p>Some of the installation steps will require information that will be provided by the instructors, as they are specific to your cohort. You will be provided a link to a box note with the relevant information, including an API key for an IBM Cloud Service Id that has sufficient permissions to install OpenShift into a specific resource group.</p> <p>The components created by the installer are depicted in the diagram below.</p> <p></p> <p>Cleaning up</p> <p>If you need / want to restart the cluster installation from scratch, follow the steps below.</p> <p>Set shell variables.</p> <pre><code>INSTALL_DIR=&lt;your_install_dir&gt;\nCLUSTER_NAME=&lt;your_cluster_name&gt;\n</code></pre> <p>Destroy the cluster.</p> <pre><code>openshift-install destroy cluster --dir ${INSTALL_DIR}\n</code></pre> <p>Delete the cloud credentials.</p> <pre><code>ccoctl ibmcloud delete-service-id --credentials-requests-dir ${INSTALL_DIR}/creds --name ${CLUSTER_NAME}\n</code></pre> <p>Remove the installation directory.</p> <pre><code>rm -fr ${INSTALL_DIR}\n</code></pre> <p>Remove the hidden files in <code>${HOME}</code>.</p> <pre><code>rm ${HOME}/.openshift_install*\n</code></pre> <p>Remove <code>install-config.yaml</code></p> <pre><code>rm ${HOME}/install-config.yaml\n</code></pre>"},{"location":"ocp-on-ibm-cloud/prepare/","title":"Prepare the Installation","text":""},{"location":"ocp-on-ibm-cloud/prepare/#prepare-the-installation","title":"Prepare the Installation","text":"<ol> <li> <p>Using the SSH key log in as user <code>itzuser</code>.</p> </li> <li> <p>Install the OpenShift installer.</p> </li> <li> <p>Install the OpenShift command line interface (CLI).</p> </li> <li> <p>Copy your pull secret from the Red Hat Hybrid Cloud Console and save it in file <code>~/.pull-secret</code>.</p> </li> <li> <p>Install the Cloud Credential Operator utility.</p> <p>Tip</p> <p>To complete the steps documented below to obtain the <code>ccoctl</code> command. Administrator access to an OpenShift cluster is required.  This command is also available for download from Redhat.  Download the <code>ccoctl</code>command </p> <p>Extract the <code>ccoctl</code> application.</p> <pre><code>RELEASE_IMAGE=$(openshift-install version | awk '/release image/ {print $3}')\n</code></pre> <pre><code>CCO_IMAGE=$(oc adm release info --image-for='cloud-credential-operator' ${RELEASE_IMAGE} -a ~/.pull-secret)\n</code></pre> <pre><code>oc image extract ${CCO_IMAGE} --file=\"/usr/bin/ccoctl\" -a ~/.pull-secret\n</code></pre> <p>Ensure the application is executable.</p> <pre><code>chmod 775 ccoctl\n</code></pre> <p>Copy it to a directory in <code>$PATH</code>.</p> <pre><code>sudo install ccoctl /usr/local/bin\n</code></pre> <p>Verify <code>ccoctl</code> is executable.</p> <pre><code>ccoctl ibmcloud -h\n</code></pre> <p>You should see output similar to the following:</p> <pre><code>Creating/deleting cloud credentials objects for IBM Cloud\n\nUsage:\n    ccoctl ibmcloud [command]\n\nAvailable Commands:\n    create-service-id Create Service ID\n    delete-service-id Delete Service ID\n    refresh-keys      Refresh API Keys for the Service ID\n\nFlags:\n    -h, --help   help for ibmcloud\n\nUse \"ccoctl ibmcloud [command] --help\" for more information about a command.\n</code></pre> <p>Clean up.</p> <pre><code>rm ccoctl\n</code></pre> </li> </ol>"},{"location":"ocp-on-vmware/","title":"Deploying OpenShift on VMware","text":""},{"location":"ocp-on-vmware/#deploying-openshift-on-vmware","title":"Deploying OpenShift on VMware","text":"<p>In this exercise, you will learn how to deploy an OpenShift cluster on a VMware virtualized environment using Installer Provisioned Infrastructure (IPI), also known as full-stack automation. By the end, you will have gained hands-on experience in installing and configuring an OpenShift cluster ready for IBM Client Engineering Pilots.</p>"},{"location":"ocp-on-vmware/#outcomes","title":"Outcomes","text":"<p>Upon completion of this exercise, you will be able to:</p> <ul> <li>Install OpenShift on vSphere<sup>1</sup> : Deploy an OpenShift cluster using IPI, which automates the installation process from scratch.<ul> <li>Install with customization that will create the Infrastructure nodes for ODF.</li> </ul> </li> <li>Configure Identity Provider for htpasswd Authentication : Set up identity provider authentication for your OpenShift cluster using htpasswd.</li> <li>Install Red Hat OpenShift Data Foundation (ODF) : Deploy ODF, a software-defined storage solution that supports both read-write many (RWX) and read-write once (RWO) storage classes.</li> <li>Configure ODF Storage for Internal Image Registry : Configure ODF to provide persistent storage for your internal image registry.</li> <li>*Enable the local image repository to support S2I images and IBM Cloudpak installations.</li> </ul>"},{"location":"ocp-on-vmware/#scenario","title":"Scenario","text":"<p>A modernization project requires containerizing existing applications using OpenShift, but the client lacks the necessary human resources to build the cluster. As part of our IBM team, you have been tasked with constructing an OpenShift cluster on their behalf.</p>"},{"location":"ocp-on-vmware/#the-required-cluster-configuration-consists-of-nine-nodes","title":"The required cluster configuration consists of nine nodes:","text":"Node type vCPU Memory in GiB Disk size in GB Node Count Control Plane 16 64 200 3 Compute 16 64 200 3 Infrastructure 16 64 200 3 <p>In addition, you will need to deploy a software-defined storage (SDS) solution that meets the following requirements:</p> <ul> <li>Supports both RWX and RWO storage classes</li> <li>Meets the specifications outlined above</li> </ul> <p>We have chosen Red Hat OpenShift Data Foundation (ODF) as our SDS solution. ODF is a highly scalable and flexible storage platform that supports various use cases, including persistent storage for applications and internal image registries.</p> <p>For reference purposes, please recall the illustration of the IPI installation process on vSphere from DO322:</p> <p></p>"},{"location":"ocp-on-vmware/#provision-the-lab-environment","title":"Provision the lab environment","text":"<ol> <li> <p>Navigate to the OpenShift Installation Cohort collection in TechZone.</p> </li> <li> <p>On the left hand side click Environments and then click the Reserve it button on the OCP Gymnasium tile.</p> <p></p> </li> <li> <p>Select Reserve now.</p> </li> <li> <p>Fill out the reservation form adding your information where relevant (use the screenshot below for guidance).</p> </li> </ol> <p>Deploy with VPN</p> <pre><code>Be sure to deploy with the VPN Access **Enabled**.\n</code></pre> <p></p> <ol> <li> <p>Click Submit. Provisioning approximately takes 30 minutes.</p> </li> <li> <p>Once your VMware vSphere environment has been successfully provisioned, you will receive an email notification from noreply@techzone.ibm.com with your reservation details. To access your provisioned environment, click on the URL provided under Reservation ID to access your reservation details.</p> </li> </ol>"},{"location":"ocp-on-vmware/#openshift-gym-overview","title":"OpenShift Gym Overview","text":""},{"location":"ocp-on-vmware/#gym-network-details","title":"Gym Network Details","text":""},{"location":"ocp-on-vmware/#complete-this-table-before-continuing-with-the-installation","title":"Complete this table before continuing with the installation","text":"Name Value Subnet Cluster Name Base Domain Gateway Address Bastion Host api.&lt;Cluster Name&gt;.&lt;Base Domain&gt; *.apps.&lt;Cluster Name&gt;.&lt;BaseDomain&gt; DHCP Assignable IP Addresses Redhat Pull Secret"},{"location":"ocp-on-vmware/#installation-details","title":"Installation Details","text":""},{"location":"ocp-on-vmware/#verifying-environment-provisioning","title":"Verifying Environment Provisioning","text":"<p>To confirm that your VMware vSphere environment has been successfully provisioned, follow these steps:</p> <ol> <li> <p>If you want to use Guacamole (Recommended for first timers) to access your environment, click on the Open your IBM Cloud environment button. Next, expand the ALL CONNECTIONS section and test both Remote Desktop and SSH sessions by attempting to establish connections. If both connections are successful, it indicates that your environment has been provisioned and is ready for installation.</p> <p>HINT: To paste in the Linux environment with your keyboard use Shift + CTRL + V</p> <p></p> </li> <li> <p>If you prefer to use WireGuard to establish a secure connection to your environment, click on the Download WireGuard VPN config button.</p> <ol> <li>Bastion's IP address: <code>192.168.252.2</code></li> <li>Username: <code>admin</code></li> <li>Password: Refer to the top of your reservation for this information</li> </ol> </li> </ol> <p>DNS and WireGuard on MacOS</p> <p>The DNS server (192.168.253.1) configured in the WireGuard client might not be queried. The Cisco Secure Client enables the DNS Proxy and Transparent Proxy by default. Disabling the proxies is a work around, when you disable them they enable themselves automatically. It might take up to 10 tries to get them in the desired state, disabled.</p> <ol> <li> <p>Collective term for VMware's cloud computing virtualization platform which includes vCenter, ESXi and more. The term is also used in the official OpenShift documentation from Red Hat.\u00a0\u21a9</p> </li> </ol>"},{"location":"ocp-on-vmware/config-identity-provider/","title":"Configure the Identity Provider","text":""},{"location":"ocp-on-vmware/config-identity-provider/#configure-the-identity-provider","title":"Configure the Identity Provider","text":"<p>To enable the clients developers to log into the cluster and meet the security requirements, an Identity Provider (IdP) must be configured. The primary purpose of an IdP is to provide a centralized authentication mechanism for users accessing the cluster. This ensures that only authorized individuals can gain access to the cluster.</p> <p>Although OpenShift supports many IdP's, for this example, we will use the HTPasswd provider, which is a popular and widely-supported option. The HTPasswd provider uses a password file to store user credentials, making it easy to manage access to the cluster.</p> <p>It is also a best practice to delete the built-in <code>kubeadmin</code> user after configuring the IdP. This ensures that the only users able to log into the cluster are those who have been explicitly authorized by the administrators.</p> <ol> <li> <p>Ensure the <code>htpasswd</code> command is available on the bastion host.</p> <pre><code>sudo dnf install -y httpd-tools\n</code></pre> </li> <li> <p>Create the htpasswd file with user <code>admin</code> and password <code>OCP4all!</code>.</p> <pre><code>htpasswd -Bbc /tmp/htpasswd admin OCP4all!\n</code></pre> </li> <li> <p>Add the clients developers.</p> <pre><code>for dev in abbott ben webb\ndo\n    htpasswd -b /tmp/htpasswd ${dev} OCP4all!\ndone\n</code></pre> </li> <li> <p>Create a Secret in the <code>openshift-config</code> namespace with data from the htpasswd file, name it <code>localusers</code>.</p> <pre><code>oc -n openshift-config create secret generic localusers \\\n--from-file htpasswd=/tmp/htpasswd\n</code></pre> </li> <li> <p>Export the OAuth resource.</p> <pre><code>oc get oauth cluster -o yaml &gt; /tmp/oauth.yaml\n</code></pre> </li> <li> <p>Edit <code>/tmp/oauth.yaml</code>, under <code>spec:</code> add the HTPasswd identity provider.</p> /tmp/oauth.yaml<pre><code>#...\nspec:\n  identityProviders:\n    - htpasswd:\n        fileData:\n          name: localusers\n      name: localusers\n      type: HTPasswd\n      mappingMethod: claim\n</code></pre> </li> <li> <p>Update the OAuth resource.</p> <pre><code>oc replace -f /tmp/oauth.yaml\n</code></pre> </li> <li> <p>Monitor the Pods in namespace <code>openshift-authentication</code>, they will restart, the rollout is not instant, so please be patient.</p> <pre><code>watch oc -n openshift-authentication get pods\n</code></pre> </li> <li> <p>Assign cluster-admin privileges to user admin.</p> <pre><code>oc adm policy add-cluster-role-to-user cluster-admin admin\n</code></pre> </li> <li> <p>Ensure user admin can log in.</p> <pre><code>oc login --insecure-skip-tls-verify=true -u admin\n</code></pre> </li> <li> <p>Verify user admin has cluster role cluster-admin, only users with this role are authorized to use get nodes.</p> <pre><code>oc auth can-i get nodes\n</code></pre> Example Output<pre><code>Warning: resource 'nodes' is not namespace scoped\n\nyes\n</code></pre> </li> <li> <p>Log in as one of the developers and verify the use of get nodes.</p> <pre><code>oc login --insecure-skip-tls-verify=true -u ben\n</code></pre> <pre><code>oc auth can-i get nodes\n</code></pre> Example Output<pre><code>Warning: resource 'nodes' is not namespace scoped\n\nno\n</code></pre> </li> <li> <p>List the users.</p> <pre><code>oc get users\n</code></pre> Example Output<pre><code>NAME    UID                                    FULL NAME   IDENTITIES\nadmin   a7cfbf3a-0892-40e0-9dcb-7ba37ecc1824               localusers:admin\nben     0e3884e4-529b-4b5f-a7e6-2e333796d03a               localusers:ben\n</code></pre> </li> </ol> <p>Note</p> <p>Only users who have logged in are listed when running the <code>oc get users</code> command.</p> <p>Note</p> <p>In a normal environment the default admin account would be deleated. For this environment please DO NOT deleate this account.  These are the steps to delete the 'kubeadmin' user in a normal environment.</p> <ol> <li> <p>Switch back to user that has cluster-admin role. The user 'admin' is a user with cluster admin role in this example.</p> <pre><code>oc login --insecure-skip-tls-verify=true -u admin\n</code></pre> </li> <li> <p>Delete user <code>kubeadmin</code>.</p> <pre><code>oc -n kube-system delete secret kubeadmin\n</code></pre> </li> </ol>"},{"location":"ocp-on-vmware/config-image-registry/","title":"Configure Internal Image Registry Storage","text":""},{"location":"ocp-on-vmware/config-image-registry/#configure-internal-image-registry-storage","title":"Configure Internal Image Registry Storage","text":"<p>On platforms that do not provide shareable object storage, vSphere without VSAN for example, the OpenShift image registry operator bootstraps itself as <code>Removed</code>. This allows the installer to complete on these platform types.</p> <p>For this project the client's developers will leverage source-to-image (S2I). One of the things that S2I does is push container images to the internal image registry. To enable this, we'll need to configure the internal image registry with storage. Let's configure the internal image registry so it uses NooBaa, an object storage type provided by OpenShift Data Foundation.</p> <ol> <li> <p>Validate that the <code>managementState</code> of the image registry operator is <code>Removed</code>.</p> <pre><code>oc get config.image/cluster -ojsonpath='{.spec.managementState}{\"\\n\"}'\n</code></pre> Example Output<pre><code>Removed\n</code></pre> </li> <li> <p>Create an object bucket claim (OBC) named image-registry in the openshift-image-registry namespace.</p> <pre><code>oc apply -f - &lt;&lt;EOF\napiVersion: objectbucket.io/v1alpha1\nkind: ObjectBucketClaim\nmetadata:\n  name: image-registry\n  namespace: openshift-image-registry\nspec:\n  storageClassName: openshift-storage.noobaa.io\n  generateBucketName: image-registry\nEOF\n</code></pre> </li> <li> <p>Save the name of the OBC in a shell variable.</p> <pre><code>export bucket_name=$(oc -n openshift-image-registry get obc image-registry -o jsonpath='{.spec.bucketName}')\n</code></pre> </li> <li> <p>Save the credentials in shell variables.</p> <pre><code>export AWS_ACCESS_KEY_ID=$(oc -n openshift-image-registry get secret image-registry -o yaml | grep -w \"AWS_ACCESS_KEY_ID:\" | head -n1 | awk '{print $2}' | base64 --decode)\nexport AWS_SECRET_ACCESS_KEY=$(oc -n openshift-image-registry get secret image-registry -o yaml | grep -w \"AWS_SECRET_ACCESS_KEY:\" | head -n1 | awk '{print $2}' | base64 --decode)\n</code></pre> </li> <li> <p>Create the secret named <code>image-registry-private-configuration-user</code>.</p> <pre><code>oc -n openshift-image-registry create secret generic image-registry-private-configuration-user \\\n  --from-literal=REGISTRY_STORAGE_S3_ACCESSKEY=${AWS_ACCESS_KEY_ID} \\\n  --from-literal=REGISTRY_STORAGE_S3_SECRETKEY=${AWS_SECRET_ACCESS_KEY}\n</code></pre> </li> <li> <p>Save the s3 route's hostname into a shell variable.</p> <pre><code>export s3_hostname=$(oc -n openshift-storage get route s3 -o=jsonpath='{.spec.host}')\n</code></pre> </li> <li> <p>Copy the Ingress CA bundle into a ConfigMap named <code>image-registry-s3-bundle</code>.</p> <pre><code>oc -n openshift-ingress extract secret/router-certs-default --confirm\noc -n openshift-config create configmap image-registry-s3-bundle --from-file=ca-bundle.crt=./tls.crt\n</code></pre> </li> <li> <p>Patch the <code>config.imageregistry/cluster</code>.</p> <pre><code>oc patch config.image/cluster -p '{\"spec\":{\"managementState\":\"Managed\",\"replicas\":2,\"storage\":{\"managementState\":\"Unmanaged\",\"s3\":{\"bucket\":'\\\"${bucket_name}\\\"',\"region\":\"us-east-1\",\"regionEndpoint\":'\\\"https://${s3_hostname}\\\"',\"virtualHostedStyle\":false,\"encrypt\":true,\"trustedCA\":{\"name\":\"image-registry-s3-bundle\"}}}}}' --type=merge\n</code></pre> </li> <li> <p>Check the image registry Pods.</p> <pre><code>oc -n openshift-image-registry get pods -l docker-registry=default\n</code></pre> Example Output<pre><code>NAME                              READY   STATUS    RESTARTS   AGE\nimage-registry-7b555754d6-5cjqg   1/1     Running   0          2m15s\nimage-registry-7b555754d6-k8hn7   1/1     Running   0          2m15s\n</code></pre> </li> <li> <p>Validate that S2I successfully pushes container images to the internal image registry.</p> <p>Ensure the <code>git</code> command is available on the bastion host.</p> <pre><code>sudo yum install -y git-core\n</code></pre> <p>Create a project.</p> <pre><code>oc new-project validate-s2i\n</code></pre> <p>Use S2I to create a <code>hello-world</code> application.</p> <pre><code>oc new-app \\\n    --name hello-world \\\n    https://github.com/RedHatTraining/DO280-apps \\\n    --context-dir hello-world-nginx\n</code></pre> <p>Follow the build logs.</p> <pre><code>oc logs -f buildconfig/hello-world\n</code></pre> Example Output<pre><code>#...\nWriting manifest to image destination\nSuccessfully pushed image-registry.openshift-image-registry.svc:5000/validate-s2i/hello-world@sha256:a6cabaa667cd38a5d90220faab76881b0cf24232709fefdc309bee9b31492cd4\nPush successful\n</code></pre> <p>Clean up.</p> <pre><code>oc delete project validate-s2i\n</code></pre> </li> </ol> <p>Congratulations the deployment is complete! You have created a platform for the client project team to work on, the application modernization journey can start.</p>"},{"location":"ocp-on-vmware/create-install-config/","title":"Creating the Install Config Asset","text":""},{"location":"ocp-on-vmware/create-install-config/#creating-the-install-config-asset","title":"Creating the Install Config Asset","text":"<p>The OpenShift installer uses a YAML configuration file, known as the install config asset, to set the parameters for the installation process.</p> <p>Collecting Install Config Asset Information</p> <p>During the provisioning process for the OCP Gymnasium, a YAML file named <code>vmware-ipi.yaml</code> was created in the <code>/home/admin</code> directory on the bastion host. This file contains essential information that will be used to create the install config asset.  This information can be found in other locations as well such as by examining the VMware environment from within vSphere.</p> Contents of /home/admin/vmware-ipi.yaml similar to the following<pre><code>---\nvsphere_username: itz-68a4902d8c33043254f6e2de@vsphere.local\nvsphere_password: -opi6hGKOnW5EE5\nvsphere_hostname: ocpgym-vc.techzone.ibm.local\nvsphere_datastore: 68a4902d8c33043254f6e2de-storage\nvsphere_cluster: ocp-gym\nvsphere_network: 68a4902d8c33043254f6e2de-segment\nvsphere_datacenter: IBMCloud\nvsphere_folder: /IBMCloud/vm/ocp-gym/reservations/68a4902d8c33043254f6e2de\nvsphere_resource_pool: /IBMCloud/host/ocp-gym/Resources/Cluster Resource Pool/Gym Member Resource Pool/68a4902d8c33043254f6e2de\nvsphere_api_vip: 192.168.252.3\nvsphere_ingress_vip: 192.168.252.4\nbase_domain: gym.lan\ncluster_name: ocpinstall\n</code></pre>"},{"location":"ocp-on-vmware/create-install-config/#run-the-wizard","title":"Run the Wizard","text":"<p>We will use the create <code>install-config</code> wizard to create the install config asset. You will need to check the provided template file for the values. Additionally, the template file has some parameters which are not requested by the wizard, you need to identify these from the template file.</p> <ol> <li> <p>Locate YOUR pull secret from Red Hat .  You can copy your pull secret from the Red Hat Hybrid Cloud Console web page.</p> </li> <li> <p>Launch the installation wizard.</p> <pre><code>openshift-install create install-config\n</code></pre> </li> <li> <p>Complete the installer survey.</p> <p>For this exercise, the cluster must be named ocpinstall; each participant uses their own instance of the OCP Gymnasium and each participant is completely isolated from the others, so this does not cause a problem.</p> Example<pre><code>? SSH Public Key /home/admin/.ssh/id_rsa.pub\n? Platform vsphere\n? vCenter ocpgymwdc-vc.techzone.ibm.local\n? Username gymuser-f6ckcc6o@techzone.ibm.local\n? Password [? for help] ********\nINFO Connecting to vCenter ocpgymwdc-vc.techzone.ibm.local\nINFO Defaulting to only available datacenter: IBMCloud\nINFO Defaulting to only available cluster: /IBMCloud/host/ocpgym-wdc\n? Default Datastore /IBMCloud/datastore/gym-50vmycg18b-f6ckcc6o-storage\n? Network gym-50vmycg18b-f6ckcc6o-segment\n? Virtual IP Address for API 192.168.252.3\n? Virtual IP Address for Ingress 192.168.252.4\n? Base Domain gym.lan\n? Cluster Name ocpinstall\n? Pull Secret [? for help] *********************************************\n\nINFO Install-Config created in: .\n</code></pre> </li> </ol>"},{"location":"ocp-on-vmware/create-install-config/#update-the-install-config-asset","title":"Update the install config asset","text":"<ol> <li> <p>In the editor of your choice, open the <code>install-config.yaml</code> created by the wizard and review the parameters created with the openshift-install.You will be required to change / add some of the settings within this file.  Use the highlighted sections in the below example to make the required changes.</p> Reference install-config.yaml<pre><code>additionalTrustBundlePolicy: Proxyonly\napiVersion: v1\nbaseDomain: gym.lan\ncompute:\n- architecture: amd64\n  hyperthreading: Enabled\n  name: worker\n  platform:\n    vsphere:\n      osDisk:\n        diskSizeGB: 200 \n      cpus: 32\n      memoryMB: 65536\n  replicas: 3\ncontrolPlane:\n  architecture: amd64\n  hyperthreading: Enabled\n  name: master\n  platform:\n    vsphere:\n      osDisk:\n        diskSizeGB: 200 \n      cpus: 32\n      memoryMB: 65536\n  replicas: 3\nmetadata:\n  creationTimestamp: null\n  name: ocpinstall\nnetworking:\n  clusterNetwork:\n  - cidr: 10.128.0.0/14\n    hostPrefix: 23\n  machineNetwork:\n  - cidr: 192.168.252.0/27\n  networkType: OVNKubernetes\n  serviceNetwork:\n  - 172.30.0.0/16\nplatform:\n  vsphere:\n    apiVIPs:\n    - 192.168.252.3\n    cluster: ocp-gym\n    datacenter: IBMCloud\n    defaultDatastore: 68a4902d8c33043254f6e2de-storage\n    ingressVIPs:\n    - 192.168.252.4\n    network: 68a4902d8c33043254f6e2de-segment\n    password: -opi6hGKOnW5EE5\n    username: itz-68a4902d8c33043254f6e2de@vsphere.local\n    vCenter: ocpgym-vc.techzone.ibm.local\n    folder: /IBMCloud/vm/ocp-gym/reservations/68a4902d8c33043254f6e2de\n    resourcePool: /IBMCloud/host/ocp-gym/Resources/Cluster Resource Pool/Gym Member Resource Pool/68a4902d8c33043254f6e2de\npublish: External\npullSecret: |\n  {...........}\nsshKey: |\n  ssh-rsa ...... dbailey@Dons-MBP\n</code></pre> </li> </ol> <p>Which parameters need changes?</p> <p>Which parameters does the template have that were missing from the survey? Which values does the template have pre-populated from the survey?</p> <p>At a minimum, the platform and network sections require modification and some of this information is unique to YOUR environment.</p> <pre><code>#...\ncompute:\n- architecture: amd64\n  name: worker\n  platform:\n    vsphere:\n      osDisk:\n        diskSizeGB: 200\n      cpus: 32\n      memoryMB: 65536\n  replicas: 3\n\nnetworking:\n  #...\nmachineNetwork:\n  - cidr: 192.168.252.0/27\n  #...\n#...\n</code></pre> <p>Hold on!</p> <p>Where are the infrastructure nodes dear instructors? Good catch! Infrastructure nodes currently can not be specified in the install config asset, we will ensure they are created in the next section.</p>"},{"location":"ocp-on-vmware/install-cluster/","title":"Customize and Install the OpenShift Cluster","text":""},{"location":"ocp-on-vmware/install-cluster/#customize-and-install-the-openshift-cluster","title":"Customize and Install the OpenShift Cluster","text":"<ol> <li> <p>Create the installation directory (by convention named after the cluster).</p> <pre><code>mkdir ocpinstall\n</code></pre> </li> <li> <p>Copy the Install Config asset into the installation directory.</p> <pre><code>cp install-config.yaml ocpinstall\n</code></pre> </li> <li> <p>Create manifests.</p> <pre><code>openshift-install create manifests --dir ocpinstall\n</code></pre> </li> <li> <p>Copy the worker MachineSet manifest to create an Infrastructure MachineSet</p> <pre><code>cp ocpinstall/openshift/99_openshift-cluster-api_worker-machineset-0.yaml \\\nocpinstall/openshift/99_openshift-cluster-api_infra-machineset-0.yaml\n</code></pre> </li> <li> <p>Edit <code>ocpinstall/openshift/99_openshift-cluster-api_infra-machineset-0.yaml</code> and make the following changes:</p> <ul> <li>Add labels to the nodes, within the <code>spec.template.spec.metadata.labels</code> section:<ul> <li><code>cluster.ocs.openshift.io/openshift-storage: ''</code></li> <li><code>node-role.kubernetes.io/infra: ''</code></li> </ul> </li> <li>Add taint <code>node.ocs.openshift.io/storage</code> with value of <code>true</code> and effect <code>NoSchedule</code> to the nodes.</li> <li>Change all values with postfix <code>-worker-0</code> to <code>-infra-0</code>.</li> <li>Change the value of <code>machine.openshift.io/cluster-api-machine-role</code> and <code>machine.openshift.io/cluster-api-machine-type</code> to <code>infra</code>.</li> <li>Change <code>memoryMiB</code> to <code>65536</code>.</li> <li>Change <code>numCPUs</code> to <code>16</code>.</li> <li>Change <code>numCoresPerSocket</code> to <code>2</code>.</li> <li>Change <code>replicas</code> to <code>3</code></li> </ul> Reference infrastructure MachineSet<pre><code>apiVersion: machine.openshift.io/v1beta1\nkind: MachineSet\nmetadata:\n  creationTimestamp: null\n  labels:\n    machine.openshift.io/cluster-api-cluster: ocpinstall-jxncd\n  name: ocpinstall-jxncd-infra-0\n  namespace: openshift-machine-api\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      machine.openshift.io/cluster-api-cluster: ocpinstall-jxncd\n      machine.openshift.io/cluster-api-machineset: ocpinstall-jxncd-infra-0\n  template:\n    metadata:\n      labels:\n        machine.openshift.io/cluster-api-cluster: ocpinstall-jxncd\n        machine.openshift.io/cluster-api-machine-role: infra\n        machine.openshift.io/cluster-api-machine-type: infra\n        machine.openshift.io/cluster-api-machineset: ocpinstall-jxncd-infra-0\n    spec:\n      lifecycleHooks: {}\n      metadata: {}\n      providerSpec:\n        value:\n          apiVersion: machine.openshift.io/v1beta1\n          credentialsSecret:\n            name: vsphere-cloud-credentials\n          diskGiB: 200\n          kind: VSphereMachineProviderSpec\n          memoryMiB: 65536\n          metadata:\n        creationTimestamp: null\n          network:\n            devices:\n            - networkName: 68a787348c33043254f6e8b6-segment\n          numCPUs: 32\n          numCoresPerSocket: 4\n          snapshot: \"\"\n          template: ocpinstall-jxncd-rhcos-generated-region-generated-zone\n          userDataSecret:\n            name: worker-user-data\n          workspace:\n            datacenter: IBMCloud\n            datastore: /IBMCloud/datastore/68a787348c33043254f6e8b6-storage\n            folder: /IBMCloud/vm/ocp-gym/reservations/68a787348c33043254f6e8b6\n            resourcePool: /IBMCloud/host/ocp-gym/Resources/Cluster Resource Pool/Gym\n              Member Resource Pool/68a787348c33043254f6e8b6\n            server: ocpgym-vc.techzone.ibm.local\nstatus:\n  replicas: 0\n</code></pre> </li> <li> <p>Create the cluster.</p> <pre><code>openshift-install create cluster --dir ocpinstall --log-level debug\n</code></pre> <p>Information</p> <p>We are suggesting you use --log-level debug in order to gain some experience with what is actually happening during the IPI installation process. The default log level is info; which is more typically used once you have experience with the process. A log file named .openshift-install.log is kept in the install directory .</p> <p>Wait for the installation to complete.</p> Example output<pre><code>#...\nINFO All cluster operators have completed progressing\nINFO Checking to see if there is a route at openshift-console/console...\nDEBUG Route found in openshift-console namespace: console\nDEBUG OpenShift console route is admitted\nINFO Install complete!\nINFO To access the cluster as the system:admin user when using 'oc', run 'export KUBECONFIG=/home/admin/ocpinstall/auth/kubeconfig'\nINFO Access the OpenShift web-console here: https://console-openshift-console.apps.ocpinstall.gym.lan\nINFO Login to the console with user: \"kubeadmin\", and password: \"AaaJH-tkqHF-VWDvh-cPku2\"\nDEBUG Time elapsed per stage:\nDEBUG               pre-bootstrap: 1m19s\nDEBUG                   bootstrap: 57s\nDEBUG                      master: 2m45s\nDEBUG          Bootstrap Complete: 16m45s\nDEBUG                         API: 54s\nDEBUG           Bootstrap Destroy: 36s\nDEBUG Cluster Operators Available: 9m47s\nDEBUG    Cluster Operators Stable: 5m32s\nINFO Time elapsed: 37m48s\n</code></pre> <p>Tip</p> <ul> <li>When using WireGuard ensure your laptop does not go to sleep and that the SSH session remains open, otherwise this can interrupt the installer.</li> <li>Consider installing and using tmux to minimize network disruptions and timeouts.</li> </ul> </li> </ol>"},{"location":"ocp-on-vmware/install-cluster/#monitor-progress-during-installation","title":"Monitor Progress During Installation","text":""},{"location":"ocp-on-vmware/install-cluster/#monitor-using-the-vsphere-console","title":"Monitor using the vSphere console","text":"<ol> <li> <p>Log into the vSphere console.</p> <p>Use the vCenter Console URL, username and password from your reservation and log into the vSphere console.</p> </li> <li> <p>Select the VMs and Templates section on the left-hand side of vSphere:</p> <p></p> <p>Expand IBMCloud &gt; ocp-gym.</p> <p>You will then be able to find your user's folder, where the virtual machines will appear during the installation.</p> <p></p> <p>Note</p> <p>You can see that there are already two virtual machines present in your folder! The first one is the VM that is your bastion, which is the one you're using now. The other is a router VM that is hosting the routing and DNS components that are needed for your cluster.</p> <p>During the installation, on the vSphere console, you will be able to see the virtual machines for your OpenShift cluster being created.</p> <p></p> <p>The OpenShift installer will first create the bootstrap node and the master nodes and power them on.</p> <p>Clicking on one of the VMs will display further information on the right-hand side of the console.</p> <p></p> <p>Once these are running and configured, the OpenShift installer will create, start and configure the worker nodes. Typically the workers are created 10-15 minutes after the masters.</p> <p>The workers will appear in the same section as the master nodes and the bootstrap node. </p> </li> </ol>"},{"location":"ocp-on-vmware/install-cluster/#monitor-using-the-openshift-cli","title":"Monitor using the OpenShift CLI","text":"<p>You can start following the installation progress with the CLI after the API server is available.</p> <ol> <li> <p>Open a new Terminal.</p> </li> <li> <p>Set up <code>system:admin</code> access for the cluster.</p> <pre><code>export KUBECONFIG=${HOME}/ocpinstall/auth/kubeconfig\n</code></pre> </li> <li> <p>Issue the following watch command.</p> <pre><code>watch oc get nodes,clusteroperators,clusterversion\n</code></pre> <p>Tip</p> <ul> <li>During the installation it is normal for the status field to occasionally show errors.</li> <li>The authentication operator is usually the last one to come up, and it may take 15-20 minutes after all the other operators are available, so don't panic. The console operator is usually next to last.</li> </ul> </li> </ol>"},{"location":"ocp-on-vmware/install-cluster/#verify-cluster-health","title":"Verify Cluster Health","text":""},{"location":"ocp-on-vmware/install-cluster/#access-the-openshift-web-console","title":"Access the OpenShift Web console","text":"<ol> <li> <p>Open https://console-openshift-console.apps.ocpinstall.gym.lan in a browser and accept the self-signed certificates.</p> </li> <li> <p>Log in using the credentials (available from the OpenShift installer output).</p> <p></p> <p>The OpenShift Web console will then display:</p> <p></p> </li> <li> <p>Click on the Control Plane and Operators in the Status window.</p> <p></p> </li> </ol>"},{"location":"ocp-on-vmware/install-cluster/#access-the-cluster-using-the-openshift-cli","title":"Access the Cluster Using the OpenShift CLI","text":"<ol> <li> <p>Log in as user <code>kubeadmin</code>.</p> <pre><code>oc login -u kubeadmin https://api.ocpinstall.gym.lan:6443 --insecure-skip-tls-verify=true\n</code></pre> Example Output<pre><code>WARNING: Using insecure TLS client config. Setting this option is not supported!\n\nConsole URL: https://api.ocpinstall.gym.lan:6443/console\nAuthentication required for https://api.ocpinstall.gym.lan:6443 (openshift)\nUsername: kubeadmin\nPassword: \nLogin successful.\n\nYou have access to 70 projects, the list has been suppressed. You can list all projects with 'oc projects'\n\nUsing project \"default\".\n</code></pre> </li> <li> <p>List the nodes.</p> <pre><code>oc get nodes\n</code></pre> Example Output<pre><code>NAME                              STATUS   ROLES                  AGE   VERSION\nocpinstall-ntfsr-infra-0-5bfqj    Ready    infra,worker           18m   v1.28.6+6216ea1\nocpinstall-ntfsr-infra-0-cscfp    Ready    infra,worker           18m   v1.28.6+6216ea1\nocpinstall-ntfsr-infra-0-p42ml    Ready    infra,worker           18m   v1.28.6+6216ea1\nocpinstall-ntfsr-master-0         Ready    control-plane,master   32m   v1.28.6+6216ea1\nocpinstall-ntfsr-master-1         Ready    control-plane,master   32m   v1.28.6+6216ea1\nocpinstall-ntfsr-master-2         Ready    control-plane,master   32m   v1.28.6+6216ea1\nocpinstall-ntfsr-worker-0-mzsp6   Ready    worker                 18m   v1.28.6+6216ea1\nocpinstall-ntfsr-worker-0-n747f   Ready    worker                 18m   v1.28.6+6216ea1\nocpinstall-ntfsr-worker-0-hsh87   Ready    worker                 18m   v1.28.6+6216ea1\n</code></pre> </li> <li> <p>Display CPU and memory usage of each node.</p> <pre><code>oc adm top nodes\n</code></pre> Example Output<pre><code>NAME                              CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%\nocpinstall-ntfsr-infra-0-5bfqj    112m         0%     2071Mi          3%\nocpinstall-ntfsr-infra-0-cscfp    109m         0%     2091Mi          3%\nocpinstall-ntfsr-infra-0-p42ml    112m         0%     2078Mi          3%\nocpinstall-ntfsr-master-0         511m         14%    6074Mi          40%\nocpinstall-ntfsr-master-1         527m         15%    6393Mi          42%\nocpinstall-ntfsr-master-2         396m         11%    4457Mi          29%\nocpinstall-ntfsr-worker-0-mzsp6   283m         3%     4301Mi          28%\nocpinstall-ntfsr-worker-0-n747f   408m         5%     4142Mi          27%\nocpinstall-ntfsr-worker-0-hsh87   408m         5%     4142Mi          27% \n</code></pre> </li> <li> <p>List the cluster operators.</p> <pre><code>oc get clusteroperators\n</code></pre> Example Output<pre><code>NAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE\nauthentication                             4.17.3    True        False         False      10m     \nbaremetal                                  4.17.3    True        False         False      61m     \ncloud-controller-manager                   4.17.3    True        False         False      69m     \ncloud-credential                           4.17.3    True        False         False      92m     \ncluster-autoscaler                         4.17.3    True        False         False      61m     \nconfig-operator                            4.17.3    True        False         False      63m     \nconsole                                    4.17.3    True        False         False      15m     \ncontrol-plane-machine-set                  4.17.3    True        False         False      61m     \ncsi-snapshot-controller                    4.17.3    True        False         False      63m     \ndns                                        4.17.3    True        False         False      59m     \netcd                                       4.17.3    True        False         False      25m     \nimage-registry                             4.17.3    True        False         False      52m     \ningress                                    4.17.3    True        False         False      18m     \ninsights                                   4.17.3    True        False         False      55m     \nkube-apiserver                             4.17.3    True        False         False      56m     \nkube-controller-manager                    4.17.3    True        False         False      56m     \nkube-scheduler                             4.17.3    True        False         False      56m     \nkube-storage-version-migrator              4.17.3    True        False         False      63m     \nmachine-api                                4.17.3    True        False         False      18m     \nmachine-approver                           4.17.3    True        False         False      61m     \nmachine-config                             4.17.3    True        False         False      25m     \nmarketplace                                4.17.3    True        False         False      61m     \nmonitoring                                 4.17.3    True        False         False      11m     \nnetwork                                    4.17.3    True        False         False      64m     \nnode-tuning                                4.17.3    True        False         False      16m     \nopenshift-apiserver                        4.17.3    True        False         False      25m     \nopenshift-controller-manager               4.17.3    True        False         False      56m     \nopenshift-samples                          4.17.3    True        False         False      56m     \noperator-lifecycle-manager                 4.17.3    True        False         False      61m     \noperator-lifecycle-manager-catalog         4.17.3    True        False         False      61m     \noperator-lifecycle-manager-packageserver   4.17.3    True        False         False      55m     \nservice-ca                                 4.17.3    True        False         False      63m     \nstorage                                    4.17.3    True        False         False      25m \n</code></pre> </li> </ol>"},{"location":"ocp-on-vmware/install-cluster/#access-cluster-nodes","title":"Access Cluster Nodes","text":"<p>You can <code>ssh</code> into the worker and master nodes once the virtual machines have been created. The user to SSH with is named <code>core</code>. The SSH key you use will be the private half of the key pair of which the public half you provided in the <code>install-config.yaml</code> file.</p> <ol> <li> <p>Get the external IP addresses of the nodes.</p> <pre><code>oc get nodes -o custom-columns=NAME:.metadata.name,EXTERNALIP:.status.addresses[0].address\n</code></pre> Example Output<pre><code>NAME                              EXTERNALIP\nocpinstall-ntfsr-infra-0-5bfqj    192.168.252.139\nocpinstall-ntfsr-infra-0-cscfp    192.168.252.135\nocpinstall-ntfsr-infra-0-p42ml    192.168.252.137\nocpinstall-ntfsr-master-0         192.168.252.132\nocpinstall-ntfsr-master-1         192.168.252.134\nocpinstall-ntfsr-master-2         192.168.252.133\nocpinstall-ntfsr-worker-0-mzsp6   192.168.252.136\nocpinstall-ntfsr-worker-0-n747f   192.168.252.138\nocpinstall-ntfsr-worker-0-hsh87   192.168.252.138 \n</code></pre> </li> <li> <p>Log in to one of the nodes.</p> <pre><code>ssh -i ~/.ssh/id_rsa core@192.168.252.132\n</code></pre> Example Output<pre><code>The authenticity of host '192.168.252.132 (192.168.252.132)' can't be established.\nED25519 key fingerprint is SHA256:39TmjFLNRRmncEWcvEuLV75DTuFVPWQI0G5p0BHTmKA.\nThis key is not known by any other names\nAre you sure you want to continue connecting (yes/no/[fingerprint])? yes\nWarning: Permanently added '192.168.252.132' (ED25519) to the list of known hosts.\nRed Hat Enterprise Linux CoreOS 417.94.202410211619-0\n  Part of OpenShift 4.17, RHCOS is a Kubernetes native operating system\n  managed by the Machine Config Operator (`clusteroperator/machine-config`).\n\nWARNING: Direct SSH access to machines is not recommended; instead,\nmake configuration changes via `machineconfig` objects:\n  https://docs.openshift.com/container-platform/4.17/architecture/architecture-rhcos.html\n\n---\n</code></pre> </li> <li> <p>Check if the kubelet is active.</p> <pre><code>systemctl is-active kubelet\n</code></pre> Example Output<pre><code>active\n</code></pre> </li> <li> <p>Log out.</p> <pre><code>exit\n</code></pre> </li> </ol>"},{"location":"ocp-on-vmware/install-data-foundation/","title":"Install OpenShift Data Foundation","text":""},{"location":"ocp-on-vmware/install-data-foundation/#install-openshift-data-foundation","title":"Install OpenShift Data Foundation","text":"<p>The OpenShift web console provides an easy way to install the ODF operator and create a storage cluster.</p> <ol> <li> <p>Open the OpenShift web console and log in as user <code>admin</code>.</p> </li> <li> <p>Click Operators &gt; OperatorHub.</p> </li> <li> <p>Scroll or type <code>odf</code> into the Filter by keyword box to find the OpenShift Data Foundation Operator.</p> </li> <li> <p>Select the OpenShift Data Foundation operator, click Install, leave the values at their default, and click Install.</p> </li> <li> <p>After the operator installed a pop-up appears stating a new version of the web console is available, ensure you click Refresh web console.</p> </li> <li> <p>Click Create StorageSystem.</p> </li> <li> <p>In the Backing storage page, Check Use Ceph RBD as the default StorageClass and leave the remaining values at their default and click Next.</p> </li> <li> <p>Set the Requested capacity to 0.5 TiB, notice that the infrastructure nodes have been preselected, and click Next.</p> </li> <li> <p>In the next two screens leave the values at their default and click Next.</p> </li> <li> <p>Review the settings and click Create StorageSystem.</p> </li> <li> <p>Detailed progress monitoring can be done on the command line.</p> <pre><code>oc login -u admin\n</code></pre> <pre><code>watch oc -n openshift-storage get storagecluster,pods\n</code></pre> Wait until the StorageCluster reaches phase Ready<pre><code>NAME                                                 AGE     PHASE   EXTERNAL   CREATED AT             VERSION\nstoragecluster.ocs.openshift.io/ocs-storagecluster   6m37s   Ready              2024-11-18T12:42:21Z   4.17.3\n\nNAME                                                                  READY   STATUS      RESTARTS   AGE\npod/csi-addons-controller-manager-79589c64b9-7nrnx                    2/2     Running     0          7m22s\npod/csi-cephfsplugin-25vdt                                            2/2     Running     0          6m36s\npod/csi-cephfsplugin-599v5                                            2/2     Running     0          6m36s\npod/csi-cephfsplugin-crkmb                                            2/2     Running     0          6m36s\npod/csi-cephfsplugin-gzsfw                                            2/2     Running     0          6m36s\npod/csi-cephfsplugin-provisioner-5d549d97d5-5z8ct                     6/6     Running     0          6m36s\npod/csi-cephfsplugin-provisioner-5d549d97d5-qgmkd                     6/6     Running     0          6m36s\npod/csi-cephfsplugin-rbm79                                            2/2     Running     0          6m36s\npod/csi-rbdplugin-9htkb                                               3/3     Running     0          6m36s\npod/csi-rbdplugin-b5g7l                                               3/3     Running     0          6m36s\npod/csi-rbdplugin-j52sh                                               3/3     Running     0          6m36s\npod/csi-rbdplugin-provisioner-5869c4d666-2mgnq                        6/6     Running     0          6m36s\npod/csi-rbdplugin-provisioner-5869c4d666-dz85n                        6/6     Running     0          6m36s\npod/csi-rbdplugin-zcq6q                                               3/3     Running     0          6m36s\npod/csi-rbdplugin-zwcdd                                               3/3     Running     0          6m36s\npod/noobaa-core-0                                                     1/1     Running     0          2m4s\npod/noobaa-db-pg-0                                                    1/1     Running     0          3m40s\npod/noobaa-endpoint-7684858565-8h2d4                                  1/1     Running     0          2m35s\npod/noobaa-operator-785bf6967-7jdb8                                   1/1     Running     0          8m5s\npod/ocs-metrics-exporter-6db44f5fc-88lwn                              1/1     Running     0          3m39s\npod/ocs-operator-bb969496f-5t9c7                                      1/1     Running     0          7m33s\npod/odf-console-6d7f89bfcd-cvg2l                                      1/1     Running     0          7m58s\npod/odf-operator-controller-manager-6ccd96449c-m94mc                  2/2     Running     0          7m58s\npod/rook-ceph-crashcollector-ocpinstall-ntfsr-infra-0-5bfqj-75vd7ws   1/1     Running     0          5m4s\npod/rook-ceph-crashcollector-ocpinstall-ntfsr-infra-0-cscfp-58k872l   1/1     Running     0          4m49s\npod/rook-ceph-crashcollector-ocpinstall-ntfsr-infra-0-p42ml-64tlpnb   1/1     Running     0          5m3s\npod/rook-ceph-exporter-ocpinstall-ntfsr-infra-0-5bfqj-75fc99752b872   1/1     Running     0          5m4s\npod/rook-ceph-exporter-ocpinstall-ntfsr-infra-0-cscfp-74c7b944fdvqb   1/1     Running     0          4m49s\npod/rook-ceph-exporter-ocpinstall-ntfsr-infra-0-p42ml-56c9847b27pw8   1/1     Running     0          5m3s\npod/rook-ceph-mds-ocs-storagecluster-cephfilesystem-a-698ff486cf478   2/2     Running     0          4m\npod/rook-ceph-mds-ocs-storagecluster-cephfilesystem-b-6dfc6bf4pprfw   2/2     Running     0          3m57s\npod/rook-ceph-mgr-a-76cfdff9c5-xf62q                                  3/3     Running     0          5m1s\npod/rook-ceph-mgr-b-6844cbb666-khcpw                                  3/3     Running     0          5m\npod/rook-ceph-mon-a-7dbf8d8f47-thx42                                  2/2     Running     0          6m6s\npod/rook-ceph-mon-b-7bfb497f67-j5rsn                                  2/2     Running     0          5m34s\npod/rook-ceph-mon-c-779c7c4697-b2lx7                                  2/2     Running     0          5m19s\npod/rook-ceph-operator-58d57dfcc9-h7wc6                               1/1     Running     0          6m35s\npod/rook-ceph-osd-0-85f667c594-q6ghs                                  2/2     Running     0          4m23s\npod/rook-ceph-osd-1-6dbb47996d-nqmbk                                  2/2     Running     0          4m21s\npod/rook-ceph-osd-2-685cd49949-v9r6r                                  2/2     Running     0          4m18s\npod/rook-ceph-osd-prepare-09c4822102c3025ef86f32f0690ab78c-p9bw7      0/1     Completed   0          4m38s\npod/rook-ceph-osd-prepare-0b70eb979aae8c4e011459b9b6e8e76a-vpw8q      0/1     Completed   0          4m39s\npod/rook-ceph-osd-prepare-40250bca7f0ab640af4d3aadd9aafcac-jz7ss      0/1     Completed   0          4m39s\npod/rook-ceph-rgw-ocs-storagecluster-cephobjectstore-a-7976995qrs8h   2/2     Running     0          3m43s\npod/ux-backend-server-84b877b699-mr9wg                                2/2     Running     0          7m32s\n</code></pre> </li> <li> <p>Verify that the OpenShift Data Foundation storage classes are available.</p> <pre><code>oc get sc\n</code></pre> Example Output<pre><code>NAME                          PROVISIONER                             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE\nocs-storagecluster-ceph-rbd   openshift-storage.rbd.csi.ceph.com      Delete          Immediate              true                   106s\nocs-storagecluster-ceph-rgw   openshift-storage.ceph.rook.io/bucket   Delete          Immediate              false                  4m55s\nocs-storagecluster-cephfs     openshift-storage.cephfs.csi.ceph.com   Delete          Immediate              true                   106s\nopenshift-storage.noobaa.io   openshift-storage.noobaa.io/obc         Delete          Immediate              false                  48s\nthin-csi (default)            csi.vsphere.vmware.com                  Delete          WaitForFirstConsumer   true                   43m\n</code></pre> </li> <li> <p>Smoke test persistent volume provisioning.</p> <p>Create a PersistentVolumeClaim (PVC) for ODF block storage.</p> <pre><code>oc create -f - &lt;&lt;EOF\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: test\n  namespace: default\nspec:\n  accessModes:\n    - ReadWriteOnce\n  storageClassName: ocs-storagecluster-ceph-rbd\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 1Gi\nEOF\n</code></pre> <p>Check if the PVC is bound to a PersistentVolume.</p> <pre><code>oc get pvc test\n</code></pre> Example Output<pre><code>NAME   STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS                  AGE\ntest   Bound    pvc-419958c8-1126-4601-ba3f-4f2c14bfb88c   1Gi        RWO            ocs-storagecluster-ceph-rbd   6s\n</code></pre> <p>Delete the test PVC.</p> <pre><code>oc delete pvc test\n</code></pre> </li> </ol>"},{"location":"ocp-on-vmware/prepare/","title":"Prepare the Installation","text":""},{"location":"ocp-on-vmware/prepare/#prepare-the-installation","title":"Prepare the Installation","text":"<p>Before proceeding with the installation, it is essential to complete some critical setup tasks on the bastion host to ensure a smooth and successful deployment. To ensure a smooth installation process, it is essential that we take care of a few crucial preparations beforehand. Specifically, we need to set up our environment on the bastion host before we begin the installation.</p>"},{"location":"ocp-on-vmware/prepare/#verify-dns-records","title":"Verify DNS records","text":"<p>To guarantee seamless operation of OpenShift Container Platform (OCP), a fully functional DNS server must be present in your environment. Fortunately, our OCP Gymnasium environment has a pre-configured DNS server with populated entries for the virtual IP addresses of both the API and Ingress components. To confirm that DNS is functioning correctly:</p> <ol> <li> <p>Log in to the bastion host using a terminal.</p> </li> <li> <p>Verify name resolution by checking both the forward and reverse lookups of the API virtual IP address:</p> <pre><code>dig +short api.ocpinstall.gym.lan -x 192.168.252.3\n</code></pre> Expected output<pre><code>192.168.252.3\napi.ocpinstall.gym.lan\n</code></pre> </li> <li> <p>Verify the name lookup for the Ingress virtual IP address:</p> <pre><code>dig +short foo.apps.ocpinstall.gym.lan\n</code></pre> Expected output<pre><code>192.168.252.4\n</code></pre> </li> </ol>"},{"location":"ocp-on-vmware/prepare/#enabling-vcenter-ca-certificate-trust","title":"Enabling vCenter CA Certificate Trust","text":"<p>Before proceeding with the installation of an OpenShift Container Platform (OCP) cluster, you need to ensure that your system trusts the root Certificate Authority (CA) of your vCenter. This is crucial because the OCP installation program relies on access to the vCenter's API.</p> <p>To complete this task, follow these steps:</p> <ol> <li> <p>Set the <code>VCENTER_HOSTNAME</code> environment variable.</p> <pre><code>export VCENTER_HOSTNAME=\"your_vsphere_hostname\"\n</code></pre> <p>Extract <code>your_vsphere_hostname</code> from the environment details (<code>vCenter Console URL</code>).</p> </li> <li> <p>Download the vCenter root CA certificates.</p> <p></p><pre><code>curl -kL https://${VCENTER_HOSTNAME}/certs/download.zip -o download.zip\n</code></pre> Example with your hostname<pre><code>curl -kL https://ocpgym-vc.techzone.ibm.local/certs/download.zip -o download.zip\n</code></pre><p></p> </li> <li> <p>Extract the certificates.</p> <pre><code>unzip download.zip\n</code></pre> </li> <li> <p>Add the certificates to the CA trust store.</p> <pre><code>sudo cp -R certs/lin/* /etc/pki/ca-trust/source/anchors/\n</code></pre> </li> <li> <p>Update the CA trust store.</p> <pre><code>sudo update-ca-trust extract\n</code></pre> </li> <li> <p>Clean up.</p> <pre><code>rm -fr download.zip certs/\n</code></pre> </li> </ol>"},{"location":"ocp-on-vmware/prepare/#installing-the-openshift-installer","title":"Installing the OpenShift Installer","text":"<p>The version of the OpenShift installer you download will determine the initial version of your OpenShift cluster.</p> <p>To complete this task, follow these steps:</p> <ol> <li> <p>Set the <code>OI_VERSION</code> environment variable</p> <pre><code>export OI_VERSION=\"4.18.22\"\n</code></pre> </li> <li> <p>Download the OpenShift Installer</p> <pre><code>curl -Lo openshift-install-linux.tar.gz \\\nhttps://mirror.openshift.com/pub/openshift-v4/clients/ocp/${OI_VERSION}/openshift-install-linux.tar.gz\n</code></pre> </li> <li> <p>Extract the installer</p> <pre><code>tar xf openshift-install-linux.tar.gz openshift-install\n</code></pre> </li> <li> <p>Install the OpenShift Installer</p> <pre><code>sudo install openshift-install /usr/local/bin\n</code></pre> </li> <li> <p>Verify the installer is executable</p> <pre><code>openshift-install version\n</code></pre> Output similar to the following<pre><code>openshift-install 4.18.22\nbuilt from commit 9094202399a3ac62bbdc22f755c6995545f3089e\nrelease image quay.io/openshift-release-dev/ocp-release@sha256:16078b671c7f5490a2136f2cd9a694d48bb38af1280ef9e2ae9ce28af075cca5\nrelease architecture amd64\n</code></pre> </li> <li> <p>Clean up</p> <pre><code>rm openshift-install-linux.tar.gz openshift-install\n</code></pre> </li> </ol>"},{"location":"ocp-on-vmware/prepare/#installing-the-openshift-cli","title":"Installing the OpenShift CLI","text":"<p>The OpenShift CLI is a crucial tool for managing your cluster. To ensure you have the latest version, follow these steps:</p> <ol> <li> <p>Set the <code>OC_VERSION</code> environment variable</p> <pre><code>export OC_VERSION=\"4.18.22\"\n</code></pre> </li> <li> <p>Download the OpenShift CLI</p> <pre><code>curl -Lo openshift-client-linux.tar.gz \\\nhttps://mirror.openshift.com/pub/openshift-v4/clients/ocp/${OC_VERSION}/openshift-client-linux-amd64-rhel8.tar.gz\n</code></pre> </li> <li> <p>Extract the CLI.</p> <pre><code>tar xf openshift-client-linux.tar.gz oc\n</code></pre> </li> <li> <p>Install the OpenShift CLI</p> <pre><code>sudo install oc /usr/local/bin\n</code></pre> </li> <li> <p>Verify the CLI is executable.</p> <pre><code>oc version\n</code></pre> Output similar to the following<pre><code>Client Version: 4.18.22\nKustomize Version: v5.4.2\n</code></pre> </li> <li> <p>Clean up.</p> <pre><code>rm openshift-client-linux.tar.gz oc\n</code></pre> </li> </ol>"},{"location":"ocp-on-vmware/prepare/#generating-a-key-pair-for-cluster-node-ssh-access","title":"Generating a Key Pair for Cluster Node SSH Access","text":"<p>During an OpenShift Container Platform installation, you can provide an SSH public key to the installation program. The key is passed to the Red Hat Enterprise Linux CoreOS (RHCOS) nodes through their Ignition config files and is used to authenticate SSH access to the nodes. After the key is passed to the nodes, you can use the key pair to SSH in to the RHCOS nodes as the user core.</p> <p>If you want to SSH in to your cluster nodes to perform installation debugging or disaster recovery, you must provide the SSH public key during the installation process. The openshift-install gather command also requires the SSH public key to be in place on the cluster nodes.</p> <pre><code>ssh-keygen -t rsa -N '' -f ~/.ssh/id_rsa\n</code></pre> Output similar to the following<pre><code>Generating public/private ed25519 key pair.\nYour identification has been saved in /home/admin/.ssh/id_rsa.\nYour public key has been saved in /home/admin/.ssh/id_rsa.pub.\nThe key fingerprint is:\nSHA256:sOpiEXVXB5TVc0jCUVlW89qcuv/Gb9DhUVs3yf2BAc8 admin@bastion\nThe key's randomart image is:\n+--[ED25519 256]--+\n|         o+****=B|\n|    . . . ..+o=*O|\n|   . ...     E oX|\n|  .    o       Bo|\n|   .  . S     oo=|\n|  .  .        o..|\n|   ..        . o |\n|  o.          . +|\n| . ..        ..+=|\n+----[SHA256]-----+\n</code></pre>"},{"location":"ocp-on-vmware/update-global-pull-secret/","title":"Add IBM Software Entitlement Key","text":""},{"location":"ocp-on-vmware/update-global-pull-secret/#add-ibm-software-entitlement-key","title":"Add IBM Software Entitlement Key","text":"<p>This procedure will update the global pull secret for the OpenShift cluster to allow for IBM Software Titles to be installed on the cluster. By updating the global pull secret IBM Software can be installed in all namespaces in the OpenShift cluster.</p> <ol> <li> <p>Retrieve IBM Software Entitlement Key</p> <p>IBM Software Entitlement Key</p> </li> <li> <p>Save the contents of the key in an environment variable</p> <pre><code>export IBM_KEY='**********************************'\n</code></pre> </li> <li> <p>Verify the entitlement key</p> <pre><code>podman login cp.icr.io --username cp --password $IBM_KEY\n</code></pre> Example Output<pre><code>Login Succeeded!\n</code></pre> </li> <li> <p>Extract the current global pull secret.</p> <pre><code>oc extract secret/pull-secret -n openshift-config --keys=.dockerconfigjson --to=. --confirm\n</code></pre> <p>Note</p> <p>To display the current contents of the file .dockerconfigjson use <code>python3 -m json.tool .dockerconfigjson</code></p> </li> <li> <p>Update the JSON file <code>.dockerconfigjson</code> with the icr.io credentials used in Step 3.</p> <pre><code>oc registry login --registry=\"cp.icr.io\" --auth-basic=\"cp:$IBM_KEY\" --to=.dockerconfigjson\n</code></pre> Example Output<pre><code>Saved credentials for cp.icr.io into .dockerconfigjson\n</code></pre> </li> <li> <p>Update the global pull secret to used the new contents of the <code>.dockerconfigjson</code> file.  This will cause all nodes in the cluster to reload configuation details to update this secret.  It could take time for all nodes to get the updated configuration.</p> <pre><code>oc set data secret/pull-secret -n openshift-config --from-file=.dockerconfigjson\n</code></pre> Example Output<pre><code>secret/pull-secret data updated\n</code></pre> </li> </ol>"},{"location":"ocp-on-vmware-airgapped/","title":"Deploy Air-Gapped OpenShift Cluster on VMware","text":""},{"location":"ocp-on-vmware-airgapped/#deploy-air-gapped-openshift-cluster-on-vmware","title":"Deploy Air-Gapped OpenShift Cluster on VMware","text":"<p>This week, we will install an OpenShift cluster using IPI in an air-gapped environment, also referred to as restricted or disconnected networks. Specifically, this type of environment is defined as follows:</p> <p>An air-gapped computer or network is one that has no network interfaces, either wired or wireless, connected to outside networks.</p>"},{"location":"ocp-on-vmware-airgapped/#objective","title":"Objective","text":"<p>Learn how to install an OpenShift cluster in an air-gapped virtualized environment.</p>"},{"location":"ocp-on-vmware-airgapped/#outcomes","title":"Outcomes","text":"<ul> <li>Install and configure a mirror registry.</li> <li>Mirror OpenShift content.</li> <li>Install Apache HTTP server.</li> <li>Install OpenShift on vSphere using Installer Provisioned Infrastructure (IPI).</li> <li>Install Red Hat OpenShift Data Foundation.</li> </ul>"},{"location":"ocp-on-vmware-airgapped/#scenario","title":"Scenario","text":"<p>Your recently acquired expertise in containerization and OpenShift cluster deployment has caught the attention of your squad manager. You have been assigned the task of installing an OpenShift cluster in an air-gapped environment for a pilot project.</p> <p>For hosting the mirrored OpenShift content you will install the mirror registry for Red Hat OpenShift. The mirror registry for Red Hat OpenShift is included with the OpenShift subscription and is a small-scale container registry that can be used to mirror the required container images of OpenShift Container Platform in disconnected installations.</p> <p>The OCP Gymnasium is not air-gapped by default, you need to configure the pfsense firewall/router to fully simulate the air-gapped environment, after the content has been been mirrored.</p> <p>The cluster requires a total of nine nodes, consisting of three each of control plane, compute, and infrastructure nodes, as specified below.</p> Node type vCPU Memory in GiB Disk size in GB control plane 4 16 120 compute 4 16 120 infrastructure 16 64 120 <p>To fulfill the storage requirements of the applications, installation of OpenShift Data Foundation is necessary.</p>"},{"location":"ocp-on-vmware-airgapped/#provision-the-lab-environment","title":"Provision the lab environment","text":"<p>Repeat the steps from VMware week.</p>"},{"location":"ocp-on-vmware-airgapped/disable-internet/","title":"Disable internet","text":"<p>NOTE: This is an optional step if you want to have fully disconnected environment.</p> <ol> <li>Validate that the bastion can currently access the internet.</li> </ol> <pre><code>ping ibm.com\n</code></pre> <p>The result will show that the bastion server can access the host.</p> Example output<pre><code>[admin@bastion ~]$ ping ibm.com\nPING ibm.com (104.69.122.4) 56(84) bytes of data.\n64 bytes from a104-69-122-4.deploy.static.akamaitechnologies.com (104.69.122.4): icmp_seq=1 ttl=52 time=1.69 ms\n64 bytes from a104-69-122-4.deploy.static.akamaitechnologies.com (104.69.122.4): icmp_seq=2 ttl=52 time=1.91 ms\n64 bytes from a104-69-122-4.deploy.static.akamaitechnologies.com (104.69.122.4): icmp_seq=3 ttl=52 time=1.85 ms\n</code></pre> <ol> <li> <p>Download the Firewall Rules :fontawesome-regular-file-zip:. Depending on where your OCP Gym has been deployed, you will need to use the rules for either Washington or Dallas.</p> </li> <li> <p>Open a browser and navigate to https://pfsense.gym.lan.</p> </li> </ol> <p>Login as user <code>admin</code> with the password from the \"Shared Reservation\" section of your reservation.</p> <ol> <li> <p>Load the aliases and firewall rules.</p> </li> <li> <p>Navigate to Diagnostics &gt; Backup &amp; Restore.</p> </li> <li> <p>In the Restore Backup section select Aliases from the Restore area drop-down box</p> </li> <li> <p>Click the button Choose file in the Configuration file field</p> </li> <li> <p>Select the file and click Open</p> </li> <li> <p>Click Restore Configuration and confirm by clicking OK</p> </li> <li> <p>In the Restore Backup section select Firewall Rules from the Restore area drop-down box</p> </li> <li> <p>Click the button Choose file in the Configuration file field</p> </li> <li> <p>Select the file and click Open</p> </li> <li> <p>Click Restore Configuration and confirm by clicking OK</p> <p>Note that it is not necessary to reboot pfsense.</p> </li> <li> <p>Check the firewall rules.</p> </li> <li> <p>Navigate to Firewall &gt; Rules</p> </li> <li> <p>Click the LAN tab</p> </li> </ol>         &lt;figcaption&gt;Reference Firewall Rules for pfsense&lt;/figcaption&gt;        ![Reference Firewall Rules for pfsense](images/firewall-rules.png)     <ol> <li>Validate that the bastion can no longer access the internet.</li> </ol> <pre><code>ping -w 10 ibm.com\n</code></pre> <p>The result will show that the bastion server can access the host.</p> Example output<pre><code>[admin@bastion ~]$ ping -w 10 ibm.com\nPING ibm.com (104.69.122.4) 56(84) bytes of data.\n\n--- ibm.com ping statistics ---\n10 packets transmitted, 0 received, 100% packet loss, time 9245ms\n</code></pre>"},{"location":"ocp-on-vmware-airgapped/install-cluster/","title":"Install cluster","text":"<ol> <li>Create a backup of the install-config.yaml file (OCP installer will delete the install-config.yaml file after consuming it).</li> </ol> <pre><code>cp install-config.yaml install-config.yaml.bak\n</code></pre> <ol> <li>Create the cluster.</li> </ol> <pre><code>openshift-install create cluster --log-level info\n</code></pre> <p>Wait for the installation to complete.</p> Example output<pre><code>#...\nDEBUG Obtaining RHCOS image file from 'http://192.168.252.2/rhcos-vmware.x86_64.ova?sha256=9b3d5a598928ec52b0d32092d0a9a41f0ec8a238eb9fff8563266b9351919e20'\n#...\nINFO All cluster operators have completed progressing\nINFO Checking to see if there is a route at openshift-console/console...\nDEBUG Route found in openshift-console namespace: console\nDEBUG OpenShift console route is admitted\nINFO Install complete!\nINFO To access the cluster as the system:admin user when using 'oc', run 'export KUBECONFIG=/home/admin/ocpinstall/auth/kubeconfig'\nINFO Access the OpenShift web-console here: https://console-openshift-console.apps.ocpinstall.gym.lan\nINFO Login to the console with user: \"kubeadmin\", and password: \"kfeBh-pgwb3-XN3Fi-JpSFj\"\nDEBUG Time elapsed per stage:\nDEBUG               pre-bootstrap: 47s\nDEBUG                   bootstrap: 1m5s\nDEBUG                      master: 2m48s\nDEBUG          Bootstrap Complete: 13m36s\nDEBUG                         API: 45s\nDEBUG           Bootstrap Destroy: 2m17s\nDEBUG Cluster Operators Available: 15m41s\nDEBUG    Cluster Operators Stable: 54s\nINFO Time elapsed: 37m17s\n</code></pre>"},{"location":"ocp-on-vmware-airgapped/install-cluster/#post-installation-configuration","title":"Post installation configuration","text":"<ol> <li>Set the <code>KUBECONFIG</code> variable.</li> </ol> <pre><code>export KUBECONFIG=${HOME}/ocpinstall/auth/kubeconfig\n</code></pre> <ol> <li>Disable the default OperatorHub catalog sources.</li> </ol> <pre><code>oc patch OperatorHub cluster --type json \\\n    -p '[{\"op\": \"add\", \"path\": \"/spec/disableAllDefaultSources\",\"value\": true}]'\n</code></pre> <ol> <li>Add the mirrored OperatorHub catalog source</li> </ol> <pre><code>oc apply -f ${HOME}/ocpinstall/oc-mirror-workspace/results-*/catalogSource-cs-redhat-operator-index.yaml\n</code></pre> <ol> <li>Add release signatures.</li> </ol> <pre><code>oc apply -f ${HOME}/ocpinstall/oc-mirror-workspace/results-*/release-signatures/\n</code></pre> <ol> <li>Verify there is a Pod named <code>cs-redhat-operator-index-*****</code> in namespace <code>openshift-marketplace</code> <pre><code>oc -n openshift-marketplace get pods\n</code></pre> Example output<pre><code>NAME                                    READY   STATUS    RESTARTS      AGE\ncs-redhat-operator-index-zxsgt          1/1     Running   0      41s\nmarketplace-operator-6bcd474fbf-6njwx   1/1     Running   2 (18m ago)   38m\n</code></pre></li> </ol>"},{"location":"ocp-on-vmware-airgapped/install-config/","title":"Install config","text":""},{"location":"ocp-on-vmware-airgapped/install-config/#collect-install-config-asset-information","title":"Collect Install Config Asset Information","text":"<ol> <li>Create a pull-secret for the mirror-registry.</li> </ol> <pre><code>REG_CREDS=$(echo -n 'admin:QuayForAll!' | base64)\n</code></pre> <pre><code>cat &lt;&lt;EOF\n{\"auths\":{\"192.168.252.2:8443\":{\"auth\":\"${REG_CREDS}\",\"email\":\"admin@quay.io\"}}}\nEOF\n</code></pre> Example output<pre><code>{\"auths\":{\"192.168.252.2:8443\":{\"auth\":\"YWRtaW46UXVheUZvckFsbCE=\",\"email\":\"admin@quay.io\"}}}\n</code></pre> <ol> <li>Calculate the checksum for the Red Hat CoreOS OVA image.</li> </ol> <pre><code>sha256sum /var/www/html/rhcos-vmware.x86_64.ova\n</code></pre> Example output<pre><code>312a12cac8c2ba7b73fdb1b0b7abada8f8048901f304fac95b0819d3058dbdca  /var/www/html/rhcos-vmware.x86_64.ova\n</code></pre>"},{"location":"ocp-on-vmware-airgapped/install-config/#run-the-wizard","title":"Run the wizard","text":"<ol> <li>Install oc &amp; openshift-install binaries</li> </ol> <pre><code>wget https://mirror.openshift.com/pub/openshift-v4/clients/ocp/${OCP_VERSION}.${OCP_MINOR}/openshift-client-linux-amd64-rhel8.tar.gz\n</code></pre> <pre><code>wget https://mirror.openshift.com/pub/openshift-v4/clients/ocp/${OCP_VERSION}.${OCP_MINOR}/openshift-install-linux.tar.gz\n</code></pre> <pre><code>tar xf openshift-client-linux-amd64-rhel8.tar.gz oc\n</code></pre> <pre><code>tar xf openshift-install-linux.tar.gz openshift-install\n</code></pre> <pre><code>chmod +x oc openshift-install\nsudo install oc openshift-install /usr/local/bin\n</code></pre> <pre><code>rm openshift-client-linux-amd64-rhel8.tar.gz oc openshift-install-linux.tar.gz openshift-install\n</code></pre> <ol> <li>Enabling vCenter CA Certificate Trust</li> </ol> <pre><code>VSPHERE_HOSTNAME=ocpgym-vc.techzone.ibm.local\n</code></pre> <pre><code>mkdir -p ~/ocpinstall/vmware\ncurl -o ~/ocpinstall/vmware/download.zip -k https://${VSPHERE_HOSTNAME}/certs/download.zip\nunzip ~/ocpinstall/vmware/download.zip -d ~/ocpinstall/vmware\nsudo cp -a ~/ocpinstall/vmware/certs/lin/. /etc/pki/ca-trust/source/anchors/\nsudo update-ca-trust extract\n</code></pre> <ol> <li>Generate SSH key for OpenShift install.</li> </ol> <pre><code>ssh-keygen -t rsa -N '' -f ~/.ssh/openshift_rsa\n</code></pre> <ol> <li>Launch the installation wizard.</li> </ol> <pre><code>openshift-install create install-config\n</code></pre> <ol> <li>Complete the installer survey.</li> </ol> <p>For this exercise, the cluster must be named <code>ocpinstall</code>.</p> Example<pre><code>? SSH Public Key /home/admin/.ssh/id_core.pub\n? Platform vsphere\n? vCenter ocpgymwdc-vc.techzone.ibm.local\n? Username gymuser-yf5tfa4q@techzone.ibm.local\n? Password [? for help] ********\nINFO Connecting to vCenter ocpgymwdc-vc.techzone.ibm.local\nINFO Defaulting to only available datacenter: IBMCloud\nINFO Defaulting to only available cluster: /IBMCloud/host/ocpgym-wdc\n? Default Datastore /IBMCloud/datastore/gym-50vmycg18b-yf5tfa4q-storage\n? Network gym-50vmycg18b-yf5tfa4q-segment\n? Virtual IP Address for API 192.168.252.3\n? Virtual IP Address for Ingress 192.168.252.4\n? Base Domain gym.lan\n? Cluster Name ocpinstall\n? Pull Secret [? for help] *********************************************************************\n\nINFO Install-Config created in: .\n</code></pre> <ol> <li>Review / compare the parameters with the <code>openshift-install.yaml</code> template below.</li> </ol> Reference install-config.yaml<pre><code>additionalTrustBundlePolicy: Proxyonly\napiVersion: v1\nbaseDomain: gym.lan\ncompute:\n- architecture: amd64\n  hyperthreading: Enabled\n  name: worker\n  platform: {}\n  replicas: 3\ncontrolPlane:\n  architecture: amd64\n  hyperthreading: Enabled\n  name: master\n  platform: {}\n  replicas: 3\nmetadata:\n  creationTimestamp: null\n  name: ocpinstall\nnetworking:\n  clusterNetwork:\n  - cidr: 10.128.0.0/14\n    hostPrefix: 23\n  machineNetwork:\n  - cidr: 192.168.252.0/24\n  networkType: OVNKubernetes\n  serviceNetwork:\n  - 172.30.0.0/16\nplatform:\n  vsphere:\n    apiVIPs:\n    - 192.168.252.3\n    clusterOSImage: http://192.168.252.2/rhcos-vmware.x86_64.ova?sha256=312a12cac8c2ba7b73fdb1b0b7abada8f8048901f304fac95b0819d3058dbdca\n    failureDomains:\n    - name: ocpgym-wdc\n      region: IBMCloud\n      server: ocpgymwdc-vc.techzone.ibm.local\n      topology:\n        computeCluster: /IBMCloud/host/ocpgym-wdc\n        datacenter: IBMCloud\n        folder: /IBMCloud/vm/ocpgym-wdc/gym-50vmycg18b-979vmfdt\n        datastore: /IBMCloud/datastore/gym-50vmycg18b-979vmfdt-storage\n        networks:\n        - gym-50vmycg18b-979vmfdt-segment\n        resourcePool: /IBMCloud/host/ocpgym-wdc/Resources/Cluster Resource Pool/Gym Member Resource Pool/gym-50vmycg18b-979vmfdt\n      zone: ocpgym-wdc\n    ingressVIPs:\n    - 192.168.252.4\n    vcenters:\n    - datacenters:\n      - IBMCloud\n      password: VRLGjoCh\n      port: 443\n      server: ocpgymwdc-vc.techzone.ibm.local\n      user: gymuser-979vmfdt@techzone.ibm.local\npublish: External\npullSecret: '{\"auths\":{\"192.168.252.2:8443\":{\"auth\":\"YWRtaW46UXVheUZvckFsbCE=\",\"email\":\"admin@quay.io\"}}}'\nsshKey: |\n  ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIKvA1NaDixwxp479SamIQqEkeY3xobH8X/M/cmgattNY core@bastion\nadditionalTrustBundle: |\n  -----BEGIN CERTIFICATE-----\n  MIIFSDCCAzCgAwIBAgIUS+LsQxm6cV2+roXKohcwc3d7d0EwDQYJKoZIhvcNAQEL\n  BQAwIjEgMB4GA1UECgwXZ3ltLENOPXJlZ2lzdHJ5Lmd5bS5sYW4wHhcNMjQwNDEy\n  MDk1NzM3WhcNMzQwNDEwMDk1NzM3WjAiMSAwHgYDVQQKDBdneW0sQ049cmVnaXN0\n  cnkuZ3ltLmxhbjCCAiIwDQYJKoZIhvcNAQEBBQADggIPADCCAgoCggIBAOWc0Bxa\n  U/oyUsE5Qyqow6kdNNa4jLGWaDPSe/mx7qUmlLS6a2+bvVbBv9ys/tW9vpkECpgM\n  TYGNpq0aNgdLP14/m/5MbCuEQ7Tc6Fj4PtxfKtJPSTcsPdRCxvBeYV/fJ2md2RaC\n  y/1+oqdky8TGAOOOkalFkVPOJ/4ptn5rkMQmKYxqX0fk/SqLRJpEvVPSNaTM9gQM\n  ufddOiWyNzTyHbADyjjwgQo2ZtAaNwMZaFaCPGO4ic1PhN87mCs+/tIO1GqLYmLb\n  GoM+wdYYggIYGclOXIUh+rLbBpFCzSsu7m3L/tL1+uKthlaLKV+dfCWpM+H6owqe\n  XtsuSyJieaNpjyTrQeJ50U7H5SV1Dq6FWMN2x2lIjnzlS4oEePzpCPB2BPmfBbBA\n  8g6miCz81yy9fPre4wvv+GrQK2ghPBQyA+QxNyE/rRDERl+e19YoNBU3cJ9mdR7A\n  E83nNX5YreZ2aOI1gk+1J/brWriGAwNEg803FxrOLDh5PWVHi0D47X227POrrHQD\n  APdwvPj72wwC/waaLWlSI9HRGik0dXPBBddMBXYxL7Cny4nrNR9efR+DIa2abhsE\n  p10PjHzWbRkVRnRCkNWLn2KbES7a4ERHFmsFD5tidYWcaUzGRptpMlRWM9G4woxL\n  ZXKC8cB2CGNw41ZPr5tFKY6xlQfdprDo9oJ7AgMBAAGjdjB0MB0GA1UdDgQWBBQY\n  2zYOaeqvUL0UB4Y86Zy1jI/aWTAfBgNVHSMEGDAWgBQY2zYOaeqvUL0UB4Y86Zy1\n  jI/aWTAPBgNVHRMBAf8EBTADAQH/MCEGA1UdEQQaMBiCEHJlZ2lzdHJ5Lmd5bS5s\n  YW6HBMCo/AIwDQYJKoZIhvcNAQELBQADggIBACJgpZrAnSR10YapjbC/TsXsXeDg\n  PBO1Xic1O316NNlUTQdY4ROJWtpwiSE+pGRCAwO+A0Ylrw5auP61yQRRjTqdDtcN\n  eEwD1Cw8xoPoRAhPwDn2hxBDN17u8LQ5JWSMfhQjsfNJrXT41XPM6cyLPaB9C0Jq\n  Pi0OWagitQNIEW5UA63EtuqyhxcWah+BJx4I6uUOrLlUdqxqS1JeciQXAgo27eTV\n  kE3qJtpgrU5KiIYytQ05QTqgfrjM+fMa1sRoGAs8W/VUJS/7QaBz+gQ8HHw8ISKA\n  CNGD5Rt+WoGSanCTHWgH3t10Su645P6ih86OGyb1w4boJeGeR2gMd+o+e0RwetJU\n  ebmLzn0Nlh3h/5GRJYl75o+moGySoUwIHAjJg+WVfc3ubEVNgzJNdqvVx07BzJ7J\n  6a6IZSGBPeL0VIn0xIjhur+gaNRGNpkyKRpeWlxKaUw2zpK9LcuOcO6Zmy0cuHpP\n  CONOYXayCeWhB0ufLI4sMOUUmPJDkXYsWR58pQhPTyO5T5ABNGXXgA9lE73ncnoV\n  3L1UIeSxEDspv0vYzE27eGUUr1192YIj545NzQgyev4JA5/SfNDLWvTG9+r6rdMT\n  sEXa5X0Von35v2dnXuvmw/wVkQCnIm55kTioFXLlt1da0sBuXO63+DLSRgP/7lpS\n  K5D4kb1yO+t7InPy\n  -----END CERTIFICATE-----\nimageDigestSources:\n- mirrors:\n  - 192.168.252.2:8443/ocp4/ubi8\n  source: registry.access.redhat.com/ubi8\n- mirrors:\n  - 192.168.252.2:8443/ocp4/rhel8\n  source: registry.redhat.io/rhel8\n- mirrors:\n  - 192.168.252.2:8443/ocp4/rhel9\n  source: registry.redhat.io/rhel9\n- mirrors:\n  - 192.168.252.2:8443/ocp4/rhceph\n  source: registry.redhat.io/rhceph\n- mirrors:\n  - 192.168.252.2:8443/ocp4/odf4\n  source: registry.redhat.io/odf4\n- mirrors:\n  - 192.168.252.2:8443/ocp4/openshift4\n  source: registry.redhat.io/openshift4\n- mirrors:\n  - 192.168.252.2:8443/ocp4/openshift/release\n  source: quay.io/openshift-release-dev/ocp-v4.0-art-dev\n- mirrors:\n  - 192.168.252.2:8443/ocp4/openshift/release-images\n  source: quay.io/openshift-release-dev/ocp-release\n</code></pre> <ol> <li>Modify your Install Config asset.</li> </ol> <p>Open the <code>install-config.yaml</code> file in your editor of choice. In addition to the VMware week modifications apply the following changes.</p> Restricted network modifications<pre><code>#...\nplatform:\n  vsphere:\n    #...\n    clusterOSImage: http://192.168.252.2/rhcos-vmware.x86_64.ova?sha256=312a12cac8c2ba7b73fdb1b0b7abada8f8048901f304fac95b0819d3058dbdca\n#...\nadditionalTrustBundle: |\n  YOUR_QUAY_CERTIFICATE\nimageDigestSources:\n- mirrors:\n  - 192.168.252.2:8443/ocp4/ubi8\n  source: registry.access.redhat.com/ubi8\n- mirrors:\n  - 192.168.252.2:8443/ocp4/rhel8\n  source: registry.redhat.io/rhel8\n- mirrors:\n  - 192.168.252.2:8443/ocp4/rhel9\n  source: registry.redhat.io/rhel9\n- mirrors:\n  - 192.168.252.2:8443/ocp4/rhceph\n  source: registry.redhat.io/rhceph\n- mirrors:\n  - 192.168.252.2:8443/ocp4/odf4\n  source: registry.redhat.io/odf4\n- mirrors:\n  - 192.168.252.2:8443/ocp4/openshift4\n  source: registry.redhat.io/openshift4\n- mirrors:\n  - 192.168.252.2:8443/ocp4/openshift/release\n  source: quay.io/openshift-release-dev/ocp-v4.0-art-dev\n- mirrors:\n  - 192.168.252.2:8443/ocp4/openshift/release-images\n  source: quay.io/openshift-release-dev/ocp-release\n</code></pre> <p>The <code>imageDigestSources</code> should be copied from <code>${HOME}/oc-mirror-workspace/results-*/imageContentSourcePolicy.yaml</code>.</p> <p>Replace the sha value of <code>clusterOSImage</code> to match the one returned by step 2.</p>"},{"location":"ocp-on-vmware-airgapped/install-odf/","title":"Install odf","text":"<p>The infrastructure nodes were not created during cluster installation so we are going to add these nodes post installation.</p> <ol> <li>List the MachineSets.</li> </ol> <p>MachineSets are defined in the <code>openshift-machine-api</code> namespace.</p> <pre><code>oc -n openshift-machine-api get machineset\n</code></pre> Example output<pre><code>NAME                        DESIRED   CURRENT   READY   AVAILABLE   AGE\nocpinstall-btcjq-worker-0   3         3         3       3           109m\n</code></pre> <ol> <li>Export the worker MachineSet.</li> </ol> <pre><code>worker_machineset=$(oc -n openshift-machine-api get machineset -o jsonpath='{.items[0].metadata.name}')\n</code></pre> <pre><code>oc -n openshift-machine-api get machineset ${worker_machineset} -o json | jq '. | del(\n  .metadata.annotations,\n  .metadata.uid,\n  .metadata.generation,\n  .metadata.resourceVersion,\n  .metadata.creationTimestamp,\n  .status\n)' &gt; infra-machineset.json\n</code></pre> <ol> <li> <p>Edit <code>infra-machineset.json</code>.</p> </li> <li> <p>Add labels to spec.metadata.lables:</p> <ul> <li><code>cluster.ocs.openshift.io/openshift-storage: \"\"</code></li> <li><code>node-role.kubernetes.io/infra: \"\"</code></li> </ul> </li> <li>Add taint <code>node.ocs.openshift.io/storage: \"true\"</code> to the spec.</li> <li>Change all values with postfix <code>-worker-0</code> to <code>-infra-0</code> (3 entries).</li> <li>Change the value of <code>machine.openshift.io/cluster-api-machine-role</code> and <code>machine.openshift.io/cluster-api-machine-type</code> to <code>infra</code>.</li> <li>Change <code>memoryMiB</code> to <code>65536</code>.</li> <li>Change <code>numCPUs</code> to <code>16</code>.</li> <li>Change <code>numCoresPerSocket</code> to <code>2</code>.</li> </ol> Reference infra-machineset.json<pre><code>{\n  \"apiVersion\": \"machine.openshift.io/v1beta1\",\n  \"kind\": \"MachineSet\",\n  \"metadata\": {\n    \"labels\": {\n      \"machine.openshift.io/cluster-api-cluster\": \"ocpinstall-btcjq\"\n    },\n    \"name\": \"ocpinstall-btcjq-infra-0\",\n    \"namespace\": \"openshift-machine-api\"\n  },\n  \"spec\": {\n    \"replicas\": 3,\n    \"selector\": {\n      \"matchLabels\": {\n        \"machine.openshift.io/cluster-api-cluster\": \"ocpinstall-btcjq\",\n        \"machine.openshift.io/cluster-api-machineset\": \"ocpinstall-btcjq-infra-0\"\n      }\n    },\n    \"template\": {\n      \"metadata\": {\n        \"labels\": {\n          \"machine.openshift.io/cluster-api-cluster\": \"ocpinstall-btcjq\",\n          \"machine.openshift.io/cluster-api-machine-role\": \"infra\",\n          \"machine.openshift.io/cluster-api-machine-type\": \"infra\",\n          \"machine.openshift.io/cluster-api-machineset\": \"ocpinstall-btcjq-infra-0\"\n        }\n      },\n      \"spec\": {\n        \"lifecycleHooks\": {},\n        \"metadata\": {\n          \"labels\": {\n            \"cluster.ocs.openshift.io/openshift-storage\": \"\",\n            \"node-role.kubernetes.io/infra\": \"\"\n          }\n        },\n        \"taints\": [\n          {\n            \"effect\": \"NoSchedule\",\n            \"key\": \"node.ocs.openshift.io/storage\",\n            \"value\": \"true\"\n          }\n        ],\n        \"providerSpec\": {\n          \"value\": {\n            \"apiVersion\": \"machine.openshift.io/v1beta1\",\n            \"credentialsSecret\": {\n              \"name\": \"vsphere-cloud-credentials\"\n            },\n            \"diskGiB\": 120,\n            \"kind\": \"VSphereMachineProviderSpec\",\n            \"memoryMiB\": 65536,\n            \"metadata\": {\n              \"creationTimestamp\": null\n            },\n            \"network\": {\n              \"devices\": [\n                {\n                  \"networkName\": \"gym-50vmycg18b-yf5tfa4q-segment\"\n                }\n              ]\n            },\n            \"numCPUs\": 16,\n            \"numCoresPerSocket\": 2,\n            \"snapshot\": \"\",\n            \"template\": \"ocpinstall-btcjq-rhcos-IBMCloud-ocpgym-wdc\",\n            \"userDataSecret\": {\n              \"name\": \"worker-user-data\"\n            },\n            \"workspace\": {\n              \"datacenter\": \"IBMCloud\",\n              \"datastore\": \"/IBMCloud/datastore/gym-50vmycg18b-yf5tfa4q-storage\",\n              \"folder\": \"/IBMCloud/vm/ocpgym-wdc/gym-50vmycg18b-yf5tfa4q\",\n              \"resourcePool\": \"/IBMCloud/host/ocpgym-wdc/Resources/Cluster Resource Pool/Gym Member Resource Pool/gym-50vmycg18b-yf5tfa4q\",\n              \"server\": \"ocpgymwdc-vc.techzone.ibm.local\"\n            }\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre> <ol> <li>Create the MachineSet.</li> </ol> <pre><code>oc apply -f infra-machineset.json\n</code></pre> <ol> <li>Monitor the nodes.</li> </ol> <pre><code>watch oc get nodes\n</code></pre> Wait until the infrastructure nodes reach status Ready<pre><code>NAME                              STATUS   ROLES                  AGE     VERSION\nocpinstall-btcjq-infra-0-2ncmd    Ready    infra,worker           2m47s   v1.30.5\nocpinstall-btcjq-infra-0-tdxvc    Ready    infra,worker           2m45s   v1.30.5\nocpinstall-btcjq-infra-0-trwwg    Ready    infra,worker           2m45s   v1.30.5\nocpinstall-btcjq-master-0         Ready    control-plane,master   123m    v1.30.5\nocpinstall-btcjq-master-1         Ready    control-plane,master   123m    v1.30.5\nocpinstall-btcjq-master-2         Ready    control-plane,master   123m    v1.30.5\nocpinstall-btcjq-worker-0-56vmd   Ready    worker                 110m    v1.30.5\nocpinstall-btcjq-worker-0-sdtg6   Ready    worker                 110m    v1.30.5\nocpinstall-btcjq-worker-0-zv829   Ready    worker                 110m    v1.30.5\n</code></pre> <ol> <li>Install the ODF operator and create a SmallScale (0.5 TiB) StorageSystem.</li> </ol>"},{"location":"ocp-on-vmware-airgapped/mirror-host/","title":"Mirror host","text":""},{"location":"ocp-on-vmware-airgapped/mirror-host/#add-storage-to-the-bastion-host","title":"Add storage to the bastion host","text":"<p>The mirror registry will be installed on the bastion host. Mirroring OpenShift platform and operator images and running the mirror registry on the same host requires tens of gigabytes of disk space. The initial disk space on the bastion host is not sufficient, you need to make more storage available.</p> <ol> <li>Log into the vSphere console.</li> </ol> <p>Use the vCenter Console URL, username and password from your reservation.</p> <ol> <li> <p>Add a 500GB disk to the bastion virtual machine.</p> </li> <li> <p>Click on the second icon at the top of the left tree view.</p> </li> <li>Expand things until you click on the bastion machine.</li> <li>Click the Edit Settings button on the top toolbar.</li> <li>Click Add New Device, choose Hard Disk.</li> <li> <p>Change the size to 500 GB for the \"New Hard disk\", then click OK.</p> </li> <li> <p>Open a Terminal and log in the bastion host.</p> </li> <li> <p>List the block devices and identify the new disk.</p> </li> </ol> <pre><code>lsblk\n</code></pre> Example output<pre><code>NAME                  MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT\nsda                     8:0    0   50G  0 disk\n\u251c\u2500sda1                  8:1    0  600M  0 part /boot/efi\n\u251c\u2500sda2                  8:2    0    1G  0 part /boot\n\u2514\u2500sda3                  8:3    0 48.4G  0 part\n  \u251c\u2500rhel_bastion-root 253:0    0 43.4G  0 lvm  /\n  \u2514\u2500rhel_bastion-swap 253:1    0    5G  0 lvm  [SWAP]\nsdb                     8:16   0  500G  0 disk\n</code></pre> <ol> <li>List the volume groups.</li> </ol> <pre><code>sudo vgs\n</code></pre> Example output<pre><code>VG           #PV #LV #SN Attr   VSize  VFree\nrhel_bastion   1   2   0 wz--n- 48.41g    0\n</code></pre> <ol> <li>Extend volume group <code>rhel_bastion</code> with the new block device <code>/dev/sdb</code>.</li> </ol> <pre><code>sudo vgextend rhel_bastion /dev/sdb\n</code></pre> Example output<pre><code>Physical volume \"/dev/sdb\" successfully created.\nVolume group \"rhel_bastion\" successfully extended\n</code></pre> <ol> <li>Extend the size of the root logical volume to 100% of it's unallocated size.</li> </ol> <pre><code>sudo lvextend -l +100%FREE /dev/rhel_bastion/root\n</code></pre> Example output<pre><code>Size of logical volume rhel_bastion/root changed from 43.41 GiB (11113 extents) to &lt;143.41 GiB (36712 extents).\nLogical volume rhel_bastion/root successfully resized.\n</code></pre> <ol> <li>Extend the size of the root filesystem.</li> </ol> <pre><code>sudo xfs_growfs /\n</code></pre> Example output<pre><code>meta-data=/dev/mapper/rhel_bastion-root isize=512    agcount=4, agsize=2844928 blks\n         =                       sectsz=512   attr=2, projid32bit=1\n         =                       crc=1        finobt=1, sparse=1, rmapbt=0\n         =                       reflink=1    bigtime=0 inobtcount=0\ndata     =                       bsize=4096   blocks=11379712, imaxpct=25\n         =                       sunit=0      swidth=0 blks\nnaming   =version 2              bsize=4096   ascii-ci=0, ftype=1\nlog      =internal log           bsize=4096   blocks=5556, version=2\n         =                       sectsz=512   sunit=0 blks, lazy-count=1\nrealtime =none                   extsz=4096   blocks=0, rtextents=0\ndata blocks changed from 11379712 to 37593088\n</code></pre>"},{"location":"ocp-on-vmware-airgapped/mirror-host/#install-the-mirror-registry","title":"Install the mirror registry","text":"<p>To ensure secure communication with image registries, which require HTTPS, we must create a TLS certificate for the mirror registry. While the mirror-registry command can generate a TLS certificate automatically, it does not provide sufficient control over the subject and subject alternate name fields of the certificate.</p> <ol> <li>Create a working directory.</li> </ol> <pre><code>mkdir ocpinstall\n</code></pre> <ol> <li>Generate a self-signed TLS certificate.</li> </ol> <pre><code>openssl req -newkey rsa:4096 -nodes -sha256 -keyout /home/admin/ocpinstall/quay.key \\\n    -x509 -out /home/admin/ocpinstall/quay.crt -days 3650 \\\n    -subj \"/O=gym,CN=registry.gym.lan\" \\\n    -addext \"subjectAltName = DNS:registry.gym.lan,IP:192.168.252.2\"\n</code></pre> <ol> <li>Create a temporary directory</li> </ol> <pre><code>MIRROR_DIR=$(mktemp -d)\n</code></pre> <ol> <li>Download the quay.io archive.</li> </ol> <pre><code>curl -Lo ${MIRROR_DIR}/mirror-registry.tar.gz \\\n    https://mirror.openshift.com/pub/cgw/mirror-registry/latest/mirror-registry-amd64.tar.gz\n</code></pre> <ol> <li>Get into the temporary directory</li> </ol> <pre><code>cd ${MIRROR_DIR}\n</code></pre> <ol> <li>Extract the quay.io archive.</li> </ol> <pre><code>tar xf mirror-registry.tar.gz\n</code></pre> <ol> <li>Run the install command.</li> </ol> <pre><code>./mirror-registry install --quayHostname 192.168.252.2 \\\n    --initUser admin --initPassword QuayForAll! \\\n    --sslKey /home/admin/ocpinstall/quay.key --sslCert /home/admin/ocpinstall/quay.crt\n</code></pre> <ol> <li>Return to the install directory</li> </ol> <pre><code>cd ~/ocpinstall\n</code></pre> <ol> <li>Ensure the mirror registry is accessible on port <code>8443</code>.</li> </ol> <pre><code>sudo firewall-cmd --add-port 8443/tcp --permanent\nsudo firewall-cmd --reload\n</code></pre> <ol> <li>Test connectivity.</li> </ol> <pre><code>curl -sk https://192.168.252.2:8443/health/instance | jq\n</code></pre> Example output<pre><code>{\n  \"data\": {\n    \"services\": {\n      \"auth\": true,\n      \"database\": true,\n      \"disk_space\": true,\n      \"registry_gunicorn\": true,\n      \"service_key\": true,\n      \"web_gunicorn\": true\n    }\n  },\n  \"status_code\": 200\n}\n</code></pre>"},{"location":"ocp-on-vmware-airgapped/mirror-host/#make-the-rhcos-vmware-ova-available","title":"Make the RHCOS VMware OVA available","text":"<p>When installing OpenShift using IPI, by default the installer downloads the Red Hat CoreOS VMware OVA from the internet. To complete an air-gapped installation, you must first obtain the OVA image and make it accessible via HTTP (HTTPS is not necessary).</p> <ol> <li>Install the <code>httpd</code> package.</li> </ol> <pre><code>sudo dnf install -y httpd\n</code></pre> <ol> <li>Start and enable the service.</li> </ol> <pre><code>sudo systemctl enable httpd --now\n</code></pre> <ol> <li>Ensure the http port is accessible.</li> </ol> <pre><code>sudo firewall-cmd --permanent --add-service http\nsudo firewall-cmd --reload\n</code></pre> <ol> <li>Download the OVA.</li> </ol> <pre><code>OCP_VERSION=4.18\n</code></pre> <pre><code>curl -Lo rhcos-vmware.x86_64.ova \\\n    https://mirror.openshift.com/pub/openshift-v4/amd64/dependencies/rhcos/${OCP_VERSION}/latest/rhcos-vmware.x86_64.ova\n</code></pre> <ol> <li>Move the OVA into <code>/var/www/html</code>.</li> </ol> <pre><code>sudo mv rhcos-vmware.x86_64.ova /var/www/html\n</code></pre> <ol> <li>Restore the default SELinux context on the directory so that the httpd service can access the ova.</li> </ol> <pre><code>sudo restorecon -Rv /var/www/html\n</code></pre>"},{"location":"ocp-on-vmware-airgapped/mirror/","title":"Mirror","text":"<p>For mirroring OpenShift content, Red Hat provides the <code>oc-mirror</code> command-line interface. What content is mirrored is configured by defining an <code>ImageSetConfiguration</code> in a file.</p>"},{"location":"ocp-on-vmware-airgapped/mirror/#install-the-oc-mirror-cli","title":"Install the oc-mirror CLI","text":"<ol> <li>Download the oc-mirror CLI.</li> </ol> <pre><code>OCP_RELEASE_CHANNEL=stable\n</code></pre> <pre><code>curl -Lo oc-mirror.tar.gz \\\n    https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/ocp/${OCP_RELEASE_CHANNEL}-${OCP_VERSION}/oc-mirror.tar.gz\n</code></pre> <ol> <li>Extract the oc-mirror CLI.</li> </ol> <pre><code>tar xf oc-mirror.tar.gz oc-mirror\n</code></pre> <ol> <li>Move the CLI to <code>/usr/local/bin</code>.</li> </ol> <pre><code>chmod +x oc-mirror\nsudo install oc-mirror /usr/local/bin\n</code></pre> <ol> <li>Verify.</li> </ol> <pre><code>oc-mirror version\n</code></pre> Example output<pre><code>W0512 15:36:58.975204   15046 mirror.go:102]\n\u26a0\ufe0f  oc-mirror v1 is deprecated (starting in 4.18 release) and will be removed in a future release - please migrate to oc-mirror --v2\n\nWARNING: This version information is deprecated and will be replaced with the output from --short. Use --output=yaml|json to get the full version.\nClient Version: version.Info{Major:\"\", Minor:\"\", GitVersion:\"4.18.0-202504231952.p0.g222e383.assembly.stream.el9-222e383\", GitCommit:\"222e38394c0d96357ba0179e88aa65956d4112a4\", GitTreeState:\"clean\", BuildDate:\"2025-04-23T20:30:36Z\", GoVersion:\"go1.22.12 (Red Hat 1.22.12-2.el8_10) X:strictfipsruntime\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n</code></pre> <ol> <li>Clean up.</li> </ol> <pre><code>rm oc-mirror.tar.gz oc-mirror\n</code></pre>"},{"location":"ocp-on-vmware-airgapped/mirror/#collect-information-for-the-mirror-operation","title":"Collect information for the mirror operation","text":"<p>In order to limit the size of the mirrored content you need to specify minimal versions of both the platform and operator images.</p> <ol> <li> <p>Copy your pull secret from the Red Hat Hybrid Cloud Console and save it in file <code>${XDG_RUNTIME_DIR}/containers/auth.json</code>.</p> </li> <li> <p>List all OpenShift versions for selected channel.</p> </li> </ol> <pre><code>oc-mirror list releases --channel ${OCP_RELEASE_CHANNEL}-${OCP_VERSION}\n</code></pre> <ol> <li>Select desired point release of OCP</li> </ol> <pre><code>OCP_MINOR=19\n</code></pre> <ol> <li>List the OpenShift Data Foundation operators in the RedHat catalog.</li> </ol> <pre><code>oc-mirror list operators \\\n    --catalog registry.redhat.io/redhat/redhat-operator-index:v${OCP_VERSION} \\\n    | grep -E 'ocs-operator|odf-operator|mcg-operator|odf-csi-addons-operator|odf-dependencies'\n</code></pre> Example output<pre><code>mcg-operator                                                stable-4.17\nocs-operator                                                stable-4.17\nodf-csi-addons-operator                                     stable-4.17\nodf-operator                                                stable-4.17\nodf-dependencies                                            stable-4.17\n</code></pre> <ol> <li>List all versions in selected channel <code>stable-4.18</code> for package <code>odf-operator</code>.</li> </ol> <pre><code>oc-mirror list operators \\\n  --catalog registry.redhat.io/redhat/redhat-operator-index:v${OCP_VERSION} \\\n  --package ocs-operator \\\n  --channel ${OCP_RELEASE_CHANNEL}-${OCP_VERSION}\n</code></pre> Example output<pre><code>VERSIONS\n4.17.0-rhodf\n</code></pre> <p>Repeat this step for operators <code>ocs-operator</code>, <code>mcg-operator</code>, <code>odf-csi-addons-operator</code> and <code>odf-dependencies</code>.</p> <ol> <li>Set rhodf package version</li> </ol> <pre><code>RHODF_MIN_VERSION=4.18.2-rhodf\n</code></pre>"},{"location":"ocp-on-vmware-airgapped/mirror/#mirror-the-platform-and-operator-images","title":"Mirror the platform and operator images","text":"<ol> <li>Create the <code>ImageSetConfiguration</code> file.</li> </ol> <pre><code>cat &lt;&lt;EOF &gt; ~/ocpinstall/isc-platform-odf.yaml\nkind: ImageSetConfiguration\napiVersion: mirror.openshift.io/v1alpha2\nmirror:\n  platform:\n    channels:\n    - name: ${OCP_RELEASE_CHANNEL}-${OCP_VERSION}\n      type: ocp\n      minVersion: ${RHODF_MIN_VERSION}\n    graph: true\n  operators:\n  - catalog: registry.redhat.io/redhat/redhat-operator-index:v${OCP_VERSION}\n    packages:\n    - name: odf-operator\n      channels:\n      - name: ${OCP_RELEASE_CHANNEL}-${OCP_VERSION}\n      minVersion: ${RHODF_MIN_VERSION}\n    - name: ocs-operator\n      channels:\n      - name: ${OCP_RELEASE_CHANNEL}-${OCP_VERSION}\n      minVersion: ${RHODF_MIN_VERSION}\n    - name: odf-csi-addons-operator\n      channels:\n      - name: ${OCP_RELEASE_CHANNEL}-${OCP_VERSION}\n      minVersion: ${RHODF_MIN_VERSION}\n    - name: mcg-operator\n      channels:\n      - name: ${OCP_RELEASE_CHANNEL}-${OCP_VERSION}\n      minVersion: ${RHODF_MIN_VERSION}\n    - name: odf-dependencies\n      channels:\n      - name: ${OCP_RELEASE_CHANNEL}-${OCP_VERSION}\n      minVersion: ${RHODF_MIN_VERSION}\nEOF\n</code></pre> <ol> <li>Log in to private image registry.</li> </ol> <pre><code>podman login 192.168.252.2:8443 -u admin -p QuayForAll! --tls-verify=false\n</code></pre> <ol> <li>Mirror the content.</li> </ol> <pre><code>oc-mirror --config=${HOME}/ocpinstall/isc-platform-odf.yaml docker://192.168.252.2:8443/ocp4 --dest-skip-tls\n</code></pre> Example output<pre><code>#...\ninfo: Mirroring completed in 1h19m55.57s (50.34MB/s)\nRendering catalog image \"192.168.252.2:8443/ocp4/redhat/redhat-operator-index:v4.18\" with file-based catalog\nWriting image mapping to oc-mirror-workspace/results-1752686279/mapping.txt\nWriting UpdateService manifests to oc-mirror-workspace/results-1752686279\nWriting CatalogSource manifests to oc-mirror-workspace/results-1752686279\nWriting ICSP manifests to oc-mirror-workspace/results-1752686279\n</code></pre>"},{"location":"ocp-on-vmware-airgapped/prepare-installation/","title":"Prepare installation","text":"<p>Repeat the steps from the VMware week exercise.</p> <ol> <li> <p>Verify DNS records.</p> </li> <li> <p>Ensure CA certificates from vCenter are trusted.</p> </li> <li> <p>Install the OpenShift installer.</p> </li> <li> <p>Generate a key pair for cluster node SSH access.</p> </li> </ol>"},{"location":"terraform/","title":"Intro to Terraform","text":"<p>Welcome to the Terraform section of the Platform Engineering Bootcamp. This introduction is intended as a quick overview on What Terraform is, Terraform core concepts and Terraform best practices. It does not aim to cover all aspects of the tool, rather just enough to get started with the lab.</p>"},{"location":"terraform/#what-is-terraform","title":"What is Terraform?","text":"<p>Terraform is an infrastructure as code tool that lets you build, change, and version infrastructure safely, efficiently and in an automated way.</p> <p>Terraform uses a declarative, high-level configuration language called HashiCorp Configuration Language (HCL) to describe the desired \u201cend-state\u201d cloud or on-premises infrastructure for running an application. It then generates a plan for reaching that end-state and runs the plan to provision the infrastructure.</p> <p>You can use terraform to manage infrastructure in the cloud and beyond.</p>"},{"location":"terraform/#terraform-glossary","title":"Terraform Glossary","text":""},{"location":"terraform/#declarative-vs-imperative","title":"Declarative (vs Imperative)","text":"<p>The term \"declarative\" is frequently used to describe Terraform. By \"declarative\" we mean the ability to describe how infrastructure should be configured via a high level configuration language.</p> <p>For example, let's look at how to define an AWS S3 bucket using terraform:</p> <pre><code>resource \"aws_s3_bucket\" \"example\" {\n  bucket = \"my-tf-test-bucket\"\n\n  tags = {\n    Name        = \"prod-bucket\"\n    Environment = \"production\"\n  }\n}\n</code></pre> <p>This block defines an S3 bucked called <code>my-tf-test-bucket</code> and tags it with a <code>Name</code> and <code>Environment</code> tag.</p> <p>Instead of outlining all the steps required to configure your environment, the above HCL code snippet defines (or declares) how you want your bucket to be configured. Terraform is able to understand this higher level configuration and translate it into the steps required to configure your infrastructure.</p> <p>By contrast, an \"imperative\" approach defines the specific commands needed to achieve the desired configuration, and looks like this:</p> <pre><code>aws s3api create-bucket --bucket my-tf-test-bucket\n\naws s3api put-bucket-tagging --bucket my-tf-test-bucket --tagging 'TagSet=[{Key=Name,Value=prod-bucket},{Key=Environment,Value=production}]'\n</code></pre> <p>Here we are using the AWS CLI to create the same S3 resource.</p> <p>While the above command produces the same result as the terraform code, it creates some challenges when working as part of a Platform Engineering team:</p> <ol> <li>How do you document what you did, allowing others to collaborate with you? (maybe try to use a shell script in git? This approach can grow in complexity very quickly)</li> <li>How do you ensure you can reliably reproduce and repeat it? (re run the shell script? What if some of the dependencies for your resources have changed?)</li> <li>How do keep track of any changes to the infrastructure you have deployed?</li> </ol> <p>By using a declarative language, terraform solves these (and many more) problems for Platform Engineering teams. It documents the configuration deployed in a way that is scalable and easy to collaborate on. Furthermore, by writing HCL you can describe how you want (desire) your resources to be configured - allowing terraform to detect any changes from it.</p>"},{"location":"terraform/#desired-end-state-vs-current-state","title":"Desired (end) State vs Current State","text":"<p>When writing HCL, you are defining the resources (and configuration) you want your infrastructure to have: this is the <code>desired state</code>. Conversely, the <code>current state</code> represents the objects (and their configuration) that actually exist within your infrastructure at present.</p> <p>Terraform is able to deploy your desired state, as well as reconcile differences between the desired state (as defined in HCL code) and the current state of your infrastructure in an automated and repeatable way. In doing so, it is considered an <code>Infrastructure as Code (IaC)</code> tool.</p>"},{"location":"terraform/#infrastructure-as-code-iac","title":"Infrastructure as Code (IaC)","text":"<p><code>Infrastructure as Code (IaC)</code> is the managing and provisioning of infrastructure through code instead of through manual processes. IaC comes with many benefits, including:</p> <ol> <li>IaC automates the deployment and configuration of cloud resources in a repeatable way, saving Platform Engineering teams time and reducing errors</li> <li>Using declarative IaC tools such as terraform makes it easy for Platform teams to collaborate, at scale</li> <li>An IaC tool such as terraform allows you to version control changes to your infrastructure, making it easy to audit when changes are introduced, and roll back unwanted changes.</li> </ol>"},{"location":"terraform/#core-terraform-core-concepts","title":"Core Terraform Core Concepts","text":""},{"location":"terraform/#terraform-resources","title":"Terraform Resources","text":"<p>Resources are the most important element in the Terraform language. Each resource block describes one or more infrastructure objects, such as virtual networks, compute instances, or higher-level components such as DNS records. For example, the following resource provisions a COS bucket in IBM Cloud:</p> <pre><code># Resource: Create an IBM Cloud Object Storage bucket\nresource \"ibm_cos_bucket\" \"example_bucket\" {\n  bucket_name          = \"my-example-bucket\"\n  resource_group_id    =  ibm_resource_group.existing_group.id\n  storage_class        = \"standard\"\n  region_location      = \"us-south\"\n  endpoint_type        = \"public\"\n}\n</code></pre> <p>You can configure resources using <code>arguments</code> (<code>bucket_name</code>, <code>resource_group_id</code>, etc.. in the example above).</p> <p>You can reference resources using <code>attributes</code>. Attributes are data and metadata related to your resource that might not be known until after creation, but you might still want to reference in your code. </p> <p>For example, you might need to reference the <code>id</code> of the COS bucket created in the example above.</p> <p>More on Terraform resources here</p>"},{"location":"terraform/#terraform-data-sources","title":"Terraform Data Sources","text":"<p>Terraform code will often need to reference resources that are not defined within it. <code>Data sources</code> allow Terraform to use information defined outside of Terraform, defined by another separate Terraform configuration, or modified by functions.</p> <pre><code># Data source: Get an existing resource group\ndata \"ibm_resource_group\" \"existing_group\" {\n  name = \"Default\"\n}\n\n# Resource: Create an IBM Cloud Object Storage bucket\nresource \"ibm_cos_bucket\" \"example_bucket\" {\n  bucket_name          = \"my-example-bucket\"\n  resource_group_id    = data.ibm_resource_group.existing_group.id\n  storage_class        = \"standard\"\n  region_location      = \"us-south\"\n  endpoint_type        = \"public\"\n}\n</code></pre> <p>More on Terraform data sources here</p>"},{"location":"terraform/#terraform-state","title":"Terraform State","text":"<p><code>Terraform State</code> stores information about your managed infrastructure and configuration. This state is used by Terraform to map real world resources to your configuration and keep track of metadata. Terraform will store state as a simple JSON in a <code>terraform.tfstate</code> file:</p> <p></p><pre><code>{\n  \"version\": 4,\n  \"terraform_version\": \"1.6.0\",\n  \"serial\": 1,\n  \"lineage\": \"a1b2c3d4-e5f6-7890-1234-56789abcdef0\",\n  \"outputs\": {},\n  \"resources\": [\n    {\n      \"mode\": \"data\",\n      \"type\": \"ibm_resource_group\",\n      \"name\": \"existing_group\",\n      \"provider\": \"provider[\\\"registry.terraform.io/ibm-cloud/ibm\\\"]\",\n      \"instances\": [\n        {\n    ...\n</code></pre> Terraform uses state to build, change and manage infrastructure, as well as when reconciling any changes to infrastructure to the desired state.<p></p> <p>More information on Terraform State here</p>"},{"location":"terraform/#terraform-providers","title":"Terraform providers","text":"<p>Terraform relies on plugins called providers to interact with cloud providers, SaaS providers, and other APIs. Each provider adds a set of resource types and/or data sources that Terraform can manage.</p> <p>There is a rich catalogue of Terraform providers which can be browsed on the Terraform Registry</p>"},{"location":"terraform/#terraform-modules","title":"Terraform Modules","text":"<p>Modules are containers for multiple infrastructure resources that are used together. For example, a module might group an IBM Cloud VPC, a Virtual Server Instance and a Volume required to deploy an application.</p> <p>One of the great benefits of Terraform is its ability to group infrastructure components into broader modules, allowing you to reliably manage infrastructure at scale</p> <p>There are a number of existing terraform modules on Github. You can find an example of a module for IBM COS here</p> <p>More information on Terraform Modules here</p>"},{"location":"terraform/#terraform-variables-and-outputs","title":"Terraform Variables and Outputs","text":""},{"location":"terraform/#input","title":"Input","text":"<p>Input variables let you customize aspects of Terraform modules without altering the module's own source code \u2013 you can think of them as a parameter for a function. You can define a variable using a variable block, allowing you to define its type and an optional default value:</p> <p></p><pre><code>variable \"image_id\" {\n  type = string\n}\n\nvariable \"availability_zone_names\" {\n  type    = list(string)\n  default = [\"us-west-1a\"]\n}\n</code></pre> Variables allow the consumers of your code to customize its behaviour without changing the underlying HCL code. <p></p>"},{"location":"terraform/#output","title":"Output","text":"<p>Output values make information about your infrastructure available on the command line, and can expose information for other Terraform configurations to use. Output values are similar to return values in programming languages:</p> <p></p><pre><code>output \"instance_ip_addr\" {\n  value = ibm_vsi_instance.primary_ip\n}\n</code></pre> Find more on variables here<p></p>"},{"location":"terraform/#terraform-cli","title":"Terraform CLI","text":"<p>The main way to interact with Terraform is via its CLI. It allows you to validate your HCL configuration, create infrastructure, reconcile changes and destroy infrastructure (and much much more)</p> <p>Find more on the Terraform CLI here</p>"},{"location":"terraform/#terraform-best-practices","title":"Terraform Best Practices","text":""},{"location":"terraform/#general","title":"General","text":"<ul> <li>You should store your project in github, and can use the following <code>.gitignore</code> to get started</li> <li>Don't hardcode values that can be passed as variables or discovered using data sources</li> </ul> <p>While Terraform can automate just about anything, it doesn't mean it should. Keep resource modules as plain as possible:</p> <ul> <li>Terraform is best at automating deployment and managment of infrastructure (usually from a cloud provider). If you are looking to automate instance configuration (e.g installing application dependencies, running prerequisite scripts and deploying an application on a remote server) consider using <code>Ansible</code></li> <li>Similarly, while Terraform can deploy resources to OpenShift and Kubernetes, it is not designed to do this. Consider using a tool like ArgoCD - or ideally the <code>OpenShift GitOps Operator</code> -  instead</li> </ul>"},{"location":"terraform/#terraform-project-structure","title":"Terraform Project structure","text":"<p>Terraform project should be composed of at least 3 files:</p> <ol> <li><code>main.tf</code> - defines the resources (e.g. resource blocks and data sources) managed by your project</li> <li><code>variables.tf</code> - defines input variables to your terraform project</li> <li><code>outputs.tf</code> - defines the output variables for your terraform project</li> </ol> <p>If your project grows in size, you will want to organise following a module structure, trying to group resources that should logically be configured together in nested modules:</p> <pre><code>\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 main.tf\n\u251c\u2500\u2500 variables.tf\n\u251c\u2500\u2500 outputs.tf\n\u251c\u2500\u2500 ...\n\u251c\u2500\u2500 modules/\n\u2502   \u251c\u2500\u2500 nestedA/\n\u2502   \u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2502   \u251c\u2500\u2500 variables.tf\n\u2502   \u2502   \u251c\u2500\u2500 main.tf\n\u2502   \u2502   \u251c\u2500\u2500 outputs.tf\n\u2502   \u251c\u2500\u2500 nestedB/\n\u2502   \u251c\u2500\u2500 .../\n</code></pre> <p>More on project structure here</p>"},{"location":"terraform/#terraform-state-management","title":"Terraform State Management","text":"<p>By default, terraform will manage state on your local machine in a <code>terraform.tfstate</code> file. This is fine for local development, but can pose a hindrance to collaborating with other Platform Engineers.</p> <p>Sensitive Terraform State</p> <p>You should never store a terraform state in git, as it may contain sensitive values such as passwords!</p> <p>As a best practice, always try to configure your code to use<code>Remote State</code>, where Terraform writes the state data to a remote data store such an <code>S3 bucket</code> or <code>Terraform Cloud</code>. Remote State is a secure way to easily share state resources with your team.</p> <p>More on remote state</p> <p>Next Steps</p> <p>You are now ready to proceed to the Terraform VSphere Lab</p>"},{"location":"terraform/terraform-vsphere-lab/","title":"Terraform Lab 1 - Terraform on VSphere","text":""},{"location":"terraform/terraform-vsphere-lab/#intro","title":"Intro","text":"<p>In this lab we will be covering the basics of Terraform infrastructure management using the VSphere environment provisioned for the Bootcamp, and introduce remote state management. We will be using the Terraform VSphere provider to manage Virtual Machines.</p>"},{"location":"terraform/terraform-vsphere-lab/#prerequisites","title":"Prerequisites","text":""},{"location":"terraform/terraform-vsphere-lab/#download-terraform","title":"Download Terraform","text":""},{"location":"terraform/terraform-vsphere-lab/#linux-bastion","title":"Linux Bastion","text":"<pre><code>sudo yum install -y yum-utils\nsudo yum-config-manager --add-repo https://rpm.releases.hashicorp.com/RHEL/hashicorp.repo\nsudo yum -y install terraform\n</code></pre>"},{"location":"terraform/terraform-vsphere-lab/#macos","title":"MacOS","text":"<pre><code>brew tap hashicorp/tap\nbrew install hashicorp/tap/terraform\n</code></pre> <p>You should then be able to run the following:</p> <pre><code>terraform version\n</code></pre> <p>Output:</p> <pre><code>Terraform v1.10.5\non darwin_arm64\n</code></pre>"},{"location":"terraform/terraform-vsphere-lab/#create-your-terraform-module","title":"Create your terraform module","text":"<p>Create a folder for your project and within it create <code>main.tf</code> , <code>variables.tf</code> and <code>outputs.tf</code> files</p> <p>In <code>main.tf</code> define the following provider block:</p> <pre><code>terraform {\n  required_providers {\n    vsphere = {\n      source = \"hashicorp/vsphere\"\n      version = \"2.11.1\"\n    }\n  }\n}\n\nprovider \"vsphere\" {\n  user                 = var.vsphere_user\n  password             = var.vsphere_password\n  vsphere_server       = var.vsphere_server\n  allow_unverified_ssl = var.vsphere_allow_unverified_ssl\n  api_timeout          = var.vsphere_api_timeout\n}\n</code></pre> <p>Using the <code>terraform</code> command, initialise your provider and prepare your working directory.</p> <pre><code>Initializing the backend...\nInitializing provider plugins...\n- Finding hashicorp/vsphere versions matching \"2.11.1\"...\n- Installing hashicorp/vsphere v2.11.1...\n- Installed hashicorp/vsphere v2.11.1 (signed by HashiCorp)\nTerraform has created a lock file .terraform.lock.hcl to record the provider\nselections it made above. Include this file in your version control repository\nso that Terraform can guarantee to make the same selections by default when\nyou run \"terraform init\" in the future.\n\nTerraform has been successfully initialized!\n\nYou may now begin working with Terraform. Try running \"terraform plan\" to see\nany changes that are required for your infrastructure. All Terraform commands\nshould now work.\n\nIf you ever set or change modules or backend configuration for Terraform,\nrerun this command to reinitialize your working directory. If you forget, other\ncommands will detect it and remind you to do so if necessary.\n</code></pre> <p>Declare the following variables in <code>variables.tf</code>:</p> <ul> <li>vsphere_user as a string with a sensitive value of <code>true</code></li> <li>vsphere_password as a string with a sensitive value of <code>true</code></li> <li>vsphere_server as a string with a sensitive value of <code>true</code></li> <li>vsphere_allow_unverified_ssl as a bool with a default value of <code>true</code> and sensitive value of <code>false</code></li> <li>vsphere_api_timeout as a number with a default value of <code>10</code> and sensitive value of <code>false</code></li> </ul> <p>We can now refactor <code>main.tf</code>:</p> <pre><code>terraform {\n  required_providers {\n    vsphere = {\n      source = \"hashicorp/vsphere\"\n      version = \"2.11.1\"\n    }\n  }\n}\n\nprovider \"vsphere\" {\n  user                 = var.vsphere_user\n  password             = var.vsphere_password\n  vsphere_server       = var.vsphere_server\n  allow_unverified_ssl  = var.vsphere_allow_unverified_ssl\n  api_timeout          = var.vsphere_api_timeout\n}\n</code></pre> <p>Using the <code>terraform</code> command, show the changes to the Terraform Plan</p> <p>Notice how terraform will now ask you to provide these values at runtime. You can store variables locally in a <code>terraform.tfvars</code> file:</p> <pre><code>vsphere_user=\"user from vmware-ipi.yaml\"\nvsphere_password=\"password from vmware-ipi.yaml\"\nvsphere_server=\"ocpgym-vc.techzone.ibm.local\"\n</code></pre> <p>Re-run a plan and notice that terraform will pick up those variables.</p> <p><code>terraform.tfvars</code> file</p> <p>Always treat your <code>terraform.tfvars</code> file as sensitive. Never commit it to git.</p> <p>Add some data sources:</p> <pre><code>data \"vsphere_datacenter\" \"datacenter\" {\n  name = var.vsphere_datacenter_name\n}\n\ndata \"vsphere_datastore\" \"datastore\" {\n  name          = \"${var.vsphere_environment_id}-storage\"\n  datacenter_id = data.vsphere_datacenter.datacenter.id\n}\n\ndata \"vsphere_compute_cluster\" \"cluster\" {\n  name          = var.vsphere_compute_cluster_name\n  datacenter_id = data.vsphere_datacenter.datacenter.id\n}\n\ndata \"vsphere_network\" \"network\" {\n  name          = \"${var.vsphere_environment_id}-segment\"\n  datacenter_id = data.vsphere_datacenter.datacenter.id\n}\n</code></pre> <p>Set some more variables.</p> <ul> <li>vsphere_datacenter_name as a string with a default value of <code>IBMCloud</code></li> <li>vsphere_environment_id as a string with a default value available from your <code>vmware-ipi.yaml</code> file</li> <li>vsphere_compute_cluster_name as a string with a default value of <code>ocp-gym</code></li> </ul> <p>Run another terraform plan. Notice how terraform will import your data sources:</p> <pre><code>data.vsphere_datacenter.datacenter: Reading...\ndata.vsphere_datacenter.datacenter: Read complete after 2s [id=datacenter-1001]\ndata.vsphere_network.network: Reading...\ndata.vsphere_datastore.datastore: Reading...\ndata.vsphere_compute_cluster.cluster: Reading...\ndata.vsphere_network.network: Read complete after 1s [id=dvportgroup-119828]\ndata.vsphere_datastore.datastore: Read complete after 1s [id=datastore-119829]\ndata.vsphere_compute_cluster.cluster: Read complete after 1s [id=domain-c1006]\n[id=421d2d34-adee-ff32-4276-876c43bcd39f]\n\nNo changes. Your infrastructure matches the configuration.\n\nTerraform has compared your real infrastructure against your configuration and found no differences, so no changes are needed.\n</code></pre> <p>Let's create some resources. Start by adding the following resource block:</p> <pre><code>locals {\n  vsphere_folder = \"${var.vsphere_compute_cluster_name}/${var.vsphere_environment_id}\"\n}\n\nresource \"vsphere_virtual_machine\" \"vm\" {\n  count                      = var.vsphere_vm_count\n  name                       = \"${var.vsphere_vm_name_prefix}-${count.index + 1}\"\n  resource_pool_id           = data.vsphere_compute_cluster.cluster.resource_pool_id\n  datastore_id               = data.vsphere_datastore.datastore.id\n  num_cpus                   = var.vsphere_vm_cpus\n  memory                     = var.vsphere_vm_memory\n  guest_id                   = var.vsphere_vm_guest_os_id\n  folder                     = local.vsphere_folder\n  firmware                   = var.vsphere_vm_firmware\n  wait_for_guest_ip_timeout  = var.vsphere_vm_wait_for_guest_ip_timeout\n  wait_for_guest_net_timeout = var.vsphere_vm_wait_for_guest_net_timeout\n\n  network_interface {\n    network_id   = data.vsphere_network.network.id\n    adapter_type = var.vsphere_vm_nic_adapter_type\n  }\n  disk {\n    label = \"${var.vsphere_vm_name_prefix}-${count.index + 1}-hard-disk\"\n    size  = var.vsphere_vm_disk_size\n  }\n}\n</code></pre> <p>Add the following variables.</p> <ul> <li>vsphere_vm_count as a number with a default value of <code>2</code></li> <li>vsphere_vm_name_prefix as a string with a default value of <code>terraform-lab-vm</code></li> <li>vsphere_vm_cpus as a number with a default value of <code>2</code></li> <li>vsphere_vm_memory as a number with a default value of <code>2048</code></li> <li>vsphere_vm_firmware as a string with a default value of <code>efi</code></li> <li>vsphere_vm_nic_adapter_type as a string with a default value of <code>vmxnet3</code></li> <li>vsphere_vm_disk_size as a number with a default value of <code>20</code></li> <li>vsphere_vm_guest_os_id as a string with a default value of <code>rhel9_64Guest</code></li> <li>vsphere_vm_wait_for_guest_ip_timeout as a number with a default value of <code>0</code></li> <li>vsphere_vm_wait_for_guest_net_timeout as a number with a default value of <code>0</code></li> </ul> <p>Run <code>terraform</code> Plan again. You will notice that terraform will figure out that some resources need to be created:</p> <pre><code>Terraform used the selected providers to generate the following execution plan. Resource actions\nare indicated with the following symbols:\n  + create\n\nTerraform will perform the following actions:\n\n  # vsphere_virtual_machine.vm[0] will be created\n  + resource \"vsphere_virtual_machine\" \"vm\" {\n      + annotation                              = (known after apply)\n      + boot_retry_delay                        = 10000\n      + change_version                          = (known after apply)\n      + cpu_limit                               = -1\n      + cpu_share_count                         = (known after apply)\n      + cpu_share_level                         = \"normal\"\n      + datastore_id                            = \"datastore-119829\"\n      + default_ip_address                      = (known after apply)\n      + ept_rvi_mode                            = (known after apply)\n      + extra_config_reboot_required            = true\n      + firmware                                = \"efi\"\n      + folder                                  = \"ocp-gym/gym-667000bg6b-56d449jt\"\n      + force_power_off                         = true\n      + guest_id                                = \"rhel9_64Guest\"\n      + guest_ip_addresses                      = (known after apply)\n      + hardware_version                        = (known after apply)\n      + host_system_id                          = (known after apply)\n      + hv_mode                                 = (known after apply)\n      + id                                      = (known after apply)\n      + ide_controller_count                    = 2\n      + imported                                = (known after apply)\n      + latency_sensitivity                     = \"normal\"\n      + memory                                  = 2048\n      + memory_limit                            = -1\n      + memory_share_count                      = (known after apply)\n      + memory_share_level                      = \"normal\"\n      + migrate_wait_timeout                    = 30\n      + moid                                    = (known after apply)\n      + name                                    = \"terraform-lab-vm-1\"\n      + num_cores_per_socket                    = 1\n      + num_cpus                                = 2\n      + nvme_controller_count                   = 0\n      + power_state                             = (known after apply)\n      + poweron_timeout                         = 300\n      + reboot_required                         = (known after apply)\n      + resource_pool_id                        = \"resgroup-1007\"\n      + run_tools_scripts_after_power_on        = true\n      + run_tools_scripts_after_resume          = true\n      + run_tools_scripts_before_guest_shutdown = true\n      + run_tools_scripts_before_guest_standby  = true\n      + sata_controller_count                   = 0\n      + scsi_bus_sharing                        = \"noSharing\"\n      + scsi_controller_count                   = 1\n      + scsi_type                               = \"pvscsi\"\n      + shutdown_wait_timeout                   = 3\n      + storage_policy_id                       = (known after apply)\n      + swap_placement_policy                   = \"inherit\"\n      + sync_time_with_host                     = true\n      + tools_upgrade_policy                    = \"manual\"\n      + uuid                                    = (known after apply)\n      + vapp_transport                          = (known after apply)\n      + vmware_tools_status                     = (known after apply)\n      + vmx_path                                = (known after apply)\n      + wait_for_guest_ip_timeout               = 0\n      + wait_for_guest_net_routable             = true\n      + wait_for_guest_net_timeout              = 0\n\n      + disk {\n          + attach            = false\n          + controller_type   = \"scsi\"\n          + datastore_id      = \"&lt;computed&gt;\"\n          + device_address    = (known after apply)\n          + disk_mode         = \"persistent\"\n          + disk_sharing      = \"sharingNone\"\n          + eagerly_scrub     = false\n          + io_limit          = -1\n          + io_reservation    = 0\n          + io_share_count    = 0\n          + io_share_level    = \"normal\"\n          + keep_on_remove    = false\n          + key               = 0\n          + label             = \"terraform-lab-vm-1-hard-disk\"\n          + path              = (known after apply)\n          + size              = 20\n          + storage_policy_id = (known after apply)\n          + thin_provisioned  = true\n          + unit_number       = 0\n          + uuid              = (known after apply)\n          + write_through     = false\n        }\n\n      + network_interface {\n          + adapter_type          = \"vmxnet3\"\n          + bandwidth_limit       = -1\n          + bandwidth_reservation = 0\n          + bandwidth_share_count = (known after apply)\n          + bandwidth_share_level = \"normal\"\n          + device_address        = (known after apply)\n          + key                   = (known after apply)\n          + mac_address           = (known after apply)\n          + network_id            = \"dvportgroup-119828\"\n        }\n    }\n\n  # vsphere_virtual_machine.vm[1] will be created\n  + resource \"vsphere_virtual_machine\" \"vm\" {\n      + annotation                              = (known after apply)\n      + boot_retry_delay                        = 10000\n      + change_version                          = (known after apply)\n      + cpu_limit                               = -1\n      + cpu_share_count                         = (known after apply)\n      + cpu_share_level                         = \"normal\"\n      + datastore_id                            = \"datastore-119829\"\n      + default_ip_address                      = (known after apply)\n      + ept_rvi_mode                            = (known after apply)\n      + extra_config_reboot_required            = true\n      + firmware                                = \"efi\"\n      + folder                                  = \"ocp-gym/gym-667000bg6b-56d449jt\"\n      + force_power_off                         = true\n      + guest_id                                = \"rhel9_64Guest\"\n      + guest_ip_addresses                      = (known after apply)\n      + hardware_version                        = (known after apply)\n      + host_system_id                          = (known after apply)\n      + hv_mode                                 = (known after apply)\n      + id                                      = (known after apply)\n      + ide_controller_count                    = 2\n      + imported                                = (known after apply)\n      + latency_sensitivity                     = \"normal\"\n      + memory                                  = 2048\n      + memory_limit                            = -1\n      + memory_share_count                      = (known after apply)\n      + memory_share_level                      = \"normal\"\n      + migrate_wait_timeout                    = 30\n      + moid                                    = (known after apply)\n      + name                                    = \"terraform-lab-vm-2\"\n      + num_cores_per_socket                    = 1\n      + num_cpus                                = 2\n      + nvme_controller_count                   = 0\n      + power_state                             = (known after apply)\n      + poweron_timeout                         = 300\n      + reboot_required                         = (known after apply)\n      + resource_pool_id                        = \"resgroup-1007\"\n      + run_tools_scripts_after_power_on        = true\n      + run_tools_scripts_after_resume          = true\n      + run_tools_scripts_before_guest_shutdown = true\n      + run_tools_scripts_before_guest_standby  = true\n      + sata_controller_count                   = 0\n      + scsi_bus_sharing                        = \"noSharing\"\n      + scsi_controller_count                   = 1\n      + scsi_type                               = \"pvscsi\"\n      + shutdown_wait_timeout                   = 3\n      + storage_policy_id                       = (known after apply)\n      + swap_placement_policy                   = \"inherit\"\n      + sync_time_with_host                     = true\n      + tools_upgrade_policy                    = \"manual\"\n      + uuid                                    = (known after apply)\n      + vapp_transport                          = (known after apply)\n      + vmware_tools_status                     = (known after apply)\n      + vmx_path                                = (known after apply)\n      + wait_for_guest_ip_timeout               = 0\n      + wait_for_guest_net_routable             = true\n      + wait_for_guest_net_timeout              = 0\n\n      + disk {\n          + attach            = false\n          + controller_type   = \"scsi\"\n          + datastore_id      = \"&lt;computed&gt;\"\n          + device_address    = (known after apply)\n          + disk_mode         = \"persistent\"\n          + disk_sharing      = \"sharingNone\"\n          + eagerly_scrub     = false\n          + io_limit          = -1\n          + io_reservation    = 0\n          + io_share_count    = 0\n          + io_share_level    = \"normal\"\n          + keep_on_remove    = false\n          + key               = 0\n          + label             = \"terraform-lab-vm-2-hard-disk\"\n          + path              = (known after apply)\n          + size              = 20\n          + storage_policy_id = (known after apply)\n          + thin_provisioned  = true\n          + unit_number       = 0\n          + uuid              = (known after apply)\n          + write_through     = false\n        }\n\n      + network_interface {\n          + adapter_type          = \"vmxnet3\"\n          + bandwidth_limit       = -1\n          + bandwidth_reservation = 0\n          + bandwidth_share_count = (known after apply)\n          + bandwidth_share_level = \"normal\"\n          + device_address        = (known after apply)\n          + key                   = (known after apply)\n          + mac_address           = (known after apply)\n          + network_id            = \"dvportgroup-119828\"\n        }\n    }\n\nPlan: 2 to add, 0 to change, 0 to destroy.\n</code></pre> <p>Create the resource using the <code>terraform</code> command:</p> <pre><code>sphere_virtual_machine.vm[1]: Creating...\nvsphere_virtual_machine.vm[0]: Creating...\nvsphere_virtual_machine.vm[1]: Still creating... [10s elapsed]\nvsphere_virtual_machine.vm[0]: Still creating... [10s elapsed]\nvsphere_virtual_machine.vm[0]: Creation complete after 13s [id=421d6005-105b-caf6-3c55-a39b81ec57ad]\nvsphere_virtual_machine.vm[1]: Creation complete after 15s [id=421d1278-4682-8bc9-e2e1-765f6ebecd1d]\n\nApply complete! Resources: 2 added, 0 changed, 0 destroyed.\n</code></pre> <p>In <code>variables.tf</code> set <code>vsphere_vm_count</code> to <code>3</code>. Re-run the <code>terraform</code> command in the previous step.</p> <pre><code>Terraform used the selected providers to generate the following execution plan. Resource actions\nare indicated with the following symbols:\n  + create\n\nTerraform will perform the following actions:\n\n  # vsphere_virtual_machine.vm[2] will be created\n  + resource \"vsphere_virtual_machine\" \"vm\" {\n      + annotation                              = (known after apply)\n      + boot_retry_delay                        = 10000\n      + change_version                          = (known after apply)\n      + cpu_limit                               = -1\n      + cpu_share_count                         = (known after apply)\n      + cpu_share_level                         = \"normal\"\n      + datastore_id                            = \"datastore-119829\"\n      + default_ip_address                      = (known after apply)\n      + ept_rvi_mode                            = (known after apply)\n      + extra_config_reboot_required            = true\n      + firmware                                = \"efi\"\n      + folder                                  = \"ocp-gym/gym-667000bg6b-56d449jt\"\n      + force_power_off                         = true\n      + guest_id                                = \"rhel9_64Guest\"\n      + guest_ip_addresses                      = (known after apply)\n      + hardware_version                        = (known after apply)\n      + host_system_id                          = (known after apply)\n      + hv_mode                                 = (known after apply)\n      + id                                      = (known after apply)\n      + ide_controller_count                    = 2\n      + imported                                = (known after apply)\n      + latency_sensitivity                     = \"normal\"\n      + memory                                  = 2048\n      + memory_limit                            = -1\n      + memory_share_count                      = (known after apply)\n      + memory_share_level                      = \"normal\"\n      + migrate_wait_timeout                    = 30\n      + moid                                    = (known after apply)\n      + name                                    = \"terraform-lab-vm-3\"\n      + num_cores_per_socket                    = 1\n      + num_cpus                                = 2\n      + nvme_controller_count                   = 0\n      + power_state                             = (known after apply)\n      + poweron_timeout                         = 300\n      + reboot_required                         = (known after apply)\n      + resource_pool_id                        = \"resgroup-1007\"\n      + run_tools_scripts_after_power_on        = true\n      + run_tools_scripts_after_resume          = true\n      + run_tools_scripts_before_guest_shutdown = true\n      + run_tools_scripts_before_guest_standby  = true\n      + sata_controller_count                   = 0\n      + scsi_bus_sharing                        = \"noSharing\"\n      + scsi_controller_count                   = 1\n      + scsi_type                               = \"pvscsi\"\n      + shutdown_wait_timeout                   = 3\n      + storage_policy_id                       = (known after apply)\n      + swap_placement_policy                   = \"inherit\"\n      + sync_time_with_host                     = true\n      + tools_upgrade_policy                    = \"manual\"\n      + uuid                                    = (known after apply)\n      + vapp_transport                          = (known after apply)\n      + vmware_tools_status                     = (known after apply)\n      + vmx_path                                = (known after apply)\n      + wait_for_guest_ip_timeout               = 0\n      + wait_for_guest_net_routable             = true\n      + wait_for_guest_net_timeout              = 0\n\n      + disk {\n          + attach            = false\n          + controller_type   = \"scsi\"\n          + datastore_id      = \"&lt;computed&gt;\"\n          + device_address    = (known after apply)\n          + disk_mode         = \"persistent\"\n          + disk_sharing      = \"sharingNone\"\n          + eagerly_scrub     = false\n          + io_limit          = -1\n          + io_reservation    = 0\n          + io_share_count    = 0\n          + io_share_level    = \"normal\"\n          + keep_on_remove    = false\n          + key               = 0\n          + label             = \"terraform-lab-vm-3-hard-disk\"\n          + path              = (known after apply)\n          + size              = 20\n          + storage_policy_id = (known after apply)\n          + thin_provisioned  = true\n          + unit_number       = 0\n          + uuid              = (known after apply)\n          + write_through     = false\n        }\n\n      + network_interface {\n          + adapter_type          = \"vmxnet3\"\n          + bandwidth_limit       = -1\n          + bandwidth_reservation = 0\n          + bandwidth_share_count = (known after apply)\n          + bandwidth_share_level = \"normal\"\n          + device_address        = (known after apply)\n          + key                   = (known after apply)\n          + mac_address           = (known after apply)\n          + network_id            = \"dvportgroup-119828\"\n        }\n    }\n\nPlan: 1 to add, 0 to change, 0 to destroy.\n\nChanges to Outputs:\n  + vsphere_vm_id    = [\n      + (known after apply),\n      + (known after apply),\n      + (known after apply),\n    ]\n  + vsphere_vm_power_state = [\n      + (known after apply),\n      + (known after apply),\n      + (known after apply),\n    ]\n</code></pre> <p>Apply those changes using <code>terraform</code>:</p> <pre><code>Apply complete! Resources: 1 added, 0 changed, 0 destroyed.\n\nOutputs:\n\nvsphere_vm_id = [\n  \"421d3b40-1360-ecb9-275f-80ed5d6283f0\",\n  \"421daa51-c028-0aac-2a44-9edf8f31cbae\",\n  \"421d7dc4-7c03-591b-cba4-089c9bc5f3cf\",\n]\nvsphere_vm_power_state = [\n  \"on\",\n  \"on\",\n  \"on\",\n]\n</code></pre> <p>Notice how terraform creates a <code>terraform.tfstate</code> file:</p> <pre><code>cat terraform.tfstate\n</code></pre> <pre><code>{\n  \"version\": 4,\n  \"terraform_version\": \"1.10.5\",\n  \"serial\": 70,\n  \"lineage\": \"2400f423-b4ee-7989-967a-643388b55c23\",\n  \"outputs\": {\n    \"vsphere_vm_id\": {\n      \"value\": [\n        \"421d72a8-5be5-71da-6aa6-57295cc9e008\",\n        \"421d4bb3-9566-b7e6-6a46-f0c780253eca\",\n  ...\n</code></pre> <p>Terraform uses this state to keep track of what resources it is managing and their configuration. By having this file on your machine terraform is able manage these resources, however no one else will be able to do so.</p>"},{"location":"terraform/terraform-vsphere-lab/#setting-up-remote-state","title":"Setting up Remote State","text":"<p>Remote state enables you to securely share state between team members. There are various way to do so - for this lab we will be using Terraform cloud. </p> <p>Navigate to: Terraform Cloud</p> <p>If you do not have a free account, create one. Create an organisation:</p> <p></p> <p>Create an API Key for Terraform:</p> <p></p> <p>Create a <code>~/.terraformrc</code> file to store your credentials:</p> <pre><code>touch ~/.terraformrc\n</code></pre> <pre><code>credentials \"app.terraform.io\" {\n    token = \"mhVn15hHLylFvQ.atlasv1.jAH...\"\n}\n</code></pre> <p>Add a remote provider to your terraform module. In <code>main.tf</code>, reconfigure the <code>terraform</code> block:</p> <pre><code>terraform {\n    backend \"remote\" {\n        organization = \"terraform-vpshere-lab\"\n        workspaces {\n            name = \"rhel-vm-deployment\"\n        }\n    }\n}\n</code></pre> <p>Re initialise the module. Terraform will be aware of the new remote provider:</p> <pre><code>Initializing the backend...\nDo you want to copy existing state to the new backend?\n  Pre-existing state was found while migrating the previous \"local\" backend to the\n  newly configured \"remote\" backend. No existing state was found in the newly\n  configured \"remote\" backend. Do you want to copy this state to the new \"remote\"\n  backend? Enter \"yes\" to copy and \"no\" to start with an empty state.\n\n  Enter a value: yes\n\n\nSuccessfully configured the backend \"remote\"! Terraform will automatically\nuse this backend unless the backend configuration changes.\nInitializing provider plugins...\n- Reusing previous version of hashicorp/vsphere from the dependency lock file\n- Using previously-installed hashicorp/vsphere v2.11.1\n\nTerraform has been successfully initialized!\n\nYou may now begin working with Terraform. Try running \"terraform plan\" to see\nany changes that are required for your infrastructure. All Terraform commands\nshould now work.\n\nIf you ever set or change modules or backend configuration for Terraform,\nrerun this command to reinitialize your working directory. If you forget, other\ncommands will detect it and remind you to do so if necessary.\n</code></pre> <p>Navigate back to Terraform Cloud. Notice how a new workspace has been created containing the resources managed by Terraform:</p> <p></p> <p>On Terraform Cloud you can safely collaborate with others on a Terraform project, controlling who has access to manage resources with Terraform. </p>"},{"location":"terraform/terraform-vsphere-lab/#clean-up","title":"Clean up","text":"<p>Use the appropriate <code>terraform</code> command to clean up all deployed resources.</p> <p>You have successfully completed the Terraform VSphere Lab!</p> <p>Extra Challenge </p> <p>You might have noticed that the RHEL VMs we have created are not able to connect to the network. As an extra challenge configure network connectivity in the RHEL VM, and save it as a templates. You can consider manually creating a template to deploy your VM. You could look at using Packer to manage your templates.</p>"},{"location":"vault/","title":"Installing Vault Enterprise on OpenShift","text":""},{"location":"vault/#installing-vault-enterprise-on-openshift","title":"Installing Vault Enterprise on OpenShift","text":"<ul> <li>Installing Vault Enterprise on OpenShift</li> <li>Requirements</li> <li>Preparing and Deploying the Vault Helm Chart</li> <li>Unsealing Vault</li> <li>Accessing the Vault UI</li> <li>Setting up Vault Enterprise (Optional)<ul> <li>Expose Vault publicly via a Route</li> <li>Set Up Simple User credentials for a Vault Admin user</li> </ul> </li> </ul> <p>This is a step to step guide to deploy Vault Enterprise on an OpenShift cluster:</p> <p></p> <ul> <li>Vault Enterprise is deployed in HA mode across 3 pods using raft storage</li> <li>All vault resources are contained in the vault-enterprise namespace</li> </ul>"},{"location":"vault/#requirements","title":"Requirements","text":"<ul> <li> <p>The following CLI tools are required to follow this guide:</p> </li> <li> <p><code>helm</code></p> </li> <li> <p><code>oc</code>. </p> <p>This guide assumes the <code>oc</code> cli is logged in to the target Cluster and has <code>ClusterAdmin</code> privileges</p> </li> <li> <p>Vault Enterprise Licences are available to IBMers, find more information here. Download it to your local machine</p> </li> </ul>"},{"location":"vault/#preparing-and-deploying-the-vault-helm-chart","title":"Preparing and Deploying the Vault Helm Chart","text":"<ul> <li>Download the Hashicorp Vault Helm chart:</li> </ul> <pre><code>helm repo add hashicorp https://helm.releases.hashicorp.com\n</code></pre> <ul> <li>Update all local helm repositories:</li> </ul> <pre><code>helm repo update\n</code></pre> <ul> <li>Create a namespace for vault enterprise:</li> </ul> <pre><code>oc new-project vault-enterprise\n</code></pre> <ul> <li> <p>Download the Vault Enterprise License from the link provided in the prerequisites. Save it as <code>vault-licence.hclic</code> file. Store it as an environment variable: </p><pre><code>secret=$(cat &lt;path-to-licence&gt;/vault-licence.hclic) &amp;&amp; \\\necho $secret\n</code></pre><p></p> </li> <li> <p>Store it as a secret in the cluster:</p> </li> </ul> <pre><code>oc create secret generic vault-ent-license -n vault-enterprise --from-literal=\"license=${secret}\"\n</code></pre> <ul> <li>Save the following file as a <code>values.yaml</code> file: <pre><code>global:\n  namespace: vault-enterprise\n  openshift: true\nserver:\n  dataStorage:\n    storageClass: ocs-storagecluster-ceph-rbd\n  image:\n    repository: \"hashicorp/vault-enterprise\"\n    tag: \"1.20.0-ent\"\n  ha:\n    enabled: true\n    replicas: 3\n    raft:\n      enabled: true\n  enterpriseLicense:\n    secretName: vault-ent-license\n</code></pre> <ul> <li>review <code>server.dataStorage.storageClass</code> and ensure it is set to a block StorageClass available on your cluster</li> <li>Review the image tag matches the version of Vault Enterprise, <code>1.20</code> is the latest version at the time of writing.</li> </ul> </li> </ul> <p>Download values.yaml</p> <ul> <li>Once happy with the <code>values.yaml</code> file install the helm chart:</li> </ul> <pre><code>helm install vault-enterprise hashicorp/vault -f vault-enterprise/values.yaml \n</code></pre> Output <pre><code>NAME: vault-enterprise\nLAST DEPLOYED: Mon Jul 14 15:21:36 2025\nNAMESPACE: vault-enterprise\nSTATUS: deployed\nREVISION: 1\nNOTES:\nThank you for installing HashiCorp Vault!\n\nNow that you have deployed Vault, you should look over the docs on using\nVault with Kubernetes available here:\n\nhttps://developer.hashicorp.com/vault/docs\n\n\nYour release is named vault-enterprise. To learn more about the release, try:\n\n  $ helm status vault-enterprise\n  $ helm get manifest vault-enterprise\n</code></pre> <ul> <li>Check Vault pods have deployed successfully:</li> </ul> <pre><code>oc get pods -n vault-enterprise\n</code></pre> Output <pre><code>NAME                                               READY   STATUS    RESTARTS   AGE\nvault-enterprise-0                                 0/1     Running   0          2m\nvault-enterprise-1                                 0/1     Running   0          2m\nvault-enterprise-2                                 0/1     Running   0          2m\nvault-enterprise-agent-injector-648bf45b79-n4lm2   1/1     Running   0          2m\n</code></pre> <p>Notice how the Vault pods are not ready. This is because Vault needs to be unsealed. </p>"},{"location":"vault/#unsealing-vault","title":"Unsealing Vault","text":"<p>Vault's raft storage elects one node (pod) as a leader, and all 3 pods as voters. In practice, only one pod (the leader) will be writing to vault, and the other 2 will be read only. </p> <ul> <li>Initialize vault on the leader pod:</li> </ul> <pre><code>oc exec -n vault-enterprise vault-enterprise-0 -- vault operator init\n</code></pre> Output <pre><code>[output omitted for brevity]\nUnseal Key 1: SzRXts...\nUnseal Key 2: 9Y1frb...\nUnseal Key 3: xAnzn1...\nUnseal Key 4: dkn7nu...\nUnseal Key 5: Ea6vf8...\n\nInitial Root Token: hvs.uZ...\n\nVault initialized with 5 key shares and a key threshold of * Please securely\ndistribute the key shares printed above. When the Vault is re-sealed,\nrestarted, or stopped, you must supply at least 3 of these keys to unseal it\nbefore it can start servicing requests.\n\nVault does not store the generated root key. Without at least 3 keys to\nreconstruct the root key, Vault will remain permanently sealed!\n\nIt is possible to generate new unseal keys, provided you have a quorum of\nexisting unseal keys shares. See \"vault operator rekey\" for more information.\n</code></pre> <p>Note down these values as they are needed to unseal vault.</p> <p>One of the limitations of this install method is if any the vault pods are restarted, the need to be unsealed again.</p> <ul> <li>Repeat the following step 3 times, passing 3 different unseal keys to successfully unseal vault:</li> </ul> <p></p><pre><code>oc exec -it -n vault-enterprise vault-enterprise-0 -- vault operator unseal\n</code></pre> * Enter the unseal key when prompted:<p></p> <pre><code>Unseal Key (will be hidden): \n</code></pre> <ul> <li>Once unsealed, run the following command:</li> </ul> <pre><code>oc exec -it -n vault-enterprise vault-enterprise-0 -- vault status  \n</code></pre> Output <pre><code>Key                     Value\n---                     -----\nSeal Type               shamir\nInitialized             true\nSealed                  false\nTotal Shares            5\nThreshold               3\nVersion                 1.20.0+ent\nBuild Date              2025-06-23T11:04:16Z\nStorage Type            raft\n[output ommitted for brevity]\n</code></pre> <ul> <li>The first vault pod is initialized and unsealed! You should now see its status as ready on OpenShift:</li> </ul> <pre><code>oc get pods -n vault-enterprise\n</code></pre> <pre><code>NAME                                               READY   STATUS    RESTARTS   AGE\nvault-enterprise-0                                 1/1     Running   0          5m\nvault-enterprise-1                                 0/1     Running   0          5m\nvault-enterprise-2                                 0/1     Running   0          5m\nvault-enterprise-agent-injector-648bf45b79-n4lm2   1/1     Running   0          5m\n</code></pre> <ul> <li>Unseal <code>vault-enterprise-1</code>. Start by joining it to the cluster:</li> </ul> <pre><code>oc exec -n vault-enterprise vault-enterprise-1 -- vault operator raft join http://vault-enterprise-0.vault-enterprise-internal:8200\n</code></pre> <ul> <li>Repeat the following step 3 times, passing 3 different unseal keys to successfully unseal vault:</li> </ul> <p></p><pre><code>oc exec -it -n vault-enterprise vault-enterprise-1 -- vault operator unseal\n</code></pre> * Enter the unseal key when prompted:<p></p> <pre><code>Unseal Key (will be hidden): \n</code></pre> <ul> <li>Once unsealed, run the following command:</li> </ul> <pre><code>oc exec -n vault-enterprise vault-enterprise-1 -- vault status  \n</code></pre> Output <pre><code>Key                                    Value\n---                                    -----\nSeal Type                              shamir\nInitialized                            true\nSealed                                 false\nTotal Shares                           5\nThreshold                              3\nVersion                                1.20.0+ent\nBuild Date                             2025-06-23T11:04:16Z\nStorage Type                           raft\nCluster Name                           vault-cluster-89d2dfbb\nCluster ID                             db583f94-0787-1efa-3c47-c2f6d76e1eef\nRemoved From Cluster                   false\nHA Enabled                             true\nHA Cluster                             https://vault-enterprise-0.vault-enterprise-internal:8201\nHA Mode                                standby\nActive Node Address                    http://10.130.1.5:8200\nPerformance Standby Node               true\n[output omitted for brevity]\n</code></pre> <ul> <li>The second vault pod is initialized and unsealed! You should now see its status as ready on OpenShift:</li> </ul> <pre><code>oc get pods -n vault-enterprise\n</code></pre> <pre><code>NAME                                               READY   STATUS    RESTARTS   AGE\nvault-enterprise-0                                 1/1     Running   0          7m\nvault-enterprise-1                                 1/1     Running   0          7m\nvault-enterprise-2                                 0/1     Running   0          7m\nvault-enterprise-agent-injector-648bf45b79-n4lm2   1/1     Running   0          7m\n</code></pre> <ul> <li>Unseal the third and final pod, <code>vault-enterprise-2</code>. Start by joining it to the cluster:</li> </ul> <pre><code>oc exec -n vault-enterprise vault-enterprise-2 -- vault operator raft join http://vault-enterprise-0.vault-enterprise-internal:8200\n</code></pre> <ul> <li>Repeat the following step 3 times, passing 3 different unseal keys to successfully unseal vault:</li> </ul> <p></p><pre><code>oc exec -it -n vault-enterprise vault-enterprise-2 -- vault operator unseal\n</code></pre> * You will be prompted to enter your unseal key:<p></p> <pre><code>Unseal Key (will be hidden): \n</code></pre> <ul> <li>Once unsealed, run the following command:</li> </ul> <pre><code>oc exec -n vault-enterprise vault-enterprise-2 -- vault status  \n</code></pre> Output <pre><code>Key                                    Value\n---                                    -----\nSeal Type                              shamir\nInitialized                            true\nSealed                                 false\nTotal Shares                           5\nThreshold                              3\nVersion                                1.20.0+ent\nBuild Date                             2025-06-23T11:04:16Z\nStorage Type                           raft\nCluster Name                           vault-cluster-89d2dfbb\nCluster ID                             db583f94-0787-1efa-3c47-c2f6d76e1eef\nRemoved From Cluster                   false\nHA Enabled                             true\nHA Cluster                             https://vault-enterprise-0.vault-enterprise-internal:8201\nHA Mode                                standby\nActive Node Address                    http://10.130.1.5:8200\n[output omitted for brevity]\n</code></pre> <ul> <li>The third vault pod is initialized and unsealed! You should now see its status as ready on OpenShift:</li> </ul> <pre><code>oc get pods -n vault-enterprise\n</code></pre> <pre><code>NAME                                               READY   STATUS    RESTARTS   AGE\nvault-enterprise-0                                 1/1     Running   0          10m\nvault-enterprise-1                                 1/1     Running   0          10m\nvault-enterprise-2                                 1/1     Running   0          10m\nvault-enterprise-agent-injector-648bf45b79-n4lm2   1/1     Running   0          10m\n</code></pre>"},{"location":"vault/#accessing-the-vault-ui","title":"Accessing the Vault UI","text":"<ul> <li>The vault system is now fully operational. You can access it from your local machine using the following command:</li> </ul> <pre><code>oc port-forward -n vault-enterprise svc/vault-enterprise 8200:8200\n</code></pre> <ul> <li>Navigate to http://localhost:8200 to start using vault. </li> </ul> <p>The following steps are optional quality of life improvements for your vault deployment</p>"},{"location":"vault/#setting-up-vault-enterprise-optional","title":"Setting up Vault Enterprise (Optional)","text":""},{"location":"vault/#expose-vault-publicly-via-a-route","title":"Expose Vault publicly via a Route","text":"<ul> <li>Run the following command to expose vault via a route:</li> </ul> <pre><code>oc create route edge vault-enterprise -n vault-enterprise --service=vault-enterprise --port=http\n</code></pre> <ul> <li>Vault will be available at the following endpoint:</li> </ul> <pre><code>echo \"https://$(oc get routes - vault-enterprise vault-enterprise -ojsonpath=\\'{..host}\\')\"\n</code></pre>"},{"location":"vault/#set-up-simple-user-credentials-for-a-vault-admin-user","title":"Set Up Simple User credentials for a Vault Admin user","text":"<p>Set up simple use password authentication for vault: </p> <ul> <li>Start by enabling the <code>userpass</code> auth method:</li> </ul> <pre><code>oc exec -n vault-enterprise vault-enterprise-0 -- vault auth enable userpass\n</code></pre> <ul> <li>Create a policy for your user. A very simple admin policy will look like below, although you will likely want to reduce permissions for your admin user:</li> </ul> <pre><code>path \"*\" {\n  capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\", \"sudo\"]\n}\n</code></pre> <p>This enables all capabilities for all paths to a user (i.e. superadmin)</p> <p>Download superadmin.hcl</p> <ul> <li>Save the policy as a file named <code>superadmin.hcl</code>. Copy it onto the vault pod:</li> </ul> <pre><code>oc cp $(pwd)/superadmin.hcl vault-enterprise/vault-enterprise-0:/vault/data/superadmin.hcl\n</code></pre> <ul> <li>Upload the policy to vault:</li> </ul> <pre><code>oc exec -n vault-enterprise vault-enterprise-0 -- vault policy write superadmin /vault/data/superadmin.hcl\n</code></pre> <ul> <li>Create an admin user:</li> </ul> <p></p><pre><code>oc exec -n vault-enterprise vault-enterprise-0 -- vault write auth/userpass/users/admin password=\"SuperSecurePassword\" policies=\"superadmin\"\n</code></pre> * Log in as an admin user using the <code>Userpass</code> method:<p></p> <p></p>"},{"location":"wca-app-dev-mod/","title":"App Development &amp; Modernization with WCA","text":""},{"location":"wca-app-dev-mod/#app-development-modernization-with-wca","title":"App Development &amp; Modernization with WCA","text":"<p>The main objective of this lab is to provide hands-on experience with some of the core features and capabilities available to developers in IBM watsonx Code Assistant (WCA). The lab content is loosely organized to align with the capabilities found in the two different plans (editions) available for WCA:</p> <p>Essentials: This accelerates software development, allowing developers to use WCA with integrated generative AI for coding tasks, including:</p> <ul> <li>Generating code</li> <li>Explaining code</li> <li>Creating unit tests</li> <li>Documenting code</li> </ul> <p>Standard: In addition to all the capabilities found in the Essentials plan, the Standard plan also includes these enterprise Java modernization capabilities:</p> <ul> <li>Java upgrades, regardless of runtime</li> <li>WebSphere to Liberty transformation</li> <li>Enhanced test generation and code explanation</li> </ul> <p>After completing this lab, you will have a deeper understanding of what is possible with WCA and will have observed how it can be a powerful tool to help accelerate software development and application modernization.</p>"},{"location":"wca-app-dev-mod/#prerequisite-installation-steps","title":"Prerequisite Installation Steps","text":"<p>As previously mentioned, there are two separate components that make up IBM watsonx Code Assistant (WCA):</p> <ul> <li>WCA service \u2013 This is the back-end software, leveraging generative AI, that services requests from developers. It is available as software-as-a-service (SaaS) on IBM Cloud and as deployable software for on-premises and cloud deployments.</li> <li>WCA extension (also referred to as a plug-in) \u2013 Installed in an integrated development environment (IDE) on a developer\u2019s own system, this is how the developer interacts with WCA. It includes a chat interface as well as in-source options for generating, explaining, and documenting code. It can also help drive Java upgrades and WebSphere to Liberty transformation, among other things. The IDEs currently supported by WCA are Visual Studio Code (VS Code) and Eclipse.</li> </ul> <p>Tip</p> <p>To avoid the need for you to setup your own development environment (which would include installation of an IDE, the WCA extension, various other software, and sample source code), a pre-configured environment has been made available for you in IBM Technology Zone (TechZone).  Likewise, a WCA service has been made available to you as well, also available through TechZone.  However, this lab assists you in setting up your own system to perform the lab on your own system using the TechZone WCA Service as the back-end.</p>"},{"location":"wca-app-dev-mod/#reserve-the-techzone-environment","title":"Reserve the TechZone Environment","text":"<p>The prerequisite environment may already exist!</p> <p>If you are performing this exercise as part of the bootcamp, the WCA environment may already exist.  Your IBM instructor will give you access to the API Key.  If this is the case you can skip forward to workstation setup</p> <p>Open the collection for WCA-related environments in TechZone. Sign in with your IBMid if prompted.</p> <p>Locate the Request watsonX Code Assistant - Standard (GA) [Approval Gated] tile and click the IBM Cloud environment / Reserve it button (A).   There are multiple tiles listed, some of which have a similar name \u2013 ensure you\u2019re selecting the correct one.</p> <p></p> <p>Accept the default for the reservation Name (A) or provide a name of your choosing. For the Purpose of the reservation, select Education (B).</p> <p></p> <ul> <li>Fill in the Purpose description box with the reason you are making the reservation. Then, select your Preferred Geography based on your desired location.</li> <li>Adjust the reservation\u2019s Start date and time and End date and time as needed (note that you can extend your reservation later if you need more time).</li> <li>In the lower-right corner, follow the links to read TechZone\u2019s terms, conditions, and security policies, and then select the checkbox to agree to them.</li> <li>Click Submit for approval.  </li> <li>A message in the upper-right corner will briefly appear stating that the reservation has been created.</li> </ul> <p>Note</p> <p>You will receive an email from IBM Technology Zone with a subject of \u201cRequest is Pending Approval\u201d. Once it has been approved you will receive another email with a subject of \u201cRequest Approved\u201d.</p> <p>When provisioning starts (based on the start time you provided) an email with a subject of \u201cReservation Provisioning on IBM Technology Zone\u201d is sent. Finally, an email with a subject of \u201cReservation Ready on IBM Technology Zone\u201d indicates that provisioning has completed.</p> <p>While waiting for this reservation to be provisioned, continue on to the next section to request the second environment you\u2019ll need.</p> <p>If you want a developer system from TechZone...</p> <p>If you chose not to use your local system as your development environment follow the below steps.</p> <p>The pre-configured development demonstration environment, which includes the following software and sample data: - Red Hat Enterprise Linux 9 - Visual Studio Code (VS Code) (including the WCA extensions) - Eclipse IDE for Java Developers - OpenJDK 11 &amp; 21 - Sample ModResorts application</p> <p>Follow the steps below to provision your WCA development demonstration environment from TechZone.</p> <ul> <li>Open the collection of WCA-related environments in TechZone. Sign in with your IBMid if prompted.</li> <li>Locate the watsonx Code Assistant - Demonstration VM tile and click the IBM Cloud environment / Reserve it button.  Note: There are multiple tiles listed, some of which have a similar name \u2013 ensure you\u2019re selecting the correct one.</li> </ul> <p>For the reservation type, select the Reserve now radio button.</p> <p>Accept the default for the reservation Name or provide a name of your choosing. For the Purpose of the reservation, select Education.</p> <p>Fill in the Purpose description box with the reason you are making the reservation. Then, select your Preferred Geography based on your desired location. </p> <p>Adjust the reservation\u2019s End date and time as needed (note that you can extend your reservation later if you need more time). Leave VPN Access as Disable.</p> <p>In the lower-right corner, follow the links to read TechZone\u2019s terms, conditions, and security policies, and then select the checkbox to agree to them. Finally, click Submit.</p> <p>A message in the upper-right corner will briefly appear stating that the reservation has been created.  First, you will receive an email from IBM Technology Zone with a subject of \u201cReservation Provisioning on IBM Technology Zone\u201d. Once it has been provisioned, an email with a subject of \u201cReservation Ready on IBM Technology Zone\u201d is sent.</p>"},{"location":"wca-app-dev-mod/#configure-the-techzone-wca-service","title":"Configure the TechZone WCA Service","text":"<p>There are three things that you will need to do before you can use the WCA service:</p> <ol> <li>Join the IBM Cloud account. Your WCA service and other resources have been provisioned for you in this account.</li> <li>Create an API key. This is how you will identify yourself and your WCA service within the WCA extension of your development environment.</li> <li>Create a deployment space. This is the serving environment for WCA.</li> </ol>"},{"location":"wca-app-dev-mod/#join-the-ibm-cloud-account","title":"Join the IBM Cloud Account","text":"<p>Once the environment has been provisioned, it is likely that would have received one additional email asking you to join an IBM Cloud account: \u201cAction required: You are invited to join an account in IBM Cloud\u201d.</p> <p>The WCA instance you are being given access to is associated with one of many accounts that have been setup (they all have a name like itz-watsonx-###). You must join this account before you can use WCA.</p> <p>Joining the Account Without Email</p> <p>Access the TechZone reservation you have for \"watsonX Code Assistant - Standard (GA) [Approval Gated]\" and Open your IBM Cloud Environment.</p> <p>Check your notifications for the invitation that belongs to this recent reservation.</p> <p></p> <p>Choose Join Now, accept the Terms and Conditions and Join Account.</p> <p>Finally, Proceed and switch to this new account within your IBM Cloud environment.</p>"},{"location":"wca-app-dev-mod/#create-an-api-key","title":"Create an API Key","text":"<p>You must do the work in this section from the account mentioned in your reservation (which you joined and switched to in the previous section).  The API key you create here will be used later to configure the WCA VSCode extension from your development environment.</p> <p>Open the Manage dropdown menu (A) and select Access (IAM) (B).</p> <p></p> <p>Select API keys (A) from the left-side menu. Then, click the Create button (B).</p> <p></p> <p>In the Create IBM Cloud API key pop-up window, enter WCA API key for the Name (A) and then click Create (B).</p> <p></p> <p>Copy or Download the new API Key.  You will need this to sign into the VSCode extension you will configure later.</p>"},{"location":"wca-app-dev-mod/#create-a-deployment-space","title":"Create a Deployment Space","text":"<p>From the TechZone account within IBM Cloud:</p> <p>Click the Navigation Menu (A) in the upper-left corner of the page. Then select Resource list (B).</p> <p></p> <p>This displays the list of services and resources that have been provisioned within the account \u2013 and that you have access to. Expand the AI / Machine Learning section (A). In here is your instance of IBM watsonx Code Assistant. Note the name; it will be in the form of itzwca-your unique identifier. Click the name of your WCA instance (B).</p> <p></p> <p>Click the Launch watsonx Code Assistant button at the bottom of the page. If prompted, log in with your IBMid.</p> <p>In the pop-up window, click the Set up button.</p> <p>For the Type of installation, ensure that Single user is selected.</p> <p>Scroll down to the Steps to complete section. Click the blue arrow icon (A) to the right of Create a deployment space.</p> <p></p> <p>In the Create a deployment space pop-up window, specify a Name (A) \u2013 for example, WCA deployment space. This is part of a global namespace, however, and has to be unique. So, if you\u2019re told that what you specified is already in use, change the name to something unique.</p> <p>Next, select your WCA service from the Code assistant service dropdown list (B). Only one service should be listed, and it should match the name of your service that you noted earlier.</p> <p>Then, select your object storage instance from the Select storage service dropdown list (C). There will be many instances listed (belonging to others also using this account), so find the one that starts with <code>itzcos</code> and includes the identifier found in your WCA service name.</p> <p>Finally, click the Create button.</p> <p>After you are informed that the deployment space has been created, click the X in the upper-right corner of the pop-up window to close it.</p> <p>The Create a deployment space step will now be marked as complete.</p> <p>Click the blue arrow icon (A) to the right of Select your license preference for response generation.</p> <p></p> <p>This opens a new browser tab or window that allow you to configure how to display code suggestions. Review the information if you wish, but do not change anything. Simply close the browser tab to return to the page with the Steps to complete section.</p> <p>Manually check the box for Select your license preference for response generations (A) to indicate that this step has been completed. Then, click the Go to home page button (B).</p> <p>You can review the information on the home page if you like (don\u2019t change anything!) but at this point you are done with WCA setup and can close the browser window.</p> <p></p>"},{"location":"wca-app-dev-mod/#workstation-setup","title":"Workstation Setup","text":""},{"location":"wca-app-dev-mod/#java-installation","title":"Java installation","text":"<p>Install Java21 using the applicable link:</p> <ul> <li>Download Java for MacOS - Arm64</li> <li>Download Java for MacOS - x86</li> <li>Download Java for Windows</li> </ul> <p>All the above are compressed files, you can extract them to any folder in your local.</p> <p>Check if Java is installed properly: </p><pre><code>java --version\n</code></pre><p></p> <p>After installing java, add java to <code>PATH</code> variable and set <code>JAVA_HOME</code> envitonment variable</p> For MacFor Windows <p>Open .zshrc or .bash_profile</p> <pre><code>nano ~/.zshrc\n</code></pre> <p>Add the following lines</p> <pre><code>export JAVA_HOME=/Library/Java/JavaVirtualMachines/&lt;java version&gt;/Contents/Home\n</code></pre> <pre><code>export PATH=$JAVA_HOME/bin:$PATH\n</code></pre> <p>Save the file and exit (press CTRL + X, then Y, and hit Enter) and reload the shell configuration so the changes take effect.</p> <pre><code>source ~/.zshrc\n</code></pre> <p>Verify the JAVA_HOME with the following command:</p> <pre><code>echo $JAVA_HOME\n</code></pre> <p>Open Environment variables using windows search bar (search for edit environment variables in the search bar)</p> <p></p> <p>Set JAVA_HOME variable using Environment variables (click on new if you do not have a JAVA_HOME set or click on edit to change the existing JAVA_HOME, and point it to the Java you installed in the earlier steps:</p> <p></p> <pre><code>JAVA_HOME= C:\\Program Files\\Java\\jdk-21\n</code></pre> <p>Add Java to PATH using Environment variables:</p> <p></p> <pre><code>%JAVA_HOME%\\bin\n</code></pre>"},{"location":"wca-app-dev-mod/#install-maven","title":"Install Maven","text":"For MacFor Windows <p>Install maven using homebrew </p><pre><code>brew install maven\n</code></pre><p></p> <p>Check if maven is installed properly:</p> <pre><code>mvn --version\n</code></pre> <p>Visit the official Maven website: Maven Download Page</p> <p>Under \"Files\", click on the binary zip archive link (e.g., apache-maven-x.x.x-bin.zip). </p> <p>Extract the zip file to a location of your choice, e.g., C:\\Apache\\maven.</p> <p>Set MAVEN_HOME variable using Environment variables: </p><pre><code>MAVEN_HOME= &lt;path-to-folder&gt;\\maven\\apache-maven-3.9.9-bin\\apache-maven-3.9.9\n</code></pre><p></p> <p>Add Maven to PATH using Environment variables:  </p><pre><code>&lt;path-to-folder&gt;\\maven\\apache-maven-3.9.9-bin\\apache-maven-3.9.9\\bin\n</code></pre><p></p>"},{"location":"wca-app-dev-mod/#install-vscode","title":"Install VSCode","text":"<p>VSCode Official Website for installation</p>"},{"location":"wca-app-dev-mod/#add-the-watsonx-code-assistant-extension-to-vscode","title":"Add the watsonx Code Assistant Extension to VSCode","text":"<p>Open VSCode and add the watsonx Code Assistant extension from the Visual Studio Code Marketplace.</p> <p>Never installed VSC extensions?</p> <p>The Visual Studio Code team has documented this process in a tutorial here</p> <p>Search for the <code>watsonx Code Assistant</code> extension.</p> <p></p> <p>Choose Install.</p> <p>After adding the extension, you will need to **Sign In* to the WCA Service via following steps:</p> <ul> <li> <p>Login with WCA4EJ API Key at the bottom left corner of VSCode. After successfully signed in, the number indicator should be gone.</p> <p></p> </li> <li> <p>If you encoutner issue during autherization that says \"administrator needs to associate you with a deployment space\", please reach out to IBMers to setup deployment space again for your API Key. </p> <p></p> </li> </ul>"},{"location":"wca-app-dev-mod/#install-the-watsonx-code-assistant-for-enterprise-java-applications","title":"Install the watsonx Code Assistant for Enterprise Java Applications","text":"<p>Open VSCode and add the watsonx Code Assistant for Enterprise Java Applications extension from the Visual Studio Code Marketplace.</p> <p>Search for <code>watsonx Code Assistant for Enterprise Java Applications</code> extension.</p> <p></p> <p>Choose Install.</p>"},{"location":"wca-app-dev-mod/#installing-liberty-tools-and-java-extension","title":"Installing Liberty Tools and Java Extension","text":"<p>Install the Liberty Tools and extension Pack for Java extensions from VSCode marketplace as shown below.</p> <p></p> <p>Next, install the Extension Pack for Java</p> <p></p>"},{"location":"wca-app-dev-mod/#start-using-wca","title":"Start Using WCA","text":"<p>You can verify the code assistant is correctly configured by navigating to the watsonx Code Assistant  chat window and beginning a chat.</p> <p></p>"},{"location":"wca-app-dev-mod/lab-1-explain/","title":"Lab 1 - Explain","text":""},{"location":"wca-app-dev-mod/lab-1-explain/#lab-1-code-explanation","title":"Lab 1 - Code Explanation","text":"<p>This document gives step-by-step guide to finish Lab1. Topics included: </p> <ul> <li>Download recommended code assets.</li> <li>Explore the example <code>modresorts</code> application</li> <li>Explain <code>modresorts</code> application.</li> </ul>"},{"location":"wca-app-dev-mod/lab-1-explain/#code-asset-download","title":"Code Asset Download","text":"<p>Git clone GitHub repository to your location of choice.</p>"},{"location":"wca-app-dev-mod/lab-1-explain/#build-application-project","title":"Build Application Project","text":"<p>Open a terminal, and go to your project folder, and navigate to <code>was_dependency</code> folder. </p><pre><code>cd &lt;your-path&gt;/wca4ej-workshop/modresorts-twas-j8/was_dependency\n</code></pre><p></p> <p>Once inside the folder, run the following command to build project:</p> <pre><code>mvn install:install-file -Dfile=was_public.jar -DpomFile=was_public-9.0.0.pom\n</code></pre> <p>if you get error with pom doesn't exist, you can goto cd /wca4ej-workshop/modresorts-twas-j8 and </p> <pre><code>mvn install:install-file -Dfile=./was_dependency/was_public.jar -DpomFile=./was_dependency/was_public-9.0.0.pom \n</code></pre> <p></p> <p>For Windows</p> <p>For Windows users, you might need to give full path for the build files <code>was_public.jar</code> and <code>was_public-9.0.0.pom</code>.</p> <p></p>"},{"location":"wca-app-dev-mod/lab-1-explain/#view-liberty-app","title":"View Liberty App","text":"<p>After you installed LibertyTools from VSCode marketplace, there should be a Liberty Dashboard section in your explorer. Click <code>Add project to Liberty Dashboard</code> and put the path to the <code>modresort-twas-j8</code> folder (this would be automatic if you open in this project).</p> <p></p> <p>Once you have selected the correct project, a <code>modresrots</code> app will show up. Right click on the app to start.</p> <p></p> <p>VSCode will go through downloading required packages, which you can see in terminal. </p> <p></p> <p>Once the app started, you can get the url (example: <code>http://localhost:9080/resorts/</code>) and open in your browser to view the web app.</p> <p>Important</p> <p>Because we are running this application using Liberty in Java21, while this application is built for WebSphere in Java8, even though the application started successfully, there are 2 places that have error because of this migration + upgrade.</p> <p></p> <p>The first one, if you click the <code>Where to?</code> dropdown and select any location, you will find the location information module showing errors.</p> <p></p> <p>The Second one, the <code>Logout</code> button does not work if you click on it.</p> <p></p> <p>We will fix these errors in the later labs.</p>"},{"location":"wca-app-dev-mod/lab-1-explain/#explain-project-code","title":"Explain Project Code","text":"<p>To understand the whole project, right click on the <code>modresorts-twas-j8</code> folder and select <code>watsonx Code Assistant</code> - <code>Explain Application</code>.</p> <p></p> <p>VSCode will prompt you that the process takes extra time. Click <code>Proceed with code analysis</code>.</p> <p></p> <p>The analysis might take 1-2 minutes to finish and a prompt will show up in the bottom right corner.</p> <p></p> <p>Now we can open the report and read through the details.</p> <p></p>"},{"location":"wca-app-dev-mod/lab-2-modernize/","title":"Lab 2 - Modernize","text":""},{"location":"wca-app-dev-mod/lab-2-modernize/#lab-2-code-modernization","title":"Lab 2 - Code Modernization","text":"<p>This document gives step-by-step guide to finish Lab2. This lab includes the step to Modernize code from WebSphere to Liberty</p>"},{"location":"wca-app-dev-mod/lab-2-modernize/#application-analysis","title":"Application analysis","text":"<p>Right click on the <code>modresorts-twas-j8</code> folder and select <code>watsonx Code Assistant</code> - <code>Modernize to Liberty</code>.</p> <p></p> <p>A <code>Modernize</code> tab will show up. Click <code>Upload migration bundle</code>, and select the file in our code asset <code>modresorts-twas-j8/migration-bundle/modresorts.ear_migrationBundle.zip</code></p> <p></p> <p>Select both files to be included and <code>Proceed</code>.</p> <p></p> <p>After analysis, the report will show you issues with this application to be fixed. 3 of them can be automated, 1 of them needs assistance. For the automated fixes, we click <code>Run automated fixes</code>.</p> <p></p>"},{"location":"wca-app-dev-mod/lab-2-modernize/#apply-fixes","title":"Apply Fixes","text":"<p>For the assisted fixes, you can click the <code>Instructions</code> dropdown to see the step which we will guide you. Now we expand the dropdowns of the issue as shown. There are clear instructions on how to fix this issue at the bottom.</p> <p></p> <p>Now we click the name of the file that has issue and needs to be fixed. In the editor window, we select the entire class defined in here that uses the <code>ResponseUtils.encodeDataString()</code> method by highlighting. Then we click <code>Help me</code> to ask help from LLM.</p> <p></p> <p>The chat with model window will show up and ask model for help. The model gives suggestions for fixing the issue.</p> <p></p> <p>There are 2 action items here.  - First need to change the code in the <code>UpperServlet.java</code> file using the copy code from chat. - Second need to update <code>pom.xml</code> to reflect the dependency.</p> <p></p>"},{"location":"wca-app-dev-mod/lab-2-modernize/#rebuild-application","title":"Rebuild Application","text":"<p>Now that we have all the fixes applied. If your <code>modresort</code> application is still running, please stop it by going to <code>TERMINAL</code> and <code>CTRL+C</code>. You can <code>Rebuild and refresh</code> to see that there are no more issues with the application.</p> <p></p> <p>We can go to browser to view the new application. This time you will find that the <code>Logout</code> button is fixed.</p> <p></p>"},{"location":"wca-app-dev-mod/lab-3-upgrade/","title":"Lab 3 - Upgrade Java","text":""},{"location":"wca-app-dev-mod/lab-3-upgrade/#lab-3-upgrade-java-version","title":"Lab 3 - Upgrade Java Version","text":"<p>This document gives step-by-step guide to finish Lab3. You will find the steps very similar to Lab2.  This lab walks through the upgrade from Java8 to Java21</p>"},{"location":"wca-app-dev-mod/lab-3-upgrade/#application-analysis","title":"Application analysis","text":"<p>Right click on the <code>modresorts-twas-j8</code> folder and select <code>watsonx Code Assistant</code> - <code>Upgrade Java Version</code>.</p> <p></p> <p>A <code>Upgrade</code> tab will show up. In the <code>Target</code> dropdown, select<code>Java 21 (LTS)</code>, and then click <code>Build and analyze</code>.</p> <p></p> <p>After analysis, the report will show you issues with this application to be fixed. 3 of them can be automated, 2 of them needs assistance. For the automated fixes, we click <code>Run automated fixes</code> (this process might take long).</p> <p></p>"},{"location":"wca-app-dev-mod/lab-3-upgrade/#apply-fixes","title":"Apply Fixes","text":"<p>For the assisted fixes, you can click the <code>Instructions</code> dropdown to see the step which we will guide you. Now we expand the dropdowns of the first issue as shown. There are clear instructions on how to fix this issue at the bottom.</p> <p></p> <p>Now we click the name of the file that has issue and needs to be fixed. In the editor window, we select the entire class defined in here that that calls the <code>MBeanOperationInfo</code> constructor by highlighting. Then we click <code>Help me</code> to ask help from LLM.</p> <p></p> <p>The chat with model window will show up and ask model for help. The model gives suggestions for fixing the issue.</p> <p></p> <p>Here we change the code in the <code>DMbeanUtils.java</code> file using the copy code from chat.</p> <p>We can ignore the second issue for now as it is a deprecation warning. Or if the rebuild still have error, you can go through the same process and fix the warning as well.</p>"},{"location":"wca-app-dev-mod/lab-3-upgrade/#rebuild-application","title":"Rebuild Application","text":"<p>Now that we have all the fixes applied, we can <code>Rebuild and refresh</code> to see that there are no more critical issues with the application.</p> <p></p> <p>We can go to browser to view the new application. This time you will find that the location module is fixed.</p> <p></p>"},{"location":"wca-app-dev-mod/lab-4-unit-test/","title":"Lab 4 - Unit Test","text":""},{"location":"wca-app-dev-mod/lab-4-unit-test/#lab-4-unit-test-generation","title":"Lab 4 - Unit Test Generation","text":"<p>This document gives step-by-step guide to finish Lab 4.  The lab covers the generation of unit test cases.</p>"},{"location":"wca-app-dev-mod/lab-4-unit-test/#generate-unit-test-for-a-given-script","title":"Generate unit test for a given script","text":"<p>Right click on the any file, for example the ``</p> <p></p> <p>The chat session will show again with the generated test cases.</p> <p>Right click on the any file, for example the ``</p> <p></p>"},{"location":"wca-app-dev-mod/lab-5-general-programming/","title":"Lab 5 - General Programming","text":""},{"location":"wca-app-dev-mod/lab-5-general-programming/#lab-5-general-programming","title":"Lab 5 - General Programming","text":"<p>This document gives step-by-step guide to finish Lab 5, but feel to add in a few of your own attempts.</p> <p>The lab covers general programming with other languages, including     - code auto completion     - code explanation     - code documentation     - unit test generation     - code translation</p> <p>To get started, you can preview options for general programming capabilities in any inline functions.</p> <p></p> <p>They align with options starting with backslash provided in the chat window.</p> <p></p>"},{"location":"wca-app-dev-mod/lab-5-general-programming/#code-auto-completion","title":"Code Auto Completion","text":"<p>When developers write code, WCA will provide real-time inline suggestions.</p> <p></p>"},{"location":"wca-app-dev-mod/lab-5-general-programming/#code-explanation","title":"Code Explanation","text":"<p>WCA can help explain the details and functions defined in the code sample.</p> <p>This you can do by clicking the explain option on top of your functions.</p> <p>Or, you can explain the entire file by typing the following in chat:</p> <pre><code>/explain @UseCase_Code_Palindrome.py\n</code></pre>"},{"location":"wca-app-dev-mod/lab-5-general-programming/#code-documentation","title":"Code Documentation","text":"<p>WCA can help generate documentation strings for a given code sample.</p> <p></p>"},{"location":"wca-app-dev-mod/lab-5-general-programming/#unit-test-generation","title":"Unit test generation","text":"<p>Similar to what we see for Java, unit test generation is available for other languages.</p> <p></p>"},{"location":"wca-app-dev-mod/lab-5-general-programming/#code-translation","title":"Code Translation","text":"<p>Code translation is provided via the following syntex:</p> <pre><code>/translate from SOURCE_LANGUAGE to TARGET_LANGUAGE @REFERENCE_FILE|CODE_SNIPPET\n</code></pre> <p>Please note from SOURCE_LANGUAGE is an optional argument, and can be skipped while doing translation.</p> <p>For this lab, we will convert the python palindrome code to Go language. Try the following prompt:</p> <pre><code>/translate from python to go UseCase_Code_Palindrome.py\n</code></pre> <p></p>"}]}